# PostgreSQL All-in-One æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜æŒ‡å— - 2025å¹´

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£æä¾›äº†PostgreSQL All-in-Oneæ¶æ„çš„å…¨é¢æ€§èƒ½ä¼˜åŒ–å’Œè°ƒä¼˜æŒ‡å—ï¼ŒåŒ…æ‹¬æ•°æ®åº“é…ç½®ä¼˜åŒ–ã€æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜ã€ç¼“å­˜ç­–ç•¥ä¼˜åŒ–ã€ç³»ç»Ÿèµ„æºä¼˜åŒ–å’Œç›‘æ§è°ƒä¼˜ã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„æ€§èƒ½ä¼˜åŒ–ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹ä¿æŒä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–ç›®æ ‡

### 1. æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ç±»å‹ | ç›®æ ‡å€¼ | ç›‘æ§æ–¹å¼ |
|---------|--------|----------|
| **OLTPæŸ¥è¯¢å»¶è¿Ÿ** | < 100ms (95%åˆ†ä½æ•°) | Prometheus + Grafana |
| **OLAPæŸ¥è¯¢å»¶è¿Ÿ** | < 5s (95%åˆ†ä½æ•°) | æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’åˆ†æ |
| **å…¨æ–‡æœç´¢å»¶è¿Ÿ** | < 1s (95%åˆ†ä½æ•°) | ç´¢å¼•ä½¿ç”¨ç‡ç›‘æ§ |
| **å¹¶å‘è¿æ¥æ•°** | æ”¯æŒ200ä¸ªè¿æ¥ | è¿æ¥æ± ç›‘æ§ |
| **ååé‡** | 10,000 TPS | äº‹åŠ¡å¤„ç†ç›‘æ§ |
| **ç¼“å­˜å‘½ä¸­ç‡** | > 95% | Redisç›‘æ§ |
| **ç³»ç»Ÿå¯ç”¨æ€§** | 99.9% | å¥åº·æ£€æŸ¥ç›‘æ§ |

### 2. ä¼˜åŒ–ç­–ç•¥

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ€§èƒ½ä¼˜åŒ–ç­–ç•¥                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  æ•°æ®åº“ä¼˜åŒ–  â”‚ â”‚  æŸ¥è¯¢ä¼˜åŒ–   â”‚ â”‚  ç¼“å­˜ä¼˜åŒ–   â”‚ â”‚ ç³»ç»Ÿä¼˜åŒ– â”‚ â”‚
â”‚  â”‚ DB Config   â”‚ â”‚ Query Tuningâ”‚ â”‚ Cache Tuningâ”‚ â”‚System   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ç´¢å¼•ä¼˜åŒ–   â”‚ â”‚  åˆ†åŒºä¼˜åŒ–   â”‚ â”‚  è¿æ¥ä¼˜åŒ–   â”‚ â”‚ ç›‘æ§ä¼˜åŒ– â”‚ â”‚
â”‚  â”‚ Index Tuningâ”‚ â”‚Partitioning â”‚ â”‚Connection   â”‚ â”‚Monitoringâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—„ï¸ æ•°æ®åº“é…ç½®ä¼˜åŒ–

### 1. PostgreSQL æ ¸å¿ƒé…ç½®ä¼˜åŒ–

#### 1.1 å†…å­˜é…ç½®ä¼˜åŒ–

```sql
-- postgresql.conf å†…å­˜é…ç½®
-- åŸºäº32GBå†…å­˜çš„æœåŠ¡å™¨é…ç½®

# å…±äº«ç¼“å†²åŒº (25% of RAM)
shared_buffers = 8GB

# æœ‰æ•ˆç¼“å­˜å¤§å° (75% of RAM)
effective_cache_size = 24GB

# å·¥ä½œå†…å­˜ (æ¯ä¸ªæŸ¥è¯¢æ“ä½œ)
work_mem = 256MB

# ç»´æŠ¤å·¥ä½œå†…å­˜ (VACUUM, CREATE INDEXç­‰)
maintenance_work_mem = 2GB

# åŠ¨æ€å…±äº«å†…å­˜
dynamic_shared_memory_type = posix

# å¤§é¡µé¢æ”¯æŒ
huge_pages = try
```

#### 1.2 è¿æ¥å’Œå¹¶å‘é…ç½®

```sql
-- è¿æ¥é…ç½®
max_connections = 200
superuser_reserved_connections = 3

# è¿æ¥æ± é…ç½®
shared_preload_libraries = 'pg_stat_statements,timescaledb'

# å¹¶å‘é…ç½®
max_worker_processes = 8
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_parallel_maintenance_workers = 4
```

#### 1.3 I/O å’Œå­˜å‚¨ä¼˜åŒ–

```sql
-- I/O é…ç½® (SSDä¼˜åŒ–)
random_page_cost = 1.1
seq_page_cost = 1.0
effective_io_concurrency = 200

# æ£€æŸ¥ç‚¹é…ç½®
checkpoint_completion_target = 0.9
checkpoint_timeout = 15min
max_wal_size = 4GB
min_wal_size = 1GB

# WAL é…ç½®
wal_level = replica
wal_buffers = 16MB
wal_writer_delay = 200ms
commit_delay = 0
commit_siblings = 5
```

#### 1.4 æŸ¥è¯¢ä¼˜åŒ–é…ç½®

```sql
-- æŸ¥è¯¢è§„åˆ’å™¨é…ç½®
default_statistics_target = 100
constraint_exclusion = partition
cursor_tuple_fraction = 0.1

# å¹¶è¡ŒæŸ¥è¯¢é…ç½®
parallel_tuple_cost = 0.1
parallel_setup_cost = 1000.0
min_parallel_table_scan_size = 8MB
min_parallel_index_scan_size = 512kB

# æˆæœ¬é…ç½®
cpu_tuple_cost = 0.01
cpu_index_tuple_cost = 0.005
cpu_operator_cost = 0.0025
```

### 2. é«˜çº§é…ç½®ä¼˜åŒ–

#### 2.1 æ—¥å¿—å’Œç›‘æ§é…ç½®

```sql
-- æ—¥å¿—é…ç½®
log_destination = 'stderr'
logging_collector = on
log_directory = 'pg_log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_rotation_age = 1d
log_rotation_size = 100MB
log_min_duration_statement = 1000
log_statement = 'mod'
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0
```

#### 2.2 è‡ªåŠ¨æ¸…ç†é…ç½®

```sql
-- è‡ªåŠ¨æ¸…ç†é…ç½®
autovacuum = on
autovacuum_max_workers = 3
autovacuum_naptime = 1min
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
autovacuum_vacuum_scale_factor = 0.2
autovacuum_analyze_scale_factor = 0.1
autovacuum_freeze_max_age = 200000000
autovacuum_multixact_freeze_max_age = 400000000
autovacuum_vacuum_cost_delay = 20ms
autovacuum_vacuum_cost_limit = 200
```

## ğŸ” æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

### 1. ç´¢å¼•ä¼˜åŒ–ç­–ç•¥

#### 1.1 å¤åˆç´¢å¼•ä¼˜åŒ–

```sql
-- ç”¨æˆ·è¡Œä¸ºåˆ†æè¡¨ç´¢å¼•ä¼˜åŒ–
CREATE TABLE user_events (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    event_data JSONB,
    timestamp TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- å¤åˆç´¢å¼•ï¼šç”¨æˆ·ID + æ—¶é—´æˆ³
CREATE INDEX CONCURRENTLY idx_user_events_user_timestamp 
ON user_events (user_id, timestamp DESC);

-- å¤åˆç´¢å¼•ï¼šäº‹ä»¶ç±»å‹ + æ—¶é—´æˆ³
CREATE INDEX CONCURRENTLY idx_user_events_type_timestamp 
ON user_events (event_type, timestamp DESC);

-- éƒ¨åˆ†ç´¢å¼•ï¼šæ´»è·ƒç”¨æˆ·äº‹ä»¶
CREATE INDEX CONCURRENTLY idx_user_events_active_users 
ON user_events (user_id, timestamp DESC) 
WHERE timestamp > NOW() - INTERVAL '30 days';

-- JSONB ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_user_events_data_gin 
ON user_events USING GIN (event_data);

-- è¡¨è¾¾å¼ç´¢å¼•ï¼šæŒ‰å°æ—¶åˆ†ç»„
CREATE INDEX CONCURRENTLY idx_user_events_hour 
ON user_events (date_trunc('hour', timestamp));
```

#### 1.2 å…¨æ–‡æœç´¢ç´¢å¼•ä¼˜åŒ–

```sql
-- æ–‡æ¡£è¡¨å…¨æ–‡æœç´¢ä¼˜åŒ–
CREATE TABLE documents (
    id BIGSERIAL PRIMARY KEY,
    title VARCHAR(200) NOT NULL,
    content TEXT NOT NULL,
    metadata JSONB,
    search_vector tsvector,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- å…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_documents_search_gin 
ON documents USING GIN (search_vector);

-- æ ‡é¢˜ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_documents_title_gin 
ON documents USING GIN (to_tsvector('simple', title));

-- å…ƒæ•°æ®ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_documents_metadata_gin 
ON documents USING GIN (metadata);

-- æ—¶é—´èŒƒå›´ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_documents_created_at 
ON documents (created_at DESC);
```

### 2. æŸ¥è¯¢é‡å†™ä¼˜åŒ–

#### 2.1 OLTP æŸ¥è¯¢ä¼˜åŒ–

```sql
-- åŸå§‹æŸ¥è¯¢ï¼ˆæ€§èƒ½è¾ƒå·®ï¼‰
SELECT * FROM user_events 
WHERE user_id = 123 
AND timestamp > '2025-01-01' 
ORDER BY timestamp DESC 
LIMIT 100;

-- ä¼˜åŒ–åæŸ¥è¯¢ï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰
SELECT id, user_id, event_type, event_data, timestamp 
FROM user_events 
WHERE user_id = 123 
AND timestamp > '2025-01-01' 
ORDER BY timestamp DESC 
LIMIT 100;

-- ä½¿ç”¨ EXPLAIN åˆ†ææŸ¥è¯¢è®¡åˆ’
EXPLAIN (ANALYZE, BUFFERS, VERBOSE) 
SELECT id, user_id, event_type, event_data, timestamp 
FROM user_events 
WHERE user_id = 123 
AND timestamp > '2025-01-01' 
ORDER BY timestamp DESC 
LIMIT 100;
```

#### 2.2 OLAP æŸ¥è¯¢ä¼˜åŒ–

```sql
-- åŸå§‹èšåˆæŸ¥è¯¢
SELECT 
    user_id,
    COUNT(*) as event_count,
    AVG((event_data->>'value')::numeric) as avg_value
FROM user_events 
WHERE timestamp > NOW() - INTERVAL '1 day'
GROUP BY user_id
ORDER BY event_count DESC;

-- ä¼˜åŒ–åæŸ¥è¯¢ï¼ˆä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼‰
SET max_parallel_workers_per_gather = 4;
SET parallel_tuple_cost = 0.1;
SET parallel_setup_cost = 1000.0;

SELECT 
    user_id,
    COUNT(*) as event_count,
    AVG((event_data->>'value')::numeric) as avg_value
FROM user_events 
WHERE timestamp > NOW() - INTERVAL '1 day'
GROUP BY user_id
ORDER BY event_count DESC;

-- ä½¿ç”¨ç‰©åŒ–è§†å›¾ä¼˜åŒ–é‡å¤æŸ¥è¯¢
CREATE MATERIALIZED VIEW mv_daily_user_stats AS
SELECT 
    date_trunc('day', timestamp) as date,
    user_id,
    COUNT(*) as event_count,
    AVG((event_data->>'value')::numeric) as avg_value,
    MAX(timestamp) as last_event
FROM user_events 
WHERE timestamp > NOW() - INTERVAL '30 days'
GROUP BY date_trunc('day', timestamp), user_id;

-- åˆ›å»ºç‰©åŒ–è§†å›¾ç´¢å¼•
CREATE INDEX idx_mv_daily_user_stats_date_user 
ON mv_daily_user_stats (date, user_id);

-- å®šæœŸåˆ·æ–°ç‰©åŒ–è§†å›¾
CREATE OR REPLACE FUNCTION refresh_daily_user_stats()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_daily_user_stats;
END;
$$ LANGUAGE plpgsql;

-- ä½¿ç”¨ cron å®šæœŸåˆ·æ–°
SELECT cron.schedule('refresh-daily-stats', '0 1 * * *', 'SELECT refresh_daily_user_stats();');
```

### 3. åˆ†åŒºè¡¨ä¼˜åŒ–

#### 3.1 æ—¶é—´åˆ†åŒºç­–ç•¥

```sql
-- åˆ›å»ºåˆ†åŒºè¡¨
CREATE TABLE events (
    id BIGSERIAL,
    user_id BIGINT NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    event_data JSONB,
    timestamp TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
) PARTITION BY RANGE (timestamp);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE events_2025_01 PARTITION OF events
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE events_2025_02 PARTITION OF events
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');

-- ä¸ºæ¯ä¸ªåˆ†åŒºåˆ›å»ºç´¢å¼•
CREATE INDEX idx_events_2025_01_user_timestamp 
ON events_2025_01 (user_id, timestamp DESC);

CREATE INDEX idx_events_2025_02_user_timestamp 
ON events_2025_02 (user_id, timestamp DESC);

-- è‡ªåŠ¨åˆ†åŒºç®¡ç†å‡½æ•°
CREATE OR REPLACE FUNCTION create_monthly_partition(table_name text, start_date date)
RETURNS void AS $$
DECLARE
    partition_name text;
    end_date date;
BEGIN
    partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM');
    end_date := start_date + INTERVAL '1 month';
    
    EXECUTE format('CREATE TABLE IF NOT EXISTS %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',
                   partition_name, table_name, start_date, end_date);
    
    -- ä¸ºæ–°åˆ†åŒºåˆ›å»ºç´¢å¼•
    EXECUTE format('CREATE INDEX IF NOT EXISTS %I ON %I (user_id, timestamp DESC)',
                   'idx_' || partition_name || '_user_timestamp', partition_name);
END;
$$ LANGUAGE plpgsql;

-- åˆ†åŒºè§¦å‘å™¨
CREATE OR REPLACE FUNCTION events_partition_trigger()
RETURNS trigger AS $$
BEGIN
    PERFORM create_monthly_partition('events', date_trunc('month', NEW.timestamp)::date);
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER events_partition_trigger
    BEFORE INSERT ON events
    FOR EACH ROW EXECUTE FUNCTION events_partition_trigger();
```

## ğŸš€ ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

### 1. Redis ç¼“å­˜é…ç½®ä¼˜åŒ–

#### 1.1 Redis æœåŠ¡å™¨é…ç½®

```bash
# redis.conf ä¼˜åŒ–é…ç½®

# å†…å­˜é…ç½®
maxmemory 8gb
maxmemory-policy allkeys-lru
maxmemory-samples 5

# æŒä¹…åŒ–é…ç½®
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb

# AOF é…ç½®
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# ç½‘ç»œé…ç½®
tcp-keepalive 300
timeout 0
tcp-backlog 511

# å®¢æˆ·ç«¯é…ç½®
maxclients 10000

# æ…¢æŸ¥è¯¢æ—¥å¿—
slowlog-log-slower-than 10000
slowlog-max-len 128
```

#### 1.2 ç¼“å­˜ç­–ç•¥å®ç°

```rust
// src/cache/strategy.rs
use redis::{Client, Connection, Commands};
use serde::{Deserialize, Serialize};
use std::time::{Duration, Instant};
use std::collections::HashMap;
use tokio::sync::RwLock;

#[derive(Debug, Clone)]
pub enum CacheStrategy {
    WriteThrough,
    WriteBehind,
    WriteAround,
    CacheAside,
}

#[derive(Debug, Clone)]
pub struct CacheConfig {
    pub strategy: CacheStrategy,
    pub default_ttl: Duration,
    pub max_size: usize,
    pub eviction_policy: EvictionPolicy,
}

#[derive(Debug, Clone)]
pub enum EvictionPolicy {
    LRU,
    LFU,
    TTL,
    Random,
}

pub struct SmartCache {
    client: Client,
    connection: Arc<RwLock<Option<Connection>>>,
    config: CacheConfig,
    stats: Arc<RwLock<CacheStats>>,
}

#[derive(Debug, Default)]
pub struct CacheStats {
    pub hits: u64,
    pub misses: u64,
    pub sets: u64,
    pub deletes: u64,
    pub evictions: u64,
    pub total_requests: u64,
}

impl SmartCache {
    pub fn new(config: CacheConfig) -> Result<Self, redis::RedisError> {
        let client = Client::open("redis://localhost:6379")?;
        
        Ok(Self {
            client,
            connection: Arc::new(RwLock::new(None)),
            config,
            stats: Arc::new(RwLock::new(CacheStats::default())),
        })
    }

    // æ™ºèƒ½ç¼“å­˜è·å–
    pub async fn get<T>(&self, key: &str) -> Result<Option<T>, redis::RedisError>
    where
        T: for<'de> Deserialize<'de>,
    {
        let start = Instant::now();
        let mut conn = self.get_connection().await?;
        
        let value: Option<String> = conn.get(key)?;
        
        let mut stats = self.stats.write().await;
        stats.total_requests += 1;
        
        match value {
            Some(v) => {
                stats.hits += 1;
                let deserialized: T = serde_json::from_str(&v)?;
                Ok(Some(deserialized))
            }
            None => {
                stats.misses += 1;
                Ok(None)
            }
        }
    }

    // æ™ºèƒ½ç¼“å­˜è®¾ç½®
    pub async fn set<T>(&self, key: &str, value: &T, ttl: Option<Duration>) -> Result<(), redis::RedisError>
    where
        T: Serialize,
    {
        let mut conn = self.get_connection().await?;
        let serialized = serde_json::to_string(value)?;
        
        let ttl_seconds = ttl.unwrap_or(self.config.default_ttl).as_secs() as usize;
        
        match self.config.strategy {
            CacheStrategy::WriteThrough => {
                // å†™å…¥ç¼“å­˜å’Œæ•°æ®åº“
                conn.set_ex(key, &serialized, ttl_seconds)?;
                // è¿™é‡Œåº”è¯¥åŒæ—¶å†™å…¥æ•°æ®åº“
            }
            CacheStrategy::WriteBehind => {
                // å¼‚æ­¥å†™å…¥æ•°æ®åº“
                conn.set_ex(key, &serialized, ttl_seconds)?;
                // å¼‚æ­¥ä»»åŠ¡å†™å…¥æ•°æ®åº“
            }
            CacheStrategy::WriteAround => {
                // åªå†™å…¥æ•°æ®åº“ï¼Œä¸å†™å…¥ç¼“å­˜
                // è¿™é‡Œåº”è¯¥å†™å…¥æ•°æ®åº“
            }
            CacheStrategy::CacheAside => {
                // åªå†™å…¥ç¼“å­˜
                conn.set_ex(key, &serialized, ttl_seconds)?;
            }
        }
        
        let mut stats = self.stats.write().await;
        stats.sets += 1;
        
        Ok(())
    }

    // æ‰¹é‡æ“ä½œ
    pub async fn mget<T>(&self, keys: &[String]) -> Result<Vec<Option<T>>, redis::RedisError>
    where
        T: for<'de> Deserialize<'de>,
    {
        let mut conn = self.get_connection().await?;
        let values: Vec<Option<String>> = conn.get(keys)?;
        
        let mut results = Vec::new();
        for value in values {
            match value {
                Some(v) => {
                    let deserialized: T = serde_json::from_str(&v)?;
                    results.push(Some(deserialized));
                }
                None => {
                    results.push(None);
                }
            }
        }
        
        Ok(results)
    }

    // æ‰¹é‡è®¾ç½®
    pub async fn mset<T>(&self, items: &[(String, T)], ttl: Option<Duration>) -> Result<(), redis::RedisError>
    where
        T: Serialize,
    {
        let mut conn = self.get_connection().await?;
        let ttl_seconds = ttl.unwrap_or(self.config.default_ttl).as_secs() as usize;
        
        for (key, value) in items {
            let serialized = serde_json::to_string(value)?;
            conn.set_ex(key, &serialized, ttl_seconds)?;
        }
        
        let mut stats = self.stats.write().await;
        stats.sets += items.len() as u64;
        
        Ok(())
    }

    // è·å–ç¼“å­˜ç»Ÿè®¡
    pub async fn get_stats(&self) -> CacheStats {
        self.stats.read().await.clone()
    }

    // è·å–ç¼“å­˜å‘½ä¸­ç‡
    pub async fn get_hit_rate(&self) -> f64 {
        let stats = self.stats.read().await;
        if stats.total_requests == 0 {
            0.0
        } else {
            stats.hits as f64 / stats.total_requests as f64
        }
    }

    async fn get_connection(&self) -> Result<Connection, redis::RedisError> {
        let mut conn_guard = self.connection.write().await;
        
        if conn_guard.is_none() {
            let conn = self.client.get_connection()?;
            *conn_guard = Some(conn);
        }
        
        Ok(conn_guard.as_ref().unwrap().clone())
    }
}
```

### 2. ç¼“å­˜é¢„çƒ­ç­–ç•¥

```rust
// src/cache/warmup.rs
use crate::cache::SmartCache;
use crate::database::DatabasePool;
use std::time::Duration;

pub struct CacheWarmup {
    cache: SmartCache,
    db_pool: DatabasePool,
}

impl CacheWarmup {
    pub fn new(cache: SmartCache, db_pool: DatabasePool) -> Self {
        Self { cache, db_pool }
    }

    // é¢„çƒ­çƒ­é—¨æ•°æ®
    pub async fn warmup_hot_data(&self) -> Result<(), Box<dyn std::error::Error>> {
        // é¢„çƒ­ç”¨æˆ·ä¿¡æ¯
        self.warmup_user_profiles().await?;
        
        // é¢„çƒ­é…ç½®æ•°æ®
        self.warmup_config_data().await?;
        
        // é¢„çƒ­ç»Ÿè®¡æ•°æ®
        self.warmup_statistics().await?;
        
        Ok(())
    }

    async fn warmup_user_profiles(&self) -> Result<(), Box<dyn std::error::Error>> {
        let users = self.db_pool.with_connection(|conn| async move {
            sqlx::query("SELECT id, username, email, profile FROM users WHERE active = true LIMIT 1000")
                .fetch_all(conn)
                .await
                .map_err(crate::core::AppError::Database)
        }).await?;

        let mut cache_items = Vec::new();
        for user in users {
            let user_id: i32 = user.get("id");
            let username: String = user.get("username");
            let email: String = user.get("email");
            let profile: serde_json::Value = user.get("profile");
            
            let user_data = serde_json::json!({
                "id": user_id,
                "username": username,
                "email": email,
                "profile": profile
            });
            
            cache_items.push((format!("user:{}", user_id), user_data));
        }
        
        self.cache.mset(&cache_items, Some(Duration::from_secs(3600))).await?;
        Ok(())
    }

    async fn warmup_config_data(&self) -> Result<(), Box<dyn std::error::Error>> {
        let configs = self.db_pool.with_connection(|conn| async move {
            sqlx::query("SELECT key, value FROM system_configs WHERE active = true")
                .fetch_all(conn)
                .await
                .map_err(crate::core::AppError::Database)
        }).await?;

        let mut cache_items = Vec::new();
        for config in configs {
            let key: String = config.get("key");
            let value: serde_json::Value = config.get("value");
            cache_items.push((format!("config:{}", key), value));
        }
        
        self.cache.mset(&cache_items, Some(Duration::from_secs(7200))).await?;
        Ok(())
    }

    async fn warmup_statistics(&self) -> Result<(), Box<dyn std::error::Error>> {
        // é¢„çƒ­æ¯æ—¥ç»Ÿè®¡
        let daily_stats = self.db_pool.with_connection(|conn| async move {
            sqlx::query("
                SELECT 
                    date_trunc('day', timestamp) as date,
                    COUNT(*) as event_count,
                    COUNT(DISTINCT user_id) as unique_users
                FROM events 
                WHERE timestamp > NOW() - INTERVAL '7 days'
                GROUP BY date_trunc('day', timestamp)
                ORDER BY date DESC
            ")
            .fetch_all(conn)
            .await
            .map_err(crate::core::AppError::Database)
        }).await?;

        let mut cache_items = Vec::new();
        for stat in daily_stats {
            let date: chrono::NaiveDate = stat.get("date");
            let event_count: i64 = stat.get("event_count");
            let unique_users: i64 = stat.get("unique_users");
            
            let stat_data = serde_json::json!({
                "date": date,
                "event_count": event_count,
                "unique_users": unique_users
            });
            
            cache_items.push((format!("stats:daily:{}", date), stat_data));
        }
        
        self.cache.mset(&cache_items, Some(Duration::from_secs(1800))).await?;
        Ok(())
    }
}
```

## ğŸ’» ç³»ç»Ÿèµ„æºä¼˜åŒ–

### 1. æ“ä½œç³»ç»Ÿä¼˜åŒ–

#### 1.1 Linux å†…æ ¸å‚æ•°ä¼˜åŒ–

```bash
# /etc/sysctl.conf ä¼˜åŒ–é…ç½®

# ç½‘ç»œä¼˜åŒ–
net.core.rmem_default = 262144
net.core.rmem_max = 16777216
net.core.wmem_default = 262144
net.core.wmem_max = 16777216
net.core.netdev_max_backlog = 5000
net.ipv4.tcp_rmem = 4096 65536 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
net.ipv4.tcp_congestion_control = bbr

# å†…å­˜ä¼˜åŒ–
vm.swappiness = 10
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
vm.dirty_expire_centisecs = 3000
vm.dirty_writeback_centisecs = 500

# æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
fs.file-max = 2097152
fs.nr_open = 1048576

# è¿›ç¨‹ä¼˜åŒ–
kernel.pid_max = 4194304
kernel.threads-max = 2097152
```

#### 1.2 ç³»ç»Ÿé™åˆ¶ä¼˜åŒ–

```bash
# /etc/security/limits.conf
postgres soft nofile 65536
postgres hard nofile 65536
postgres soft nproc 32768
postgres hard nproc 32768
postgres soft memlock unlimited
postgres hard memlock unlimited

# /etc/systemd/system/postgresql.service.d/override.conf
[Service]
LimitNOFILE=65536
LimitNPROC=32768
LimitMEMLOCK=infinity
```

### 2. ç¡¬ä»¶ä¼˜åŒ–å»ºè®®

#### 2.1 CPU ä¼˜åŒ–

```text
æ¨èé…ç½®ï¼š
- CPU: Intel Xeon Gold 6248R (24æ ¸48çº¿ç¨‹) æˆ– AMD EPYC 7542 (32æ ¸64çº¿ç¨‹)
- é¢‘ç‡: 3.0GHz+ åŸºç¡€é¢‘ç‡
- ç¼“å­˜: 32MB+ L3ç¼“å­˜
- æ¶æ„: æ”¯æŒAVX-512æŒ‡ä»¤é›†

ä¼˜åŒ–å»ºè®®ï¼š
- å¯ç”¨CPUæ€§èƒ½æ¨¡å¼
- å…³é—­ä¸å¿…è¦çš„CPUèŠ‚èƒ½åŠŸèƒ½
- ä½¿ç”¨CPUäº²å’Œæ€§ç»‘å®šPostgreSQLè¿›ç¨‹
```

#### 2.2 å†…å­˜ä¼˜åŒ–

```text
æ¨èé…ç½®ï¼š
- å†…å­˜: 64GB+ DDR4-3200 ECCå†…å­˜
- å†…å­˜é€šé“: 8é€šé“é…ç½®
- å†…å­˜ç±»å‹: ä¼ä¸šçº§ECCå†…å­˜

ä¼˜åŒ–å»ºè®®ï¼š
- é…ç½®å¤§é¡µé¢æ”¯æŒ
- å¯ç”¨å†…å­˜å‹ç¼©
- ä¼˜åŒ–NUMAé…ç½®
```

#### 2.3 å­˜å‚¨ä¼˜åŒ–

```text
æ¨èé…ç½®ï¼š
- ä¸»å­˜å‚¨: NVMe SSD (å¦‚Intel P5510 3.84TB)
- å¤‡ä»½å­˜å‚¨: SATA SSD æˆ– HDD
- å­˜å‚¨æ¥å£: PCIe 4.0 x4

ä¼˜åŒ–å»ºè®®ï¼š
- ä½¿ç”¨RAID 1é…ç½®ä¿è¯æ•°æ®å®‰å…¨
- å¯ç”¨TRIMæ”¯æŒ
- ä¼˜åŒ–I/Oè°ƒåº¦å™¨
```

## ğŸ“Š ç›‘æ§å’Œè°ƒä¼˜

### 1. æ€§èƒ½ç›‘æ§æŒ‡æ ‡

#### 1.1 æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡

```sql
-- æŸ¥è¯¢æ€§èƒ½ç›‘æ§
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    stddev_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements 
ORDER BY total_time DESC 
LIMIT 10;

-- è¿æ¥ç›‘æ§
SELECT 
    state,
    count(*) as connections
FROM pg_stat_activity 
GROUP BY state;

-- é”ç›‘æ§
SELECT 
    mode,
    count(*) as locks
FROM pg_locks 
GROUP BY mode;

-- ç´¢å¼•ä½¿ç”¨ç›‘æ§
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes 
ORDER BY idx_tup_read DESC;
```

#### 1.2 ç³»ç»Ÿèµ„æºç›‘æ§

```bash
# CPU ä½¿ç”¨ç‡ç›‘æ§
top -p $(pgrep postgres)

# å†…å­˜ä½¿ç”¨ç›‘æ§
free -h
cat /proc/meminfo

# I/O ç›‘æ§
iostat -x 1
iotop -p $(pgrep postgres)

# ç½‘ç»œç›‘æ§
netstat -i
ss -tuln
```

### 2. è‡ªåŠ¨åŒ–è°ƒä¼˜è„šæœ¬

#### 2.1 æ€§èƒ½åˆ†æè„šæœ¬

```bash
#!/bin/bash
# performance_analysis.sh

echo "=== PostgreSQL æ€§èƒ½åˆ†ææŠ¥å‘Š ==="
echo "ç”Ÿæˆæ—¶é—´: $(date)"
echo

# æ•°æ®åº“è¿æ¥æ•°
echo "=== æ•°æ®åº“è¿æ¥æ•° ==="
psql -c "SELECT count(*) as total_connections FROM pg_stat_activity;"

# æ…¢æŸ¥è¯¢åˆ†æ
echo "=== æ…¢æŸ¥è¯¢åˆ†æ ==="
psql -c "
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements 
WHERE mean_time > 1000
ORDER BY mean_time DESC 
LIMIT 5;
"

# ç¼“å­˜å‘½ä¸­ç‡
echo "=== ç¼“å­˜å‘½ä¸­ç‡ ==="
psql -c "
SELECT 
    round(100.0 * sum(blks_hit) / (sum(blks_hit) + sum(blks_read)), 2) as cache_hit_ratio
FROM pg_stat_database;
"

# ç´¢å¼•ä½¿ç”¨æƒ…å†µ
echo "=== ç´¢å¼•ä½¿ç”¨æƒ…å†µ ==="
psql -c "
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes 
WHERE idx_tup_read > 0
ORDER BY idx_tup_read DESC 
LIMIT 10;
"

# è¡¨å¤§å°åˆ†æ
echo "=== è¡¨å¤§å°åˆ†æ ==="
psql -c "
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC 
LIMIT 10;
"
```

#### 2.2 è‡ªåŠ¨è°ƒä¼˜è„šæœ¬

```bash
#!/bin/bash
# auto_tuning.sh

echo "=== PostgreSQL è‡ªåŠ¨è°ƒä¼˜ ==="

# æ£€æŸ¥å½“å‰é…ç½®
echo "æ£€æŸ¥å½“å‰é…ç½®..."
psql -c "SHOW shared_buffers;"
psql -c "SHOW work_mem;"
psql -c "SHOW maintenance_work_mem;"

# è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
TOTAL_MEM=$(free -m | awk 'NR==2{printf "%.0f", $2}')
SHARED_BUFFERS=$((TOTAL_MEM * 25 / 100))
WORK_MEM=$((TOTAL_MEM * 5 / 100))
MAINTENANCE_WORK_MEM=$((TOTAL_MEM * 10 / 100))

echo "ç³»ç»Ÿæ€»å†…å­˜: ${TOTAL_MEM}MB"
echo "å»ºè®® shared_buffers: ${SHARED_BUFFERS}MB"
echo "å»ºè®® work_mem: ${WORK_MEM}MB"
echo "å»ºè®® maintenance_work_mem: ${MAINTENANCE_WORK_MEM}MB"

# ç”Ÿæˆä¼˜åŒ–é…ç½®
cat > /tmp/postgresql_optimized.conf << EOF
# è‡ªåŠ¨ç”Ÿæˆçš„ä¼˜åŒ–é…ç½®
shared_buffers = ${SHARED_BUFFERS}MB
work_mem = ${WORK_MEM}MB
maintenance_work_mem = ${MAINTENANCE_WORK_MEM}MB
effective_cache_size = $((TOTAL_MEM * 75 / 100))MB
max_connections = 200
random_page_cost = 1.1
effective_io_concurrency = 200
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100
EOF

echo "ä¼˜åŒ–é…ç½®å·²ç”Ÿæˆ: /tmp/postgresql_optimized.conf"
echo "è¯·æ‰‹åŠ¨åº”ç”¨é…ç½®å¹¶é‡å¯PostgreSQLæœåŠ¡"
```

## ğŸ“‹ æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†PostgreSQL All-in-Oneæ¶æ„çš„å…¨é¢æ€§èƒ½ä¼˜åŒ–æŒ‡å—ï¼ŒåŒ…æ‹¬ï¼š

### 1. æ•°æ®åº“é…ç½®ä¼˜åŒ–

- **å†…å­˜é…ç½®**: åˆç†åˆ†é…å…±äº«ç¼“å†²åŒºå’Œå·¥ä½œå†…å­˜
- **è¿æ¥é…ç½®**: ä¼˜åŒ–å¹¶å‘è¿æ¥å’Œè¿æ¥æ± è®¾ç½®
- **I/Oé…ç½®**: SSDä¼˜åŒ–å’Œæ£€æŸ¥ç‚¹é…ç½®
- **æŸ¥è¯¢ä¼˜åŒ–**: è§„åˆ’å™¨å‚æ•°å’Œå¹¶è¡ŒæŸ¥è¯¢é…ç½®

### 2. æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

- **ç´¢å¼•ä¼˜åŒ–**: å¤åˆç´¢å¼•ã€éƒ¨åˆ†ç´¢å¼•ã€è¡¨è¾¾å¼ç´¢å¼•
- **æŸ¥è¯¢é‡å†™**: OLTPå’ŒOLAPæŸ¥è¯¢ä¼˜åŒ–
- **åˆ†åŒºä¼˜åŒ–**: æ—¶é—´åˆ†åŒºå’Œè‡ªåŠ¨åˆ†åŒºç®¡ç†

### 3. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

- **Redisé…ç½®**: å†…å­˜ç®¡ç†å’ŒæŒä¹…åŒ–é…ç½®
- **ç¼“å­˜ç­–ç•¥**: å¤šç§ç¼“å­˜æ¨¡å¼å®ç°
- **ç¼“å­˜é¢„çƒ­**: æ™ºèƒ½æ•°æ®é¢„çƒ­æœºåˆ¶

### 4. ç³»ç»Ÿèµ„æºä¼˜åŒ–

- **æ“ä½œç³»ç»Ÿ**: å†…æ ¸å‚æ•°å’Œç³»ç»Ÿé™åˆ¶ä¼˜åŒ–
- **ç¡¬ä»¶é…ç½®**: CPUã€å†…å­˜ã€å­˜å‚¨ä¼˜åŒ–å»ºè®®
- **ç›‘æ§è°ƒä¼˜**: æ€§èƒ½æŒ‡æ ‡ç›‘æ§å’Œè‡ªåŠ¨åŒ–è°ƒä¼˜

é€šè¿‡ç³»ç»Ÿæ€§çš„æ€§èƒ½ä¼˜åŒ–ï¼ŒPostgreSQL All-in-Oneæ¶æ„èƒ½å¤Ÿåœ¨é«˜è´Ÿè½½ä¸‹ä¿æŒä¼˜å¼‚çš„æ€§èƒ½è¡¨ç°ï¼Œæ»¡è¶³ä¸­å°å‹å›¢é˜Ÿçš„æ•°æ®å¤„ç†éœ€æ±‚ã€‚
