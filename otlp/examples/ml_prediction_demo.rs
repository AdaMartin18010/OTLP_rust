//! # æœºå™¨å­¦ä¹ é”™è¯¯é¢„æµ‹ç³»ç»Ÿæ¼”ç¤º
//!
//! å±•ç¤ºå¦‚ä½•ä½¿ç”¨ OTLP Rust çš„æœºå™¨å­¦ä¹ é”™è¯¯é¢„æµ‹ç³»ç»Ÿè¿›è¡Œæ™ºèƒ½é”™è¯¯é¢„æµ‹ã€
//! åœ¨çº¿å­¦ä¹ å’Œè‡ªé€‚åº”æ¢å¤ç­–ç•¥ã€‚

use otlp::error::ErrorSeverity;
use otlp::ml_error_prediction::HealthStatus;
use otlp::{
    ErrorSample, MLErrorPrediction, MLPredictionConfig, PredictionFeedback, PredictionResult,
    Result, SystemContext,
};
use std::collections::HashMap;
use std::time::Duration;

#[tokio::main]
async fn main() -> Result<()> {
    // åˆå§‹åŒ–æ—¥å¿—
    tracing_subscriber::fmt::init();

    println!("ğŸ¤– OTLP Rust æœºå™¨å­¦ä¹ é”™è¯¯é¢„æµ‹ç³»ç»Ÿæ¼”ç¤º");
    println!("===========================================");

    // ç¤ºä¾‹ 1: åŸºæœ¬é¢„æµ‹ç³»ç»Ÿè®¾ç½®
    basic_prediction_demo().await?;

    // ç¤ºä¾‹ 2: é”™è¯¯é¢„æµ‹
    error_prediction_demo().await?;

    // ç¤ºä¾‹ 3: æ¨¡å‹è®­ç»ƒ
    model_training_demo().await?;

    // ç¤ºä¾‹ 4: åœ¨çº¿å­¦ä¹ 
    online_learning_demo().await?;

    // ç¤ºä¾‹ 5: è‡ªé€‚åº”æ¢å¤ç­–ç•¥
    adaptive_recovery_demo().await?;

    println!("\nâœ… æ‰€æœ‰MLé¢„æµ‹æ¼”ç¤ºå®Œæˆï¼");
    Ok(())
}

/// ç¤ºä¾‹ 1: åŸºæœ¬é¢„æµ‹ç³»ç»Ÿè®¾ç½®
async fn basic_prediction_demo() -> Result<()> {
    println!("\nğŸ¤– ç¤ºä¾‹ 1: åŸºæœ¬é¢„æµ‹ç³»ç»Ÿè®¾ç½®");
    println!("---------------------------");

    // åˆ›å»ºMLé¢„æµ‹é…ç½®
    let config = MLPredictionConfig::default();

    // åˆ›å»ºMLé¢„æµ‹ç³»ç»Ÿ
    let ml_predictor = MLErrorPrediction::new(config)?;

    println!("  âœ… MLé¢„æµ‹ç³»ç»Ÿåˆ›å»ºæˆåŠŸ");

    // è·å–æ¨¡å‹çŠ¶æ€
    let status = ml_predictor.get_model_status().await?;
    println!("  ğŸ“Š æ¨¡å‹çŠ¶æ€:");
    println!("    - æ¨¡å‹ç‰ˆæœ¬: {}", status.model_version);
    println!("    - æ€»é¢„æµ‹æ¬¡æ•°: {}", status.total_predictions);
    println!("    - å½“å‰å‡†ç¡®ç‡: {:.3}", status.accuracy);
    println!("    - ç¼“å­˜å¤§å°: {}", status.cache_size);
    println!("    - æ˜¯å¦æ­£åœ¨è®­ç»ƒ: {}", status.is_training);

    Ok(())
}

/// ç¤ºä¾‹ 2: é”™è¯¯é¢„æµ‹
async fn error_prediction_demo() -> Result<()> {
    println!("\nğŸ”® ç¤ºä¾‹ 2: é”™è¯¯é¢„æµ‹");
    println!("-------------------");

    let config = MLPredictionConfig::default();
    let ml_predictor = MLErrorPrediction::new(config)?;

    // åˆ›å»ºä¸åŒçš„ç³»ç»Ÿä¸Šä¸‹æ–‡åœºæ™¯
    let scenarios = vec![
        ("é«˜è´Ÿè½½åœºæ™¯", create_high_load_context()),
        ("èµ„æºè€—å°½åœºæ™¯", create_resource_exhausted_context()),
        ("ç½‘ç»œé—®é¢˜åœºæ™¯", create_network_issue_context()),
        ("å¥åº·ç³»ç»Ÿåœºæ™¯", create_healthy_context()),
        ("ä¸´ç•ŒçŠ¶æ€åœºæ™¯", create_critical_context()),
    ];

    for (name, context) in scenarios {
        println!("  ğŸ” é¢„æµ‹åœºæ™¯: {}", name);

        // è¿›è¡Œé”™è¯¯é¢„æµ‹
        let prediction = ml_predictor.predict_error_probability(&context).await?;

        println!("    ğŸ“Š é¢„æµ‹ç»“æœ:");
        println!("      - é”™è¯¯æ¦‚ç‡: {:.3}", prediction.probability);
        println!("      - ç½®ä¿¡åº¦: {:.3}", prediction.confidence);
        println!("      - é¢„æµ‹é”™è¯¯ç±»å‹: {:?}", prediction.error_types);
        println!("      - æ—¶é—´çª—å£: {:?}", prediction.time_window);
        println!("      - æ¨¡å‹ç‰ˆæœ¬: {}", prediction.model_version);

        // æ˜¾ç¤ºé¢„æµ‹è§£é‡Š
        println!("    ğŸ’¡ é¢„æµ‹è§£é‡Š:");
        println!("      - æ€»ç»“: {}", prediction.explanation.summary);
        println!(
            "      - ç½®ä¿¡æ°´å¹³: {}",
            prediction.explanation.confidence_level
        );
        for detail in &prediction.explanation.details {
            println!("      - {}", detail);
        }

        // æ˜¾ç¤ºå»ºè®®æªæ–½
        println!("    ğŸ›¡ï¸  å»ºè®®æªæ–½:");
        for (i, action) in prediction.recommended_actions.iter().enumerate() {
            println!(
                "      {}. {} (ä¼˜å…ˆçº§: {}, é¢„æœŸæ•ˆæœ: {:.1}%)",
                i + 1,
                action.description,
                action.priority,
                action.estimated_effectiveness * 100.0
            );
        }

        println!();
    }

    Ok(())
}

/// ç¤ºä¾‹ 3: æ¨¡å‹è®­ç»ƒ
async fn model_training_demo() -> Result<()> {
    println!("\nğŸ“ ç¤ºä¾‹ 3: æ¨¡å‹è®­ç»ƒ");
    println!("-------------------");

    let config = MLPredictionConfig::default();
    let ml_predictor = MLErrorPrediction::new(config)?;

    // åˆ›å»ºè®­ç»ƒæ•°æ®
    let training_data = create_training_data();
    println!("  ğŸ“š åˆ›å»ºäº† {} ä¸ªè®­ç»ƒæ ·æœ¬", training_data.len());

    // è®­ç»ƒæ¨¡å‹
    println!("  ğŸ“ å¼€å§‹è®­ç»ƒæ¨¡å‹...");
    let training_result = ml_predictor.train_model(&training_data).await?;

    println!("  âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ:");
    println!("    - è®­ç»ƒæˆåŠŸ: {}", training_result.success);
    println!("    - å‡†ç¡®ç‡: {:.3}", training_result.accuracy);
    println!("    - ç²¾ç¡®ç‡: {:.3}", training_result.precision);
    println!("    - å¬å›ç‡: {:.3}", training_result.recall);
    println!("    - F1åˆ†æ•°: {:.3}", training_result.f1_score);
    println!("    - è®­ç»ƒæ ·æœ¬æ•°: {}", training_result.training_samples);
    println!("    - æ–°æ¨¡å‹ç‰ˆæœ¬: {}", training_result.model_version);

    // éªŒè¯è®­ç»ƒåçš„æ¨¡å‹çŠ¶æ€
    let status = ml_predictor.get_model_status().await?;
    println!("  ğŸ“Š è®­ç»ƒåæ¨¡å‹çŠ¶æ€:");
    println!("    - æ¨¡å‹ç‰ˆæœ¬: {}", status.model_version);
    println!("    - å‡†ç¡®ç‡: {:.3}", status.accuracy);

    Ok(())
}

/// ç¤ºä¾‹ 4: åœ¨çº¿å­¦ä¹ 
async fn online_learning_demo() -> Result<()> {
    println!("\nğŸ”„ ç¤ºä¾‹ 4: åœ¨çº¿å­¦ä¹ ");
    println!("-------------------");

    let config = MLPredictionConfig::default();
    let ml_predictor = MLErrorPrediction::new(config)?;

    // æ¨¡æ‹Ÿé¢„æµ‹å’Œåé¦ˆå¾ªç¯
    println!("  ğŸ”„ æ¨¡æ‹Ÿé¢„æµ‹å’Œåé¦ˆå¾ªç¯...");

    for i in 1..=5 {
        println!("  ğŸ“Š ç¬¬ {} è½®é¢„æµ‹å’Œåé¦ˆ:", i);

        // åˆ›å»ºç³»ç»Ÿä¸Šä¸‹æ–‡
        let context = create_learning_context(i);

        // è¿›è¡Œé¢„æµ‹
        let prediction = ml_predictor.predict_error_probability(&context).await?;
        println!("    - é¢„æµ‹é”™è¯¯æ¦‚ç‡: {:.3}", prediction.probability);

        // æ¨¡æ‹Ÿå®é™…ç»“æœ
        let actual_outcome = simulate_actual_outcome(&context, i);

        // åˆ›å»ºåé¦ˆ
        let feedback = PredictionFeedback {
            prediction_id: format!("prediction-{}", i),
            actual_outcome: actual_outcome.clone(),
            feedback_type: determine_feedback_type(&prediction, &actual_outcome),
            timestamp: std::time::SystemTime::now(),
            context,
        };

        // åœ¨çº¿å­¦ä¹ 
        ml_predictor.online_learn(feedback).await?;
        println!("    - åé¦ˆå·²å¤„ç†ï¼Œæ¨¡å‹å·²æ›´æ–°");

        // ç­‰å¾…ä¸€æ®µæ—¶é—´
        tokio::time::sleep(Duration::from_millis(100)).await;
    }

    // æ£€æŸ¥å­¦ä¹ æ•ˆæœ
    let status = ml_predictor.get_model_status().await?;
    println!("  ğŸ“ˆ åœ¨çº¿å­¦ä¹ å®Œæˆï¼Œå½“å‰æ¨¡å‹çŠ¶æ€:");
    println!("    - æ¨¡å‹ç‰ˆæœ¬: {}", status.model_version);
    println!("    - æ€»é¢„æµ‹æ¬¡æ•°: {}", status.total_predictions);
    println!("    - å‡†ç¡®ç‡: {:.3}", status.accuracy);

    Ok(())
}

/// ç¤ºä¾‹ 5: è‡ªé€‚åº”æ¢å¤ç­–ç•¥
async fn adaptive_recovery_demo() -> Result<()> {
    println!("\nğŸ›¡ï¸  ç¤ºä¾‹ 5: è‡ªé€‚åº”æ¢å¤ç­–ç•¥");
    println!("---------------------------");

    let config = MLPredictionConfig::default();
    let ml_predictor = MLErrorPrediction::new(config)?;

    // æ¨¡æ‹Ÿä¸åŒçš„ç³»ç»ŸçŠ¶æ€
    let system_states = vec![
        ("æ­£å¸¸çŠ¶æ€", create_normal_state()),
        ("è­¦å‘ŠçŠ¶æ€", create_warning_state()),
        ("å±é™©çŠ¶æ€", create_danger_state()),
        ("ä¸´ç•ŒçŠ¶æ€", create_critical_state()),
    ];

    for (state_name, context) in system_states {
        println!("  ğŸ” ç³»ç»ŸçŠ¶æ€: {}", state_name);

        // é¢„æµ‹é”™è¯¯æ¦‚ç‡
        let prediction = ml_predictor.predict_error_probability(&context).await?;

        // æ ¹æ®é¢„æµ‹ç»“æœåˆ¶å®šæ¢å¤ç­–ç•¥
        let recovery_strategy = determine_recovery_strategy(&prediction);

        println!("    ğŸ“Š é¢„æµ‹ç»“æœ:");
        println!("      - é”™è¯¯æ¦‚ç‡: {:.3}", prediction.probability);
        println!("      - ç½®ä¿¡åº¦: {:.3}", prediction.confidence);

        println!("    ğŸ›¡ï¸  è‡ªé€‚åº”æ¢å¤ç­–ç•¥:");
        println!("      - ç­–ç•¥ç±»å‹: {}", recovery_strategy.strategy_type);
        println!("      - æ‰§è¡Œä¼˜å…ˆçº§: {}", recovery_strategy.priority);
        println!(
            "      - é¢„æœŸæ•ˆæœ: {:.1}%",
            recovery_strategy.expected_effectiveness * 100.0
        );
        println!("      - æ‰§è¡Œæ—¶é—´: {:?}", recovery_strategy.execution_time);

        // æ¨¡æ‹Ÿç­–ç•¥æ‰§è¡Œ
        execute_recovery_strategy(&recovery_strategy).await?;

        println!();
    }

    Ok(())
}

// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºä¸åŒçš„ç³»ç»Ÿä¸Šä¸‹æ–‡

fn create_high_load_context() -> SystemContext {
    SystemContext {
        timestamp: std::time::SystemTime::now(),
        cpu_usage: 0.95,
        memory_usage: 0.90,
        system_load: 4.5,
        active_services: 25,
        network_latency: Duration::from_millis(300),
        error_history: vec![
            create_error_history_entry("resource", ErrorSeverity::High),
            create_error_history_entry("processing", ErrorSeverity::Medium),
        ],
        service_health: create_service_health_map(vec![
            ("service1", HealthStatus::Warning),
            ("service2", HealthStatus::Healthy),
            ("service3", HealthStatus::Critical),
        ]),
        resource_metrics: create_resource_metrics(0.8, 0.9),
    }
}

fn create_resource_exhausted_context() -> SystemContext {
    SystemContext {
        timestamp: std::time::SystemTime::now(),
        cpu_usage: 0.98,
        memory_usage: 0.99,
        system_load: 8.0,
        active_services: 50,
        network_latency: Duration::from_millis(100),
        error_history: vec![
            create_error_history_entry("resource", ErrorSeverity::Critical),
            create_error_history_entry("resource", ErrorSeverity::Critical),
            create_error_history_entry("processing", ErrorSeverity::High),
        ],
        service_health: create_service_health_map(vec![
            ("service1", HealthStatus::Critical),
            ("service2", HealthStatus::Critical),
            ("service3", HealthStatus::Warning),
        ]),
        resource_metrics: create_resource_metrics(0.95, 0.98),
    }
}

fn create_network_issue_context() -> SystemContext {
    SystemContext {
        timestamp: std::time::SystemTime::now(),
        cpu_usage: 0.6,
        memory_usage: 0.7,
        system_load: 2.0,
        active_services: 15,
        network_latency: Duration::from_secs(2),
        error_history: vec![
            create_error_history_entry("transport", ErrorSeverity::High),
            create_error_history_entry("transport", ErrorSeverity::Medium),
        ],
        service_health: create_service_health_map(vec![
            ("service1", HealthStatus::Healthy),
            ("service2", HealthStatus::Warning),
            ("service3", HealthStatus::Healthy),
        ]),
        resource_metrics: create_resource_metrics(0.6, 0.7),
    }
}

fn create_healthy_context() -> SystemContext {
    SystemContext {
        timestamp: std::time::SystemTime::now(),
        cpu_usage: 0.3,
        memory_usage: 0.4,
        system_load: 1.0,
        active_services: 8,
        network_latency: Duration::from_millis(50),
        error_history: Vec::new(),
        service_health: create_service_health_map(vec![
            ("service1", HealthStatus::Healthy),
            ("service2", HealthStatus::Healthy),
            ("service3", HealthStatus::Healthy),
        ]),
        resource_metrics: create_resource_metrics(0.3, 0.4),
    }
}

fn create_critical_context() -> SystemContext {
    SystemContext {
        timestamp: std::time::SystemTime::now(),
        cpu_usage: 1.0,
        memory_usage: 1.0,
        system_load: 10.0,
        active_services: 100,
        network_latency: Duration::from_secs(5),
        error_history: vec![
            create_error_history_entry("resource", ErrorSeverity::Critical),
            create_error_history_entry("transport", ErrorSeverity::Critical),
            create_error_history_entry("processing", ErrorSeverity::Critical),
        ],
        service_health: create_service_health_map(vec![
            ("service1", HealthStatus::Critical),
            ("service2", HealthStatus::Critical),
            ("service3", HealthStatus::Critical),
        ]),
        resource_metrics: create_resource_metrics(1.0, 1.0),
    }
}

fn create_learning_context(round: i32) -> SystemContext {
    SystemContext {
        timestamp: std::time::SystemTime::now(),
        cpu_usage: 0.5 + (round as f64 * 0.1),
        memory_usage: 0.6 + (round as f64 * 0.05),
        system_load: 1.0 + (round as f64 * 0.2),
        active_services: 10 + (round as usize),
        network_latency: Duration::from_millis(100 + (round * 50) as u64),
        error_history: if round > 3 {
            vec![create_error_history_entry(
                "transport",
                ErrorSeverity::Medium,
            )]
        } else {
            Vec::new()
        },
        service_health: create_service_health_map(vec![
            ("service1", HealthStatus::Healthy),
            ("service2", HealthStatus::Warning),
        ]),
        resource_metrics: create_resource_metrics(0.5, 0.6),
    }
}

fn create_normal_state() -> SystemContext {
    create_healthy_context()
}

fn create_warning_state() -> SystemContext {
    SystemContext {
        timestamp: std::time::SystemTime::now(),
        cpu_usage: 0.7,
        memory_usage: 0.8,
        system_load: 2.5,
        active_services: 20,
        network_latency: Duration::from_millis(200),
        error_history: vec![create_error_history_entry(
            "processing",
            ErrorSeverity::Medium,
        )],
        service_health: create_service_health_map(vec![
            ("service1", HealthStatus::Warning),
            ("service2", HealthStatus::Healthy),
        ]),
        resource_metrics: create_resource_metrics(0.7, 0.8),
    }
}

fn create_danger_state() -> SystemContext {
    create_high_load_context()
}

fn create_critical_state() -> SystemContext {
    create_critical_context()
}

// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºè®­ç»ƒæ•°æ®

fn create_training_data() -> Vec<ErrorSample> {
    let mut training_data = Vec::new();

    // æ­£å¸¸æ ·æœ¬
    for i in 0..20 {
        training_data.push(ErrorSample {
            id: format!("normal-{}", i),
            timestamp: std::time::SystemTime::now(),
            context: create_healthy_context(),
            actual_error: None,
            predicted_error: None,
            prediction_accuracy: Some(0.9),
        });
    }

    // èµ„æºé”™è¯¯æ ·æœ¬
    for i in 0..15 {
        training_data.push(ErrorSample {
            id: format!("resource-{}", i),
            timestamp: std::time::SystemTime::now(),
            context: create_high_load_context(),
            actual_error: Some(create_error_history_entry("resource", ErrorSeverity::High)),
            predicted_error: None,
            prediction_accuracy: Some(0.85),
        });
    }

    // ä¼ è¾“é”™è¯¯æ ·æœ¬
    for i in 0..10 {
        training_data.push(ErrorSample {
            id: format!("transport-{}", i),
            timestamp: std::time::SystemTime::now(),
            context: create_network_issue_context(),
            actual_error: Some(create_error_history_entry(
                "transport",
                ErrorSeverity::Medium,
            )),
            predicted_error: None,
            prediction_accuracy: Some(0.8),
        });
    }

    training_data
}

// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºå…¶ä»–æ•°æ®ç»“æ„

fn create_error_history_entry(
    error_type: &str,
    severity: ErrorSeverity,
) -> otlp::ml_error_prediction::ErrorHistoryEntry {
    otlp::ml_error_prediction::ErrorHistoryEntry {
        timestamp: std::time::SystemTime::now(),
        error_type: error_type.to_string(),
        severity: severity.clone(),
        source: "demo".to_string(),
    }
}

fn create_service_health_map(
    services: Vec<(&str, HealthStatus)>,
) -> HashMap<String, otlp::ml_error_prediction::ServiceHealth> {
    let mut health_map = HashMap::new();

    for (name, status) in services {
        health_map.insert(
            name.to_string(),
            otlp::ml_error_prediction::ServiceHealth {
                status: match status {
                    HealthStatus::Healthy => otlp::ml_error_prediction::HealthStatus::Healthy,
                    HealthStatus::Warning => otlp::ml_error_prediction::HealthStatus::Warning,
                    HealthStatus::Critical => otlp::ml_error_prediction::HealthStatus::Critical,
                },
                response_time: Duration::from_millis(100),
                error_rate: match status {
                    HealthStatus::Healthy => 0.01,
                    HealthStatus::Warning => 0.05,
                    HealthStatus::Critical => 0.2,
                },
                last_check: std::time::SystemTime::now(),
            },
        );
    }

    health_map
}

fn create_resource_metrics(
    _cpu_usage: f64,
    memory_usage: f64,
) -> otlp::ml_error_prediction::ResourceMetrics {
    otlp::ml_error_prediction::ResourceMetrics {
        cpu_cores: 4,
        total_memory: 8192,
        available_memory: ((1.0 - memory_usage) * 8192.0) as u64,
        disk_usage: 0.5,
        network_bandwidth: 1000,
    }
}

// è¾…åŠ©å‡½æ•°ï¼šæ¨¡æ‹Ÿå’Œå†³ç­–é€»è¾‘

fn simulate_actual_outcome(
    context: &SystemContext,
    round: i32,
) -> otlp::ml_error_prediction::ActualOutcome {
    // åŸºäºç³»ç»ŸçŠ¶æ€æ¨¡æ‹Ÿå®é™…ç»“æœ
    if context.cpu_usage > 0.8 || context.memory_usage > 0.8 {
        otlp::ml_error_prediction::ActualOutcome::ErrorOccurred(create_error_history_entry(
            "resource",
            ErrorSeverity::High,
        ))
    } else if context.network_latency > Duration::from_secs(1) {
        otlp::ml_error_prediction::ActualOutcome::ErrorOccurred(create_error_history_entry(
            "transport",
            ErrorSeverity::Medium,
        ))
    } else if round > 4 {
        otlp::ml_error_prediction::ActualOutcome::ErrorOccurred(create_error_history_entry(
            "processing",
            ErrorSeverity::Low,
        ))
    } else {
        otlp::ml_error_prediction::ActualOutcome::NoError
    }
}

fn determine_feedback_type(
    prediction: &PredictionResult,
    actual: &otlp::ml_error_prediction::ActualOutcome,
) -> otlp::ml_error_prediction::FeedbackType {
    match actual {
        otlp::ml_error_prediction::ActualOutcome::ErrorOccurred(_) => {
            if prediction.probability > 0.7 {
                otlp::ml_error_prediction::FeedbackType::Positive
            } else {
                otlp::ml_error_prediction::FeedbackType::Negative
            }
        }
        otlp::ml_error_prediction::ActualOutcome::NoError => {
            if prediction.probability < 0.3 {
                otlp::ml_error_prediction::FeedbackType::Positive
            } else {
                otlp::ml_error_prediction::FeedbackType::Negative
            }
        }
    }
}

#[derive(Debug)]
struct RecoveryStrategy {
    strategy_type: String,
    priority: u32,
    expected_effectiveness: f64,
    execution_time: Duration,
}

fn determine_recovery_strategy(prediction: &PredictionResult) -> RecoveryStrategy {
    if prediction.probability > 0.8 {
        RecoveryStrategy {
            strategy_type: "ç´§æ€¥æ‰©å®¹".to_string(),
            priority: 1,
            expected_effectiveness: 0.9,
            execution_time: Duration::from_secs(30),
        }
    } else if prediction.probability > 0.6 {
        RecoveryStrategy {
            strategy_type: "å¯ç”¨ç†”æ–­å™¨".to_string(),
            priority: 2,
            expected_effectiveness: 0.8,
            execution_time: Duration::from_secs(10),
        }
    } else if prediction.probability > 0.4 {
        RecoveryStrategy {
            strategy_type: "å¢åŠ é‡è¯•æœºåˆ¶".to_string(),
            priority: 3,
            expected_effectiveness: 0.7,
            execution_time: Duration::from_secs(5),
        }
    } else {
        RecoveryStrategy {
            strategy_type: "ç›‘æ§å¢å¼º".to_string(),
            priority: 4,
            expected_effectiveness: 0.6,
            execution_time: Duration::from_secs(1),
        }
    }
}

async fn execute_recovery_strategy(strategy: &RecoveryStrategy) -> Result<()> {
    println!("      ğŸ”§ æ‰§è¡Œæ¢å¤ç­–ç•¥: {}...", strategy.strategy_type);

    // æ¨¡æ‹Ÿç­–ç•¥æ‰§è¡Œæ—¶é—´
    tokio::time::sleep(strategy.execution_time).await;

    println!("      âœ… æ¢å¤ç­–ç•¥æ‰§è¡Œå®Œæˆ");
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_basic_prediction() {
        let result = basic_prediction_demo().await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_error_prediction() {
        let result = error_prediction_demo().await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_model_training() {
        let result = model_training_demo().await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_online_learning() {
        let result = online_learning_demo().await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_adaptive_recovery() {
        let result = adaptive_recovery_demo().await;
        assert!(result.is_ok());
    }
}
