# 分布式系统一致性模型

## 目录

- [分布式系统一致性模型](#分布式系统一致性模型)
  - [目录](#目录)
  - [概述](#概述)
  - [一致性模型层次](#一致性模型层次)
  - [强一致性模型](#强一致性模型)
    - [线性一致性 (Linearizability)](#线性一致性-linearizability)
    - [顺序一致性 (Sequential Consistency)](#顺序一致性-sequential-consistency)
  - [弱一致性模型](#弱一致性模型)
    - [因果一致性 (Causal Consistency)](#因果一致性-causal-consistency)
    - [最终一致性 (Eventual Consistency)](#最终一致性-eventual-consistency)
  - [CAP 定理](#cap-定理)
    - [定理陈述](#定理陈述)
    - [形式化证明](#形式化证明)
    - [实践权衡](#实践权衡)
  - [PACELC 定理](#pacelc-定理)
  - [在 OTLP 中的应用](#在-otlp-中的应用)
    - [Trace 数据一致性](#trace-数据一致性)
    - [Span 完整性保证](#span-完整性保证)
    - [一致性级别选择](#一致性级别选择)
  - [一致性协议实现](#一致性协议实现)
    - [Paxos 算法](#paxos-算法)
    - [Raft 算法](#raft-算法)
    - [向量时钟协议](#向量时钟协议)
  - [性能与一致性权衡](#性能与一致性权衡)
  - [参考文献](#参考文献)

## 概述

一致性模型定义了分布式系统中并发操作的正确性标准。对于 OTLP 分布式追踪系统，理解不同一致性模型对于设计高可用、高性能的追踪数据收集和查询系统至关重要。

**核心问题**：

- 如何在分布式环境中保证数据一致性？
- 不同一致性模型的性能代价是什么？
- OTLP 应该选择哪种一致性模型？

## 一致性模型层次

一致性模型可以按强度排序（从强到弱）：

```text
线性一致性 (Linearizability)
    ↓
顺序一致性 (Sequential Consistency)
    ↓
因果一致性 (Causal Consistency)
    ↓
最终一致性 (Eventual Consistency)
```

**关键特性**：

- 更强的一致性 → 更简单的编程模型，但更低的性能
- 更弱的一致性 → 更高的性能和可用性，但更复杂的编程模型

## 强一致性模型

### 线性一致性 (Linearizability)

**定义**：

一个并发执行是线性一致的，当且仅当存在一个全局的操作序列，满足：

1. **实时约束**：如果操作 \( op_1 \) 在实时时间上完成于操作 \( op_2 \) 开始之前，则 \( op_1 \) 在序列中出现在 \( op_2 \) 之前

2. **顺序规范**：序列满足对象的顺序规范

**形式化定义**：

设 \( H \) 是一个历史（操作序列），\( H \) 是线性一致的当且仅当：

\[
\exists S: \text{complete}(H) \equiv S \land S \text{ 是合法的} \land \prec_H \subseteq \prec_S
\]

其中：

- \( S \) 是一个顺序历史
- \( \prec_H \) 是 \( H \) 中的实时顺序
- \( \prec_S \) 是 \( S \) 中的顺序

**示例**：

```rust
use std::sync::Arc;
use tokio::sync::RwLock;

/// 线性一致的寄存器
pub struct LinearizableRegister<T> {
    value: Arc<RwLock<T>>,
}

impl<T: Clone> LinearizableRegister<T> {
    pub fn new(initial: T) -> Self {
        Self {
            value: Arc::new(RwLock::new(initial)),
        }
    }

    /// 原子读操作
    pub async fn read(&self) -> T {
        self.value.read().await.clone()
    }

    /// 原子写操作
    pub async fn write(&self, value: T) {
        *self.value.write().await = value;
    }

    /// 比较并交换（CAS）
    pub async fn compare_and_swap(&self, expected: T, new: T) -> Result<T, T>
    where
        T: PartialEq,
    {
        let mut guard = self.value.write().await;
        if *guard == expected {
            *guard = new.clone();
            Ok(new)
        } else {
            Err(guard.clone())
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_linearizable_register() {
        let register = LinearizableRegister::new(0);

        // 并发写入
        let r1 = register.clone();
        let r2 = register.clone();

        let h1 = tokio::spawn(async move { r1.write(1).await });
        let h2 = tokio::spawn(async move { r2.write(2).await });

        h1.await.unwrap();
        h2.await.unwrap();

        // 读取应该看到最后一次写入
        let value = register.read().await;
        assert!(value == 1 || value == 2);
    }
}
```

**性能代价**：

- 需要全局同步
- 每次操作需要与所有副本通信
- 延迟：\( O(n) \)，其中 \( n \) 是副本数

### 顺序一致性 (Sequential Consistency)

**定义**：

一个并发执行是顺序一致的，当且仅当存在一个全局的操作序列，满足：

1. **程序顺序**：每个进程的操作按照程序顺序出现
2. **顺序规范**：序列满足对象的顺序规范

**与线性一致性的区别**：

- 顺序一致性不要求实时约束
- 允许操作被重排序，只要保持每个进程的程序顺序

**形式化定义**：

\[
\exists S: \forall P_i: \text{order}(P_i, H) = \text{order}(P_i, S) \land S \text{ 是合法的}
\]

**示例**：

```rust
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;

/// 顺序一致的计数器
pub struct SequentialCounter {
    value: Arc<AtomicU64>,
}

impl SequentialCounter {
    pub fn new() -> Self {
        Self {
            value: Arc::new(AtomicU64::new(0)),
        }
    }

    /// 递增计数器
    pub fn increment(&self) -> u64 {
        // SeqCst 保证顺序一致性
        self.value.fetch_add(1, Ordering::SeqCst)
    }

    /// 读取计数器
    pub fn get(&self) -> u64 {
        self.value.load(Ordering::SeqCst)
    }
}
```

**性能代价**：

- 比线性一致性更高效
- 但仍需要全局顺序
- 延迟：\( O(n) \)

## 弱一致性模型

### 因果一致性 (Causal Consistency)

**定义**：

一个系统是因果一致的，当且仅当：

\[
\forall a, b: a \rightarrow b \Rightarrow \text{所有进程以相同顺序观察到 } a \text{ 和 } b
\]

其中 \( \rightarrow \) 是 happens-before 关系。

**关键特性**：

- 只保证因果相关的操作顺序
- 并发操作可以被不同进程以不同顺序观察到
- 使用向量时钟实现

**实现**（见[因果关系与偏序理论](./因果关系与偏序理论.md)）：

```rust
use std::collections::HashMap;

/// 因果一致性存储（简化版）
pub struct CausalStore {
    clock: VectorClock,
    data: HashMap<String, (String, VectorClock)>,
}

impl CausalStore {
    pub fn write(&mut self, key: String, value: String) {
        self.clock.tick();
        self.data.insert(key, (value, self.clock.clone()));
    }

    pub fn read(&self, key: &str) -> Option<&String> {
        self.data.get(key).map(|(v, _)| v)
    }
}
```

**性能优势**：

- 不需要全局同步
- 延迟：\( O(1) \) 本地操作
- 空间开销：\( O(n) \) 向量时钟

### 最终一致性 (Eventual Consistency)

**定义**：

一个系统是最终一致的，当且仅当：

\[
\forall \text{更新 } u: \exists t: \forall t' > t, \forall \text{副本 } r: r \text{ 在时间 } t' \text{ 包含 } u
\]

即：如果没有新的更新，所有副本最终会收敛到相同的值。

**关键特性**：

- 最弱的一致性保证
- 最高的可用性和性能
- 需要冲突解决机制

**冲突解决策略**：

1. **Last-Write-Wins (LWW)**：

    \[
    \text{value} = \max_{timestamp}(\text{所有写入})
    \]

2. **Version Vector**：保留所有并发版本，由应用解决

3. **CRDT (Conflict-free Replicated Data Types)**：数学上保证收敛

**示例 - LWW 寄存器**：

```rust
use std::time::SystemTime;

/// Last-Write-Wins 寄存器
#[derive(Debug, Clone)]
pub struct LWWRegister<T> {
    value: T,
    timestamp: SystemTime,
}

impl<T: Clone> LWWRegister<T> {
    pub fn new(value: T) -> Self {
        Self {
            value,
            timestamp: SystemTime::now(),
        }
    }

    /// 写入新值
    pub fn write(&mut self, value: T) {
        self.value = value;
        self.timestamp = SystemTime::now();
    }

    /// 合并来自其他副本的值
    pub fn merge(&mut self, other: &LWWRegister<T>) {
        if other.timestamp > self.timestamp {
            self.value = other.value.clone();
            self.timestamp = other.timestamp;
        }
    }

    /// 读取当前值
    pub fn read(&self) -> &T {
        &self.value
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::thread;
    use std::time::Duration;

    #[test]
    fn test_lww_register() {
        let mut r1 = LWWRegister::new(1);
        thread::sleep(Duration::from_millis(10));
        let mut r2 = LWWRegister::new(2);

        // r2 的时间戳更新，应该获胜
        r1.merge(&r2);
        assert_eq!(*r1.read(), 2);
    }
}
```

**示例 - CRDT Counter**：

```rust
use std::collections::HashMap;

/// CRDT 增长计数器 (G-Counter)
#[derive(Debug, Clone)]
pub struct GCounter {
    /// 每个节点的计数
    counts: HashMap<String, u64>,
    /// 当前节点 ID
    node_id: String,
}

impl GCounter {
    pub fn new(node_id: String) -> Self {
        Self {
            counts: HashMap::new(),
            node_id,
        }
    }

    /// 递增计数器
    pub fn increment(&mut self) {
        *self.counts.entry(self.node_id.clone()).or_insert(0) += 1;
    }

    /// 获取总计数
    pub fn value(&self) -> u64 {
        self.counts.values().sum()
    }

    /// 合并其他副本
    pub fn merge(&mut self, other: &GCounter) {
        for (node, &count) in &other.counts {
            let entry = self.counts.entry(node.clone()).or_insert(0);
            *entry = (*entry).max(count);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_g_counter() {
        let mut c1 = GCounter::new("node1".to_string());
        let mut c2 = GCounter::new("node2".to_string());

        c1.increment();
        c1.increment();
        c2.increment();

        // 合并前
        assert_eq!(c1.value(), 2);
        assert_eq!(c2.value(), 1);

        // 合并后
        c1.merge(&c2);
        c2.merge(&c1);

        assert_eq!(c1.value(), 3);
        assert_eq!(c2.value(), 3);
    }
}
```

## CAP 定理

### 定理陈述

**CAP 定理** (Brewer, 2000)：

对于一个分布式数据存储系统，不可能同时满足以下三个属性：

- **C (Consistency)**：一致性 - 所有节点在同一时间看到相同的数据
- **A (Availability)**：可用性 - 每个请求都能得到响应（成功或失败）
- **P (Partition Tolerance)**：分区容错性 - 系统在网络分区时仍能继续工作

**结论**：最多只能同时满足两个属性。

### 形式化证明

**证明**（简化版）：

假设系统同时满足 C、A、P。考虑以下场景：

1. 系统有两个节点 \( N_1 \) 和 \( N_2 \)
2. 发生网络分区，\( N_1 \) 和 \( N_2 \) 无法通信
3. 客户端向 \( N_1 \) 写入值 \( v_1 \)
4. 客户端向 \( N_2 \) 读取值

**根据可用性 (A)**：\( N_2 \) 必须响应读请求

**根据分区容错性 (P)**：\( N_2 \) 无法从 \( N_1 \) 获取 \( v_1 \)

**根据一致性 (C)**：\( N_2 \) 必须返回 \( v_1 \)

矛盾！因此不可能同时满足 C、A、P。∎

### 实践权衡

**CP 系统**（牺牲可用性）：

- 示例：HBase, MongoDB (强一致性模式), Zookeeper
- 特点：在分区时拒绝服务，保证一致性
- 适用场景：金融系统、库存管理

**AP 系统**（牺牲一致性）：

- 示例：Cassandra, DynamoDB, Riak
- 特点：在分区时继续服务，接受最终一致性
- 适用场景：社交网络、内容分发、日志收集

**CA 系统**（牺牲分区容错性）：

- 示例：传统关系数据库（单节点）
- 特点：不能容忍网络分区
- 适用场景：单数据中心、低延迟网络

## PACELC 定理

**PACELC 定理** (Abadi, 2012)：

扩展 CAP 定理，考虑正常运行时的权衡：

\[
\text{if } \textbf{P}artition \text{ then } \textbf{A}vailability \text{ vs } \textbf{C}onsistency \text{ else } \textbf{L}atency \text{ vs } \textbf{C}onsistency
\]

**分类**：

- **PA/EL**：Cassandra, Riak - 分区时选择可用性，正常时选择低延迟
- **PC/EC**：HBase, MongoDB - 分区时选择一致性，正常时选择一致性
- **PA/EC**：DynamoDB - 分区时选择可用性，正常时选择一致性

## 在 OTLP 中的应用

### Trace 数据一致性

OTLP 追踪数据的特点：

1. **写入密集**：大量 Span 数据持续写入
2. **时效性要求**：需要快速查询最近的追踪数据
3. **容错要求**：不能因为单个节点故障而丢失数据

**推荐模型**：**因果一致性 + 最终一致性**

理由：

- Span 之间有明确的因果关系（父子关系）
- 不需要强一致性（追踪数据是观测性数据，不是事务数据）
- 高可用性和低延迟更重要

### Span 完整性保证

**问题**：如何保证一个 Trace 的所有 Span 都被收集到？

**解决方案 - 基于向量时钟的完整性检查**：

```rust
use std::collections::{HashMap, HashSet};

/// Trace 完整性检查器
pub struct TraceCompletenessChecker {
    /// Trace ID -> (已收到的 Span IDs, 预期的 Span IDs)
    traces: HashMap<String, (HashSet<String>, HashSet<String>)>,
}

impl TraceCompletenessChecker {
    pub fn new() -> Self {
        Self {
            traces: HashMap::new(),
        }
    }

    /// 添加 Span
    pub fn add_span(&mut self, trace_id: String, span_id: String, parent_span_id: Option<String>) {
        let (received, expected) = self
            .traces
            .entry(trace_id.clone())
            .or_insert_with(|| (HashSet::new(), HashSet::new()));

        received.insert(span_id.clone());

        // 如果有父 Span，将其添加到预期列表
        if let Some(parent_id) = parent_span_id {
            expected.insert(parent_id);
        }
    }

    /// 检查 Trace 是否完整
    pub fn is_complete(&self, trace_id: &str) -> bool {
        if let Some((received, expected)) = self.traces.get(trace_id) {
            // 所有预期的 Span 都已收到
            expected.is_subset(received)
        } else {
            false
        }
    }

    /// 获取缺失的 Span
    pub fn get_missing_spans(&self, trace_id: &str) -> Vec<String> {
        if let Some((received, expected)) = self.traces.get(trace_id) {
            expected.difference(received).cloned().collect()
        } else {
            Vec::new()
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_trace_completeness() {
        let mut checker = TraceCompletenessChecker::new();

        let trace_id = "trace1".to_string();

        // 添加根 Span
        checker.add_span(trace_id.clone(), "span1".to_string(), None);

        // 添加子 Span
        checker.add_span(trace_id.clone(), "span2".to_string(), Some("span1".to_string()));

        // Trace 应该完整
        assert!(checker.is_complete(&trace_id));

        // 添加另一个子 Span，但父 Span 缺失
        checker.add_span(trace_id.clone(), "span3".to_string(), Some("span_missing".to_string()));

        // Trace 不完整
        assert!(!checker.is_complete(&trace_id));

        let missing = checker.get_missing_spans(&trace_id);
        assert_eq!(missing, vec!["span_missing".to_string()]);
    }
}
```

### 一致性级别选择

**OTLP 一致性级别配置**：

```rust
/// OTLP 一致性级别
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ConsistencyLevel {
    /// 最终一致性 - 最高性能
    Eventual,
    /// 因果一致性 - 平衡性能和正确性
    Causal,
    /// 强一致性 - 最高正确性
    Strong,
}

impl ConsistencyLevel {
    /// 获取读取超时
    pub fn read_timeout_ms(&self) -> u64 {
        match self {
            ConsistencyLevel::Eventual => 100,
            ConsistencyLevel::Causal => 500,
            ConsistencyLevel::Strong => 5000,
        }
    }

    /// 获取需要确认的副本数
    pub fn required_acks(&self, total_replicas: usize) -> usize {
        match self {
            ConsistencyLevel::Eventual => 1,
            ConsistencyLevel::Causal => (total_replicas / 2) + 1,
            ConsistencyLevel::Strong => total_replicas,
        }
    }
}
```

## 一致性协议实现

### Paxos 算法

**Paxos** 是一个用于在分布式系统中达成共识的算法。

**角色**：

- **Proposer**：提议者
- **Acceptor**：接受者
- **Learner**：学习者

**阶段**：

1. **Prepare 阶段**：
   - Proposer 发送 `Prepare(n)` 给多数 Acceptor
   - Acceptor 承诺不接受编号小于 \( n \) 的提议

2. **Accept 阶段**：
   - Proposer 发送 `Accept(n, v)` 给多数 Acceptor
   - Acceptor 接受提议并通知 Learner

**正确性保证**：

- **安全性**：最多只有一个值被选定
- **活性**：最终会选定一个值（在合理的网络假设下）

### Raft 算法

**Raft** 是 Paxos 的简化版本，更易于理解和实现。

**核心概念**：

- **Leader Election**：选举领导者
- **Log Replication**：日志复制
- **Safety**：安全性保证

**状态**：

- **Follower**：跟随者
- **Candidate**：候选者
- **Leader**：领导者

**日志复制流程**：

1. 客户端向 Leader 发送命令
2. Leader 将命令追加到本地日志
3. Leader 向所有 Follower 发送 `AppendEntries` RPC
4. 多数 Follower 确认后，Leader 提交日志
5. Leader 通知 Follower 提交日志

### 向量时钟协议

见[因果关系与偏序理论](./因果关系与偏序理论.md)。

## 性能与一致性权衡

**延迟对比**：

| 一致性模型 | 写延迟 | 读延迟 | 可用性 |
|-----------|--------|--------|--------|
| 线性一致性 | O(n) RTT | O(n) RTT | 低 |
| 顺序一致性 | O(n) RTT | O(1) | 中 |
| 因果一致性 | O(1) | O(1) | 高 |
| 最终一致性 | O(1) | O(1) | 最高 |

**OTLP 推荐配置**：

```rust
/// OTLP 存储配置
pub struct OtlpStorageConfig {
    /// 默认一致性级别
    pub default_consistency: ConsistencyLevel,
    /// 写入副本数
    pub replication_factor: usize,
    /// 读取副本数
    pub read_replicas: usize,
    /// 是否启用因果一致性
    pub enable_causal_consistency: bool,
}

impl Default for OtlpStorageConfig {
    fn default() -> Self {
        Self {
            // 默认使用因果一致性
            default_consistency: ConsistencyLevel::Causal,
            // 3 副本
            replication_factor: 3,
            // 读取 2 个副本
            read_replicas: 2,
            // 启用因果一致性
            enable_causal_consistency: true,
        }
    }
}
```

## 参考文献

1. **Lamport, L.** (1979). "How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs". *IEEE Transactions on Computers*, C-28(9), 690-691.

2. **Herlihy, M. P., & Wing, J. M.** (1990). "Linearizability: A Correctness Condition for Concurrent Objects". *ACM Transactions on Programming Languages and Systems*, 12(3), 463-492.

3. **Brewer, E. A.** (2000). "Towards Robust Distributed Systems". *PODC Keynote*.

4. **Gilbert, S., & Lynch, N.** (2002). "Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services". *ACM SIGACT News*, 33(2), 51-59.

5. **Abadi, D.** (2012). "Consistency Tradeoffs in Modern Distributed Database System Design". *IEEE Computer*, 45(2), 37-42.

6. **Shapiro, M., Preguiça, N., Baquero, C., & Zawirski, M.** (2011). "Conflict-Free Replicated Data Types". *Stabilization, Safety, and Security of Distributed Systems*, 386-400.

7. **Ongaro, D., & Ousterhout, J.** (2014). "In Search of an Understandable Consensus Algorithm". *USENIX ATC*, 305-319.

---

*本文档提供了分布式系统一致性模型的完整理论基础，包括形式化定义、证明和实践指导，为 OTLP 系统的设计提供了理论支撑。*
