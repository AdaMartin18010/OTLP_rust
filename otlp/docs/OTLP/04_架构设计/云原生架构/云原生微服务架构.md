# OTLP Rust äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„æŒ‡å—

## ğŸ“š æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†OTLP Ruståœ¨äº‘åŸç”Ÿç¯å¢ƒä¸­çš„å¾®æœåŠ¡æ¶æ„è®¾è®¡ï¼ŒåŒ…æ‹¬å®¹å™¨åŒ–ã€æœåŠ¡ç½‘æ ¼ã€äº‘åŸç”Ÿæ¨¡å¼ã€CI/CDæµæ°´çº¿ç­‰ç°ä»£äº‘åŸç”Ÿæ¶æ„å®è·µã€‚

## â˜ï¸ äº‘åŸç”Ÿæ¶æ„æ¦‚è§ˆ

### 1. äº‘åŸç”Ÿæ¶æ„åŸåˆ™

| åŸåˆ™ | æè¿° | å®ç°æ–¹å¼ |
|------|------|----------|
| **å®¹å™¨åŒ–** | åº”ç”¨æ‰“åŒ…åœ¨å®¹å™¨ä¸­ | Docker + Kubernetes |
| **å¾®æœåŠ¡** | æ‹†åˆ†ä¸ºå°å‹ç‹¬ç«‹æœåŠ¡ | æœåŠ¡æ‹†åˆ† + APIç½‘å…³ |
| **å¯è§‚æµ‹æ€§** | å®Œæ•´çš„ç›‘æ§å’Œè¿½è¸ª | OTLP + Prometheus + Grafana |
| **è‡ªåŠ¨åŒ–** | è‡ªåŠ¨åŒ–çš„éƒ¨ç½²å’Œè¿ç»´ | CI/CD + GitOps |
| **å¼¹æ€§** | è‡ªåŠ¨æ‰©ç¼©å®¹å’Œæ•…éšœæ¢å¤ | HPA + VPA + ç†”æ–­å™¨ |
| **å®‰å…¨** | é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹ | mTLS + RBAC + ç½‘ç»œç­–ç•¥ |

### 2. äº‘åŸç”Ÿæ¶æ„å±‚æ¬¡

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           åº”ç”¨å±‚ (Application)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         æœåŠ¡ç½‘æ ¼å±‚ (Service Mesh)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         å®¹å™¨ç¼–æ’å±‚ (Orchestration)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         åŸºç¡€è®¾æ–½å±‚ (Infrastructure)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ³ å®¹å™¨åŒ–æ¶æ„

### 1. å¤šé˜¶æ®µDockeræ„å»º

```dockerfile
# Dockerfile.otlp-rust
# æ„å»ºé˜¶æ®µ
FROM rust:1.90-slim as builder

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /usr/src/app

# å¤åˆ¶Cargoæ–‡ä»¶
COPY Cargo.toml Cargo.lock ./

# åˆ›å»ºè™šæ‹Ÿä¾èµ–æ¥åˆ©ç”¨Dockerç¼“å­˜
RUN mkdir src && echo "fn main() {}" > src/main.rs

# æ„å»ºä¾èµ–
RUN cargo build --release && rm -rf src

# å¤åˆ¶æºä»£ç 
COPY src ./src
COPY examples ./examples
COPY benches ./benches

# é‡æ–°æ„å»ºåº”ç”¨
RUN cargo build --release

# è¿è¡Œæ—¶é˜¶æ®µ
FROM debian:bookworm-slim

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r otlp && useradd -r -g otlp otlp

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶äºŒè¿›åˆ¶æ–‡ä»¶
COPY --from=builder /usr/src/app/target/release/otlp /app/otlp

# å¤åˆ¶é…ç½®æ–‡ä»¶
COPY --from=builder /usr/src/app/examples/config.yaml /app/config.yaml

# è®¾ç½®æƒé™
RUN chown -R otlp:otlp /app
USER otlp

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8080 4317 4318

# å¯åŠ¨å‘½ä»¤
CMD ["./otlp"]
```

### 2. å®¹å™¨ç¼–æ’é…ç½®

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: otlp-system
  labels:
    name: otlp-system
    istio-injection: enabled

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otlp-config
  namespace: otlp-system
data:
  config.yaml: |
    otlp:
      endpoint: "http://otel-collector:4317"
      service_name: "otlp-rust-service"
      service_version: "1.0.0"
      environment: "production"
      
    batch:
      max_export_batch_size: 1000
      export_timeout: 5s
      max_queue_size: 10000
      scheduled_delay: 1s
      
    retry:
      enabled: true
      initial_interval: 1s
      max_interval: 30s
      max_elapsed_time: 300s
      multiplier: 2.0
      
    compression:
      enabled: true
      algorithm: "gzip"
      level: 6

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: otlp-secrets
  namespace: otlp-system
type: Opaque
data:
  api-key: <base64-encoded-api-key>
  tls-cert: <base64-encoded-cert>
  tls-key: <base64-encoded-key>

---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-app
  namespace: otlp-system
  labels:
    app: otlp-app
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: otlp-app
  template:
    metadata:
      labels:
        app: otlp-app
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: otlp-app-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      containers:
      - name: otlp-app
        image: otlp-rust:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        env:
        - name: RUST_LOG
          value: "info"
        - name: OTLP_ENDPOINT
          valueFrom:
            configMapKeyRef:
              name: otlp-config
              key: otlp.endpoint
        - name: SERVICE_NAME
          valueFrom:
            configMapKeyRef:
              name: otlp-config
              key: otlp.service_name
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: otlp-secrets
              key: api-key
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: config-volume
          mountPath: /app/config.yaml
          subPath: config.yaml
      volumes:
      - name: config-volume
        configMap:
          name: otlp-config
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - otlp-app
              topologyKey: kubernetes.io/hostname
```

## ğŸŒ æœåŠ¡ç½‘æ ¼æ¶æ„

### 1. IstioæœåŠ¡ç½‘æ ¼é…ç½®

```yaml
# istio/gateway.yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: otlp-gateway
  namespace: otlp-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "otlp.example.com"
    tls:
      httpsRedirect: true
  - port:
      number: 443
      name: https
      protocol: HTTPS
    hosts:
    - "otlp.example.com"
    tls:
      mode: SIMPLE
      credentialName: otlp-tls-cert

---
# istio/virtualservice.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: otlp-vs
  namespace: otlp-system
spec:
  hosts:
  - "otlp.example.com"
  gateways:
  - otlp-gateway
  http:
  - match:
    - uri:
        prefix: /api/v1/traces
    route:
    - destination:
        host: otlp-app
        port:
          number: 4317
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
  - match:
    - uri:
        prefix: /api/v1/metrics
    route:
    - destination:
        host: otlp-app
        port:
          number: 4318
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
  - match:
    - uri:
        prefix: /health
    route:
    - destination:
        host: otlp-app
        port:
          number: 8080
    timeout: 5s

---
# istio/destinationrule.yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: otlp-app-dr
  namespace: otlp-system
spec:
  host: otlp-app
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
        maxRequestsPerConnection: 10
        maxRetries: 3
    circuitBreaker:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    loadBalancer:
      simple: LEAST_CONN
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    tls:
      mode: ISTIO_MUTUAL
```

### 2. æœåŠ¡ç½‘æ ¼ç›‘æ§

```yaml
# istio/telemetry.yaml
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: otlp-telemetry
  namespace: otlp-system
spec:
  metrics:
  - providers:
    - name: prometheus
  - overrides:
    - match:
        metric: ALL_METRICS
      tagOverrides:
        destination_service:
          value: "otlp-app"
        source_service:
          value: "client"
  tracing:
  - providers:
    - name: otlp
  - customTags:
    environment:
      literal:
        value: "production"
    service_version:
      header:
        name: "x-service-version"
        defaultValue: "unknown"
```

## ğŸ”„ å¾®æœåŠ¡æ¶æ„å®ç°

### 1. å¾®æœåŠ¡æ‹†åˆ†ç­–ç•¥

```rust
// src/microservices/mod.rs
use std::collections::HashMap;
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};

pub mod trace_service;
pub mod metric_service;
pub mod log_service;
pub mod config_service;
pub mod health_service;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MicroserviceConfig {
    pub service_name: String,
    pub version: String,
    pub port: u16,
    pub dependencies: Vec<ServiceDependency>,
    pub health_check: HealthCheckConfig,
    pub metrics: MetricsConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceDependency {
    pub service_name: String,
    pub endpoint: String,
    pub timeout: u64,
    pub retry_count: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheckConfig {
    pub enabled: bool,
    pub interval: u64,
    pub timeout: u64,
    pub failure_threshold: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricsConfig {
    pub enabled: bool,
    pub port: u16,
    pub path: String,
}

pub struct MicroserviceManager {
    services: RwLock<HashMap<String, Box<dyn Microservice + Send + Sync>>>,
    config: MicroserviceConfig,
}

impl MicroserviceManager {
    pub fn new(config: MicroserviceConfig) -> Self {
        Self {
            services: RwLock::new(HashMap::new()),
            config,
        }
    }
    
    pub async fn register_service<T>(&self, name: &str, service: T)
    where
        T: Microservice + Send + Sync + 'static,
    {
        let mut services = self.services.write().await;
        services.insert(name.to_string(), Box::new(service));
    }
    
    pub async fn start_all_services(&self) -> Result<(), Box<dyn std::error::Error>> {
        let services = self.services.read().await;
        
        for (name, service) in services.iter() {
            log::info!("Starting service: {}", name);
            service.start().await?;
        }
        
        Ok(())
    }
    
    pub async fn stop_all_services(&self) -> Result<(), Box<dyn std::error::Error>> {
        let services = self.services.read().await;
        
        for (name, service) in services.iter() {
            log::info!("Stopping service: {}", name);
            service.stop().await?;
        }
        
        Ok(())
    }
}

pub trait Microservice {
    async fn start(&self) -> Result<(), Box<dyn std::error::Error>>;
    async fn stop(&self) -> Result<(), Box<dyn std::error::Error>>;
    async fn health_check(&self) -> Result<HealthStatus, Box<dyn std::error::Error>>;
    async fn get_metrics(&self) -> Result<String, Box<dyn std::error::Error>>;
}

#[derive(Debug, Clone)]
pub enum HealthStatus {
    Healthy,
    Unhealthy,
    Degraded,
}
```

### 2. è¿½è¸ªæœåŠ¡å®ç°

```rust
// src/microservices/trace_service.rs
use super::{Microservice, HealthStatus};
use crate::data::TelemetryData;
use std::collections::HashMap;
use tokio::sync::RwLock;

pub struct TraceService {
    traces: RwLock<HashMap<String, TelemetryData>>,
    metrics: TraceMetrics,
}

struct TraceMetrics {
    total_traces: std::sync::atomic::AtomicU64,
    active_traces: std::sync::atomic::AtomicU64,
    failed_traces: std::sync::atomic::AtomicU64,
}

impl TraceService {
    pub fn new() -> Self {
        Self {
            traces: RwLock::new(HashMap::new()),
            metrics: TraceMetrics {
                total_traces: std::sync::atomic::AtomicU64::new(0),
                active_traces: std::sync::atomic::AtomicU64::new(0),
                failed_traces: std::sync::atomic::AtomicU64::new(0),
            },
        }
    }
    
    pub async fn create_trace(&self, service_name: &str, operation_name: &str) -> Result<String, Box<dyn std::error::Error>> {
        let trace_id = uuid::Uuid::new_v4().to_string();
        let trace = TelemetryData::trace(format!("{}-{}", service_name, operation_name))
            .with_attribute("service.name", service_name)
            .with_attribute("operation.name", operation_name)
            .with_attribute("trace.id", &trace_id);
        
        let mut traces = self.traces.write().await;
        traces.insert(trace_id.clone(), trace);
        
        self.metrics.total_traces.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        self.metrics.active_traces.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        
        Ok(trace_id)
    }
    
    pub async fn complete_trace(&self, trace_id: &str, status: &str) -> Result<(), Box<dyn std::error::Error>> {
        let mut traces = self.traces.write().await;
        if let Some(trace) = traces.get_mut(trace_id) {
            trace.with_attribute("status", status);
            self.metrics.active_traces.fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
            
            if status == "error" {
                self.metrics.failed_traces.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
            }
        }
        
        Ok(())
    }
    
    pub async fn get_trace(&self, trace_id: &str) -> Result<Option<TelemetryData>, Box<dyn std::error::Error>> {
        let traces = self.traces.read().await;
        Ok(traces.get(trace_id).cloned())
    }
}

#[async_trait::async_trait]
impl Microservice for TraceService {
    async fn start(&self) -> Result<(), Box<dyn std::error::Error>> {
        log::info!("Trace service started");
        Ok(())
    }
    
    async fn stop(&self) -> Result<(), Box<dyn std::error::Error>> {
        log::info!("Trace service stopped");
        Ok(())
    }
    
    async fn health_check(&self) -> Result<HealthStatus, Box<dyn std::error::Error>> {
        let traces_count = self.traces.read().await.len();
        if traces_count < 10000 {
            Ok(HealthStatus::Healthy)
        } else {
            Ok(HealthStatus::Degraded)
        }
    }
    
    async fn get_metrics(&self) -> Result<String, Box<dyn std::error::Error>> {
        let metrics = format!(
            "# HELP otlp_traces_total Total number of traces\n\
             # TYPE otlp_traces_total counter\n\
             otlp_traces_total {}\n\
             # HELP otlp_traces_active Active number of traces\n\
             # TYPE otlp_traces_active gauge\n\
             otlp_traces_active {}\n\
             # HELP otlp_traces_failed Failed number of traces\n\
             # TYPE otlp_traces_failed counter\n\
             otlp_traces_failed {}",
            self.metrics.total_traces.load(std::sync::atomic::Ordering::Relaxed),
            self.metrics.active_traces.load(std::sync::atomic::Ordering::Relaxed),
            self.metrics.failed_traces.load(std::sync::atomic::Ordering::Relaxed)
        );
        
        Ok(metrics)
    }
}
```

## ğŸš€ CI/CDæµæ°´çº¿

### 1. GitOpsé…ç½®

```yaml
# .github/workflows/ci-cd.yaml
name: OTLP Rust CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        components: rustfmt, clippy
    
    - name: Cache cargo registry
      uses: actions/cache@v3
      with:
        path: ~/.cargo/registry
        key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Cache cargo index
      uses: actions/cache@v3
      with:
        path: ~/.cargo/git
        key: ${{ runner.os }}-cargo-index-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Cache cargo build
      uses: actions/cache@v3
      with:
        path: target
        key: ${{ runner.os }}-cargo-build-target-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libssl-dev pkg-config
    
    - name: Run tests
      run: cargo test --verbose
    
    - name: Run clippy
      run: cargo clippy --all-targets --all-features -- -D warnings
    
    - name: Run fmt
      run: cargo fmt --all -- --check
    
    - name: Run security audit
      run: |
        cargo install cargo-audit
        cargo audit

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.otlp-rust
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: 'v3.12.0'
    
    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig
    
    - name: Deploy to Kubernetes
      run: |
        # Update image tag in deployment
        sed -i "s|image: otlp-rust:.*|image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest|g" k8s/deployment.yaml
        
        # Apply configurations
        kubectl apply -f k8s/namespace.yaml
        kubectl apply -f k8s/configmap.yaml
        kubectl apply -f k8s/secret.yaml
        kubectl apply -f k8s/service.yaml
        kubectl apply -f k8s/deployment.yaml
        kubectl apply -f istio/gateway.yaml
        kubectl apply -f istio/virtualservice.yaml
        kubectl apply -f istio/destinationrule.yaml
        
        # Wait for deployment to be ready
        kubectl rollout status deployment/otlp-app -n otlp-system --timeout=300s
    
    - name: Run smoke tests
      run: |
        # Wait for service to be ready
        kubectl wait --for=condition=available --timeout=300s deployment/otlp-app -n otlp-system
        
        # Get service endpoint
        SERVICE_IP=$(kubectl get svc otlp-app-service -n otlp-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
        
        # Run smoke tests
        curl -f http://$SERVICE_IP:8080/health || exit 1
        curl -f http://$SERVICE_IP:8080/ready || exit 1
        curl -f http://$SERVICE_IP:8080/metrics || exit 1
```

### 2. ArgoCDåº”ç”¨é…ç½®

```yaml
# argocd/application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: otlp-rust-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/otlp-rust
    targetRevision: HEAD
    path: k8s
  destination:
    server: https://kubernetes.default.svc
    namespace: otlp-system
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    - PrunePropagationPolicy=foreground
    - PruneLast=true
  revisionHistoryLimit: 10
```

## ğŸ“Š äº‘åŸç”Ÿç›‘æ§

### 1. Prometheusé…ç½®

```yaml
# monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "otlp_rules.yml"
    
    scrape_configs:
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
    
    - job_name: 'istio-mesh'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - istio-system
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: istio-telemetry;prometheus
    
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otlp-rules
  namespace: monitoring
data:
  otlp_rules.yml: |
    groups:
    - name: otlp.rules
      rules:
      - alert: OtlpServiceDown
        expr: up{job="otlp-app"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "OTLP service is down"
          description: "OTLP service has been down for more than 1 minute."
      
      - alert: OtlpHighErrorRate
        expr: rate(otlp_requests_total{status="error"}[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate in OTLP service"
          description: "Error rate is {{ $value }} errors per second."
      
      - alert: OtlpHighLatency
        expr: histogram_quantile(0.95, rate(otlp_request_duration_seconds_bucket[5m])) > 1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High latency in OTLP service"
          description: "95th percentile latency is {{ $value }} seconds."
      
      - alert: OtlpMemoryUsage
        expr: (container_memory_usage_bytes{pod=~"otlp-app-.*"} / container_spec_memory_limit_bytes{pod=~"otlp-app-.*"}) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage in OTLP service"
          description: "Memory usage is {{ $value }}% of limit."
```

### 2. Grafanaä»ªè¡¨ç›˜

```json
{
  "dashboard": {
    "id": null,
    "title": "OTLP Rust Service Dashboard",
    "tags": ["otlp", "rust", "microservices"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(otlp_requests_total[5m])",
            "legendFormat": "Requests/sec"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "reqps"
          }
        }
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(otlp_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          },
          {
            "expr": "histogram_quantile(0.95, rate(otlp_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.99, rate(otlp_request_duration_seconds_bucket[5m]))",
            "legendFormat": "99th percentile"
          }
        ],
        "yAxes": [
          {
            "unit": "s"
          }
        ]
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(otlp_requests_total{status=\"error\"}[5m]) / rate(otlp_requests_total[5m])",
            "legendFormat": "Error Rate"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit"
          }
        }
      },
      {
        "id": 4,
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "container_memory_usage_bytes{pod=~\"otlp-app-.*\"}",
            "legendFormat": "{{pod}}"
          }
        ],
        "yAxes": [
          {
            "unit": "bytes"
          }
        ]
      },
      {
        "id": 5,
        "title": "CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total{pod=~\"otlp-app-.*\"}[5m])",
            "legendFormat": "{{pod}}"
          }
        ],
        "yAxes": [
          {
            "unit": "percent"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

## ğŸ”’ äº‘åŸç”Ÿå®‰å…¨

### 1. å®‰å…¨ç­–ç•¥é…ç½®

```yaml
# security/network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: otlp-network-policy
  namespace: otlp-system
spec:
  podSelector:
    matchLabels:
      app: otlp-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: istio-system
    - podSelector:
        matchLabels:
          app: otel-collector
    ports:
    - protocol: TCP
      port: 8080
    - protocol: TCP
      port: 4317
    - protocol: TCP
      port: 4318
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: otel-collector
    ports:
    - protocol: TCP
      port: 4317
  - to: []
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53

---
# security/pod-security-policy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: otlp-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'

---
# security/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: otlp-app-role
  namespace: otlp-system
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: otlp-app-rolebinding
  namespace: otlp-system
subjects:
- kind: ServiceAccount
  name: otlp-app-sa
  namespace: otlp-system
roleRef:
  kind: Role
  name: otlp-app-role
  apiGroup: rbac.authorization.k8s.io

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otlp-app-sa
  namespace: otlp-system
automountServiceAccountToken: false
```

## ğŸ“š æœ€ä½³å®è·µæ€»ç»“

### 1. äº‘åŸç”Ÿæ¶æ„åŸåˆ™1

- **å®¹å™¨ä¼˜å…ˆ**: æ‰€æœ‰åº”ç”¨éƒ½å®¹å™¨åŒ–éƒ¨ç½²
- **å¾®æœåŠ¡æ¶æ„**: æ‹†åˆ†ä¸ºå°å‹ç‹¬ç«‹æœåŠ¡
- **APIé©±åŠ¨**: é€šè¿‡APIè¿›è¡ŒæœåŠ¡é—´é€šä¿¡
- **è‡ªåŠ¨åŒ–è¿ç»´**: å®ç°è‡ªåŠ¨åŒ–çš„éƒ¨ç½²å’Œè¿ç»´
- **å¯è§‚æµ‹æ€§**: å»ºç«‹å®Œæ•´çš„ç›‘æ§å’Œè¿½è¸ªä½“ç³»
- **å®‰å…¨ä¼˜å…ˆ**: å®æ–½é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹

### 2. å®æ–½å»ºè®®

- **æ¸è¿›å¼è¿ç§»**: ä»å•ä½“åº”ç”¨é€æ­¥è¿ç§»åˆ°å¾®æœåŠ¡
- **æœåŠ¡ç½‘æ ¼**: ä½¿ç”¨Istioç­‰æœåŠ¡ç½‘æ ¼ç®¡ç†å¾®æœåŠ¡
- **GitOps**: é‡‡ç”¨GitOpsæ¨¡å¼ç®¡ç†é…ç½®å’Œéƒ¨ç½²
- **ç›‘æ§å‘Šè­¦**: å»ºç«‹å®Œå–„çš„ç›‘æ§å‘Šè­¦ä½“ç³»
- **å®‰å…¨ç­–ç•¥**: å®æ–½å¤šå±‚å®‰å…¨é˜²æŠ¤ç­–ç•¥
- **æ€§èƒ½ä¼˜åŒ–**: æŒç»­ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½

### 3. è¿ç»´æœ€ä½³å®è·µ

- **è‡ªåŠ¨åŒ–æµ‹è¯•**: å»ºç«‹å®Œæ•´çš„è‡ªåŠ¨åŒ–æµ‹è¯•ä½“ç³»
- **æŒç»­é›†æˆ**: å®ç°æŒç»­é›†æˆå’ŒæŒç»­éƒ¨ç½²
- **é…ç½®ç®¡ç†**: ç»Ÿä¸€ç®¡ç†é…ç½®å’Œå¯†é’¥
- **æ—¥å¿—èšåˆ**: é›†ä¸­æ”¶é›†å’Œåˆ†ææ—¥å¿—
- **æ•…éšœæ¢å¤**: å»ºç«‹å¿«é€Ÿæ•…éšœæ¢å¤æœºåˆ¶
- **å®¹é‡è§„åˆ’**: åˆç†è§„åˆ’èµ„æºå®¹é‡

---

**äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ27æ—¥  
**ç»´æŠ¤è€…**: OTLP 2025 æ–‡æ¡£å›¢é˜Ÿ
