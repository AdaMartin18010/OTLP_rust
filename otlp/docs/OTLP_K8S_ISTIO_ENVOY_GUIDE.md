# OTLP on Kubernetes with Istio/Envoy - å®Œæ•´ç”Ÿäº§å®è·µæŒ‡å—

> **ç‰ˆæœ¬**: v2.0  
> **çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª  
> **æœ€åæ›´æ–°**: 2025å¹´10æœˆ17æ—¥

---

## ğŸ“‹ ç›®å½•

- [OTLP on Kubernetes with Istio/Envoy - å®Œæ•´ç”Ÿäº§å®è·µæŒ‡å—](#otlp-on-kubernetes-with-istioenvoy---å®Œæ•´ç”Ÿäº§å®è·µæŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ“‹ æ¦‚è¿°](#-æ¦‚è¿°)
    - [æ ¸å¿ƒç›®æ ‡](#æ ¸å¿ƒç›®æ ‡)
    - [æŠ€æœ¯æ ˆ](#æŠ€æœ¯æ ˆ)
  - [ğŸ—ï¸ æ¶æ„è®¾è®¡](#ï¸-æ¶æ„è®¾è®¡)
    - [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
    - [æ•°æ®æµå‘](#æ•°æ®æµå‘)
  - [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
    - [å‰ç½®è¦æ±‚](#å‰ç½®è¦æ±‚)
    - [1. éƒ¨ç½²OpenTelemetry Collector](#1-éƒ¨ç½²opentelemetry-collector)
    - [2. å¯ç”¨Istioæ³¨å…¥](#2-å¯ç”¨istioæ³¨å…¥)
    - [3. éƒ¨ç½²ç¤ºä¾‹åº”ç”¨](#3-éƒ¨ç½²ç¤ºä¾‹åº”ç”¨)
    - [4. éªŒè¯æ•°æ®æµ](#4-éªŒè¯æ•°æ®æµ)
  - [ğŸ“¦ ç”Ÿäº§çº§éƒ¨ç½²](#-ç”Ÿäº§çº§éƒ¨ç½²)
    - [Helm Chartéƒ¨ç½²](#helm-chartéƒ¨ç½²)
    - [é«˜å¯ç”¨é…ç½®](#é«˜å¯ç”¨é…ç½®)
    - [HPAè‡ªåŠ¨ä¼¸ç¼©](#hpaè‡ªåŠ¨ä¼¸ç¼©)
  - [ğŸŒ Istio/Envoyé›†æˆ](#-istioenvoyé›†æˆ)
    - [TraceContextä¼ æ’­](#tracecontextä¼ æ’­)
    - [VirtualServiceé…ç½®](#virtualserviceé…ç½®)
    - [PeerAuthentication (mTLS)](#peerauthentication-mtls)
    - [DestinationRuleé…ç½®](#destinationruleé…ç½®)
    - [Envoy Access Log](#envoy-access-log)
  - [ğŸ”’ å®‰å…¨åŠ å›º](#-å®‰å…¨åŠ å›º)
    - [mTLSé…ç½®](#mtlsé…ç½®)
    - [è®¤è¯ç­–ç•¥](#è®¤è¯ç­–ç•¥)
    - [æˆæƒç­–ç•¥](#æˆæƒç­–ç•¥)
    - [ç½‘ç»œç­–ç•¥](#ç½‘ç»œç­–ç•¥)
  - [ğŸ“Š ç›‘æ§å’Œå‘Šè­¦](#-ç›‘æ§å’Œå‘Šè­¦)
    - [Prometheusç›‘æ§](#prometheusç›‘æ§)
    - [Grafanaä»ªè¡¨æ¿](#grafanaä»ªè¡¨æ¿)
    - [å‘Šè­¦è§„åˆ™](#å‘Šè­¦è§„åˆ™)
  - [ğŸ” æ•…éšœæ’æŸ¥](#-æ•…éšœæ’æŸ¥)
    - [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
      - [é—®é¢˜1: Collectoræ— æ³•æ¥æ”¶æ•°æ®](#é—®é¢˜1-collectoræ— æ³•æ¥æ”¶æ•°æ®)
      - [é—®é¢˜2: mTLSè®¤è¯å¤±è´¥](#é—®é¢˜2-mtlsè®¤è¯å¤±è´¥)
    - [è¯Šæ–­å·¥å…·](#è¯Šæ–­å·¥å…·)
    - [æ—¥å¿—åˆ†æ](#æ—¥å¿—åˆ†æ)
  - [âš¡ æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
    - [Collectorä¼˜åŒ–](#collectorä¼˜åŒ–)
    - [Envoyä¼˜åŒ–](#envoyä¼˜åŒ–)
    - [ç½‘ç»œä¼˜åŒ–](#ç½‘ç»œä¼˜åŒ–)
  - [ğŸ“š å®Œæ•´é…ç½®ç¤ºä¾‹](#-å®Œæ•´é…ç½®ç¤ºä¾‹)
    - [åŸºç¡€éƒ¨ç½²](#åŸºç¡€éƒ¨ç½²)
    - [Helm Values](#helm-values)
  - [ğŸ§ª æµ‹è¯•éªŒè¯](#-æµ‹è¯•éªŒè¯)
    - [åŠŸèƒ½æµ‹è¯•](#åŠŸèƒ½æµ‹è¯•)
    - [æ€§èƒ½æµ‹è¯•](#æ€§èƒ½æµ‹è¯•)
    - [æ··æ²Œæµ‹è¯•](#æ··æ²Œæµ‹è¯•)
  - [ğŸ“ è¿ç»´æ‰‹å†Œ](#-è¿ç»´æ‰‹å†Œ)
    - [æ—¥å¸¸è¿ç»´](#æ—¥å¸¸è¿ç»´)
    - [æ•…éšœæ¢å¤](#æ•…éšœæ¢å¤)
    - [å‡çº§æµç¨‹](#å‡çº§æµç¨‹)
  - [ğŸ“š ç›¸å…³èµ„æº](#-ç›¸å…³èµ„æº)
  - [ğŸ“… å˜æ›´å†å²](#-å˜æ›´å†å²)

---

## ğŸ“‹ æ¦‚è¿°

### æ ¸å¿ƒç›®æ ‡

1. âœ… åœ¨Kubernetesä¸­éƒ¨ç½²ç”Ÿäº§çº§OpenTelemetry Collector
2. âœ… é›†æˆIstioæœåŠ¡ç½‘æ ¼å®ç°æµé‡æ²»ç†
3. âœ… é€šè¿‡Envoy Sidecarå®ç°ç»Ÿä¸€å¯è§‚æµ‹æ€§
4. âœ… ä½¿ç”¨OTLPåè®®ä¼ è¾“Traces/Metrics/Logs
5. âœ… å®ç°å®‰å…¨ã€é«˜å¯ç”¨ã€å¯æ‰©å±•çš„å¯è§‚æµ‹æ€§å¹³å°

### æŠ€æœ¯æ ˆ

| ç»„ä»¶ | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|
| Kubernetes | v1.28+ | å®¹å™¨ç¼–æ’å¹³å° |
| Istio | v1.20+ | æœåŠ¡ç½‘æ ¼ |
| OpenTelemetry Collector | v0.95+ | é¥æµ‹æ•°æ®æ”¶é›† |
| Envoy | v1.29+ | Service Proxy |
| Prometheus | v2.49+ | æŒ‡æ ‡å­˜å‚¨ |
| Jaeger | v1.54+ | è¿½è¸ªåç«¯ |
| Grafana | v10.3+ | å¯è§†åŒ– |

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Namespace: observability                â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚      OpenTelemetry Collector (DaemonSet)     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  Receivers  â”‚  â”‚ Processors  â”‚           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â€¢ OTLP     â”‚â†’â”‚ â€¢ Batch     â”‚â†’Exporters â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  â€¢ Jaeger   â”‚  â”‚ â€¢ Filter    â”‚           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚           â†“           â†“           â†“                 â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚
â”‚  â”‚    â”‚ Jaeger   â”‚ â”‚Prometheusâ”‚ â”‚  Loki    â”‚         â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â†‘                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           Namespace: default (with Istio)            â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚           Application Pod                   â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Application  â”‚    â”‚ Envoy Proxy  â”‚     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  Container   â”‚â†â”€â”€â†’â”‚  (Sidecar)   â”‚     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚              â”‚    â”‚              â”‚     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ OTLP Export  â”‚    â”‚ TraceContext â”‚     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â”‚   â†“          â”‚    â”‚ Propagation  â”‚     â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®æµå‘

```text
Application â†’ Envoy Sidecar â†’ OTLP Collector â†’ Backend
     â”‚            â”‚                  â”‚              â”‚
     â”‚            â”‚                  â”‚              â”œâ”€â†’ Jaeger (Traces)
     â”‚            â”‚                  â”‚              â”œâ”€â†’ Prometheus (Metrics)
     â”‚            â”‚                  â”‚              â””â”€â†’ Loki (Logs)
     â”‚            â”‚                  â”‚
     â”‚            â”‚                  â””â”€â†’ Batch Processing
     â”‚            â”‚                       â€¢ Filtering
     â”‚            â”‚                       â€¢ Sampling
     â”‚            â”‚                       â€¢ Enrichment
     â”‚            â”‚
     â”‚            â””â”€â†’ TraceContext Propagation
     â”‚                â€¢ traceparent
     â”‚                â€¢ b3 headers
     â”‚
     â””â”€â†’ OTLP/gRPC or OTLP/HTTP
         â€¢ Traces
         â€¢ Metrics
         â€¢ Logs
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

```bash
# 1. Kubernetesé›†ç¾¤ï¼ˆv1.28+ï¼‰
kubectl version --client

# 2. Istioå®‰è£…ï¼ˆv1.20+ï¼‰
istioctl version

# 3. Helmï¼ˆv3.12+ï¼‰
helm version

# 4. å¯è®¿é—®çš„å®¹å™¨registry
# ä¾‹å¦‚ï¼šdocker.io, gcr.io, æˆ–ç§æœ‰registry
```

### 1. éƒ¨ç½²OpenTelemetry Collector

**åˆ›å»ºå‘½åç©ºé—´**:

```bash
kubectl create namespace observability
```

**éƒ¨ç½²Collector (DaemonSetæ¨¡å¼)**:

```yaml
# otel-collector.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: observability
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  otel-collector-config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
            cors:
              allowed_origins:
                - "*"
      
      # Kubernetesé›†ç¾¤æŒ‡æ ‡
      k8s_cluster:
        auth_type: serviceAccount
        node_conditions_to_report: [Ready, MemoryPressure, DiskPressure]
      
      # KubeletæŒ‡æ ‡
      kubeletstats:
        collection_interval: 30s
        auth_type: serviceAccount
        endpoint: "${K8S_NODE_NAME}:10250"
        insecure_skip_verify: true
    
    processors:
      # æ‰¹å¤„ç†
      batch:
        timeout: 10s
        send_batch_size: 1024
      
      # å†…å­˜é™åˆ¶
      memory_limiter:
        check_interval: 1s
        limit_mib: 512
        spike_limit_mib: 128
      
      # èµ„æºæ£€æµ‹
      resourcedetection/env:
        detectors: [env, system]
        timeout: 2s
      
      # Kuberneteså±æ€§
      k8sattributes:
        auth_type: serviceAccount
        passthrough: false
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.deployment.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
          labels:
            - tag_name: app
              key: app
              from: pod
      
      # é‡‡æ ·
      probabilistic_sampler:
        hash_seed: 22
        sampling_percentage: 10
    
    exporters:
      # æ—¥å¿—å¯¼å‡ºï¼ˆè°ƒè¯•ï¼‰
      logging:
        loglevel: info
        sampling_initial: 5
        sampling_thereafter: 200
      
      # Jaegerå¯¼å‡º
      otlp/jaeger:
        endpoint: jaeger-collector.observability.svc.cluster.local:4317
        tls:
          insecure: true
      
      # Prometheuså¯¼å‡º
      prometheusremotewrite:
        endpoint: http://prometheus.observability.svc.cluster.local:9090/api/v1/write
        tls:
          insecure: true
      
      # Lokiå¯¼å‡º
      loki:
        endpoint: http://loki.observability.svc.cluster.local:3100/loki/api/v1/push
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, resourcedetection/env, probabilistic_sampler, batch]
          exporters: [logging, otlp/jaeger]
        
        metrics:
          receivers: [otlp, k8s_cluster, kubeletstats]
          processors: [memory_limiter, k8sattributes, resourcedetection/env, batch]
          exporters: [logging, prometheusremotewrite]
        
        logs:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, resourcedetection/env, batch]
          exporters: [logging, loki]
      
      telemetry:
        logs:
          level: info
        metrics:
          address: :8888
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
spec:
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      serviceAccountName: otel-collector
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-k8s:0.95.0
        command:
          - "/otelcol-k8s"
          - "--config=/conf/otel-collector-config.yaml"
          ports:
        - name: otlp-grpc
          containerPort: 4317
          protocol: TCP
        - name: otlp-http
          containerPort: 4318
          protocol: TCP
        - name: metrics
          containerPort: 8888
          protocol: TCP
        env:
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: K8S_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
          volumeMounts:
        - name: otel-collector-config-vol
          mountPath: /conf
        resources:
          limits:
            cpu: 500m
            memory: 1Gi
          requests:
            cpu: 200m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 10
          periodSeconds: 5
      volumes:
      - name: otel-collector-config-vol
          configMap:
          name: otel-collector-config
          items:
          - key: otel-collector-config.yaml
            path: otel-collector-config.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
spec:
  type: ClusterIP
  selector:
    app: otel-collector
  ports:
  - name: otlp-grpc
      port: 4317
      targetPort: 4317
    protocol: TCP
  - name: otlp-http
      port: 4318
      targetPort: 4318
    protocol: TCP
  - name: metrics
    port: 8888
    targetPort: 8888
    protocol: TCP
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: observability
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - nodes/stats
  - services
  - endpoints
  - pods
  - events
  - namespaces
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources:
  - replicasets
  - deployments
  - daemonsets
  - statefulsets
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch"]
  resources:
  - jobs
  - cronjobs
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector
subjects:
- kind: ServiceAccount
  name: otel-collector
  namespace: observability
```

**åº”ç”¨é…ç½®**:

```bash
kubectl apply -f otel-collector.yaml

# éªŒè¯éƒ¨ç½²
kubectl get pods -n observability
kubectl logs -n observability -l app=otel-collector --tail=50
```

### 2. å¯ç”¨Istioæ³¨å…¥

```bash
# ä¸ºdefaultå‘½åç©ºé—´å¯ç”¨Istio sidecaræ³¨å…¥
kubectl label namespace default istio-injection=enabled

# éªŒè¯
kubectl get namespace default --show-labels
```

### 3. éƒ¨ç½²ç¤ºä¾‹åº”ç”¨

```yaml
# demo-app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-demo
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: otlp-demo
      version: v1
  template:
    metadata:
      labels:
        app: otlp-demo
        version: v1
    spec:
      containers:
        - name: app
          image: your-registry/otlp-demo:latest
        ports:
        - containerPort: 8080
          env:
            - name: OTLP_ENDPOINT
          value: "http://otel-collector.observability.svc.cluster.local:4318"
            - name: OTLP_PROTOCOL
          value: "http"
        - name: SERVICE_NAME
          value: "otlp-demo"
        - name: SERVICE_VERSION
          value: "v1"
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://otel-collector.observability.svc.cluster.local:4318"
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: "service.name=otlp-demo,service.version=v1,deployment.environment=production"
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi
---
apiVersion: v1
kind: Service
metadata:
  name: otlp-demo
  namespace: default
spec:
  selector:
    app: otlp-demo
  ports:
  - port: 8080
    targetPort: 8080
    name: http
```

**åº”ç”¨é…ç½®**:

```bash
kubectl apply -f demo-app.yaml

# éªŒè¯Istio sidecaræ³¨å…¥
kubectl get pods -l app=otlp-demo
# åº”è¯¥çœ‹åˆ°2/2 READYï¼ˆapp + envoyï¼‰

# æ£€æŸ¥sidecar
kubectl describe pod -l app=otlp-demo | grep -A 5 "istio-proxy"
```

### 4. éªŒè¯æ•°æ®æµ

```bash
# 1. æ£€æŸ¥Collectoræ˜¯å¦æ¥æ”¶æ•°æ®
kubectl logs -n observability -l app=otel-collector --tail=100 | grep "TracesExporter"

# 2. ç”Ÿæˆæµ‹è¯•æµé‡
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl -X POST http://otlp-demo.default.svc.cluster.local:8080/api/test

# 3. æ£€æŸ¥Jaeger UI
kubectl port-forward -n observability svc/jaeger-query 16686:16686

# 4. æ£€æŸ¥PrometheusæŒ‡æ ‡
kubectl port-forward -n observability svc/prometheus 9090:9090

# è®¿é—® http://localhost:16686 æŸ¥çœ‹traces
# è®¿é—® http://localhost:9090 æŸ¥è¯¢metrics
```

---

## ğŸ“¦ ç”Ÿäº§çº§éƒ¨ç½²

### Helm Chartéƒ¨ç½²

**Chartç»“æ„**:

```text
otlp-chart/
â”œâ”€â”€ Chart.yaml
â”œâ”€â”€ values.yaml
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ collector-daemonset.yaml
â”‚   â”œâ”€â”€ collector-service.yaml
â”‚   â”œâ”€â”€ collector-configmap.yaml
â”‚   â”œâ”€â”€ collector-serviceaccount.yaml
â”‚   â”œâ”€â”€ collector-clusterrole.yaml
â”‚   â”œâ”€â”€ collector-clusterrolebinding.yaml
â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â”œâ”€â”€ pdb.yaml
â”‚   â”œâ”€â”€ servicemonitor.yaml
â”‚   â””â”€â”€ NOTES.txt
â””â”€â”€ README.md
```

**å®‰è£…å‘½ä»¤**:

```bash
# æ·»åŠ Helm repo
helm repo add opentelemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm repo update

# å®‰è£…Collector
helm install otel-collector opentelemetry/opentelemetry-collector \
  --namespace observability \
  --create-namespace \
  --values values-production.yaml

# æˆ–ä½¿ç”¨æœ¬åœ°chart
helm install otel-collector ./otlp-chart \
  --namespace observability \
  --create-namespace \
  --values values-production.yaml
```

### é«˜å¯ç”¨é…ç½®

```yaml
# values-production.yaml
replicaCount: 3

mode: deployment  # ä½¿ç”¨Deploymentè€ŒéDaemonSet

podDisruptionBudget:
  enabled: true
  minAvailable: 2

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - otel-collector
        topologyKey: kubernetes.io/hostname

resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

persistence:
  enabled: true
  storageClass: "fast-ssd"
  size: 10Gi
```

### HPAè‡ªåŠ¨ä¼¸ç¼©

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: otel-collector-hpa
  namespace: observability
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otel-collector
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: otelcol_receiver_accepted_spans
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 15
      selectPolicy: Max
```

---

## ğŸŒ Istio/Envoyé›†æˆ

### TraceContextä¼ æ’­

**Envoyé…ç½®ï¼ˆé€šè¿‡Istioæ³¨å…¥ï¼‰**:

```yaml
# istio-telemetry.yaml
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: mesh-default
  namespace: istio-system
spec:
  tracing:
  - providers:
    - name: otlp
    randomSamplingPercentage: 10.0
    customTags:
      environment:
        literal:
          value: production
      version:
        header:
          name: x-app-version
          defaultValue: unknown
---
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-install
  namespace: istio-system
spec:
  meshConfig:
    defaultConfig:
      tracing:
        tlsSettings:
          mode: DISABLE
        zipkin:
          address: otel-collector.observability.svc.cluster.local:9411
        sampling: 10.0
        custom_tags:
          environment:
            literal:
              value: production
    enableTracing: true
    extensionProviders:
    - name: otlp
      envoyOtelAls:
        service: otel-collector.observability.svc.cluster.local
        port: 4317
```

**åº”ç”¨é…ç½®**:

```bash
kubectl apply -f istio-telemetry.yaml

# é‡å¯Envoy sidecarsä»¥åº”ç”¨æ–°é…ç½®
kubectl rollout restart deployment -n default
```

### VirtualServiceé…ç½®

```yaml
# virtualservice.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: otel-collector
  namespace: observability
spec:
  hosts:
  - otel-collector.observability.svc.cluster.local
  http:
  - match:
    - uri:
        prefix: /v1/traces
    route:
    - destination:
        host: otel-collector.observability.svc.cluster.local
        port:
          number: 4318
      weight: 100
    timeout: 10s
    retries:
      attempts: 3
      perTryTimeout: 2s
      retryOn: 5xx,reset,connect-failure,refused-stream
  - match:
    - uri:
        prefix: /v1/metrics
    route:
    - destination:
        host: otel-collector.observability.svc.cluster.local
        port:
          number: 4318
      weight: 100
    timeout: 5s
  - match:
    - uri:
        prefix: /v1/logs
    route:
    - destination:
        host: otel-collector.observability.svc.cluster.local
        port:
          number: 4318
      weight: 100
    timeout: 5s
  tcp:
  - match:
    - port: 4317
    route:
    - destination:
        host: otel-collector.observability.svc.cluster.local
        port:
          number: 4317
```

### PeerAuthentication (mTLS)

```yaml
# peerauthentication.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: observability
spec:
  mtls:
    mode: STRICT
---
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: otel-collector-mtls
  namespace: observability
spec:
  selector:
    matchLabels:
      app: otel-collector
  mtls:
    mode: PERMISSIVE  # å…è®¸mTLSå’Œplaintext
  portLevelMtls:
    4317:
      mode: PERMISSIVE  # gRPC port
    4318:
      mode: PERMISSIVE  # HTTP port
```

### DestinationRuleé…ç½®

```yaml
# destinationrule.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: otel-collector
  namespace: observability
spec:
  host: otel-collector.observability.svc.cluster.local
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
    connectionPool:
      tcp:
        maxConnections: 100
        connectTimeout: 3s
      http:
        http1MaxPendingRequests: 1024
        http2MaxRequests: 1024
        maxRequestsPerConnection: 100
        maxRetries: 3
    loadBalancer:
      simple: LEAST_REQUEST
      localityLbSetting:
        enabled: true
        distribute:
        - from: us-west-1
          to:
            us-west-1: 80
            us-east-1: 20
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minHealthPercent: 50
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

### Envoy Access Log

```yaml
# envoy-filter.yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: otel-access-log
  namespace: istio-system
spec:
  configPatches:
  - applyTo: NETWORK_FILTER
    match:
      context: ANY
      listener:
        filterChain:
          filter:
            name: envoy.filters.network.http_connection_manager
    patch:
      operation: MERGE
      value:
        typed_config:
          "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
          access_log:
          - name: envoy.access_loggers.open_telemetry
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.access_loggers.open_telemetry.v3.OpenTelemetryAccessLogConfig
              common_config:
                log_name: otel_envoy_accesslog
                transport_api_version: V3
                grpc_service:
                  envoy_grpc:
                    cluster_name: outbound|4317||otel-collector.observability.svc.cluster.local
              body:
                string_value: |
                  {
                    "start_time": "%START_TIME%",
                    "method": "%REQ(:METHOD)%",
                    "path": "%REQ(X-ENVOY-ORIGINAL-PATH?:PATH)%",
                    "protocol": "%PROTOCOL%",
                    "response_code": "%RESPONSE_CODE%",
                    "bytes_sent": "%BYTES_SENT%",
                    "duration": "%DURATION%",
                    "trace_id": "%REQ(X-B3-TRACEID)%",
                    "span_id": "%REQ(X-B3-SPANID)%"
                  }
```

---

## ğŸ”’ å®‰å…¨åŠ å›º

### mTLSé…ç½®

```yaml
# mtls-strict.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: strict-mtls
  namespace: observability
spec:
  mtls:
    mode: STRICT
```

### è®¤è¯ç­–ç•¥

```yaml
# requestauthentication.yaml
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-auth
  namespace: observability
spec:
  selector:
    matchLabels:
      app: otel-collector
  jwtRules:
  - issuer: "https://your-issuer.com"
    jwksUri: "https://your-issuer.com/.well-known/jwks.json"
    audiences:
    - "otel-collector"
    forwardOriginalToken: true
```

### æˆæƒç­–ç•¥

```yaml
# authorizationpolicy.yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: otel-collector-authz
  namespace: observability
spec:
  selector:
    matchLabels:
      app: otel-collector
  action: ALLOW
  rules:
  # å…è®¸æ¥è‡ªåŒä¸€å‘½åç©ºé—´çš„è¯·æ±‚
  - from:
    - source:
        namespaces: ["observability"]
    to:
    - operation:
        ports: ["4317", "4318"]
  # å…è®¸æ¥è‡ªdefaultå‘½åç©ºé—´çš„åº”ç”¨
  - from:
    - source:
        namespaces: ["default"]
        principals: ["cluster.local/ns/default/sa/otlp-demo"]
    to:
    - operation:
        methods: ["POST"]
        paths: ["/v1/traces", "/v1/metrics", "/v1/logs"]
```

### ç½‘ç»œç­–ç•¥

```yaml
# networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: otel-collector-netpol
  namespace: observability
spec:
  podSelector:
    matchLabels:
      app: otel-collector
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # å…è®¸æ¥è‡ªåº”ç”¨çš„OTLPæµé‡
  - from:
    - namespaceSelector:
        matchLabels:
          name: default
    ports:
    - protocol: TCP
      port: 4317
    - protocol: TCP
      port: 4318
  # å…è®¸Prometheus scrape
  - from:
    - namespaceSelector:
        matchLabels:
          name: observability
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 8888
  egress:
  # å…è®¸è®¿é—®Jaeger
  - to:
    - podSelector:
        matchLabels:
          app: jaeger
    ports:
    - protocol: TCP
      port: 4317
  # å…è®¸è®¿é—®Prometheus
  - to:
    - podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 9090
  # å…è®¸DNSæŸ¥è¯¢
  - to:
    - namespaceSelector: {}
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
```

---

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦

### Prometheusç›‘æ§

```yaml
# servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector
  namespace: observability
spec:
  selector:
    matchLabels:
      app: otel-collector
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

**å…³é”®æŒ‡æ ‡**:

```promql
# Collectoræ¥æ”¶é€Ÿç‡
rate(otelcol_receiver_accepted_spans[5m])

# Collectorå¯¼å‡ºé€Ÿç‡
rate(otelcol_exporter_sent_spans[5m])

# å¤±è´¥ç‡
rate(otelcol_exporter_send_failed_spans[5m]) / rate(otelcol_exporter_sent_spans[5m])

# é˜Ÿåˆ—å¤§å°
otelcol_exporter_queue_size

# å†…å­˜ä½¿ç”¨
process_resident_memory_bytes{job="otel-collector"}

# CPUä½¿ç”¨
rate(process_cpu_seconds_total{job="otel-collector"}[5m])
```

### Grafanaä»ªè¡¨æ¿

**å¯¼å…¥ä»ªè¡¨æ¿**:

```bash
# å®˜æ–¹ä»ªè¡¨æ¿ID: 15983
# æˆ–ä½¿ç”¨è‡ªå®šä¹‰ä»ªè¡¨æ¿
kubectl apply -f dashboards/otel-collector-dashboard.json
```

### å‘Šè­¦è§„åˆ™

```yaml
# prometheusrule.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: otel-collector-alerts
  namespace: observability
spec:
  groups:
  - name: otel-collector
    interval: 30s
    rules:
    - alert: OTLPCollectorDown
      expr: up{job="otel-collector"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "OTLP Collectorå®ä¾‹å®•æœº"
        description: "Collector {{ $labels.instance }} å·²å®•æœºè¶…è¿‡5åˆ†é’Ÿ"
    
    - alert: OTLPCollectorHighErrorRate
      expr: |
        rate(otelcol_exporter_send_failed_spans[5m]) /
        rate(otelcol_exporter_sent_spans[5m]) > 0.05
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "OTLP Collectoré”™è¯¯ç‡è¿‡é«˜"
        description: "é”™è¯¯ç‡: {{ $value | humanizePercentage }}"
    
    - alert: OTLPCollectorHighMemory
      expr: |
        process_resident_memory_bytes{job="otel-collector"} /
        on(pod) container_spec_memory_limit_bytes{container="otel-collector"} > 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "OTLP Collectorå†…å­˜ä½¿ç”¨è¿‡é«˜"
        description: "å†…å­˜ä½¿ç”¨: {{ $value | humanizePercentage }}"
    
    - alert: OTLPCollectorQueueBacklog
      expr: otelcol_exporter_queue_size > 1000
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "OTLP Collectoré˜Ÿåˆ—ç§¯å‹"
        description: "é˜Ÿåˆ—å¤§å°: {{ $value }}"
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### é—®é¢˜1: Collectoræ— æ³•æ¥æ”¶æ•°æ®

**ç—‡çŠ¶**:

- åº”ç”¨æ—¥å¿—æ˜¾ç¤ºOTLPå‘é€å¤±è´¥
- Collectoræ—¥å¿—æ— incoming traces

**è¯Šæ–­**:

```bash
# 1. æ£€æŸ¥Collectoræ—¥å¿—
kubectl logs -n observability -l app=otel-collector --tail=100

# 2. æ£€æŸ¥Serviceæ˜¯å¦æ­£å¸¸
kubectl get svc -n observability otel-collector
kubectl get endpoints -n observability otel-collector

# 3. æµ‹è¯•è¿é€šæ€§
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl -v http://otel-collector.observability.svc.cluster.local:4318/v1/traces

# 4. æ£€æŸ¥NetworkPolicy
kubectl get networkpolicy -n observability
```

**è§£å†³æ–¹æ¡ˆ**:

- ç¡®è®¤ç«¯å£é…ç½®æ­£ç¡®ï¼ˆ4317 gRPC, 4318 HTTPï¼‰
- æ£€æŸ¥NetworkPolicyæ˜¯å¦é˜»æ­¢æµé‡
- éªŒè¯DNSè§£ææ­£å¸¸
- æ£€æŸ¥Istio sidecaræ˜¯å¦æ­£ç¡®æ³¨å…¥

#### é—®é¢˜2: mTLSè®¤è¯å¤±è´¥

**ç—‡çŠ¶**:

- 503 Service Unavailable
- Envoyæ—¥å¿—æ˜¾ç¤ºTLS handshake failed

**è¯Šæ–­**:

```bash
# æ£€æŸ¥mTLSçŠ¶æ€
istioctl x describe pod <pod-name> -n <namespace>

# æ£€æŸ¥PeerAuthentication
kubectl get peerauthentication -A

# æ£€æŸ¥è¯ä¹¦
kubectl exec -it <pod-name> -c istio-proxy -n <namespace> -- \
  cat /etc/certs/cert-chain.pem | openssl x509 -text -noout
```

**è§£å†³æ–¹æ¡ˆ**:

- ç¡®è®¤PeerAuthenticationé…ç½®ä¸ºPERMISSIVEæˆ–STRICT
- æ£€æŸ¥è¯ä¹¦æœ‰æ•ˆæœŸ
- éªŒè¯Istioæ§åˆ¶å¹³é¢æ­£å¸¸
- ç¡®è®¤æ‰€æœ‰podéƒ½æ³¨å…¥äº†sidecar

### è¯Šæ–­å·¥å…·

```bash
# 1. Istioctlåˆ†æ
istioctl analyze -n observability

# 2. Envoyé…ç½®dump
kubectl exec -it <pod-name> -c istio-proxy -n <namespace> -- \
  curl localhost:15000/config_dump > envoy-config.json

# 3. æŸ¥çœ‹Envoyè®¿é—®æ—¥å¿—
kubectl logs <pod-name> -c istio-proxy -n <namespace> --tail=100

# 4. å®æ—¶ç›‘æ§CollectoræŒ‡æ ‡
kubectl port-forward -n observability svc/otel-collector 8888:8888
# è®¿é—® http://localhost:8888/metrics

# 5. æµ‹è¯•gRPCè¿æ¥
grpcurl -plaintext \
  otel-collector.observability.svc.cluster.local:4317 \
  list
```

### æ—¥å¿—åˆ†æ

```bash
# æŸ¥æ‰¾é”™è¯¯
kubectl logs -n observability -l app=otel-collector \
  --tail=1000 | grep -i error

# åˆ†ææ‹’ç»åŸå› 
kubectl logs -n observability -l app=otel-collector \
  --tail=1000 | grep "refused\|rejected\|denied"

# æŸ¥çœ‹é‡‡æ ·æƒ…å†µ
kubectl logs -n observability -l app=otel-collector \
  --tail=1000 | grep "sampled"

# åˆ†ææ€§èƒ½
kubectl logs -n observability -l app=otel-collector \
  --tail=1000 | grep "duration\|latency"
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### Collectorä¼˜åŒ–

```yaml
# ä¼˜åŒ–çš„Collectoré…ç½®
processors:
  batch:
    timeout: 5s                    # é™ä½å»¶è¿Ÿ
    send_batch_size: 2048          # å¢åŠ æ‰¹å¤§å°
    send_batch_max_size: 4096
  
  memory_limiter:
    check_interval: 1s
    limit_mib: 2048                # å¢åŠ å†…å­˜é™åˆ¶
    spike_limit_mib: 512
  
  # ä»…å¤„ç†å¿…è¦çš„å±æ€§
  resource:
    attributes:
    - key: unnecessary_attribute
      action: delete

resources:
  limits:
    cpu: 2000m                     # å¢åŠ CPU
    memory: 4Gi                    # å¢åŠ å†…å­˜
  requests:
    cpu: 1000m
    memory: 2Gi
```

### Envoyä¼˜åŒ–

```yaml
# Envoyèµ„æºä¼˜åŒ–
apiVersion: v1
kind: ConfigMap
metadata:
  name: istio-sidecar-injector
  namespace: istio-system
data:
  values: |
    global:
      proxy:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 2000m
            memory: 1024Mi
        concurrency: 2             # CPUæ ¸å¿ƒæ•°
```

### ç½‘ç»œä¼˜åŒ–

```yaml
# ä½¿ç”¨NodePortæˆ–LoadBalanceræš´éœ²Collector
apiVersion: v1
kind: Service
metadata:
  name: otel-collector-external
  namespace: observability
spec:
  type: LoadBalancer
  selector:
    app: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
    protocol: TCP
  - name: otlp-http
    port: 4318
    targetPort: 4318
    protocol: TCP
  externalTrafficPolicy: Local     # ä¿ç•™æºIP
  sessionAffinity: ClientIP        # ä¼šè¯äº²å’Œæ€§
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
```

---

## ğŸ“š å®Œæ•´é…ç½®ç¤ºä¾‹

### åŸºç¡€éƒ¨ç½²

å‚è§ä¸Šæ–‡"å¿«é€Ÿå¼€å§‹"éƒ¨åˆ†çš„å®Œæ•´YAMLé…ç½®ã€‚

### Helm Values

```yaml
# values-production.yaml
mode: daemonset

image:
  repository: otel/opentelemetry-collector-k8s
  tag: 0.95.0
  pullPolicy: IfNotPresent

replicaCount: 1

resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8888"
  prometheus.io/path: "/metrics"

service:
  type: ClusterIP
  ports:
    otlp-grpc:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
    otlp-http:
      enabled: true
      containerPort: 4318
      servicePort: 4318
      protocol: TCP
    metrics:
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP

ingress:
  enabled: false

podDisruptionBudget:
  enabled: true
  minAvailable: 1

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70

serviceMonitor:
  enabled: true
  interval: 30s

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
  
  processors:
    batch:
      timeout: 10s
      send_batch_size: 1024
    memory_limiter:
      check_interval: 1s
      limit_mib: 1536
      spike_limit_mib: 512
  
  exporters:
    logging:
      loglevel: info
    otlp/jaeger:
      endpoint: jaeger-collector:4317
      tls:
        insecure: true
  
  service:
    pipelines:
      traces:
        receivers: [otlp]
        processors: [memory_limiter, batch]
        exporters: [logging, otlp/jaeger]
```

---

## ğŸ§ª æµ‹è¯•éªŒè¯

### åŠŸèƒ½æµ‹è¯•

```bash
#!/bin/bash
# test-otlp-k8s.sh

set -e

echo "=== OTLP K8såŠŸèƒ½æµ‹è¯• ==="

# 1. æµ‹è¯•gRPCç«¯ç‚¹
echo "æµ‹è¯•gRPCç«¯ç‚¹..."
grpcurl -plaintext \
  otel-collector.observability.svc.cluster.local:4317 \
  list

# 2. æµ‹è¯•HTTPç«¯ç‚¹
echo "æµ‹è¯•HTTPç«¯ç‚¹..."
curl -v -X POST \
  http://otel-collector.observability.svc.cluster.local:4318/v1/traces \
  -H "Content-Type: application/json" \
  -d '{
    "resourceSpans": [
      {
        "resource": {
          "attributes": [
            {"key": "service.name", "value": {"stringValue": "test-service"}}
          ]
        },
        "scopeSpans": [
          {
            "spans": [
              {
                "traceId": "5B8EFFF798038103D269B633813FC60C",
                "spanId": "EEE19B7EC3C1B174",
                "name": "test-span",
                "startTimeUnixNano": "1544712660000000000",
                "endTimeUnixNano": "1544712661000000000"
              }
            ]
          }
        ]
      }
    ]
  }'

# 3. æ£€æŸ¥Collectoræ—¥å¿—
echo "æ£€æŸ¥Collectoræ—¥å¿—..."
kubectl logs -n observability -l app=otel-collector --tail=50 | grep "test-service"

echo "âœ… åŠŸèƒ½æµ‹è¯•å®Œæˆ"
```

### æ€§èƒ½æµ‹è¯•

```bash
#!/bin/bash
# benchmark-otlp.sh

echo "=== OTLPæ€§èƒ½åŸºå‡†æµ‹è¯• ==="

# ä½¿ç”¨k6è¿›è¡Œè´Ÿè½½æµ‹è¯•
kubectl run k6 --image=grafana/k6 --rm -it --restart=Never -- \
  run - <<EOF
import http from 'k6/http';
import { check } from 'k6';

export let options = {
  stages: [
    { duration: '1m', target: 100 },
    { duration: '3m', target: 100 },
    { duration: '1m', target: 0 },
  ],
};

export default function() {
  let url = 'http://otel-collector.observability.svc.cluster.local:4318/v1/traces';
  let payload = JSON.stringify({
    resourceSpans: [{
      resource: {
        attributes: [
          {key: "service.name", value: {stringValue: "load-test"}}
        ]
      },
      scopeSpans: [{
        spans: [{
          traceId: "5B8EFFF798038103D269B633813FC60C",
          spanId: "EEE19B7EC3C1B174",
          name: "test-span",
          startTimeUnixNano: "1544712660000000000",
          endTimeUnixNano: "1544712661000000000"
        }]
      }]
    }]
  });
  
  let res = http.post(url, payload, {
    headers: { 'Content-Type': 'application/json' },
  });
  
  check(res, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
}
EOF
```

### æ··æ²Œæµ‹è¯•

```bash
# ä½¿ç”¨Chaos Meshæµ‹è¯•æ•…éšœæ¢å¤
kubectl apply -f - <<EOF
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: otel-collector-pod-failure
  namespace: observability
spec:
  action: pod-failure
  mode: one
  duration: "30s"
  selector:
    namespaces:
      - observability
    labelSelectors:
      app: otel-collector
  scheduler:
    cron: "@every 5m"
EOF

# ç›‘æ§æ¢å¤è¿‡ç¨‹
kubectl get pods -n observability -w
```

---

## ğŸ“ è¿ç»´æ‰‹å†Œ

### æ—¥å¸¸è¿ç»´

**æ¯æ—¥æ£€æŸ¥**:

```bash
#!/bin/bash
# daily-check.sh

echo "=== OTLPæ¯æ—¥æ£€æŸ¥ ==="

# 1. æ£€æŸ¥PodçŠ¶æ€
echo "1. PodçŠ¶æ€:"
kubectl get pods -n observability -l app=otel-collector

# 2. æ£€æŸ¥é”™è¯¯ç‡
echo "2. é”™è¯¯ç‡:"
kubectl logs -n observability -l app=otel-collector --since=24h | \
  grep -c "error" || echo "0"

# 3. æ£€æŸ¥èµ„æºä½¿ç”¨
echo "3. èµ„æºä½¿ç”¨:"
kubectl top pods -n observability -l app=otel-collector

# 4. æ£€æŸ¥HPAçŠ¶æ€
echo "4. HPAçŠ¶æ€:"
kubectl get hpa -n observability

# 5. æ£€æŸ¥å‘Šè­¦
echo "5. æ´»è·ƒå‘Šè­¦:"
curl -s http://prometheus.observability:9090/api/v1/alerts | \
  jq '.data.alerts[] | select(.state=="firing") | .labels.alertname'

echo "âœ… æ¯æ—¥æ£€æŸ¥å®Œæˆ"
```

### æ•…éšœæ¢å¤

**Collectoré‡å¯**:

```bash
# ä¼˜é›…é‡å¯
kubectl rollout restart daemonset/otel-collector -n observability

# å¼ºåˆ¶é‡å¯
kubectl delete pods -n observability -l app=otel-collector

# éªŒè¯æ¢å¤
kubectl rollout status daemonset/otel-collector -n observability
```

**é…ç½®å›æ»š**:

```bash
# æŸ¥çœ‹å†å²ç‰ˆæœ¬
kubectl rollout history daemonset/otel-collector -n observability

# å›æ»šåˆ°ä¸Šä¸€ç‰ˆæœ¬
kubectl rollout undo daemonset/otel-collector -n observability

# å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬
kubectl rollout undo daemonset/otel-collector -n observability --to-revision=3
```

### å‡çº§æµç¨‹

```bash
#!/bin/bash
# upgrade-collector.sh

set -e

VERSION="0.96.0"

echo "=== å‡çº§OTLP Collectoråˆ° $VERSION ==="

# 1. å¤‡ä»½å½“å‰é…ç½®
kubectl get configmap -n observability otel-collector-config -o yaml > \
  backup-config-$(date +%Y%m%d).yaml

# 2. æ›´æ–°é•œåƒ
kubectl set image daemonset/otel-collector -n observability \
  otel-collector=otel/opentelemetry-collector-k8s:$VERSION

# 3. ç›‘æ§æ»šåŠ¨æ›´æ–°
kubectl rollout status daemonset/otel-collector -n observability

# 4. éªŒè¯æ–°ç‰ˆæœ¬
kubectl get pods -n observability -l app=otel-collector -o \
  jsonpath='{.items[0].spec.containers[0].image}'

# 5. è¿è¡Œå¥åº·æ£€æŸ¥
./daily-check.sh

echo "âœ… å‡çº§å®Œæˆ"
```

---

## ğŸ“š ç›¸å…³èµ„æº

**å®˜æ–¹æ–‡æ¡£**:

- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)
- [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/)
- [Istio Documentation](https://istio.io/latest/docs/)
- [Envoy Proxy Documentation](https://www.envoyproxy.io/docs/envoy/latest/)

**Helm Charts**:

- [OpenTelemetry Helm Charts](https://github.com/open-telemetry/opentelemetry-helm-charts)
- [Jaeger Helm Chart](https://github.com/jaegertracing/helm-charts)
- [Prometheus Helm Chart](https://github.com/prometheus-community/helm-charts)

**ç¤ºä¾‹å’Œæ•™ç¨‹**:

- [OpenTelemetry Demo](https://github.com/open-telemetry/opentelemetry-demo)
- [Istio Telemetry](https://istio.io/latest/docs/tasks/observability/)

**ç¤¾åŒºæ”¯æŒ**:

- [CNCF Slack - OpenTelemetry](https://cloud-native.slack.com/archives/C01N3AT62SJ)
- [Istio Discuss](https://discuss.istio.io/)

---

## ğŸ“… å˜æ›´å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | å˜æ›´å†…å®¹ | ä½œè€… |
|------|------|---------|------|
| v2.0 | 2025-10-17 | å®Œæ•´ç”Ÿäº§çº§æŒ‡å—ï¼šæ¶æ„ã€éƒ¨ç½²ã€ç›‘æ§ã€æ•…éšœæ’æŸ¥ | OTLPå›¢é˜Ÿ |
| v1.0 | 2025-01-20 | åˆå§‹ç‰ˆæœ¬ï¼šåŸºç¡€æŒ‡å¼• | OTLPå›¢é˜Ÿ |

---

**æ–‡æ¡£çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª  
**å®Œæˆæ—¶é—´**: 2025å¹´10æœˆ17æ—¥  
**ç»´æŠ¤å›¢é˜Ÿ**: OTLP Cloud Nativeå›¢é˜Ÿ
