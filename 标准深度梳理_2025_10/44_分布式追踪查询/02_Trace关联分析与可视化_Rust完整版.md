# Trace å…³è”åˆ†æä¸å¯è§†åŒ– - Rust å®Œæ•´å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **Rust ç‰ˆæœ¬**: 1.90+  
> **æœ€åæ›´æ–°**: 2025-10-10

---

## ğŸ“‹ ç›®å½•

- [Trace å…³è”åˆ†æä¸å¯è§†åŒ– - Rust å®Œæ•´å®ç°](#trace-å…³è”åˆ†æä¸å¯è§†åŒ–---rust-å®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
    - [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
    - [åº”ç”¨åœºæ™¯](#åº”ç”¨åœºæ™¯)
  - [ğŸ”— Trace å…³è”åˆ†æ](#-trace-å…³è”åˆ†æ)
    - [1. è°ƒç”¨é“¾è·¯åˆ†æ](#1-è°ƒç”¨é“¾è·¯åˆ†æ)
    - [2. å…³é”®è·¯å¾„åˆ†æ](#2-å…³é”®è·¯å¾„åˆ†æ)
    - [3. ç“¶é¢ˆè¯†åˆ«](#3-ç“¶é¢ˆè¯†åˆ«)
  - [ğŸ“Š æœåŠ¡ä¾èµ–åˆ†æ](#-æœåŠ¡ä¾èµ–åˆ†æ)
    - [1. æœåŠ¡æ‹“æ‰‘å›¾](#1-æœåŠ¡æ‹“æ‰‘å›¾)
    - [2. ä¾èµ–æ·±åº¦åˆ†æ](#2-ä¾èµ–æ·±åº¦åˆ†æ)
    - [3. å¾ªç¯ä¾èµ–æ£€æµ‹](#3-å¾ªç¯ä¾èµ–æ£€æµ‹)
  - [âš¡ æ€§èƒ½åˆ†æ](#-æ€§èƒ½åˆ†æ)
    - [1. è€—æ—¶åˆ†å¸ƒåˆ†æ](#1-è€—æ—¶åˆ†å¸ƒåˆ†æ)
    - [2. P95/P99 åˆ†æ](#2-p95p99-åˆ†æ)
    - [3. å¼‚å¸¸æ£€æµ‹](#3-å¼‚å¸¸æ£€æµ‹)
  - [ğŸ¨ Trace å¯è§†åŒ–](#-trace-å¯è§†åŒ–)
    - [1. ç€‘å¸ƒå›¾ç”Ÿæˆ](#1-ç€‘å¸ƒå›¾ç”Ÿæˆ)
    - [2. ç«ç„°å›¾ç”Ÿæˆ](#2-ç«ç„°å›¾ç”Ÿæˆ)
    - [3. Gantt å›¾ç”Ÿæˆ](#3-gantt-å›¾ç”Ÿæˆ)
  - [ğŸ“ˆ èšåˆåˆ†æ](#-èšåˆåˆ†æ)
    - [1. æ—¶é—´çª—å£èšåˆ](#1-æ—¶é—´çª—å£èšåˆ)
    - [2. æœåŠ¡çº§åˆ«èšåˆ](#2-æœåŠ¡çº§åˆ«èšåˆ)
  - [ğŸ” æ ¹å› åˆ†æ](#-æ ¹å› åˆ†æ)
    - [1. é”™è¯¯ä¼ æ’­åˆ†æ](#1-é”™è¯¯ä¼ æ’­åˆ†æ)
    - [2. å¼‚å¸¸æ ¹å› å®šä½](#2-å¼‚å¸¸æ ¹å› å®šä½)
  - [ğŸ“Š å®Œæ•´ç¤ºä¾‹ï¼šTrace åˆ†æå¹³å°](#-å®Œæ•´ç¤ºä¾‹trace-åˆ†æå¹³å°)
  - [ğŸ¯ æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)
    - [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
    - [å¯è§†åŒ–å»ºè®®](#å¯è§†åŒ–å»ºè®®)
    - [åˆ†æç­–ç•¥](#åˆ†æç­–ç•¥)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## ğŸ¯ æ¦‚è¿°

Trace å…³è”åˆ†æä¸å¯è§†åŒ–æ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿçš„é«˜çº§åŠŸèƒ½ï¼Œé€šè¿‡å¤šç»´åº¦åˆ†æå’Œç›´è§‚çš„å¯è§†åŒ–ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆå’Œæ•…éšœæ ¹å› ã€‚

### æ ¸å¿ƒåŠŸèƒ½

- âœ… **è°ƒç”¨é“¾è·¯åˆ†æ** - å®Œæ•´çš„è¯·æ±‚è°ƒç”¨è·¯å¾„
- âœ… **å…³é”®è·¯å¾„è¯†åˆ«** - æ‰¾å‡ºå½±å“æ€§èƒ½çš„å…³é”®èŠ‚ç‚¹
- âœ… **ç“¶é¢ˆæ£€æµ‹** - è‡ªåŠ¨è¯†åˆ«æ…¢æŸ¥è¯¢å’Œèµ„æºç“¶é¢ˆ
- âœ… **æœåŠ¡ä¾èµ–å›¾** - å¯è§†åŒ–æœåŠ¡é—´è°ƒç”¨å…³ç³»
- âœ… **æ€§èƒ½åˆ†å¸ƒåˆ†æ** - ç»Ÿè®¡åˆ†æå’Œå¼‚å¸¸æ£€æµ‹
- âœ… **å¯è§†åŒ–æ¸²æŸ“** - ç€‘å¸ƒå›¾ã€ç«ç„°å›¾ã€Gantt å›¾
- âœ… **æ ¹å› åˆ†æ** - é”™è¯¯ä¼ æ’­é“¾è¿½è¸ª

### åº”ç”¨åœºæ™¯

- ğŸ”§ **æ€§èƒ½è°ƒä¼˜** - è¯†åˆ«æ…¢è¯·æ±‚å’Œæ€§èƒ½çƒ­ç‚¹
- ğŸ”§ **æ•…éšœæ’æŸ¥** - å¿«é€Ÿå®šä½é”™è¯¯æ ¹å› 
- ğŸ”§ **å®¹é‡è§„åˆ’** - åˆ†ææœåŠ¡è´Ÿè½½å’Œä¾èµ–å…³ç³»
- ğŸ”§ **æ¶æ„ä¼˜åŒ–** - å‘ç°å†—ä½™è°ƒç”¨å’Œå¾ªç¯ä¾èµ–

---

## ğŸ”— Trace å…³è”åˆ†æ

### 1. è°ƒç”¨é“¾è·¯åˆ†æ

```rust
use std::sync::Arc;
use std::collections::{HashMap, VecDeque};
use chrono::{DateTime, Utc};

/// è°ƒç”¨é“¾è·¯åˆ†æå™¨
pub struct CallChainAnalyzer {
    trace_reconstructor: Arc<TraceReconstructor>,
}

impl CallChainAnalyzer {
    pub fn new(trace_reconstructor: Arc<TraceReconstructor>) -> Self {
        Self { trace_reconstructor }
    }
    
    /// åˆ†æè°ƒç”¨é“¾è·¯
    pub async fn analyze_call_chain(
        &self,
        trace_id: &TraceId,
    ) -> Result<CallChainAnalysis, AnalysisError> {
        // é‡å»º Trace æ ‘
        let trace_tree = self.trace_reconstructor
            .reconstruct_trace_tree(trace_id)
            .await?;
        
        let mut chains = Vec::new();
        
        // éå†æ¯ä¸ªæ ¹èŠ‚ç‚¹
        for root in &trace_tree.root_nodes {
            let chain = self.extract_call_chain(root, Vec::new());
            chains.push(chain);
        }
        
        // è®¡ç®—æ€»çš„è°ƒç”¨æ·±åº¦
        let max_depth = chains
            .iter()
            .map(|chain| chain.len())
            .max()
            .unwrap_or(0);
        
        // è®¡ç®—è°ƒç”¨æ€»æ•°
        let total_calls = chains
            .iter()
            .map(|chain| chain.len())
            .sum();
        
        Ok(CallChainAnalysis {
            trace_id: *trace_id,
            chains,
            max_depth,
            total_calls,
        })
    }
    
    /// æå–è°ƒç”¨é“¾
    fn extract_call_chain(
        &self,
        node: &TraceTreeNode,
        mut current_chain: Vec<CallChainNode>,
    ) -> Vec<CallChainNode> {
        // æ·»åŠ å½“å‰èŠ‚ç‚¹
        current_chain.push(CallChainNode {
            span_id: node.span.span_id,
            span_name: node.span.name.clone(),
            service_name: node.span.service_name.clone(),
            start_time: node.span.start_time,
            end_time: node.span.end_time,
            duration_ms: node.span.duration_ms,
            depth: current_chain.len(),
        });
        
        // é€’å½’å¤„ç†å­èŠ‚ç‚¹ï¼ˆæ·±åº¦ä¼˜å…ˆï¼‰
        if let Some(child) = node.children.first() {
            self.extract_call_chain(child, current_chain)
        } else {
            current_chain
        }
    }
}

#[derive(Debug, Clone)]
pub struct CallChainAnalysis {
    pub trace_id: TraceId,
    pub chains: Vec<Vec<CallChainNode>>,
    pub max_depth: usize,
    pub total_calls: usize,
}

#[derive(Debug, Clone)]
pub struct CallChainNode {
    pub span_id: SpanId,
    pub span_name: String,
    pub service_name: String,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub duration_ms: i64,
    pub depth: usize,
}

pub type TraceId = [u8; 16];
pub type SpanId = [u8; 8];

#[derive(Debug)]
pub enum AnalysisError {
    TraceNotFound,
    ReconstructionFailed(String),
    AnalysisFailed(String),
}
```

---

### 2. å…³é”®è·¯å¾„åˆ†æ

```rust
/// å…³é”®è·¯å¾„åˆ†æå™¨
pub struct CriticalPathAnalyzer {
    call_chain_analyzer: Arc<CallChainAnalyzer>,
}

impl CriticalPathAnalyzer {
    pub fn new(call_chain_analyzer: Arc<CallChainAnalyzer>) -> Self {
        Self { call_chain_analyzer }
    }
    
    /// è¯†åˆ«å…³é”®è·¯å¾„ï¼ˆæœ€é•¿è€—æ—¶è·¯å¾„ï¼‰
    pub async fn find_critical_path(
        &self,
        trace_id: &TraceId,
    ) -> Result<CriticalPath, AnalysisError> {
        let analysis = self.call_chain_analyzer
            .analyze_call_chain(trace_id)
            .await?;
        
        // æ‰¾å‡ºè€—æ—¶æœ€é•¿çš„é“¾è·¯
        let critical_chain = analysis
            .chains
            .into_iter()
            .max_by_key(|chain| {
                chain.iter().map(|node| node.duration_ms).sum::<i64>()
            })
            .ok_or(AnalysisError::AnalysisFailed("No chains found".to_string()))?;
        
        let total_duration = critical_chain
            .iter()
            .map(|node| node.duration_ms)
            .sum();
        
        // è¯†åˆ«å…³é”®èŠ‚ç‚¹ï¼ˆè€—æ—¶è¶…è¿‡æ€»æ—¶é—´çš„ 20%ï¼‰
        let threshold = (total_duration as f64 * 0.2) as i64;
        
        let critical_nodes: Vec<CallChainNode> = critical_chain
            .iter()
            .filter(|node| node.duration_ms >= threshold)
            .cloned()
            .collect();
        
        Ok(CriticalPath {
            trace_id: *trace_id,
            full_chain: critical_chain,
            critical_nodes,
            total_duration,
        })
    }
    
    /// è®¡ç®—æ¯ä¸ª Span åœ¨å…³é”®è·¯å¾„ä¸­çš„å æ¯”
    pub fn calculate_span_contribution(
        &self,
        critical_path: &CriticalPath,
    ) -> Vec<SpanContribution> {
        let total_duration = critical_path.total_duration as f64;
        
        critical_path
            .full_chain
            .iter()
            .map(|node| {
                let percentage = (node.duration_ms as f64 / total_duration) * 100.0;
                
                SpanContribution {
                    span_id: node.span_id,
                    span_name: node.span_name.clone(),
                    service_name: node.service_name.clone(),
                    duration_ms: node.duration_ms,
                    percentage,
                }
            })
            .collect()
    }
}

#[derive(Debug, Clone)]
pub struct CriticalPath {
    pub trace_id: TraceId,
    pub full_chain: Vec<CallChainNode>,
    pub critical_nodes: Vec<CallChainNode>,
    pub total_duration: i64,
}

#[derive(Debug, Clone)]
pub struct SpanContribution {
    pub span_id: SpanId,
    pub span_name: String,
    pub service_name: String,
    pub duration_ms: i64,
    pub percentage: f64,
}
```

---

### 3. ç“¶é¢ˆè¯†åˆ«

```rust
/// ç“¶é¢ˆè¯†åˆ«å™¨
pub struct BottleneckDetector {
    critical_path_analyzer: Arc<CriticalPathAnalyzer>,
}

impl BottleneckDetector {
    pub fn new(critical_path_analyzer: Arc<CriticalPathAnalyzer>) -> Self {
        Self { critical_path_analyzer }
    }
    
    /// è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
    pub async fn detect_bottlenecks(
        &self,
        trace_id: &TraceId,
        threshold_percentage: f64, // ä¾‹å¦‚ 20.0 è¡¨ç¤ºå æ€»æ—¶é—´çš„ 20%
    ) -> Result<Vec<Bottleneck>, AnalysisError> {
        let critical_path = self.critical_path_analyzer
            .find_critical_path(trace_id)
            .await?;
        
        let contributions = self.critical_path_analyzer
            .calculate_span_contribution(&critical_path);
        
        let bottlenecks: Vec<Bottleneck> = contributions
            .into_iter()
            .filter(|contrib| contrib.percentage >= threshold_percentage)
            .map(|contrib| {
                // è¯†åˆ«ç“¶é¢ˆç±»å‹
                let bottleneck_type = self.classify_bottleneck(&contrib);
                
                Bottleneck {
                    span_id: contrib.span_id,
                    span_name: contrib.span_name,
                    service_name: contrib.service_name,
                    duration_ms: contrib.duration_ms,
                    percentage: contrib.percentage,
                    bottleneck_type,
                }
            })
            .collect();
        
        Ok(bottlenecks)
    }
    
    /// åˆ†ç±»ç“¶é¢ˆç±»å‹
    fn classify_bottleneck(&self, contrib: &SpanContribution) -> BottleneckType {
        // åŸºäº Span åç§°æ¨æ–­ç±»å‹
        let name_lower = contrib.span_name.to_lowercase();
        
        if name_lower.contains("sql") || name_lower.contains("query") || name_lower.contains("database") {
            BottleneckType::Database
        } else if name_lower.contains("http") || name_lower.contains("rpc") || name_lower.contains("grpc") {
            BottleneckType::Network
        } else if name_lower.contains("compute") || name_lower.contains("process") {
            BottleneckType::Computation
        } else if name_lower.contains("lock") || name_lower.contains("wait") {
            BottleneckType::Lock
        } else {
            BottleneckType::Unknown
        }
    }
}

#[derive(Debug, Clone)]
pub struct Bottleneck {
    pub span_id: SpanId,
    pub span_name: String,
    pub service_name: String,
    pub duration_ms: i64,
    pub percentage: f64,
    pub bottleneck_type: BottleneckType,
}

#[derive(Debug, Clone, PartialEq)]
pub enum BottleneckType {
    Database,
    Network,
    Computation,
    Lock,
    Unknown,
}
```

---

## ğŸ“Š æœåŠ¡ä¾èµ–åˆ†æ

### 1. æœåŠ¡æ‹“æ‰‘å›¾

```rust
use std::collections::{HashMap, HashSet};

/// æœåŠ¡æ‹“æ‰‘åˆ†æå™¨
pub struct ServiceTopologyAnalyzer {
    dependency_graph_builder: Arc<DependencyGraphBuilder>,
}

impl ServiceTopologyAnalyzer {
    pub fn new(dependency_graph_builder: Arc<DependencyGraphBuilder>) -> Self {
        Self { dependency_graph_builder }
    }
    
    /// æ„å»ºæœåŠ¡æ‹“æ‰‘å›¾
    pub async fn build_topology(
        &self,
        trace_ids: Vec<TraceId>,
    ) -> Result<ServiceTopology, AnalysisError> {
        let mut aggregated_graph = AggregatedDependencyGraph {
            nodes: HashMap::new(),
            edges: HashMap::new(),
        };
        
        // èšåˆå¤šä¸ª Trace çš„ä¾èµ–å›¾
        for trace_id in trace_ids {
            if let Ok(graph) = self.dependency_graph_builder
                .build_service_dependency_graph(&trace_id)
                .await
            {
                self.merge_graph(&mut aggregated_graph, graph);
            }
        }
        
        // è®¡ç®—æ‹“æ‰‘ç»Ÿè®¡
        let entry_services = self.find_entry_services(&aggregated_graph);
        let leaf_services = self.find_leaf_services(&aggregated_graph);
        
        Ok(ServiceTopology {
            nodes: aggregated_graph.nodes.into_values().collect(),
            edges: aggregated_graph.edges.into_values().collect(),
            entry_services,
            leaf_services,
        })
    }
    
    /// åˆå¹¶ä¾èµ–å›¾
    fn merge_graph(
        &self,
        target: &mut AggregatedDependencyGraph,
        source: ServiceDependencyGraph,
    ) {
        // åˆå¹¶èŠ‚ç‚¹
        for (service_name, node) in source.nodes {
            target
                .nodes
                .entry(service_name.clone())
                .and_modify(|existing| {
                    existing.span_count += node.span_count;
                    existing.total_duration_ms += node.total_duration_ms;
                    existing.trace_count += 1;
                })
                .or_insert(AggregatedServiceNode {
                    service_name: node.service_name,
                    span_count: node.span_count,
                    total_duration_ms: node.total_duration_ms,
                    trace_count: 1,
                });
        }
        
        // åˆå¹¶è¾¹
        for edge in source.edges {
            let edge_key = (edge.from.clone(), edge.to.clone());
            
            target
                .edges
                .entry(edge_key.clone())
                .and_modify(|existing| {
                    existing.call_count += edge.call_count;
                })
                .or_insert(AggregatedDependencyEdge {
                    from: edge.from,
                    to: edge.to,
                    call_count: edge.call_count,
                });
        }
    }
    
    /// æ‰¾å‡ºå…¥å£æœåŠ¡ï¼ˆæ²¡æœ‰å…¥è¾¹çš„æœåŠ¡ï¼‰
    fn find_entry_services(&self, graph: &AggregatedDependencyGraph) -> Vec<String> {
        let mut services_with_incoming: HashSet<String> = graph
            .edges
            .values()
            .map(|edge| edge.to.clone())
            .collect();
        
        graph
            .nodes
            .keys()
            .filter(|service| !services_with_incoming.contains(*service))
            .cloned()
            .collect()
    }
    
    /// æ‰¾å‡ºå¶å­æœåŠ¡ï¼ˆæ²¡æœ‰å‡ºè¾¹çš„æœåŠ¡ï¼‰
    fn find_leaf_services(&self, graph: &AggregatedDependencyGraph) -> Vec<String> {
        let services_with_outgoing: HashSet<String> = graph
            .edges
            .values()
            .map(|edge| edge.from.clone())
            .collect();
        
        graph
            .nodes
            .keys()
            .filter(|service| !services_with_outgoing.contains(*service))
            .cloned()
            .collect()
    }
}

#[derive(Debug, Clone)]
pub struct AggregatedDependencyGraph {
    pub nodes: HashMap<String, AggregatedServiceNode>,
    pub edges: HashMap<(String, String), AggregatedDependencyEdge>,
}

#[derive(Debug, Clone)]
pub struct AggregatedServiceNode {
    pub service_name: String,
    pub span_count: usize,
    pub total_duration_ms: i64,
    pub trace_count: usize,
}

#[derive(Debug, Clone)]
pub struct AggregatedDependencyEdge {
    pub from: String,
    pub to: String,
    pub call_count: usize,
}

#[derive(Debug, Clone)]
pub struct ServiceTopology {
    pub nodes: Vec<AggregatedServiceNode>,
    pub edges: Vec<AggregatedDependencyEdge>,
    pub entry_services: Vec<String>,
    pub leaf_services: Vec<String>,
}
```

---

### 2. ä¾èµ–æ·±åº¦åˆ†æ

```rust
/// ä¾èµ–æ·±åº¦åˆ†æå™¨
pub struct DependencyDepthAnalyzer {
    topology_analyzer: Arc<ServiceTopologyAnalyzer>,
}

impl DependencyDepthAnalyzer {
    pub fn new(topology_analyzer: Arc<ServiceTopologyAnalyzer>) -> Self {
        Self { topology_analyzer }
    }
    
    /// è®¡ç®—æœåŠ¡ä¾èµ–æ·±åº¦
    pub fn calculate_dependency_depth(
        &self,
        topology: &ServiceTopology,
    ) -> HashMap<String, usize> {
        let mut depths = HashMap::new();
        
        // æ„å»ºé‚»æ¥è¡¨
        let mut adjacency: HashMap<String, Vec<String>> = HashMap::new();
        
        for edge in &topology.edges {
            adjacency
                .entry(edge.from.clone())
                .or_insert_with(Vec::new)
                .push(edge.to.clone());
        }
        
        // ä»å…¥å£æœåŠ¡å¼€å§‹ BFS
        for entry_service in &topology.entry_services {
            let mut queue = VecDeque::new();
            queue.push_back((entry_service.clone(), 0));
            
            while let Some((service, depth)) = queue.pop_front() {
                // æ›´æ–°æ·±åº¦ï¼ˆå–æœ€å¤§å€¼ï¼‰
                depths
                    .entry(service.clone())
                    .and_modify(|d| *d = (*d).max(depth))
                    .or_insert(depth);
                
                // å°†å­æœåŠ¡åŠ å…¥é˜Ÿåˆ—
                if let Some(children) = adjacency.get(&service) {
                    for child in children {
                        queue.push_back((child.clone(), depth + 1));
                    }
                }
            }
        }
        
        depths
    }
    
    /// æŸ¥æ‰¾æœ€é•¿ä¾èµ–é“¾
    pub fn find_longest_dependency_chain(
        &self,
        topology: &ServiceTopology,
    ) -> Vec<String> {
        let depths = self.calculate_dependency_depth(topology);
        
        // æ‰¾å‡ºæœ€å¤§æ·±åº¦
        let max_depth = depths.values().max().cloned().unwrap_or(0);
        
        // å›æº¯æ‰¾å‡ºæœ€é•¿é“¾
        let mut chain = Vec::new();
        
        // ä»å…¥å£æœåŠ¡å¼€å§‹
        for entry_service in &topology.entry_services {
            if let Some(&depth) = depths.get(entry_service) {
                if depth == max_depth {
                    chain = self.backtrack_chain(entry_service, topology, &depths);
                    break;
                }
            }
        }
        
        chain
    }
    
    /// å›æº¯æ„å»ºä¾èµ–é“¾
    fn backtrack_chain(
        &self,
        start_service: &str,
        topology: &ServiceTopology,
        depths: &HashMap<String, usize>,
    ) -> Vec<String> {
        let mut chain = vec![start_service.to_string()];
        let mut current_service = start_service.to_string();
        let mut current_depth = *depths.get(start_service).unwrap_or(&0);
        
        loop {
            // æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ·±åº¦çš„å­æœåŠ¡
            let next_service = topology
                .edges
                .iter()
                .filter(|edge| edge.from == current_service)
                .find_map(|edge| {
                    let child_depth = *depths.get(&edge.to)?;
                    
                    if child_depth == current_depth + 1 {
                        Some(edge.to.clone())
                    } else {
                        None
                    }
                });
            
            match next_service {
                Some(next) => {
                    chain.push(next.clone());
                    current_service = next;
                    current_depth += 1;
                }
                None => break,
            }
        }
        
        chain
    }
}
```

---

### 3. å¾ªç¯ä¾èµ–æ£€æµ‹

```rust
/// å¾ªç¯ä¾èµ–æ£€æµ‹å™¨
pub struct CircularDependencyDetector {
    topology_analyzer: Arc<ServiceTopologyAnalyzer>,
}

impl CircularDependencyDetector {
    pub fn new(topology_analyzer: Arc<ServiceTopologyAnalyzer>) -> Self {
        Self { topology_analyzer }
    }
    
    /// æ£€æµ‹å¾ªç¯ä¾èµ–
    pub fn detect_circular_dependencies(
        &self,
        topology: &ServiceTopology,
    ) -> Vec<Vec<String>> {
        let mut cycles = Vec::new();
        
        // æ„å»ºé‚»æ¥è¡¨
        let mut adjacency: HashMap<String, Vec<String>> = HashMap::new();
        
        for edge in &topology.edges {
            adjacency
                .entry(edge.from.clone())
                .or_insert_with(Vec::new)
                .push(edge.to.clone());
        }
        
        let mut visited = HashSet::new();
        let mut rec_stack = HashSet::new();
        let mut path = Vec::new();
        
        // DFS æ£€æµ‹ç¯
        for service in topology.nodes.iter().map(|n| &n.service_name) {
            if !visited.contains(service) {
                self.dfs_detect_cycle(
                    service,
                    &adjacency,
                    &mut visited,
                    &mut rec_stack,
                    &mut path,
                    &mut cycles,
                );
            }
        }
        
        cycles
    }
    
    /// DFS æ£€æµ‹ç¯
    fn dfs_detect_cycle(
        &self,
        service: &str,
        adjacency: &HashMap<String, Vec<String>>,
        visited: &mut HashSet<String>,
        rec_stack: &mut HashSet<String>,
        path: &mut Vec<String>,
        cycles: &mut Vec<Vec<String>>,
    ) {
        visited.insert(service.to_string());
        rec_stack.insert(service.to_string());
        path.push(service.to_string());
        
        if let Some(children) = adjacency.get(service) {
            for child in children {
                if !visited.contains(child) {
                    self.dfs_detect_cycle(child, adjacency, visited, rec_stack, path, cycles);
                } else if rec_stack.contains(child) {
                    // å‘ç°ç¯
                    let cycle_start = path.iter().position(|s| s == child).unwrap();
                    let cycle = path[cycle_start..].to_vec();
                    cycles.push(cycle);
                }
            }
        }
        
        path.pop();
        rec_stack.remove(service);
    }
}
```

---

## âš¡ æ€§èƒ½åˆ†æ

### 1. è€—æ—¶åˆ†å¸ƒåˆ†æ

```rust
/// è€—æ—¶åˆ†å¸ƒåˆ†æå™¨
pub struct DurationDistributionAnalyzer {
    trace_query_engine: Arc<TraceIdQueryEngine>,
}

impl DurationDistributionAnalyzer {
    pub fn new(trace_query_engine: Arc<TraceIdQueryEngine>) -> Self {
        Self { trace_query_engine }
    }
    
    /// åˆ†æè€—æ—¶åˆ†å¸ƒ
    pub async fn analyze_duration_distribution(
        &self,
        trace_ids: Vec<TraceId>,
    ) -> Result<DurationDistribution, AnalysisError> {
        let mut durations = Vec::new();
        
        for trace_id in trace_ids {
            if let Ok(Some(trace)) = self.trace_query_engine.query_trace(&trace_id).await {
                durations.push(trace.duration_ms);
            }
        }
        
        if durations.is_empty() {
            return Err(AnalysisError::AnalysisFailed("No traces found".to_string()));
        }
        
        durations.sort();
        
        let count = durations.len();
        let sum: i64 = durations.iter().sum();
        let avg = sum / count as i64;
        
        let min = *durations.first().unwrap();
        let max = *durations.last().unwrap();
        
        let median = if count % 2 == 0 {
            (durations[count / 2 - 1] + durations[count / 2]) / 2
        } else {
            durations[count / 2]
        };
        
        // è®¡ç®—æ ‡å‡†å·®
        let variance: f64 = durations
            .iter()
            .map(|d| {
                let diff = *d as f64 - avg as f64;
                diff * diff
            })
            .sum::<f64>()
            / count as f64;
        
        let std_dev = variance.sqrt();
        
        Ok(DurationDistribution {
            count,
            min,
            max,
            avg,
            median,
            std_dev,
            percentiles: self.calculate_percentiles(&durations),
        })
    }
    
    /// è®¡ç®—ç™¾åˆ†ä½æ•°
    fn calculate_percentiles(&self, sorted_durations: &[i64]) -> Percentiles {
        let len = sorted_durations.len();
        
        Percentiles {
            p50: sorted_durations[len / 2],
            p75: sorted_durations[len * 3 / 4],
            p90: sorted_durations[len * 9 / 10],
            p95: sorted_durations[len * 95 / 100],
            p99: sorted_durations[len * 99 / 100],
        }
    }
}

#[derive(Debug, Clone)]
pub struct DurationDistribution {
    pub count: usize,
    pub min: i64,
    pub max: i64,
    pub avg: i64,
    pub median: i64,
    pub std_dev: f64,
    pub percentiles: Percentiles,
}

#[derive(Debug, Clone)]
pub struct Percentiles {
    pub p50: i64,
    pub p75: i64,
    pub p90: i64,
    pub p95: i64,
    pub p99: i64,
}
```

---

### 2. P95/P99 åˆ†æ

```rust
/// P95/P99 åˆ†æå™¨
pub struct PercentileAnalyzer {
    duration_analyzer: Arc<DurationDistributionAnalyzer>,
}

impl PercentileAnalyzer {
    pub fn new(duration_analyzer: Arc<DurationDistributionAnalyzer>) -> Self {
        Self { duration_analyzer }
    }
    
    /// åˆ†æ P95/P99 è¶‹åŠ¿
    pub async fn analyze_percentile_trends(
        &self,
        trace_ids_by_window: Vec<(DateTime<Utc>, Vec<TraceId>)>,
    ) -> Result<Vec<PercentileTrend>, AnalysisError> {
        let mut trends = Vec::new();
        
        for (timestamp, trace_ids) in trace_ids_by_window {
            let distribution = self.duration_analyzer
                .analyze_duration_distribution(trace_ids)
                .await?;
            
            trends.push(PercentileTrend {
                timestamp,
                p95: distribution.percentiles.p95,
                p99: distribution.percentiles.p99,
                avg: distribution.avg,
            });
        }
        
        Ok(trends)
    }
    
    /// æ£€æµ‹ P99 å¼‚å¸¸
    pub fn detect_p99_anomalies(
        &self,
        trends: &[PercentileTrend],
        threshold_multiplier: f64, // ä¾‹å¦‚ 2.0 è¡¨ç¤ºè¶…è¿‡å¹³å‡å€¼çš„ 2 å€
    ) -> Vec<PercentileAnomaly> {
        if trends.is_empty() {
            return Vec::new();
        }
        
        // è®¡ç®— P99 çš„å¹³å‡å€¼
        let avg_p99 = trends
            .iter()
            .map(|t| t.p99)
            .sum::<i64>() as f64
            / trends.len() as f64;
        
        let threshold = avg_p99 * threshold_multiplier;
        
        trends
            .iter()
            .filter(|trend| trend.p99 as f64 > threshold)
            .map(|trend| PercentileAnomaly {
                timestamp: trend.timestamp,
                p99: trend.p99,
                expected_p99: avg_p99 as i64,
                deviation_percentage: ((trend.p99 as f64 - avg_p99) / avg_p99 * 100.0),
            })
            .collect()
    }
}

#[derive(Debug, Clone)]
pub struct PercentileTrend {
    pub timestamp: DateTime<Utc>,
    pub p95: i64,
    pub p99: i64,
    pub avg: i64,
}

#[derive(Debug, Clone)]
pub struct PercentileAnomaly {
    pub timestamp: DateTime<Utc>,
    pub p99: i64,
    pub expected_p99: i64,
    pub deviation_percentage: f64,
}
```

---

### 3. å¼‚å¸¸æ£€æµ‹

```rust
/// å¼‚å¸¸æ£€æµ‹å™¨
pub struct AnomalyDetector {
    duration_analyzer: Arc<DurationDistributionAnalyzer>,
}

impl AnomalyDetector {
    pub fn new(duration_analyzer: Arc<DurationDistributionAnalyzer>) -> Self {
        Self { duration_analyzer }
    }
    
    /// æ£€æµ‹å¼‚å¸¸ Traceï¼ˆåŸºäºç»Ÿè®¡æ–¹æ³•ï¼‰
    pub async fn detect_anomalies(
        &self,
        trace_ids: Vec<TraceId>,
        sigma_threshold: f64, // ä¾‹å¦‚ 3.0 è¡¨ç¤º 3 å€æ ‡å‡†å·®
    ) -> Result<Vec<AnomalyTrace>, AnalysisError> {
        let distribution = self.duration_analyzer
            .analyze_duration_distribution(trace_ids.clone())
            .await?;
        
        let upper_bound = distribution.avg as f64 + sigma_threshold * distribution.std_dev;
        let lower_bound = (distribution.avg as f64 - sigma_threshold * distribution.std_dev).max(0.0);
        
        let mut anomalies = Vec::new();
        
        for trace_id in trace_ids {
            if let Ok(Some(trace)) = self.duration_analyzer
                .trace_query_engine
                .query_trace(&trace_id)
                .await
            {
                let duration = trace.duration_ms as f64;
                
                if duration > upper_bound || duration < lower_bound {
                    anomalies.push(AnomalyTrace {
                        trace_id,
                        duration_ms: trace.duration_ms,
                        expected_range: (lower_bound as i64, upper_bound as i64),
                        anomaly_type: if duration > upper_bound {
                            AnomalyType::SlowResponse
                        } else {
                            AnomalyType::FastResponse
                        },
                    });
                }
            }
        }
        
        Ok(anomalies)
    }
}

#[derive(Debug, Clone)]
pub struct AnomalyTrace {
    pub trace_id: TraceId,
    pub duration_ms: i64,
    pub expected_range: (i64, i64),
    pub anomaly_type: AnomalyType,
}

#[derive(Debug, Clone)]
pub enum AnomalyType {
    SlowResponse,
    FastResponse,
}
```

---

## ğŸ¨ Trace å¯è§†åŒ–

### 1. ç€‘å¸ƒå›¾ç”Ÿæˆ

```rust
/// ç€‘å¸ƒå›¾ç”Ÿæˆå™¨
pub struct WaterfallChartGenerator {
    trace_reconstructor: Arc<TraceReconstructor>,
}

impl WaterfallChartGenerator {
    pub fn new(trace_reconstructor: Arc<TraceReconstructor>) -> Self {
        Self { trace_reconstructor }
    }
    
    /// ç”Ÿæˆç€‘å¸ƒå›¾æ•°æ®
    pub async fn generate_waterfall(
        &self,
        trace_id: &TraceId,
    ) -> Result<WaterfallChart, AnalysisError> {
        let trace_tree = self.trace_reconstructor
            .reconstruct_trace_tree(trace_id)
            .await?;
        
        let mut bars = Vec::new();
        
        // æ‰¾åˆ°æœ€æ—©çš„å¼€å§‹æ—¶é—´ä½œä¸ºåŸºå‡†
        let base_time = trace_tree
            .root_nodes
            .first()
            .map(|root| root.span.start_time)
            .ok_or(AnalysisError::AnalysisFailed("No root nodes".to_string()))?;
        
        // éå†æ ‘ç”Ÿæˆç€‘å¸ƒå›¾æ¡ç›®
        for root in &trace_tree.root_nodes {
            self.traverse_for_waterfall(root, &mut bars, base_time, 0);
        }
        
        Ok(WaterfallChart {
            trace_id: *trace_id,
            base_time,
            bars,
        })
    }
    
    /// é€’å½’éå†ç”Ÿæˆç€‘å¸ƒå›¾æ¡ç›®
    fn traverse_for_waterfall(
        &self,
        node: &TraceTreeNode,
        bars: &mut Vec<WaterfallBar>,
        base_time: DateTime<Utc>,
        depth: usize,
    ) {
        let start_offset = (node.span.start_time - base_time).num_milliseconds();
        
        bars.push(WaterfallBar {
            span_id: node.span.span_id,
            span_name: node.span.name.clone(),
            service_name: node.span.service_name.clone(),
            start_offset_ms: start_offset,
            duration_ms: node.span.duration_ms,
            depth,
        });
        
        for child in &node.children {
            self.traverse_for_waterfall(child, bars, base_time, depth + 1);
        }
    }
}

#[derive(Debug, Clone)]
pub struct WaterfallChart {
    pub trace_id: TraceId,
    pub base_time: DateTime<Utc>,
    pub bars: Vec<WaterfallBar>,
}

#[derive(Debug, Clone)]
pub struct WaterfallBar {
    pub span_id: SpanId,
    pub span_name: String,
    pub service_name: String,
    pub start_offset_ms: i64,
    pub duration_ms: i64,
    pub depth: usize,
}
```

---

### 2. ç«ç„°å›¾ç”Ÿæˆ

```rust
/// ç«ç„°å›¾ç”Ÿæˆå™¨
pub struct FlameGraphGenerator {
    trace_reconstructor: Arc<TraceReconstructor>,
}

impl FlameGraphGenerator {
    pub fn new(trace_reconstructor: Arc<TraceReconstructor>) -> Self {
        Self { trace_reconstructor }
    }
    
    /// ç”Ÿæˆç«ç„°å›¾æ•°æ®
    pub async fn generate_flame_graph(
        &self,
        trace_id: &TraceId,
    ) -> Result<FlameGraph, AnalysisError> {
        let trace_tree = self.trace_reconstructor
            .reconstruct_trace_tree(trace_id)
            .await?;
        
        let mut flame_nodes = Vec::new();
        
        for root in &trace_tree.root_nodes {
            let node = self.build_flame_node(root);
            flame_nodes.push(node);
        }
        
        Ok(FlameGraph {
            trace_id: *trace_id,
            roots: flame_nodes,
        })
    }
    
    /// é€’å½’æ„å»ºç«ç„°å›¾èŠ‚ç‚¹
    fn build_flame_node(&self, tree_node: &TraceTreeNode) -> FlameNode {
        let children: Vec<FlameNode> = tree_node
            .children
            .iter()
            .map(|child| self.build_flame_node(child))
            .collect();
        
        FlameNode {
            name: format!("{} ({})", tree_node.span.service_name, tree_node.span.name),
            value: tree_node.span.duration_ms,
            children,
        }
    }
}

#[derive(Debug, Clone)]
pub struct FlameGraph {
    pub trace_id: TraceId,
    pub roots: Vec<FlameNode>,
}

#[derive(Debug, Clone)]
pub struct FlameNode {
    pub name: String,
    pub value: i64,
    pub children: Vec<FlameNode>,
}
```

---

### 3. Gantt å›¾ç”Ÿæˆ

```rust
/// Gantt å›¾ç”Ÿæˆå™¨
pub struct GanttChartGenerator {
    waterfall_generator: Arc<WaterfallChartGenerator>,
}

impl GanttChartGenerator {
    pub fn new(waterfall_generator: Arc<WaterfallChartGenerator>) -> Self {
        Self { waterfall_generator }
    }
    
    /// ç”Ÿæˆ Gantt å›¾æ•°æ®
    pub async fn generate_gantt(
        &self,
        trace_id: &TraceId,
    ) -> Result<GanttChart, AnalysisError> {
        let waterfall = self.waterfall_generator
            .generate_waterfall(trace_id)
            .await?;
        
        // æŒ‰æœåŠ¡åˆ†ç»„
        let mut service_groups: HashMap<String, Vec<GanttTask>> = HashMap::new();
        
        for bar in waterfall.bars {
            let task = GanttTask {
                task_id: format!("{:?}", bar.span_id),
                task_name: bar.span_name,
                start_offset_ms: bar.start_offset_ms,
                duration_ms: bar.duration_ms,
            };
            
            service_groups
                .entry(bar.service_name)
                .or_insert_with(Vec::new)
                .push(task);
        }
        
        Ok(GanttChart {
            trace_id: *trace_id,
            base_time: waterfall.base_time,
            service_groups,
        })
    }
}

#[derive(Debug, Clone)]
pub struct GanttChart {
    pub trace_id: TraceId,
    pub base_time: DateTime<Utc>,
    pub service_groups: HashMap<String, Vec<GanttTask>>,
}

#[derive(Debug, Clone)]
pub struct GanttTask {
    pub task_id: String,
    pub task_name: String,
    pub start_offset_ms: i64,
    pub duration_ms: i64,
}
```

---

## ğŸ“ˆ èšåˆåˆ†æ

### 1. æ—¶é—´çª—å£èšåˆ

```rust
/// æ—¶é—´çª—å£èšåˆå™¨
pub struct TimeWindowAggregator {
    trace_query_engine: Arc<TraceIdQueryEngine>,
}

impl TimeWindowAggregator {
    pub fn new(trace_query_engine: Arc<TraceIdQueryEngine>) -> Self {
        Self { trace_query_engine }
    }
    
    /// æŒ‰æ—¶é—´çª—å£èšåˆ
    pub async fn aggregate_by_time_window(
        &self,
        trace_ids: Vec<TraceId>,
        window_size_minutes: i64,
    ) -> Result<Vec<TimeWindowAggregation>, AnalysisError> {
        let mut trace_by_window: HashMap<i64, Vec<CompleteTrace>> = HashMap::new();
        
        for trace_id in trace_ids {
            if let Ok(Some(trace)) = self.trace_query_engine.query_trace(&trace_id).await {
                let window_key = trace.start_time.timestamp() / (window_size_minutes * 60);
                
                trace_by_window
                    .entry(window_key)
                    .or_insert_with(Vec::new)
                    .push(trace);
            }
        }
        
        let mut aggregations = Vec::new();
        
        for (window_key, traces) in trace_by_window {
            let window_start = DateTime::from_timestamp(window_key * window_size_minutes * 60, 0)
                .unwrap();
            
            let count = traces.len();
            let total_duration: i64 = traces.iter().map(|t| t.duration_ms).sum();
            let avg_duration = total_duration / count as i64;
            
            let error_count = traces
                .iter()
                .filter(|t| {
                    t.spans
                        .iter()
                        .any(|s| matches!(s.status, SpanStatus::Error { .. }))
                })
                .count();
            
            aggregations.push(TimeWindowAggregation {
                window_start,
                window_size_minutes,
                trace_count: count,
                avg_duration_ms: avg_duration,
                error_count,
            });
        }
        
        aggregations.sort_by_key(|a| a.window_start);
        
        Ok(aggregations)
    }
}

#[derive(Debug, Clone)]
pub struct TimeWindowAggregation {
    pub window_start: DateTime<Utc>,
    pub window_size_minutes: i64,
    pub trace_count: usize,
    pub avg_duration_ms: i64,
    pub error_count: usize,
}
```

---

### 2. æœåŠ¡çº§åˆ«èšåˆ

```rust
/// æœåŠ¡çº§åˆ«èšåˆå™¨
pub struct ServiceLevelAggregator {
    trace_query_engine: Arc<TraceIdQueryEngine>,
}

impl ServiceLevelAggregator {
    pub fn new(trace_query_engine: Arc<TraceIdQueryEngine>) -> Self {
        Self { trace_query_engine }
    }
    
    /// æŒ‰æœåŠ¡èšåˆç»Ÿè®¡
    pub async fn aggregate_by_service(
        &self,
        trace_ids: Vec<TraceId>,
    ) -> Result<Vec<ServiceAggregation>, AnalysisError> {
        let mut service_stats: HashMap<String, ServiceStatsAccumulator> = HashMap::new();
        
        for trace_id in trace_ids {
            if let Ok(Some(trace)) = self.trace_query_engine.query_trace(&trace_id).await {
                for span in &trace.spans {
                    let stats = service_stats
                        .entry(span.service_name.clone())
                        .or_insert_with(|| ServiceStatsAccumulator {
                            service_name: span.service_name.clone(),
                            span_count: 0,
                            total_duration_ms: 0,
                            error_count: 0,
                            durations: Vec::new(),
                        });
                    
                    stats.span_count += 1;
                    stats.total_duration_ms += span.duration_ms;
                    stats.durations.push(span.duration_ms);
                    
                    if matches!(span.status, SpanStatus::Error { .. }) {
                        stats.error_count += 1;
                    }
                }
            }
        }
        
        let aggregations: Vec<ServiceAggregation> = service_stats
            .into_values()
            .map(|mut stats| {
                stats.durations.sort();
                
                let avg_duration = stats.total_duration_ms / stats.span_count as i64;
                let p95_duration = stats.durations[stats.durations.len() * 95 / 100];
                
                ServiceAggregation {
                    service_name: stats.service_name,
                    span_count: stats.span_count,
                    avg_duration_ms: avg_duration,
                    p95_duration_ms: p95_duration,
                    error_count: stats.error_count,
                    error_rate: (stats.error_count as f64 / stats.span_count as f64) * 100.0,
                }
            })
            .collect();
        
        Ok(aggregations)
    }
}

#[derive(Debug)]
struct ServiceStatsAccumulator {
    service_name: String,
    span_count: usize,
    total_duration_ms: i64,
    error_count: usize,
    durations: Vec<i64>,
}

#[derive(Debug, Clone)]
pub struct ServiceAggregation {
    pub service_name: String,
    pub span_count: usize,
    pub avg_duration_ms: i64,
    pub p95_duration_ms: i64,
    pub error_count: usize,
    pub error_rate: f64,
}
```

---

## ğŸ” æ ¹å› åˆ†æ

### 1. é”™è¯¯ä¼ æ’­åˆ†æ

```rust
/// é”™è¯¯ä¼ æ’­åˆ†æå™¨
pub struct ErrorPropagationAnalyzer {
    trace_reconstructor: Arc<TraceReconstructor>,
}

impl ErrorPropagationAnalyzer {
    pub fn new(trace_reconstructor: Arc<TraceReconstructor>) -> Self {
        Self { trace_reconstructor }
    }
    
    /// åˆ†æé”™è¯¯ä¼ æ’­é“¾
    pub async fn analyze_error_propagation(
        &self,
        trace_id: &TraceId,
    ) -> Result<ErrorPropagationChain, AnalysisError> {
        let trace_tree = self.trace_reconstructor
            .reconstruct_trace_tree(trace_id)
            .await?;
        
        let mut error_spans = Vec::new();
        
        // æ”¶é›†æ‰€æœ‰é”™è¯¯ Span
        for root in &trace_tree.root_nodes {
            self.collect_error_spans(root, &mut error_spans);
        }
        
        if error_spans.is_empty() {
            return Err(AnalysisError::AnalysisFailed("No errors found".to_string()));
        }
        
        // æ‰¾å‡ºæœ€æ—©çš„é”™è¯¯ï¼ˆæ ¹å› ï¼‰
        error_spans.sort_by_key(|span| span.start_time);
        
        let root_cause = error_spans.first().unwrap().clone();
        
        Ok(ErrorPropagationChain {
            trace_id: *trace_id,
            root_cause,
            propagated_errors: error_spans,
        })
    }
    
    /// æ”¶é›†é”™è¯¯ Span
    fn collect_error_spans(&self, node: &TraceTreeNode, errors: &mut Vec<SpanRecord>) {
        if matches!(node.span.status, SpanStatus::Error { .. }) {
            errors.push(node.span.clone());
        }
        
        for child in &node.children {
            self.collect_error_spans(child, errors);
        }
    }
}

#[derive(Debug, Clone)]
pub struct ErrorPropagationChain {
    pub trace_id: TraceId,
    pub root_cause: SpanRecord,
    pub propagated_errors: Vec<SpanRecord>,
}
```

---

### 2. å¼‚å¸¸æ ¹å› å®šä½

```rust
/// å¼‚å¸¸æ ¹å› å®šä½å™¨
pub struct RootCauseLocator {
    error_propagation_analyzer: Arc<ErrorPropagationAnalyzer>,
    bottleneck_detector: Arc<BottleneckDetector>,
}

impl RootCauseLocator {
    pub fn new(
        error_propagation_analyzer: Arc<ErrorPropagationAnalyzer>,
        bottleneck_detector: Arc<BottleneckDetector>,
    ) -> Self {
        Self {
            error_propagation_analyzer,
            bottleneck_detector,
        }
    }
    
    /// å®šä½å¼‚å¸¸æ ¹å› 
    pub async fn locate_root_cause(
        &self,
        trace_id: &TraceId,
    ) -> Result<RootCauseAnalysis, AnalysisError> {
        let mut root_causes = Vec::new();
        
        // 1. æ£€æŸ¥é”™è¯¯ä¼ æ’­
        if let Ok(error_chain) = self.error_propagation_analyzer
            .analyze_error_propagation(trace_id)
            .await
        {
            root_causes.push(RootCause {
                cause_type: RootCauseType::Error,
                span_id: error_chain.root_cause.span_id,
                span_name: error_chain.root_cause.name.clone(),
                service_name: error_chain.root_cause.service_name.clone(),
                description: format!(
                    "Error originated in {} service",
                    error_chain.root_cause.service_name
                ),
            });
        }
        
        // 2. æ£€æŸ¥æ€§èƒ½ç“¶é¢ˆ
        if let Ok(bottlenecks) = self.bottleneck_detector
            .detect_bottlenecks(trace_id, 20.0)
            .await
        {
            for bottleneck in bottlenecks {
                root_causes.push(RootCause {
                    cause_type: RootCauseType::PerformanceBottleneck,
                    span_id: bottleneck.span_id,
                    span_name: bottleneck.span_name.clone(),
                    service_name: bottleneck.service_name.clone(),
                    description: format!(
                        "{:?} bottleneck taking {:.1}% of total time",
                        bottleneck.bottleneck_type,
                        bottleneck.percentage
                    ),
                });
            }
        }
        
        Ok(RootCauseAnalysis {
            trace_id: *trace_id,
            root_causes,
        })
    }
}

#[derive(Debug, Clone)]
pub struct RootCauseAnalysis {
    pub trace_id: TraceId,
    pub root_causes: Vec<RootCause>,
}

#[derive(Debug, Clone)]
pub struct RootCause {
    pub cause_type: RootCauseType,
    pub span_id: SpanId,
    pub span_name: String,
    pub service_name: String,
    pub description: String,
}

#[derive(Debug, Clone)]
pub enum RootCauseType {
    Error,
    PerformanceBottleneck,
    Timeout,
    ResourceExhaustion,
}
```

---

## ğŸ“Š å®Œæ•´ç¤ºä¾‹ï¼šTrace åˆ†æå¹³å°

```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt::init();
    
    println!("ğŸ¯ Trace åˆ†æå¹³å°å¯åŠ¨ä¸­...\n");
    
    // åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
    let trace_query_engine = Arc::new(TraceIdQueryEngine::new());
    let trace_reconstructor = Arc::new(TraceReconstructor::new(trace_query_engine.clone()));
    
    // åˆ›å»ºåˆ†æå™¨
    let call_chain_analyzer = Arc::new(CallChainAnalyzer::new(trace_reconstructor.clone()));
    let critical_path_analyzer = Arc::new(CriticalPathAnalyzer::new(call_chain_analyzer.clone()));
    let bottleneck_detector = Arc::new(BottleneckDetector::new(critical_path_analyzer.clone()));
    
    // åˆ›å»ºå¯è§†åŒ–ç”Ÿæˆå™¨
    let waterfall_generator = Arc::new(WaterfallChartGenerator::new(trace_reconstructor.clone()));
    let flame_generator = Arc::new(FlameGraphGenerator::new(trace_reconstructor.clone()));
    
    let trace_id = [1u8; 16];
    
    println!("ğŸ“Š æ‰§è¡Œå®Œæ•´çš„ Trace åˆ†æ...\n");
    
    // 1. è°ƒç”¨é“¾è·¯åˆ†æ
    if let Ok(chain_analysis) = call_chain_analyzer.analyze_call_chain(&trace_id).await {
        println!("ğŸ”— è°ƒç”¨é“¾è·¯åˆ†æ:");
        println!("  æœ€å¤§æ·±åº¦: {}", chain_analysis.max_depth);
        println!("  æ€»è°ƒç”¨æ•°: {}", chain_analysis.total_calls);
    }
    
    // 2. å…³é”®è·¯å¾„åˆ†æ
    if let Ok(critical_path) = critical_path_analyzer.find_critical_path(&trace_id).await {
        println!("\nâš¡ å…³é”®è·¯å¾„:");
        println!("  æ€»è€—æ—¶: {}ms", critical_path.total_duration);
        println!("  å…³é”®èŠ‚ç‚¹æ•°: {}", critical_path.critical_nodes.len());
        
        let contributions = critical_path_analyzer.calculate_span_contribution(&critical_path);
        
        println!("\n  Span è€—æ—¶å æ¯”:");
        for contrib in contributions.iter().take(5) {
            println!(
                "    - {} ({}) : {:.1}%",
                contrib.span_name,
                contrib.service_name,
                contrib.percentage
            );
        }
    }
    
    // 3. ç“¶é¢ˆæ£€æµ‹
    if let Ok(bottlenecks) = bottleneck_detector.detect_bottlenecks(&trace_id, 15.0).await {
        println!("\nğŸ” æ€§èƒ½ç“¶é¢ˆ:");
        
        for bottleneck in bottlenecks {
            println!(
                "  - {} ({:?}): {}ms ({:.1}%)",
                bottleneck.span_name,
                bottleneck.bottleneck_type,
                bottleneck.duration_ms,
                bottleneck.percentage
            );
        }
    }
    
    // 4. ç”Ÿæˆç€‘å¸ƒå›¾
    if let Ok(waterfall) = waterfall_generator.generate_waterfall(&trace_id).await {
        println!("\nğŸŒŠ ç€‘å¸ƒå›¾æ•°æ®:");
        println!("  åŸºå‡†æ—¶é—´: {}", waterfall.base_time);
        println!("  æ€»æ¡ç›®æ•°: {}", waterfall.bars.len());
    }
    
    // 5. ç”Ÿæˆç«ç„°å›¾
    if let Ok(flame_graph) = flame_generator.generate_flame_graph(&trace_id).await {
        println!("\nğŸ”¥ ç«ç„°å›¾æ•°æ®:");
        println!("  æ ¹èŠ‚ç‚¹æ•°: {}", flame_graph.roots.len());
    }
    
    println!("\nâœ… åˆ†æå®Œæˆï¼");
    
    Ok(())
}

// å ä½å®ç°
pub struct TraceIdQueryEngine {}
impl TraceIdQueryEngine {
    pub fn new() -> Self { Self {} }
    pub async fn query_trace(&self, _trace_id: &TraceId) -> Result<Option<CompleteTrace>, AnalysisError> {
        Ok(None)
    }
}

pub struct TraceReconstructor {
    _trace_query_engine: Arc<TraceIdQueryEngine>,
}
impl TraceReconstructor {
    pub fn new(_trace_query_engine: Arc<TraceIdQueryEngine>) -> Self {
        Self { _trace_query_engine }
    }
    pub async fn reconstruct_trace_tree(&self, _trace_id: &TraceId) -> Result<TraceTree, AnalysisError> {
        Err(AnalysisError::TraceNotFound)
    }
}

pub struct DependencyGraphBuilder {}
impl DependencyGraphBuilder {
    pub async fn build_service_dependency_graph(&self, _trace_id: &TraceId) -> Result<ServiceDependencyGraph, AnalysisError> {
        Err(AnalysisError::TraceNotFound)
    }
}

#[derive(Debug, Clone)]
pub struct TraceTree {
    pub trace_id: TraceId,
    pub root_nodes: Vec<TraceTreeNode>,
}

#[derive(Debug, Clone)]
pub struct TraceTreeNode {
    pub span: SpanRecord,
    pub children: Vec<TraceTreeNode>,
}

#[derive(Debug, Clone)]
pub struct SpanRecord {
    pub span_id: SpanId,
    pub parent_span_id: Option<SpanId>,
    pub trace_id: TraceId,
    pub name: String,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub duration_ms: i64,
    pub service_name: String,
    pub attributes: HashMap<String, String>,
    pub status: SpanStatus,
}

#[derive(Debug, Clone)]
pub enum SpanStatus {
    Ok,
    Error { message: String },
}

#[derive(Debug, Clone)]
pub struct CompleteTrace {
    pub trace_id: TraceId,
    pub spans: Vec<SpanRecord>,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub duration_ms: i64,
    pub service_count: usize,
    pub span_count: usize,
}

#[derive(Debug, Clone)]
pub struct ServiceDependencyGraph {
    pub nodes: HashMap<String, ServiceNode>,
    pub edges: Vec<ServiceDependencyEdge>,
}

#[derive(Debug, Clone)]
pub struct ServiceNode {
    pub service_name: String,
    pub span_count: usize,
    pub total_duration_ms: i64,
}

#[derive(Debug, Clone)]
pub struct ServiceDependencyEdge {
    pub from: String,
    pub to: String,
    pub call_count: usize,
}
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### æ€§èƒ½ä¼˜åŒ–

1. **ç¼“å­˜é‡å»ºç»“æœ** - Trace æ ‘é‡å»ºç»“æœåº”ç¼“å­˜
2. **å¹¶è¡Œåˆ†æ** - å¤šä¸ªåˆ†æä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ
3. **å¢é‡è®¡ç®—** - èšåˆç»Ÿè®¡ä½¿ç”¨å¢é‡æ›´æ–°

### å¯è§†åŒ–å»ºè®®

1. **ç€‘å¸ƒå›¾** - é€‚åˆæŸ¥çœ‹æ—¶åºå…³ç³»
2. **ç«ç„°å›¾** - é€‚åˆè¯†åˆ«çƒ­ç‚¹
3. **Gantt å›¾** - é€‚åˆå¹¶å‘åˆ†æ

### åˆ†æç­–ç•¥

1. **å…ˆå®è§‚åå¾®è§‚** - å…ˆçœ‹æ•´ä½“å†çœ‹ç»†èŠ‚
2. **å…³æ³¨å…³é”®è·¯å¾„** - ä¼˜å…ˆåˆ†æå…³é”®è·¯å¾„
3. **ç»“åˆå¤šç»´åº¦** - é”™è¯¯ã€æ€§èƒ½ã€ä¾èµ–ç»¼åˆåˆ†æ

---

## ğŸ“š å‚è€ƒèµ„æº

- [Jaeger UI](https://www.jaegertracing.io/docs/latest/frontend-ui/)
- [Zipkin UI](https://zipkin.io/pages/instrumenting.html)
- [Grafana Tempo](https://grafana.com/docs/tempo/latest/)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-10  
**ä½œè€…**: OTLP Rust é¡¹ç›®ç»„
