# Rust OTLP ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–å®æˆ˜æŒ‡å—

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **Rust ç‰ˆæœ¬**: 1.90  
> **OpenTelemetry**: 0.31.0  
> **æœ€åæ›´æ–°**: 2025-10-08  
> **æ–‡æ¡£çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª

---

## ğŸ“‹ ç›®å½•

- [Rust OTLP ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–å®æˆ˜æŒ‡å—](#rust-otlp-ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–å®æˆ˜æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [æ¦‚è¿°](#æ¦‚è¿°)
    - [ç”Ÿäº§ç¯å¢ƒæŒ‘æˆ˜](#ç”Ÿäº§ç¯å¢ƒæŒ‘æˆ˜)
    - [æ ¸å¿ƒæŒ‡æ ‡](#æ ¸å¿ƒæŒ‡æ ‡)
  - [å¤§è§„æ¨¡éƒ¨ç½²æ¶æ„](#å¤§è§„æ¨¡éƒ¨ç½²æ¶æ„)
    - [å¤šå±‚æ¶æ„è®¾è®¡](#å¤šå±‚æ¶æ„è®¾è®¡)
    - [Rust Agent å®ç°](#rust-agent-å®ç°)
    - [Kubernetes éƒ¨ç½²é…ç½®](#kubernetes-éƒ¨ç½²é…ç½®)
  - [æˆæœ¬ä¼˜åŒ–ç­–ç•¥](#æˆæœ¬ä¼˜åŒ–ç­–ç•¥)
    - [æ™ºèƒ½é‡‡æ ·ç­–ç•¥](#æ™ºèƒ½é‡‡æ ·ç­–ç•¥)
    - [åˆ†å±‚å­˜å‚¨ç­–ç•¥](#åˆ†å±‚å­˜å‚¨ç­–ç•¥)
    - [æˆæœ¬ç›‘æ§ Dashboard](#æˆæœ¬ç›‘æ§-dashboard)
  - [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
    - [é›¶æ‹·è´åºåˆ—åŒ–](#é›¶æ‹·è´åºåˆ—åŒ–)
    - [æ‰¹é‡å¤„ç†ä¼˜åŒ–](#æ‰¹é‡å¤„ç†ä¼˜åŒ–)
    - [è¿æ¥æ± ä¼˜åŒ–](#è¿æ¥æ± ä¼˜åŒ–)
  - [SLOç›‘æ§ä¸å‘Šè­¦](#sloç›‘æ§ä¸å‘Šè­¦)
    - [SLO å®šä¹‰](#slo-å®šä¹‰)
    - [SLO ç›‘æ§å®ç°](#slo-ç›‘æ§å®ç°)
    - [Prometheus å‘Šè­¦è§„åˆ™](#prometheus-å‘Šè­¦è§„åˆ™)
  - [å®¹é‡è§„åˆ’](#å®¹é‡è§„åˆ’)
    - [å®¹é‡è®¡ç®—æ¨¡å‹](#å®¹é‡è®¡ç®—æ¨¡å‹)
  - [æ•…éšœæ¢å¤](#æ•…éšœæ¢å¤)
    - [æ–­è·¯å™¨å®ç°](#æ–­è·¯å™¨å®ç°)
    - [é‡è¯•æœºåˆ¶](#é‡è¯•æœºåˆ¶)
  - [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
    - [1. ç›‘æ§æŒ‡æ ‡](#1-ç›‘æ§æŒ‡æ ‡)
    - [2. é…ç½®ç®¡ç†](#2-é…ç½®ç®¡ç†)
    - [3. è¿ç»´æ£€æŸ¥æ¸…å•](#3-è¿ç»´æ£€æŸ¥æ¸…å•)
  - [æ€»ç»“](#æ€»ç»“)
    - [æ¶æ„æ¼”è¿›è·¯å¾„](#æ¶æ„æ¼”è¿›è·¯å¾„)
    - [å…³é”®æˆåŠŸå› ç´ ](#å…³é”®æˆåŠŸå› ç´ )

---

## æ¦‚è¿°

### ç”Ÿäº§ç¯å¢ƒæŒ‘æˆ˜

åœ¨å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½² OTLP è¿½è¸ªç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ï¼š

- âš ï¸ **é«˜ååé‡**: æ¯ç§’ç™¾ä¸‡çº§ Span å¤„ç†
- âš ï¸ **æˆæœ¬æ§åˆ¶**: å­˜å‚¨å’Œç½‘ç»œæˆæœ¬
- âš ï¸ **å¯é æ€§**: 99.9%+ å¯ç”¨æ€§è¦æ±‚
- âš ï¸ **å»¶è¿Ÿæ•æ„Ÿ**: å¯¹åº”ç”¨æ€§èƒ½å½±å“ < 1%
- âš ï¸ **æ‰©å±•æ€§**: éšä¸šåŠ¡å¢é•¿è‡ªåŠ¨æ‰©å±•

### æ ¸å¿ƒæŒ‡æ ‡

```text
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ç”Ÿäº§ç¯å¢ƒå…³é”®æŒ‡æ ‡                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ååé‡:      1M+ spans/sec                       â•‘
â•‘  å»¶è¿Ÿ:        P99 < 10ms                          â•‘
â•‘  å¼€é”€:        CPU < 2%, å†…å­˜ < 200MB              â•‘
â•‘  å¯ç”¨æ€§:      99.95%+                             â•‘
â•‘  æ•°æ®ä¸¢å¤±:    < 0.01%                             â•‘
â•‘  é‡‡æ ·ç‡:      1-10% (åŠ¨æ€è°ƒæ•´)                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## å¤§è§„æ¨¡éƒ¨ç½²æ¶æ„

### å¤šå±‚æ¶æ„è®¾è®¡

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     åº”ç”¨å±‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚Service A â”‚  â”‚Service B â”‚  â”‚Service C â”‚  (1000s)   â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è¾¹ç¼˜æ”¶é›†å±‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  OTLP Collector (Agent Mode)                 â”‚     â”‚
â”‚  â”‚  - æœ¬åœ°ç¼“å­˜                                   â”‚     â”‚
â”‚  â”‚  - æ‰¹é‡å‘é€                                   â”‚     â”‚
â”‚  â”‚  - å¤±è´¥é‡è¯•                                   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  èšåˆå¤„ç†å±‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  OTLP Collector (Gateway Mode)               â”‚     â”‚
â”‚  â”‚  - é‡‡æ ·å†³ç­–                                   â”‚     â”‚
â”‚  â”‚  - æ•°æ®æ¸…æ´—                                   â”‚     â”‚
â”‚  â”‚  - è´Ÿè½½å‡è¡¡                                   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  å­˜å‚¨å±‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Hot: ES  â”‚  â”‚ Warm: S3  â”‚  â”‚Cold: GCS â”‚           â”‚
â”‚  â”‚ (7 days) â”‚  â”‚ (30 days) â”‚  â”‚(90 days) â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Rust Agent å®ç°

```rust
// src/agent/otlp_agent.rs
use std::sync::Arc;
use tokio::sync::{mpsc, Semaphore};
use std::time::Duration;

pub struct OtlpAgent {
    /// æœ¬åœ°ç¼“å†²é˜Ÿåˆ—
    buffer: Arc<Mutex<VecDeque<SpanData>>>,
    /// æ‰¹é‡å¤§å°
    batch_size: usize,
    /// æ‰¹é‡è¶…æ—¶
    batch_timeout: Duration,
    /// å¹¶å‘é™åˆ¶
    semaphore: Arc<Semaphore>,
    /// å¯¼å‡ºå™¨
    exporter: Arc<dyn Exporter>,
}

impl OtlpAgent {
    pub fn new(config: AgentConfig) -> Self {
        Self {
            buffer: Arc::new(Mutex::new(VecDeque::with_capacity(config.buffer_capacity))),
            batch_size: config.batch_size,
            batch_timeout: config.batch_timeout,
            semaphore: Arc::new(Semaphore::new(config.max_concurrent_exports)),
            exporter: config.exporter,
        }
    }

    /// æ¥æ”¶ Span
    pub async fn receive_span(&self, span: SpanData) -> Result<(), AgentError> {
        let mut buffer = self.buffer.lock().await;
        
        // æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æ»¡
        if buffer.len() >= buffer.capacity() {
            // ä¸¢å¼ƒç­–ç•¥ï¼šä¸¢å¼ƒæœ€æ—§çš„
            buffer.pop_front();
            metrics::counter!("otlp_agent_spans_dropped").increment(1);
        }

        buffer.push_back(span);
        Ok(())
    }

    /// æ‰¹é‡å¯¼å‡º
    pub async fn run(&self) -> Result<(), AgentError> {
        let mut interval = tokio::time::interval(self.batch_timeout);

        loop {
            interval.tick().await;

            let mut buffer = self.buffer.lock().await;
            if buffer.is_empty() {
                continue;
            }

            // å–å‡ºä¸€æ‰¹æ•°æ®
            let batch: Vec<SpanData> = buffer
                .drain(..buffer.len().min(self.batch_size))
                .collect();

            drop(buffer); // é‡Šæ”¾é”

            // è·å–ä¿¡å·é‡
            let permit = self.semaphore.clone().acquire_owned().await?;
            let exporter = self.exporter.clone();

            // å¼‚æ­¥å¯¼å‡º
            tokio::spawn(async move {
                match exporter.export(batch.clone()).await {
                    Ok(_) => {
                        metrics::counter!("otlp_agent_spans_exported").increment(batch.len() as u64);
                    }
                    Err(e) => {
                        tracing::error!("Export failed: {}", e);
                        metrics::counter!("otlp_agent_export_errors").increment(1);
                        // TODO: å®ç°é‡è¯•é€»è¾‘
                    }
                }
                drop(permit);
            });
        }
    }
}

#[derive(Debug, Clone)]
pub struct AgentConfig {
    pub buffer_capacity: usize,
    pub batch_size: usize,
    pub batch_timeout: Duration,
    pub max_concurrent_exports: usize,
    pub exporter: Arc<dyn Exporter>,
}

impl Default for AgentConfig {
    fn default() -> Self {
        Self {
            buffer_capacity: 10_000,
            batch_size: 1_000,
            batch_timeout: Duration::from_secs(10),
            max_concurrent_exports: 5,
            exporter: Arc::new(StdoutExporter::new()),
        }
    }
}
```

### Kubernetes éƒ¨ç½²é…ç½®

```yaml
# k8s/agent-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otlp-agent
  namespace: observability
spec:
  selector:
    matchLabels:
      app: otlp-agent
  template:
    metadata:
      labels:
        app: otlp-agent
    spec:
      containers:
      - name: agent
        image: my-registry/otlp-agent:latest
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        env:
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://otlp-gateway.observability.svc:4317"
        - name: BATCH_SIZE
          value: "1000"
        - name: BATCH_TIMEOUT
          value: "10s"
        ports:
        - containerPort: 4317
          name: grpc
          protocol: TCP
        volumeMounts:
        - name: varlog
          mountPath: /var/log
          readOnly: true
      volumes:
      - name: varlog
        hostPath:
          path: /var/log

---
# k8s/gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-gateway
  namespace: observability
spec:
  replicas: 5
  selector:
    matchLabels:
      app: otlp-gateway
  template:
    metadata:
      labels:
        app: otlp-gateway
    spec:
      containers:
      - name: gateway
        image: my-registry/otlp-gateway:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        env:
        - name: SAMPLING_RATE
          value: "0.05"  # 5% é‡‡æ ·ç‡
        - name: BACKEND_ENDPOINT
          value: "http://jaeger-collector:14268/api/traces"
        ports:
        - containerPort: 4317
          name: grpc
        - containerPort: 8888
          name: metrics
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8888
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8888
          initialDelaySeconds: 10
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: otlp-gateway
  namespace: observability
spec:
  selector:
    app: otlp-gateway
  ports:
  - name: grpc
    port: 4317
    targetPort: 4317
  - name: metrics
    port: 8888
    targetPort: 8888
  type: ClusterIP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: otlp-gateway-hpa
  namespace: observability
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otlp-gateway
  minReplicas: 5
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: otlp_spans_per_second
      target:
        type: AverageValue
        averageValue: "50000"
```

---

## æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### æ™ºèƒ½é‡‡æ ·ç­–ç•¥

```rust
// src/sampling/intelligent_sampler.rs
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

/// æ™ºèƒ½é‡‡æ ·å™¨ - åŸºäºå¤šç§ç­–ç•¥
pub struct IntelligentSampler {
    /// åŸºç¡€é‡‡æ ·ç‡
    base_rate: f64,
    /// æœåŠ¡ç‰¹å®šé‡‡æ ·ç‡
    service_rates: Arc<RwLock<HashMap<String, f64>>>,
    /// é”™è¯¯å§‹ç»ˆé‡‡æ ·
    sample_errors: bool,
    /// æ…¢è¯·æ±‚å§‹ç»ˆé‡‡æ ·
    slow_threshold_ms: u64,
}

impl IntelligentSampler {
    pub fn new(base_rate: f64) -> Self {
        Self {
            base_rate,
            service_rates: Arc::new(RwLock::new(HashMap::new())),
            sample_errors: true,
            slow_threshold_ms: 1000,
        }
    }

    /// è®¾ç½®æœåŠ¡ç‰¹å®šé‡‡æ ·ç‡
    pub async fn set_service_rate(&self, service: String, rate: f64) {
        self.service_rates.write().await.insert(service, rate);
    }

    pub async fn should_sample(&self, span: &SpanData) -> bool {
        // 1. å§‹ç»ˆé‡‡æ ·é”™è¯¯
        if self.sample_errors && span.status().is_error() {
            metrics::counter!("sampling_reason", "reason" => "error").increment(1);
            return true;
        }

        // 2. å§‹ç»ˆé‡‡æ ·æ…¢è¯·æ±‚
        let duration_ms = span.end_time()
            .duration_since(span.start_time())
            .unwrap()
            .as_millis() as u64;

        if duration_ms > self.slow_threshold_ms {
            metrics::counter!("sampling_reason", "reason" => "slow").increment(1);
            return true;
        }

        // 3. æ£€æŸ¥æœåŠ¡ç‰¹å®šé‡‡æ ·ç‡
        let service = span.resource()
            .get("service.name")
            .map(|v| v.to_string())
            .unwrap_or_default();

        let service_rates = self.service_rates.read().await;
        let rate = service_rates.get(&service).copied().unwrap_or(self.base_rate);

        // 4. åŸºäºé‡‡æ ·ç‡éšæœºé‡‡æ ·
        let should_sample = rand::random::<f64>() < rate;
        
        if should_sample {
            metrics::counter!("sampling_reason", "reason" => "probabilistic").increment(1);
        }

        should_sample
    }
}

/// åŠ¨æ€è°ƒæ•´é‡‡æ ·ç‡
pub struct DynamicSamplingAdjuster {
    sampler: Arc<IntelligentSampler>,
    target_spans_per_sec: f64,
}

impl DynamicSamplingAdjuster {
    pub async fn adjust(&self) {
        let current_rate = tokio::time::interval(Duration::from_secs(60));

        loop {
            current_rate.tick().await;

            // è·å–å½“å‰ååé‡
            let current_throughput = metrics::gauge!("otlp_spans_per_second").get();

            // è®¡ç®—ç›®æ ‡é‡‡æ ·ç‡
            let target_rate = self.target_spans_per_sec / current_throughput;
            let target_rate = target_rate.clamp(0.01, 1.0);

            // æ›´æ–°åŸºç¡€é‡‡æ ·ç‡
            tracing::info!(
                "Adjusting sampling rate: {} (throughput: {})",
                target_rate,
                current_throughput
            );

            // è¿™é‡Œéœ€è¦è®¿é—® sampler çš„å†…éƒ¨çŠ¶æ€æ¥æ›´æ–°
            // å®é™…å®ç°ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
        }
    }
}
```

### åˆ†å±‚å­˜å‚¨ç­–ç•¥

```rust
// src/storage/tiered_storage.rs
use chrono::{DateTime, Duration, Utc};

#[derive(Debug, Clone)]
pub enum StorageTier {
    Hot {
        backend: String,        // "elasticsearch"
        retention_days: u32,    // 7 å¤©
    },
    Warm {
        backend: String,        // "s3"
        retention_days: u32,    // 30 å¤©
    },
    Cold {
        backend: String,        // "glacier"
        retention_days: u32,    // 90 å¤©
    },
}

pub struct TieredStorageManager {
    tiers: Vec<StorageTier>,
}

impl TieredStorageManager {
    pub fn new() -> Self {
        Self {
            tiers: vec![
                StorageTier::Hot {
                    backend: "elasticsearch".to_string(),
                    retention_days: 7,
                },
                StorageTier::Warm {
                    backend: "s3".to_string(),
                    retention_days: 30,
                },
                StorageTier::Cold {
                    backend: "glacier".to_string(),
                    retention_days: 90,
                },
            ],
        }
    }

    /// æ ¹æ®æ—¶é—´å†³å®šå­˜å‚¨å±‚
    pub fn get_storage_tier(&self, timestamp: DateTime<Utc>) -> &StorageTier {
        let age = Utc::now() - timestamp;

        if age <= Duration::days(7) {
            &self.tiers[0] // Hot
        } else if age <= Duration::days(30) {
            &self.tiers[1] // Warm
        } else {
            &self.tiers[2] // Cold
        }
    }

    /// æ‰§è¡Œæ•°æ®è¿ç§»
    pub async fn migrate_data(&self) -> Result<(), Box<dyn std::error::Error>> {
        // 1. æŸ¥è¯¢ Hot å­˜å‚¨ä¸­ > 7å¤© çš„æ•°æ®
        let old_spans = self.query_old_spans_from_hot(7).await?;

        // 2. å†™å…¥ Warm å­˜å‚¨
        self.write_to_warm(old_spans).await?;

        // 3. ä» Hot å­˜å‚¨åˆ é™¤
        self.delete_from_hot(7).await?;

        // 4. ç±»ä¼¼åœ°è¿ç§» Warm -> Cold
        let very_old_spans = self.query_old_spans_from_warm(30).await?;
        self.write_to_cold(very_old_spans).await?;
        self.delete_from_warm(30).await?;

        Ok(())
    }

    async fn query_old_spans_from_hot(&self, days: u32) -> Result<Vec<SpanData>, Box<dyn std::error::Error>> {
        // å®ç°æŸ¥è¯¢é€»è¾‘
        Ok(vec![])
    }

    async fn write_to_warm(&self, spans: Vec<SpanData>) -> Result<(), Box<dyn std::error::Error>> {
        // å‹ç¼©å¹¶å†™å…¥ S3
        let compressed = Self::compress_spans(&spans)?;
        
        // ä¸Šä¼ åˆ° S3
        let s3_client = aws_sdk_s3::Client::new(&aws_config::load_from_env().await);
        let key = format!("traces/{}/{}.gz", 
            chrono::Utc::now().format("%Y-%m-%d"),
            uuid::Uuid::new_v4()
        );

        s3_client
            .put_object()
            .bucket("my-traces-bucket")
            .key(&key)
            .body(compressed.into())
            .content_encoding("gzip")
            .send()
            .await?;

        Ok(())
    }

    fn compress_spans(spans: &[SpanData]) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
        use flate2::write::GzEncoder;
        use flate2::Compression;
        use std::io::Write;

        let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
        
        for span in spans {
            let json = serde_json::to_string(span)?;
            writeln!(encoder, "{}", json)?;
        }

        Ok(encoder.finish()?)
    }

    async fn delete_from_hot(&self, days: u32) -> Result<(), Box<dyn std::error::Error>> {
        // å®ç°åˆ é™¤é€»è¾‘
        Ok(())
    }

    // Warm -> Cold ç±»ä¼¼å®ç°
    async fn query_old_spans_from_warm(&self, days: u32) -> Result<Vec<SpanData>, Box<dyn std::error::Error>> {
        Ok(vec![])
    }

    async fn write_to_cold(&self, spans: Vec<SpanData>) -> Result<(), Box<dyn std::error::Error>> {
        // å†™å…¥ Glacier
        Ok(())
    }

    async fn delete_from_warm(&self, days: u32) -> Result<(), Box<dyn std::error::Error>> {
        Ok(())
    }
}

/// æˆæœ¬ä¼°ç®—
pub struct CostEstimator {
    /// æ¯ GB å­˜å‚¨æˆæœ¬ (ç¾å…ƒ/æœˆ)
    hot_storage_cost_per_gb: f64,
    warm_storage_cost_per_gb: f64,
    cold_storage_cost_per_gb: f64,
    /// æ¯ç™¾ä¸‡æ¬¡æŸ¥è¯¢æˆæœ¬
    query_cost_per_million: f64,
}

impl CostEstimator {
    pub fn aws_default() -> Self {
        Self {
            hot_storage_cost_per_gb: 0.10,   // Elasticsearch
            warm_storage_cost_per_gb: 0.023, // S3 Standard
            cold_storage_cost_per_gb: 0.004, // S3 Glacier
            query_cost_per_million: 0.50,
        }
    }

    pub fn estimate_monthly_cost(&self, metrics: &UsageMetrics) -> MonthlyCost {
        let hot_cost = metrics.hot_storage_gb * self.hot_storage_cost_per_gb;
        let warm_cost = metrics.warm_storage_gb * self.warm_storage_cost_per_gb;
        let cold_cost = metrics.cold_storage_gb * self.cold_storage_cost_per_gb;

        let query_cost = (metrics.monthly_queries as f64 / 1_000_000.0) 
            * self.query_cost_per_million;

        MonthlyCost {
            hot_storage: hot_cost,
            warm_storage: warm_cost,
            cold_storage: cold_cost,
            query_cost,
            total: hot_cost + warm_cost + cold_cost + query_cost,
        }
    }
}

#[derive(Debug)]
pub struct UsageMetrics {
    pub hot_storage_gb: f64,
    pub warm_storage_gb: f64,
    pub cold_storage_gb: f64,
    pub monthly_queries: u64,
}

#[derive(Debug)]
pub struct MonthlyCost {
    pub hot_storage: f64,
    pub warm_storage: f64,
    pub cold_storage: f64,
    pub query_cost: f64,
    pub total: f64,
}
```

### æˆæœ¬ç›‘æ§ Dashboard

```yaml
# grafana-dashboard-cost.json (ç®€åŒ–ç‰ˆ)
{
  "title": "OTLP Cost Monitoring",
  "panels": [
    {
      "title": "Monthly Cost Estimate",
      "targets": [
        {
          "expr": "sum(otlp_storage_cost_usd) by (tier)"
        }
      ]
    },
    {
      "title": "Storage by Tier",
      "targets": [
        {
          "expr": "otlp_storage_size_gb{tier=\"hot\"}"
        },
        {
          "expr": "otlp_storage_size_gb{tier=\"warm\"}"
        },
        {
          "expr": "otlp_storage_size_gb{tier=\"cold\"}"
        }
      ]
    },
    {
      "title": "Cost per Service",
      "targets": [
        {
          "expr": "sum(otlp_service_cost_usd) by (service_name)"
        }
      ]
    }
  ]
}
```

---

## æ€§èƒ½ä¼˜åŒ–

### é›¶æ‹·è´åºåˆ—åŒ–

```rust
// src/optimization/zero_copy.rs
use bytes::{Bytes, BytesMut};
use prost::Message;

/// é›¶æ‹·è´å¯¼å‡ºå™¨
pub struct ZeroCopyExporter {
    client: tonic::client::Grpc<tonic::transport::Channel>,
    buffer_pool: Arc<Mutex<Vec<BytesMut>>>,
}

impl ZeroCopyExporter {
    pub async fn export_zero_copy(&self, spans: Vec<SpanData>) -> Result<(), Box<dyn std::error::Error>> {
        // 1. ä»æ± ä¸­è·å– buffer
        let mut buffer = self.get_buffer_from_pool();

        // 2. ç›´æ¥åºåˆ—åŒ–åˆ° bufferï¼ˆé›¶æ‹·è´ï¼‰
        let proto_spans = self.convert_to_proto(spans);
        proto_spans.encode(&mut buffer)?;

        // 3. è½¬æ¢ä¸º Bytesï¼ˆé›¶æ‹·è´ï¼‰
        let bytes = buffer.freeze();

        // 4. å‘é€ï¼ˆé›¶æ‹·è´ï¼‰
        self.send_bytes(bytes).await?;

        Ok(())
    }

    fn get_buffer_from_pool(&self) -> BytesMut {
        self.buffer_pool.lock().unwrap()
            .pop()
            .unwrap_or_else(|| BytesMut::with_capacity(64 * 1024))
    }

    fn return_buffer_to_pool(&self, mut buffer: BytesMut) {
        buffer.clear();
        self.buffer_pool.lock().unwrap().push(buffer);
    }

    async fn send_bytes(&self, bytes: Bytes) -> Result<(), Box<dyn std::error::Error>> {
        // ä½¿ç”¨ tonic çš„é›¶æ‹·è´ API
        // ...
        Ok(())
    }
}
```

### æ‰¹é‡å¤„ç†ä¼˜åŒ–

```rust
// src/optimization/batching.rs

/// è‡ªé€‚åº”æ‰¹é‡å¤„ç†å™¨
pub struct AdaptiveBatcher {
    min_batch_size: usize,
    max_batch_size: usize,
    current_batch_size: Arc<AtomicUsize>,
    latency_target_ms: u64,
}

impl AdaptiveBatcher {
    pub fn new(latency_target_ms: u64) -> Self {
        Self {
            min_batch_size: 100,
            max_batch_size: 10_000,
            current_batch_size: Arc::new(AtomicUsize::new(1000)),
            latency_target_ms,
        }
    }

    pub async fn adjust_batch_size(&self) {
        let mut interval = tokio::time::interval(Duration::from_secs(30));

        loop {
            interval.tick().await;

            // è·å–å½“å‰å¹³å‡å»¶è¿Ÿ
            let current_latency_ms = metrics::histogram!("otlp_export_latency_ms")
                .percentile(0.95);

            let current_size = self.current_batch_size.load(Ordering::Relaxed);

            let new_size = if current_latency_ms > self.latency_target_ms as f64 {
                // å»¶è¿Ÿå¤ªé«˜ï¼Œå‡å°æ‰¹æ¬¡
                (current_size as f64 * 0.9) as usize
            } else {
                // å»¶è¿Ÿå¯æ¥å—ï¼Œå¢å¤§æ‰¹æ¬¡
                (current_size as f64 * 1.1) as usize
            };

            let new_size = new_size.clamp(self.min_batch_size, self.max_batch_size);
            
            self.current_batch_size.store(new_size, Ordering::Relaxed);

            tracing::info!(
                "Adjusted batch size: {} -> {} (latency: {}ms)",
                current_size,
                new_size,
                current_latency_ms
            );
        }
    }

    pub fn get_batch_size(&self) -> usize {
        self.current_batch_size.load(Ordering::Relaxed)
    }
}
```

### è¿æ¥æ± ä¼˜åŒ–

```rust
// src/optimization/connection_pool.rs
use deadpool::managed::{Manager, Pool, RecycleResult};

pub struct GrpcConnectionManager {
    endpoint: String,
}

#[async_trait]
impl Manager for GrpcConnectionManager {
    type Type = tonic::transport::Channel;
    type Error = Box<dyn std::error::Error>;

    async fn create(&self) -> Result<Self::Type, Self::Error> {
        let channel = tonic::transport::Channel::from_shared(self.endpoint.clone())?
            .http2_keep_alive_interval(Duration::from_secs(30))
            .keep_alive_timeout(Duration::from_secs(10))
            .keep_alive_while_idle(true)
            .tcp_keepalive(Some(Duration::from_secs(30)))
            .connect()
            .await?;

        Ok(channel)
    }

    async fn recycle(&self, conn: &mut Self::Type) -> RecycleResult<Self::Error> {
        // æ£€æŸ¥è¿æ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        // è¿™é‡Œå¯ä»¥å‘é€ä¸€ä¸ªç®€å•çš„å¥åº·æ£€æŸ¥è¯·æ±‚
        Ok(())
    }
}

pub type ConnectionPool = Pool<GrpcConnectionManager>;

pub fn create_connection_pool(endpoint: String, max_size: usize) -> ConnectionPool {
    let manager = GrpcConnectionManager { endpoint };
    Pool::builder(manager)
        .max_size(max_size)
        .build()
        .unwrap()
}
```

---

## SLOç›‘æ§ä¸å‘Šè­¦

### SLO å®šä¹‰

```rust
// src/slo/definition.rs
use std::time::Duration;

#[derive(Debug, Clone)]
pub struct ServiceLevelObjective {
    pub name: String,
    pub description: String,
    pub target_percentage: f64,  // 99.9% = 0.999
    pub window: Duration,         // 30 å¤©
    pub metrics: Vec<SLOMetric>,
}

#[derive(Debug, Clone)]
pub enum SLOMetric {
    /// å¯ç”¨æ€§ SLO
    Availability {
        success_query: String,
        total_query: String,
    },
    /// å»¶è¿Ÿ SLO
    Latency {
        query: String,
        threshold_ms: u64,
        percentile: f64,  // 0.99 for P99
    },
    /// é”™è¯¯ç‡ SLO
    ErrorRate {
        error_query: String,
        total_query: String,
    },
}

impl ServiceLevelObjective {
    pub fn tracing_availability() -> Self {
        Self {
            name: "Tracing Availability".to_string(),
            description: "99.9% of traces should be successfully exported".to_string(),
            target_percentage: 0.999,
            window: Duration::from_secs(30 * 24 * 3600), // 30å¤©
            metrics: vec![
                SLOMetric::Availability {
                    success_query: "sum(rate(otlp_spans_exported_total[5m]))".to_string(),
                    total_query: "sum(rate(otlp_spans_total[5m]))".to_string(),
                },
            ],
        }
    }

    pub fn tracing_latency() -> Self {
        Self {
            name: "Tracing Latency".to_string(),
            description: "99% of traces should complete within 10ms".to_string(),
            target_percentage: 0.99,
            window: Duration::from_secs(7 * 24 * 3600), // 7å¤©
            metrics: vec![
                SLOMetric::Latency {
                    query: "histogram_quantile(0.99, sum(rate(otlp_export_duration_seconds_bucket[5m])) by (le))".to_string(),
                    threshold_ms: 10,
                    percentile: 0.99,
                },
            ],
        }
    }
}
```

### SLO ç›‘æ§å®ç°

```rust
// src/slo/monitor.rs
use prometheus::{Histogram, Counter, Registry};

pub struct SloMonitor {
    slo: ServiceLevelObjective,
    registry: Registry,
    /// é”™è¯¯é¢„ç®—å‰©ä½™
    error_budget_remaining: Arc<AtomicU64>,
}

impl SloMonitor {
    pub fn new(slo: ServiceLevelObjective) -> Self {
        let registry = Registry::new();
        
        Self {
            slo,
            registry,
            error_budget_remaining: Arc::new(AtomicU64::new(0)),
        }
    }

    /// è®¡ç®—é”™è¯¯é¢„ç®—
    pub fn calculate_error_budget(&self) -> ErrorBudget {
        let total_seconds = self.slo.window.as_secs() as f64;
        let target = self.slo.target_percentage;
        
        // å…è®¸çš„åœæœºæ—¶é—´
        let allowed_downtime_seconds = total_seconds * (1.0 - target);
        
        ErrorBudget {
            total_seconds: allowed_downtime_seconds,
            consumed_seconds: self.get_consumed_seconds(),
        }
    }

    fn get_consumed_seconds(&self) -> f64 {
        // ä» Prometheus æŸ¥è¯¢å®é™…æ¶ˆè€—çš„é”™è¯¯é¢„ç®—
        // è¿™é‡Œç®€åŒ–å®ç°
        0.0
    }

    /// æ£€æŸ¥ SLO æ˜¯å¦æ»¡è¶³
    pub async fn check_slo_compliance(&self) -> bool {
        let budget = self.calculate_error_budget();
        budget.remaining_percentage() > 0.0
    }

    /// ç”Ÿæˆ SLO æŠ¥å‘Š
    pub async fn generate_report(&self) -> SloReport {
        let budget = self.calculate_error_budget();
        
        SloReport {
            slo_name: self.slo.name.clone(),
            target: self.slo.target_percentage,
            actual: self.get_actual_performance().await,
            error_budget: budget,
            status: if budget.remaining_percentage() > 10.0 {
                SloStatus::Healthy
            } else if budget.remaining_percentage() > 0.0 {
                SloStatus::Warning
            } else {
                SloStatus::Exhausted
            },
        }
    }

    async fn get_actual_performance(&self) -> f64 {
        // ä» Prometheus æŸ¥è¯¢å®é™…æ€§èƒ½
        0.999
    }
}

#[derive(Debug)]
pub struct ErrorBudget {
    pub total_seconds: f64,
    pub consumed_seconds: f64,
}

impl ErrorBudget {
    pub fn remaining_seconds(&self) -> f64 {
        self.total_seconds - self.consumed_seconds
    }

    pub fn remaining_percentage(&self) -> f64 {
        (self.remaining_seconds() / self.total_seconds) * 100.0
    }

    pub fn is_exhausted(&self) -> bool {
        self.remaining_seconds() <= 0.0
    }
}

#[derive(Debug)]
pub struct SloReport {
    pub slo_name: String,
    pub target: f64,
    pub actual: f64,
    pub error_budget: ErrorBudget,
    pub status: SloStatus,
}

#[derive(Debug, PartialEq)]
pub enum SloStatus {
    Healthy,
    Warning,
    Exhausted,
}
```

### Prometheus å‘Šè­¦è§„åˆ™

```yaml
# prometheus/slo-alerts.yml
groups:
  - name: slo_tracing
    interval: 1m
    rules:
      # é”™è¯¯é¢„ç®—æ¶ˆè€—é€Ÿç‡å‘Šè­¦
      - alert: ErrorBudgetBurnRateTooHigh
        expr: |
          (
            1 - (
              sum(rate(otlp_spans_exported_total[1h]))
              /
              sum(rate(otlp_spans_total[1h]))
            )
          ) > 0.001 * 14.4  # 14.4x burn rate
        for: 5m
        labels:
          severity: critical
          slo: availability
        annotations:
          summary: "Error budget burn rate is too high"
          description: "At current rate, error budget will be exhausted in < 2 days"

      - alert: ErrorBudgetNearlyExhausted
        expr: |
          (
            otlp_error_budget_remaining_seconds / otlp_error_budget_total_seconds
          ) < 0.10
        for: 10m
        labels:
          severity: warning
          slo: availability
        annotations:
          summary: "Error budget nearly exhausted"
          description: "Only {{ $value | humanizePercentage }} of error budget remaining"

      # å»¶è¿Ÿ SLO å‘Šè­¦
      - alert: LatencySLOViolation
        expr: |
          histogram_quantile(0.99,
            sum(rate(otlp_export_duration_seconds_bucket[5m])) by (le)
          ) > 0.010  # 10ms
        for: 10m
        labels:
          severity: warning
          slo: latency
        annotations:
          summary: "Latency SLO violation"
          description: "P99 latency is {{ $value }}s, exceeds 10ms target"

      # å¯ç”¨æ€§ SLO å‘Šè­¦
      - alert: AvailabilitySLOViolation
        expr: |
          (
            sum(rate(otlp_spans_exported_total[30d]))
            /
            sum(rate(otlp_spans_total[30d]))
          ) < 0.999
        for: 1h
        labels:
          severity: critical
          slo: availability
        annotations:
          summary: "Availability SLO violation"
          description: "Current availability is {{ $value | humanizePercentage }}, below 99.9% target"
```

---

## å®¹é‡è§„åˆ’

### å®¹é‡è®¡ç®—æ¨¡å‹

```rust
// src/capacity/planner.rs

#[derive(Debug, Clone)]
pub struct CapacityRequirements {
    /// æ¯ç§’ Span æ•°é‡
    pub spans_per_second: u64,
    /// å¹³å‡ Span å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    pub avg_span_size_bytes: usize,
    /// é‡‡æ ·ç‡
    pub sampling_rate: f64,
    /// æ•°æ®ä¿ç•™å¤©æ•°
    pub retention_days: u32,
}

impl CapacityRequirements {
    pub fn calculate_storage_needed(&self) -> StorageEstimate {
        let sampled_spans_per_day = (self.spans_per_second as f64 
            * 86400.0 
            * self.sampling_rate) as u64;

        let storage_per_day_gb = (sampled_spans_per_day as f64 
            * self.avg_span_size_bytes as f64) 
            / 1_073_741_824.0; // è½¬æ¢ä¸º GB

        let total_storage_gb = storage_per_day_gb * self.retention_days as f64;

        StorageEstimate {
            daily_gb: storage_per_day_gb,
            total_gb: total_storage_gb,
            compressed_gb: total_storage_gb * 0.3, // å‡è®¾ 30% å‹ç¼©ç‡
        }
    }

    pub fn calculate_compute_needed(&self) -> ComputeEstimate {
        // å‡è®¾æ¯ 10K spans/sec éœ€è¦ 1 CPU æ ¸å¿ƒ
        let cores_needed = (self.spans_per_second as f64 / 10_000.0).ceil() as u32;
        
        // æ¯ CPU æ ¸å¿ƒ 2GB å†…å­˜
        let memory_gb_needed = cores_needed as f64 * 2.0;

        ComputeEstimate {
            cpu_cores: cores_needed,
            memory_gb: memory_gb_needed,
            instances: (cores_needed as f64 / 4.0).ceil() as u32, // æ¯å®ä¾‹ 4 æ ¸
        }
    }

    pub fn calculate_network_needed(&self) -> NetworkEstimate {
        let bytes_per_second = self.spans_per_second as f64 * self.avg_span_size_bytes as f64;
        let mbps = (bytes_per_second * 8.0) / 1_000_000.0;

        NetworkEstimate {
            inbound_mbps: mbps,
            outbound_mbps: mbps * 0.8, // è€ƒè™‘å‹ç¼©
            monthly_gb: (bytes_per_second * 86400.0 * 30.0) / 1_073_741_824.0,
        }
    }
}

#[derive(Debug)]
pub struct StorageEstimate {
    pub daily_gb: f64,
    pub total_gb: f64,
    pub compressed_gb: f64,
}

#[derive(Debug)]
pub struct ComputeEstimate {
    pub cpu_cores: u32,
    pub memory_gb: f64,
    pub instances: u32,
}

#[derive(Debug)]
pub struct NetworkEstimate {
    pub inbound_mbps: f64,
    pub outbound_mbps: f64,
    pub monthly_gb: f64,
}

// ä½¿ç”¨ç¤ºä¾‹
pub fn example_capacity_planning() {
    let requirements = CapacityRequirements {
        spans_per_second: 100_000,
        avg_span_size_bytes: 1024,
        sampling_rate: 0.05, // 5%
        retention_days: 7,
    };

    let storage = requirements.calculate_storage_needed();
    let compute = requirements.calculate_compute_needed();
    let network = requirements.calculate_network_needed();

    println!("Capacity Planning:");
    println!("  Storage: {:.2} GB/day, {:.2} GB total", storage.daily_gb, storage.total_gb);
    println!("  Compute: {} cores, {:.2} GB RAM, {} instances", 
        compute.cpu_cores, compute.memory_gb, compute.instances);
    println!("  Network: {:.2} Mbps in, {:.2} GB/month", 
        network.inbound_mbps, network.monthly_gb);
}
```

---

## æ•…éšœæ¢å¤

### æ–­è·¯å™¨å®ç°

```rust
// src/reliability/circuit_breaker.rs
use std::sync::Arc;
use tokio::sync::RwLock;
use std::time::{Duration, Instant};

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum CircuitState {
    Closed,      // æ­£å¸¸è¿è¡Œ
    Open,        // æ–­å¼€ï¼Œæ‹’ç»è¯·æ±‚
    HalfOpen,    // åŠå¼€ï¼Œå°è¯•æ¢å¤
}

pub struct CircuitBreaker {
    state: Arc<RwLock<CircuitBreakerState>>,
    config: CircuitBreakerConfig,
}

#[derive(Debug)]
struct CircuitBreakerState {
    current_state: CircuitState,
    failure_count: u32,
    success_count: u32,
    last_failure_time: Option<Instant>,
    last_state_change: Instant,
}

#[derive(Debug, Clone)]
pub struct CircuitBreakerConfig {
    pub failure_threshold: u32,     // è¿ç»­å¤±è´¥æ¬¡æ•°é˜ˆå€¼
    pub success_threshold: u32,     // åŠå¼€çŠ¶æ€éœ€è¦çš„æˆåŠŸæ¬¡æ•°
    pub timeout: Duration,          // æ–­å¼€åç­‰å¾…æ—¶é—´
    pub half_open_timeout: Duration,
}

impl CircuitBreaker {
    pub fn new(config: CircuitBreakerConfig) -> Self {
        Self {
            state: Arc::new(RwLock::new(CircuitBreakerState {
                current_state: CircuitState::Closed,
                failure_count: 0,
                success_count: 0,
                last_failure_time: None,
                last_state_change: Instant::now(),
            })),
            config,
        }
    }

    pub async fn call<F, T, E>(&self, f: F) -> Result<T, CircuitBreakerError<E>>
    where
        F: Future<Output = Result<T, E>>,
    {
        // æ£€æŸ¥å½“å‰çŠ¶æ€
        {
            let state = self.state.read().await;
            match state.current_state {
                CircuitState::Open => {
                    // æ£€æŸ¥æ˜¯å¦å¯ä»¥è½¬åˆ°åŠå¼€çŠ¶æ€
                    if state.last_state_change.elapsed() >= self.config.timeout {
                        drop(state);
                        self.transition_to_half_open().await;
                    } else {
                        return Err(CircuitBreakerError::CircuitOpen);
                    }
                }
                _ => {}
            }
        }

        // æ‰§è¡Œè°ƒç”¨
        match f.await {
            Ok(result) => {
                self.on_success().await;
                Ok(result)
            }
            Err(e) => {
                self.on_failure().await;
                Err(CircuitBreakerError::CallFailed(e))
            }
        }
    }

    async fn on_success(&self) {
        let mut state = self.state.write().await;
        
        match state.current_state {
            CircuitState::HalfOpen => {
                state.success_count += 1;
                if state.success_count >= self.config.success_threshold {
                    // è½¬åˆ°å…³é—­çŠ¶æ€
                    state.current_state = CircuitState::Closed;
                    state.failure_count = 0;
                    state.success_count = 0;
                    state.last_state_change = Instant::now();
                    tracing::info!("Circuit breaker closed");
                }
            }
            CircuitState::Closed => {
                // é‡ç½®å¤±è´¥è®¡æ•°
                state.failure_count = 0;
            }
            _ => {}
        }
    }

    async fn on_failure(&self) {
        let mut state = self.state.write().await;
        
        state.failure_count += 1;
        state.last_failure_time = Some(Instant::now());

        match state.current_state {
            CircuitState::Closed => {
                if state.failure_count >= self.config.failure_threshold {
                    // è½¬åˆ°æ–­å¼€çŠ¶æ€
                    state.current_state = CircuitState::Open;
                    state.last_state_change = Instant::now();
                    tracing::warn!("Circuit breaker opened after {} failures", state.failure_count);
                }
            }
            CircuitState::HalfOpen => {
                // è½¬å›æ–­å¼€çŠ¶æ€
                state.current_state = CircuitState::Open;
                state.success_count = 0;
                state.last_state_change = Instant::now();
                tracing::warn!("Circuit breaker reopened");
            }
            _ => {}
        }
    }

    async fn transition_to_half_open(&self) {
        let mut state = self.state.write().await;
        state.current_state = CircuitState::HalfOpen;
        state.success_count = 0;
        state.failure_count = 0;
        state.last_state_change = Instant::now();
        tracing::info!("Circuit breaker transitioned to half-open");
    }

    pub async fn get_state(&self) -> CircuitState {
        self.state.read().await.current_state
    }
}

#[derive(Debug, thiserror::Error)]
pub enum CircuitBreakerError<E> {
    #[error("Circuit breaker is open")]
    CircuitOpen,
    
    #[error("Call failed: {0}")]
    CallFailed(E),
}
```

### é‡è¯•æœºåˆ¶

```rust
// src/reliability/retry.rs
use backoff::{ExponentialBackoff, backoff::Backoff};

pub struct RetryPolicy {
    pub max_retries: u32,
    pub initial_interval: Duration,
    pub max_interval: Duration,
    pub multiplier: f64,
    pub randomization_factor: f64,
}

impl Default for RetryPolicy {
    fn default() -> Self {
        Self {
            max_retries: 3,
            initial_interval: Duration::from_millis(100),
            max_interval: Duration::from_secs(10),
            multiplier: 2.0,
            randomization_factor: 0.1,
        }
    }
}

pub async fn retry_with_backoff<F, T, E>(
    policy: &RetryPolicy,
    mut operation: F,
) -> Result<T, E>
where
    F: FnMut() -> Fut<Output = Result<T, E>>,
{
    let mut backoff = ExponentialBackoff {
        current_interval: policy.initial_interval,
        initial_interval: policy.initial_interval,
        max_interval: policy.max_interval,
        multiplier: policy.multiplier,
        randomization_factor: policy.randomization_factor,
        ..Default::default()
    };

    let mut attempts = 0;

    loop {
        match operation().await {
            Ok(result) => {
                if attempts > 0 {
                    tracing::info!("Operation succeeded after {} retries", attempts);
                }
                return Ok(result);
            }
            Err(e) => {
                attempts += 1;
                
                if attempts >= policy.max_retries {
                    tracing::error!("Operation failed after {} attempts", attempts);
                    return Err(e);
                }

                if let Some(duration) = backoff.next_backoff() {
                    tracing::warn!(
                        "Operation failed (attempt {}), retrying in {:?}",
                        attempts,
                        duration
                    );
                    tokio::time::sleep(duration).await;
                } else {
                    return Err(e);
                }
            }
        }
    }
}
```

---

## æœ€ä½³å®è·µ

### 1. ç›‘æ§æŒ‡æ ‡

```rust
// å¿…é¡»ç›‘æ§çš„æ ¸å¿ƒæŒ‡æ ‡
pub fn register_core_metrics(registry: &Registry) {
    // ååé‡
    register_counter!(
        "otlp_spans_total",
        "Total number of spans processed"
    );

    register_counter!(
        "otlp_spans_exported_total",
        "Total number of spans successfully exported"
    );

    // å»¶è¿Ÿ
    register_histogram!(
        "otlp_export_duration_seconds",
        "Time spent exporting spans"
    );

    // é”™è¯¯
    register_counter!(
        "otlp_export_errors_total",
        "Total number of export errors"
    );

    // é˜Ÿåˆ—
    register_gauge!(
        "otlp_queue_length",
        "Current length of the span queue"
    );

    // é‡‡æ ·
    register_counter!(
        "otlp_spans_sampled_total",
        "Total number of sampled spans"
    );

    // èµ„æºä½¿ç”¨
    register_gauge!(
        "otlp_memory_usage_bytes",
        "Current memory usage"
    );

    register_gauge!(
        "otlp_cpu_usage_percent",
        "Current CPU usage percentage"
    );
}
```

### 2. é…ç½®ç®¡ç†

```yaml
# config/production.yaml
agent:
  buffer_capacity: 50000
  batch_size: 2000
  batch_timeout: 5s
  max_concurrent_exports: 10

sampling:
  base_rate: 0.05  # 5%
  sample_errors: true
  slow_threshold_ms: 1000
  
  # æœåŠ¡ç‰¹å®šé‡‡æ ·ç‡
  service_overrides:
    critical-service: 0.5   # 50%
    analytics-service: 0.01 # 1%

storage:
  hot:
    backend: elasticsearch
    retention_days: 7
  warm:
    backend: s3
    retention_days: 30
  cold:
    backend: glacier
    retention_days: 90

slo:
  availability:
    target: 0.999  # 99.9%
    window_days: 30
  latency:
    target_p99_ms: 10
    window_days: 7
```

### 3. è¿ç»´æ£€æŸ¥æ¸…å•

```text
æ—¥å¸¸æ£€æŸ¥ (æ¯å¤©):
â˜ æ£€æŸ¥é”™è¯¯é¢„ç®—æ¶ˆè€—
â˜ æ£€æŸ¥ P99 å»¶è¿Ÿ
â˜ æ£€æŸ¥å­˜å‚¨ä½¿ç”¨é‡
â˜ æŸ¥çœ‹å‘Šè­¦å†å²

æ¯å‘¨æ£€æŸ¥:
â˜ Review SLO è¾¾æˆæƒ…å†µ
â˜ åˆ†ææˆæœ¬è¶‹åŠ¿
â˜ æ£€æŸ¥å®¹é‡è§„åˆ’
â˜ æ›´æ–°é‡‡æ ·ç­–ç•¥

æ¯æœˆæ£€æŸ¥:
â˜ æ€§èƒ½åŸºå‡†æµ‹è¯•
â˜ æ•…éšœæ¼”ç»ƒ
â˜ å®¹é‡è¯„ä¼°
â˜ æˆæœ¬ä¼˜åŒ–Review
```

---

## æ€»ç»“

### æ¶æ„æ¼”è¿›è·¯å¾„

```text
Phase 1: åŸºç¡€éƒ¨ç½² (0-3ä¸ªæœˆ)
  âœ… å•ç‚¹éƒ¨ç½²
  âœ… åŸºç¡€ç›‘æ§
  âœ… å›ºå®šé‡‡æ ·

Phase 2: ä¼˜åŒ–æå‡ (3-6ä¸ªæœˆ)
  âœ… å¤šå±‚æ¶æ„
  âœ… æ™ºèƒ½é‡‡æ ·
  âœ… SLOç›‘æ§

Phase 3: è§„æ¨¡åŒ– (6-12ä¸ªæœˆ)
  âœ… å¤§è§„æ¨¡éƒ¨ç½²
  âœ… æˆæœ¬ä¼˜åŒ–
  âœ… è‡ªåŠ¨åŒ–è¿ç»´

Phase 4: æŒç»­æ”¹è¿› (12ä¸ªæœˆ+)
  âœ… AI/ML ä¼˜åŒ–
  âœ… è‡ªé€‚åº”ç³»ç»Ÿ
  âœ… é¢„æµ‹æ€§ç»´æŠ¤
```

### å…³é”®æˆåŠŸå› ç´ 

```text
âœ… æ˜ç¡®çš„ SLO ç›®æ ‡
âœ… å…¨é¢çš„ç›‘æ§ä½“ç³»
âœ… æ™ºèƒ½çš„é‡‡æ ·ç­–ç•¥
âœ… åˆ†å±‚çš„å­˜å‚¨æ¶æ„
âœ… è‡ªåŠ¨åŒ–çš„è¿ç»´æµç¨‹
âœ… æŒç»­çš„æˆæœ¬ä¼˜åŒ–
```

---

**æ–‡æ¡£ä½œè€…**: OTLP Rust Team  
**åˆ›å»ºæ—¥æœŸ**: 2025-10-08  
**è®¸å¯è¯**: MIT OR Apache-2.0  
**ç›¸å…³æ–‡æ¡£**:

- [Kuberneteséƒ¨ç½²æŒ‡å—](../19_å®¹å™¨åŒ–ä¸Kubernetes/01_Rust_OTLP_Kuberneteså®Œæ•´éƒ¨ç½²æŒ‡å—.md)
- [ç›‘æ§ä¸å‘Šè­¦](../20_ç›‘æ§ä¸å‘Šè­¦/01_Prometheus_Grafanaå®Œæ•´é›†æˆæŒ‡å—.md)
- [æ€§èƒ½åŸºå‡†æµ‹è¯•](../14_æ€§èƒ½ä¸åŸºå‡†æµ‹è¯•/02_Rust_OTLPæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæ•´æ¡†æ¶.md)
