# OpenTelemetry 故障排查指南

> **最后更新**: 2025年10月8日  
> **目标读者**: SRE、运维工程师、开发者

---

## 目录

- [OpenTelemetry 故障排查指南](#opentelemetry-故障排查指南)
  - [目录](#目录)
  - [1. 故障排查概述](#1-故障排查概述)
    - [1.1 排查流程](#11-排查流程)
    - [1.2 诊断工具](#12-诊断工具)
  - [2. 连接问题](#2-连接问题)
    - [2.1 无法连接Collector](#21-无法连接collector)
    - [2.2 TLS握手失败](#22-tls握手失败)
    - [2.3 认证失败](#23-认证失败)
  - [3. 数据丢失问题](#3-数据丢失问题)
    - [3.1 Span丢失](#31-span丢失)
    - [3.2 部分Trace缺失](#32-部分trace缺失)
    - [3.3 采样问题](#33-采样问题)
  - [4. 性能问题](#4-性能问题)
    - [4.1 应用延迟增加](#41-应用延迟增加)
    - [4.2 Collector CPU高](#42-collector-cpu高)
    - [4.3 内存泄漏](#43-内存泄漏)
  - [5. 数据质量问题](#5-数据质量问题)
    - [5.1 属性缺失](#51-属性缺失)
    - [5.2 错误的Trace关系](#52-错误的trace关系)
    - [5.3 时间戳错误](#53-时间戳错误)
  - [6. Collector问题](#6-collector问题)
    - [6.1 Collector崩溃](#61-collector崩溃)
    - [6.2 队列积压](#62-队列积压)
    - [6.3 导出失败](#63-导出失败)
  - [7. Kubernetes特有问题](#7-kubernetes特有问题)
    - [7.1 Pod无法连接Collector](#71-pod无法连接collector)
    - [7.2 Resource属性缺失](#72-resource属性缺失)
    - [7.3 Service Mesh问题](#73-service-mesh问题)
  - [8. 后端问题](#8-后端问题)
    - [8.1 Jaeger查询慢](#81-jaeger查询慢)
    - [8.2 Prometheus基数爆炸](#82-prometheus基数爆炸)
    - [8.3 存储空间不足](#83-存储空间不足)
  - [9. 调试技巧](#9-调试技巧)
  - [10. 监控与告警](#10-监控与告警)
  - [11. 故障案例库](#11-故障案例库)
  - [12. 参考资源](#12-参考资源)

---

## 1. 故障排查概述

### 1.1 排查流程

**标准排查流程**：

```text
1. 现象识别 (Symptom Identification)
   - 用户报告
   - 监控告警
   - 日志异常

2. 问题定位 (Problem Localization)
   - 确定故障组件
   - 缩小范围
   - 收集证据

3. 根因分析 (Root Cause Analysis)
   - 分析日志
   - 检查配置
   - 复现问题

4. 解决方案 (Solution)
   - 临时修复
   - 永久修复
   - 验证修复

5. 预防措施 (Prevention)
   - 更新文档
   - 改进监控
   - 自动化测试

示例：Span丢失问题
1. 现象：Jaeger查询不到某些Span
2. 定位：检查SDK → Collector → Jaeger链路
3. 根因：Collector队列满，拒绝新数据
4. 解决：增大队列，增加Collector副本
5. 预防：配置队列长度告警
```

### 1.2 诊断工具

**常用工具**：

```bash
# 1. 网络诊断
ping collector.example.com
telnet collector.example.com 4317
curl -v http://collector.example.com:4318/v1/traces

# 2. DNS诊断
nslookup collector.example.com
dig collector.example.com

# 3. TLS诊断
openssl s_client -connect collector.example.com:4317
openssl s_client -connect collector.example.com:4317 -showcerts

# 4. Collector状态
curl http://localhost:13133/  # 健康检查
curl http://localhost:8888/metrics  # Prometheus指标

# 5. Kubernetes诊断
kubectl get pods -n observability
kubectl logs otel-collector-xxx -n observability
kubectl describe pod otel-collector-xxx -n observability
kubectl exec -it otel-collector-xxx -n observability -- /bin/sh

# 6. 抓包分析
tcpdump -i any port 4317 -w capture.pcap
wireshark capture.pcap
```

---

## 2. 连接问题

### 2.1 无法连接Collector

**症状**：

```text
应用日志:
error: failed to connect to collector: context deadline exceeded
error: rpc error: code = Unavailable desc = connection refused
```

**排查步骤**：

```bash
# 1. 检查Collector是否运行
kubectl get pods -n observability
# 状态应该是Running

# 2. 检查端口
kubectl get svc otel-collector -n observability
# 确认4317/4318端口

# 3. 测试连接
telnet otel-collector.observability.svc.cluster.local 4317

# 4. 检查Network Policy
kubectl get networkpolicy -n observability
kubectl describe networkpolicy otel-collector-policy

# 5. 检查DNS
nslookup otel-collector.observability.svc.cluster.local

# 6. 查看Collector日志
kubectl logs otel-collector-xxx -n observability
```

**常见原因与解决**：

```text
原因1: Collector未运行
解决: 
  kubectl rollout restart deployment/otel-collector -n observability

原因2: 端口配置错误
解决:
  检查应用配置的端口是否匹配
  SDK: otlptracegrpc.WithEndpoint("collector:4317")
  Service: port: 4317

原因3: Network Policy阻止
解决:
  修改Network Policy允许来源Pod

原因4: Service不存在
解决:
  创建Service:
  kubectl expose deployment otel-collector \
    --port=4317 --target-port=4317 \
    --name=otel-collector -n observability

原因5: DNS解析失败
解决:
  使用IP地址: "10.0.1.5:4317"
  或修复DNS配置
```

### 2.2 TLS握手失败

**症状**：

```text
应用日志:
error: tls: failed to verify certificate
error: x509: certificate signed by unknown authority
error: x509: certificate has expired
```

**排查步骤**：

```bash
# 1. 验证证书
openssl s_client -connect collector:4317 -showcerts

# 2. 检查证书有效期
openssl x509 -in server.crt -noout -dates

# 3. 检查证书链
openssl verify -CAfile ca.crt server.crt

# 4. 检查SAN (Subject Alternative Name)
openssl x509 -in server.crt -noout -text | grep -A 1 "Subject Alternative Name"
```

**常见原因与解决**：

```text
原因1: CA证书不匹配
症状: x509: certificate signed by unknown authority
解决:
  确保客户端加载正确的CA证书
  tlsConfig.RootCAs = caCertPool

原因2: 证书过期
症状: x509: certificate has expired
解决:
  重新生成证书
  使用cert-manager自动续期

原因3: 主机名不匹配
症状: x509: certificate is valid for X, not Y
解决:
  使用正确的主机名
  或配置SAN包含所有主机名

原因4: 时钟偏移
症状: certificate is not yet valid
解决:
  同步时钟: ntpdate ntp.ubuntu.com

解决方案示例:
# Go SDK配置正确的CA
caCert, _ := os.ReadFile("/path/to/ca.crt")
caCertPool := x509.NewCertPool()
caCertPool.AppendCertsFromPEM(caCert)

tlsConfig := &tls.Config{
    RootCAs:    caCertPool,
    ServerName: "collector.example.com",  // 匹配证书CN
}
```

### 2.3 认证失败

**症状**：

```text
应用日志:
error: rpc error: code = Unauthenticated desc = invalid credentials
error: authentication failed

Collector日志:
authentication failed for request from 10.0.1.5
```

**排查步骤**：

```bash
# 1. 检查Collector认证配置
kubectl get configmap otel-collector-config -o yaml

# 2. 测试认证
curl -H "Authorization: Bearer token123" \
  http://collector:4318/v1/traces

# 3. 检查Token
echo $OTEL_EXPORTER_OTLP_HEADERS
```

**常见原因与解决**：

```text
原因1: Token错误
解决:
  检查Token是否正确
  OTEL_EXPORTER_OTLP_HEADERS="authorization=Bearer correct-token"

原因2: Token过期
解决:
  使用OAuth2自动刷新
  或手动更新Token

原因3: 认证方式不匹配
解决:
  确保客户端和服务器使用相同认证方式
  Bearer Token / mTLS / OAuth2

示例配置:
// Go SDK
ctx := metadata.AppendToOutgoingContext(
    context.Background(),
    "authorization", "Bearer "+token,
)
```

---

## 3. 数据丢失问题

### 3.1 Span丢失

**症状**：

```text
- Jaeger查询不到某些Span
- Trace不完整
- 监控显示数据量下降
```

**排查步骤**：

```bash
# 1. 检查SDK日志
# 查找export错误

# 2. 检查Collector指标
curl http://collector:8888/metrics | grep refused
# otelcol_receiver_refused_spans

# 3. 检查Collector日志
kubectl logs otel-collector-xxx | grep -i "refuse\|drop\|fail"

# 4. 检查队列
curl http://collector:8888/metrics | grep queue_size
# otelcol_exporter_queue_size

# 5. 检查后端
# Jaeger: 检查存储容量
# Elasticsearch: 检查索引状态
```

**常见原因与解决**：

```text
原因1: Collector队列满
症状: otelcol_receiver_refused_spans 增加
解决:
  # 增大队列
  exporters:
    otlp:
      sending_queue:
        queue_size: 10000  # 增大

  # 增加Collector副本
  kubectl scale deployment otel-collector --replicas=5

原因2: 内存限制
症状: memory_limiter拒绝数据
解决:
  # 增大内存限制
  processors:
    memory_limiter:
      limit_mib: 2048  # 增大

原因3: 采样丢弃
症状: 正常，按采样率丢弃
解决:
  # 如果需要更多数据，提高采样率
  trace.TraceIDRatioBased(0.5)  # 从0.1提高到0.5

原因4: 导出失败
症状: otelcol_exporter_send_failed_spans 增加
解决:
  # 检查后端连接
  # 启用重试
  exporters:
    otlp:
      retry_on_failure:
        enabled: true
        max_elapsed_time: 300s

原因5: 批处理丢弃
症状: 批次超过最大大小
解决:
  processors:
    batch:
      send_batch_max_size: 20000  # 增大
```

### 3.2 部分Trace缺失

**症状**：

```text
- Trace只显示部分Span
- 父子关系断裂
- 跨服务的Span缺失
```

**排查步骤**：

```bash
# 1. 检查采样决策
# 查看Span的sampled flag

# 2. 检查TraceContext传播
# 确认HTTP header包含traceparent

# 3. 检查不同服务的采样配置
# 确保一致的采样策略

# 4. 检查时间窗口
# Tail sampling需要等待trace完成
```

**常见原因与解决**：

```text
原因1: 采样不一致
症状: 父Span采样，子Span未采样
解决:
  # 使用ParentBased采样器
  sampler := trace.ParentBased(
      trace.TraceIDRatioBased(0.1),
  )

原因2: TraceContext未传播
症状: 跨服务Span没有关联
解决:
  # 确保传播TraceContext
  // HTTP客户端
  req = req.WithContext(ctx)  // 包含TraceContext
  otelhttp.NewTransport(http.DefaultTransport)

原因3: 时钟不同步
症状: Span时间戳错误
解决:
  # 同步时钟
  ntpdate ntp.ubuntu.com

原因4: Tail sampling等待不足
症状: Trace未完成就采样
解决:
  processors:
    tail_sampling:
      decision_wait: 30s  # 增加等待时间
```

### 3.3 采样问题

**症状**：

```text
- 采样率与预期不符
- 重要Trace被丢弃
- 所有Trace都被采样(0%丢弃)
```

**排查步骤**：

```bash
# 1. 检查采样配置
# SDK和Collector的采样配置

# 2. 计算实际采样率
# accepted / (accepted + refused)

# 3. 检查采样决策逻辑
# 查看tail_sampling策略
```

**解决方案**：

```yaml
# 智能采样配置
processors:
  tail_sampling:
    decision_wait: 10s
    num_traces: 100000
    policies:
      # 1. 总是采样错误
      - name: errors
        type: status_code
        status_code:
          status_codes: [ERROR]
      
      # 2. 总是采样慢请求
      - name: slow
        type: latency
        latency:
          threshold_ms: 1000
      
      # 3. 概率采样其他
      - name: probabilistic
        type: probabilistic
        probabilistic:
          sampling_percentage: 10

# 预期结果:
# 错误: 100%采样
# 慢请求: 100%采样
# 快速成功: 10%采样
```

---

## 4. 性能问题

### 4.1 应用延迟增加

**症状**：

```text
- API响应时间增加
- p99延迟升高
- 用户投诉慢
```

**排查步骤**：

```bash
# 1. 测量可观测性开销
# 对比开启前后性能

# 2. 检查同步导出
# 确保使用BatchSpanProcessor

# 3. 检查属性数量
# 过多属性导致序列化慢

# 4. 分析火焰图
go tool pprof http://localhost:6060/debug/pprof/profile
```

**常见原因与解决**：

```text
原因1: 使用SimpleSpanProcessor
症状: 每个Span立即同步导出
解决:
  # 改用BatchSpanProcessor
  processor := trace.NewBatchSpanProcessor(exporter)

原因2: 属性过多
症状: 序列化和网络传输慢
解决:
  # 限制属性数量
  trace.WithSpanLimits(trace.SpanLimits{
      AttributeCountLimit: 64,  # 从128降低
  })

原因3: 网络慢
症状: 导出阻塞
解决:
  # 使用本地Collector (sidecar/agent)
  # 减少网络跳转

原因4: CPU密集型操作
症状: 序列化占用CPU
解决:
  # 降低采样率
  sampler := trace.TraceIDRatioBased(0.05)  # 从0.1降低

原因5: 日志级别过高
症状: 大量DEBUG日志
解决:
  telemetry:
    logs:
      level: info  # 从debug改为info

性能基准:
无可观测性: 100 req/s, 10ms p99
有可观测性(优化后): 98 req/s, 11ms p99 (2%开销 ✅)
有可观测性(未优化): 80 req/s, 25ms p99 (20%开销 ❌)
```

### 4.2 Collector CPU高

**症状**：

```text
- Collector CPU > 80%
- 延迟增加
- 偶尔拒绝数据
```

**排查步骤**：

```bash
# 1. 查看CPU使用
kubectl top pod otel-collector-xxx

# 2. 分析CPU占用
kubectl exec -it otel-collector-xxx -- pprof \
  http://localhost:1777/debug/pprof/profile

# 3. 检查处理器配置
# 是否有CPU密集型processor

# 4. 检查吞吐量
curl http://collector:8888/metrics | grep accepted
```

**常见原因与解决**：

```text
原因1: Transform processor
症状: 复杂转换占用CPU
解决:
  # 移除或简化transform
  # 或在SDK层处理

原因2: Tail sampling
症状: 缓存大量trace
解决:
  processors:
    tail_sampling:
      num_traces: 50000  # 降低from 100000
      decision_wait: 5s  # 降低from 10s

原因3: 批次太小
症状: 频繁序列化
解决:
  processors:
    batch:
      send_batch_size: 8192  # 增大from 512

原因4: 压缩级别高
症状: gzip压缩占用CPU
解决:
  # 使用默认压缩级别
  # 或使用snappy压缩

原因5: 吞吐量过高
症状: 单实例处理不过来
解决:
  # 水平扩展
  kubectl scale deployment otel-collector --replicas=5
  
  # 配置负载均衡
  apiVersion: v1
  kind: Service
  metadata:
    name: otel-collector
  spec:
    type: LoadBalancer
    ports:
    - port: 4317
```

### 4.3 内存泄漏

**症状**：

```text
- Collector内存持续增长
- 最终OOM Killed
- Restart次数增加
```

**排查步骤**：

```bash
# 1. 查看内存使用
kubectl top pod otel-collector-xxx

# 2. 分析堆内存
kubectl exec -it otel-collector-xxx -- \
  curl http://localhost:1777/debug/pprof/heap > heap.prof
go tool pprof heap.prof

# 3. 检查goroutine泄漏
kubectl exec -it otel-collector-xxx -- \
  curl http://localhost:1777/debug/pprof/goroutine

# 4. 查看事件
kubectl describe pod otel-collector-xxx | grep -A 5 Events
```

**常见原因与解决**：

```text
原因1: 未配置memory_limiter
症状: 无内存保护
解决:
  processors:
    memory_limiter:
      check_interval: 1s
      limit_mib: 1536  # 容器2GB的75%
      spike_limit_mib: 384

原因2: tail_sampling缓存过大
症状: 缓存大量未完成trace
解决:
  processors:
    tail_sampling:
      num_traces: 50000  # 降低
      decision_wait: 5s  # 缩短

原因3: 队列过大
症状: 队列缓冲大量数据
解决:
  exporters:
    otlp:
      sending_queue:
        queue_size: 5000  # 降低from 10000

原因4: 导出慢
症状: 数据积压
解决:
  # 检查后端性能
  # 增加导出并发
  exporters:
    otlp:
      sending_queue:
        num_consumers: 20  # 增加

原因5: 实际需要更多内存
症状: 正常工作负载
解决:
  # 增加资源限制
  resources:
    limits:
      memory: 4Gi  # 从2Gi增加
```

---

## 5. 数据质量问题

### 5.1 属性缺失

**症状**：

```text
- Span缺少预期属性
- Resource属性为空
- http.url等关键属性缺失
```

**排查步骤**：

```bash
# 1. 检查SDK配置
# 确认属性添加代码

# 2. 检查Processor
# 是否有删除属性的配置

# 3. 检查属性限制
# AttributeCountLimit

# 4. 查看原始数据
# 使用logging exporter
```

**常见原因与解决**：

```text
原因1: 未添加属性
症状: 代码中未设置
解决:
  span.SetAttributes(
      semconv.HTTPMethod("GET"),
      semconv.HTTPRoute("/api/users/:id"),
  )

原因2: Resource未配置
症状: service.name等缺失
解决:
  res, _ := resource.New(ctx,
      resource.WithAttributes(
          semconv.ServiceName("my-service"),
      ),
  )

原因3: 属性被删除
症状: Processor删除了
解决:
  # 检查attributes processor
  processors:
    attributes:
      actions:
        - key: sensitive_data
          action: delete  # 确认是否误删

原因4: 属性数量超限
症状: 超过AttributeCountLimit
解决:
  # 增加限制或减少属性
  trace.WithSpanLimits(trace.SpanLimits{
      AttributeCountLimit: 256,  # 增加
  })

原因5: 自动instrumentation未启用
症状: 缺少HTTP/DB属性
解决:
  # 使用instrumentation库
  import "go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp"
  http.Handle("/", otelhttp.NewHandler(handler, "server"))
```

### 5.2 错误的Trace关系

**症状**：

```text
- Span的parent_span_id错误
- Trace中Span没有关联
- 显示为独立Trace
```

**排查步骤**：

```bash
# 1. 检查Context传播
# 确认context正确传递

# 2. 检查TraceID和SpanID
# 查看是否一致

# 3. 检查propagator配置
# 确认使用W3C Trace Context
```

**常见原因与解决**：

```text
原因1: Context未传播
症状: 子Span没有parent_span_id
解决:
  # 正确传播Context
  ctx, span := tracer.Start(ctx, "parent")
  defer span.End()
  
  // 传递ctx给子函数
  childFunc(ctx)  // ✅ 传递ctx
  
  func childFunc(ctx context.Context) {
      _, childSpan := tracer.Start(ctx, "child")
      defer childSpan.End()
  }

原因2: 跨goroutine未传递Context
症状: goroutine中的Span断裂
解决:
  go func(ctx context.Context) {  // ✅ 传递ctx
      _, span := tracer.Start(ctx, "async")
      defer span.End()
  }(ctx)

原因3: 跨服务传播失败
症状: HTTP调用后Trace断裂
解决:
  # 使用instrumentation
  client := http.Client{
      Transport: otelhttp.NewTransport(http.DefaultTransport),
  }

原因4: Propagator配置错误
症状: TraceContext header格式不对
解决:
  # 配置W3C propagator
  otel.SetTextMapPropagator(
      propagation.NewCompositeTextMapPropagator(
          propagation.TraceContext{},
          propagation.Baggage{},
      ),
  )

原因5: 自定义传播逻辑错误
症状: 手动提取/注入TraceContext错误
解决:
  # 使用标准propagator
  // 注入
  otel.GetTextMapPropagator().Inject(ctx, carrier)
  // 提取
  ctx = otel.GetTextMapPropagator().Extract(ctx, carrier)
```

### 5.3 时间戳错误

**症状**：

```text
- Span duration为负数
- Span开始时间在结束时间之后
- 时间戳为0或未来时间
```

**常见原因与解决**：

```text
原因1: 时钟不同步
症状: 不同机器时钟偏移
解决:
  # 同步时钟 (所有机器)
  sudo ntpdate ntp.ubuntu.com
  
  # 或启用NTP服务
  sudo systemctl enable systemd-timesyncd
  sudo systemctl start systemd-timesyncd

原因2: 时区问题
症状: 时间戳时区不一致
解决:
  # 使用UTC时间
  time.Now().UTC()

原因3: 手动设置时间戳错误
症状: 代码中错误设置
解决:
  # 使用SDK自动设置
  // ❌ 错误
  span.SetStartTime(wrongTime)
  
  // ✅ 正确: 不手动设置，SDK自动处理
  _, span := tracer.Start(ctx, "operation")

原因4: 容器时钟
症状: Docker/K8s容器时钟不准
解决:
  # 挂载主机时间
  volumes:
    - /etc/localtime:/etc/localtime:ro
```

---

## 6. Collector问题

### 6.1 Collector崩溃

**症状**：

```text
- Pod状态: CrashLoopBackOff
- 频繁重启
- 日志显示panic
```

**排查步骤**：

```bash
# 1. 查看Pod状态
kubectl get pod otel-collector-xxx

# 2. 查看日志
kubectl logs otel-collector-xxx --previous

# 3. 查看事件
kubectl describe pod otel-collector-xxx

# 4. 检查配置
kubectl get configmap otel-collector-config -o yaml
```

**常见原因与解决**：

```text
原因1: 配置错误
症状: failed to load config
解决:
  # 验证配置
  otelcol validate --config=config.yaml
  
  # 检查YAML语法
  yamllint config.yaml

原因2: OOM Killed
症状: 内存超限
解决:
  # 增加内存限制
  resources:
    limits:
      memory: 2Gi

  # 配置memory_limiter
  processors:
    memory_limiter:
      limit_mib: 1536

原因3: 无效的exporter配置
症状: failed to create exporter
解决:
  # 检查exporter endpoint
  exporters:
    otlp:
      endpoint: "backend:4317"  # 确认可访问

原因4: TLS配置错误
症状: failed to load TLS config
解决:
  # 确认证书文件存在
  ls -la /certs/

原因5: 端口冲突
症状: failed to start: address already in use
解决:
  # 更改端口或查找占用进程
  netstat -tulpn | grep 4317
```

### 6.2 队列积压

**症状**：

```text
- otelcol_exporter_queue_size持续增长
- 延迟增加
- 最终开始拒绝数据
```

**排查步骤**：

```bash
# 1. 检查队列大小
curl http://collector:8888/metrics | grep queue_size

# 2. 检查导出速率
curl http://collector:8888/metrics | grep exporter_sent

# 3. 检查导出失败
curl http://collector:8888/metrics | grep exporter_send_failed

# 4. 检查后端健康
curl http://backend:health
```

**解决方案**：

```text
解决1: 增加导出并发
exporters:
  otlp:
    sending_queue:
      num_consumers: 20  # 增加并发

解决2: 增大队列
exporters:
  otlp:
    sending_queue:
      queue_size: 10000  # 增大缓冲

解决3: 增加Collector副本
kubectl scale deployment otel-collector --replicas=5

解决4: 优化批处理
processors:
  batch:
    send_batch_size: 8192  # 增大批次

解决5: 检查后端性能
# 确保后端能处理负载
# 扩展后端或优化配置
```

### 6.3 导出失败

**症状**：

```text
- otelcol_exporter_send_failed_spans增加
- 日志显示export error
- 数据无法到达后端
```

**常见原因与解决**：

```text
原因1: 后端不可达
症状: connection refused
解决:
  # 检查后端健康
  curl http://backend:4317/health
  
  # 检查网络连接
  telnet backend 4317

原因2: 认证失败
症状: authentication failed
解决:
  # 检查认证配置
  exporters:
    otlp:
      headers:
        authorization: "Bearer correct-token"

原因3: 超时
症状: context deadline exceeded
解决:
  exporters:
    otlp:
      timeout: 60s  # 增加超时

原因4: 负载过高
症状: 后端返回503
解决:
  # 扩展后端
  # 或增加Collector分片

原因5: TLS错误
症状: tls handshake failure
解决:
  # 检查证书配置
  exporters:
    otlp:
      tls:
        ca_file: /path/to/ca.crt
```

---

## 7. Kubernetes特有问题

### 7.1 Pod无法连接Collector

**症状**：

```text
- 应用Pod无法连接到Collector
- DNS解析失败
- Network Policy阻止
```

**解决方案**：

```yaml
# 1. 创建Service
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
spec:
  selector:
    app: otel-collector
  ports:
  - name: grpc
    port: 4317
    targetPort: 4317
  - name: http
    port: 4318
    targetPort: 4318

# 2. 配置Network Policy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-otel
  namespace: observability
spec:
  podSelector:
    matchLabels:
      app: otel-collector
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector: {}  # 允许所有namespace
    ports:
    - port: 4317
    - port: 4318

# 3. 应用连接
# 使用FQDN
endpoint: "otel-collector.observability.svc.cluster.local:4317"
```

### 7.2 Resource属性缺失

**症状**：

```text
- k8s.pod.name等属性缺失
- k8s.namespace.name为空
```

**解决方案**：

```yaml
# 1. 使用Downward API
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      - name: app
        env:
        - name: K8S_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: K8S_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: "k8s.pod.name=$(K8S_POD_NAME),k8s.namespace.name=$(K8S_NAMESPACE)"

# 2. 使用k8sattributes processor (Collector)
processors:
  k8sattributes:
    auth_type: "serviceAccount"
    passthrough: false
    extract:
      metadata:
        - k8s.pod.name
        - k8s.pod.uid
        - k8s.namespace.name
        - k8s.node.name
      labels:
        - tag_name: app.label
          key: app
          from: pod
```

### 7.3 Service Mesh问题

**症状**：

```text
- Istio/Linkerd环境下连接失败
- mTLS冲突
- TraceContext传播问题
```

**解决方案**：

```yaml
# Istio环境
# 1. 允许mTLS
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: otel-collector
spec:
  selector:
    matchLabels:
      app: otel-collector
  mtls:
    mode: PERMISSIVE  # 允许plaintext

# 2. 配置DestinationRule
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: otel-collector
spec:
  host: otel-collector
  trafficPolicy:
    tls:
      mode: DISABLE  # 禁用Istio mTLS for Collector

# 3. TraceContext传播
# Istio自动注入traceparent header
# 确保应用使用compatible propagator
```

---

## 8. 后端问题

### 8.1 Jaeger查询慢

**症状**：

```text
- UI查询超时
- 查询响应时间>10秒
```

**解决方案**：

```text
1. 添加索引
CREATE INDEX idx_service_operation 
ON jaeger_spans(service_name, operation_name);

2. 限制查询范围
# 查询最近1小时，不要查询1个月

3. 使用服务+操作过滤
# 不要只按trace_id查询

4. 扩展Elasticsearch
# 增加节点，提高查询性能

5. 使用Cassandra
# 替代Elasticsearch，更适合时序数据
```

### 8.2 Prometheus基数爆炸

**症状**：

```text
- Prometheus内存暴涨
- 查询超时
- 抓取失败
```

**解决方案**：

```yaml
# 1. 控制标签基数
processors:
  attributes:
    actions:
      # 参数化URL
      - key: http.url
        pattern: ^(.*)/users/[0-9]+(.*)$
        replacement: ${1}/users/:id${2}
        action: update
      
      # 删除高基数标签
      - key: request.id
        action: delete

# 2. 使用直方图而非计数器
# ❌ 每个user_id一个metric
counter{user_id="123"}

# ✅ 使用histogram
http_request_duration_seconds_bucket

# 3. 配置Prometheus recording rules
# 预聚合高基数metrics
```

### 8.3 存储空间不足

**症状**：

```text
- 磁盘使用率>90%
- 写入失败
- 查询异常
```

**解决方案**：

```text
1. 缩短保留期
# Jaeger: 7天 → 3天
# Elasticsearch: 30天 → 7天

2. 增加采样率
# 降低数据量
tail_sampling: 10% → 5%

3. 启用压缩
# Elasticsearch启用best_compression

4. 冷热分离
# 热数据SSD，冷数据HDD/S3

5. 增加存储
# 扩容存储卷

6. 清理旧数据
# Elasticsearch: curator清理
curator delete indices --older-than 7 --time-unit days
```

---

## 9. 调试技巧

**通用调试技巧**：

```bash
# 1. 启用DEBUG日志
export OTEL_LOG_LEVEL=debug

# 2. 使用logging exporter
exporters:
  logging:
    loglevel: debug
    sampling_initial: 100  # 前100条全部打印

# 3. 抓取原始数据
tcpdump -i any port 4317 -w trace.pcap

# 4. 使用telemetrygen生成测试数据
telemetrygen traces --otlp-insecure --rate 10 --duration 60s

# 5. 检查健康状态
curl http://localhost:13133/

# 6. 查看zpages
curl http://localhost:55679/debug/tracez

# 7. 分析pprof
go tool pprof http://localhost:1777/debug/pprof/profile
```

---

## 10. 监控与告警

**推荐告警规则**：

```yaml
groups:
- name: otel_alerts
  rules:
    - alert: CollectorDown
      expr: up{job="otel-collector"} == 0
      for: 1m
    
    - alert: HighRefuseRate
      expr: rate(otelcol_receiver_refused_spans[5m]) > 100
      for: 5m
    
    - alert: ExportFailure
      expr: rate(otelcol_exporter_send_failed_spans[5m]) > 10
      for: 5m
    
    - alert: QueueBacklog
      expr: otelcol_exporter_queue_size > 8000
      for: 5m
    
    - alert: HighMemory
      expr: process_resident_memory_bytes > 1.5e9
      for: 5m
```

---

## 11. 故障案例库

**案例1: 生产环境数据丢失**:

```text
现象: Jaeger查询不到近1小时的trace
原因: Collector OOM被kill
解决:
  1. 配置memory_limiter
  2. 增加内存限制到2GB
  3. 减少tail_sampling缓存
教训: 必须配置memory_limiter保护
```

**案例2: 应用延迟暴涨**:

```text
现象: API p99延迟从50ms→500ms
原因: 使用SimpleSpanProcessor同步导出
解决:
  改用BatchSpanProcessor
教训: 始终使用批处理
```

**案例3: Prometheus内存溢出**:

```text
现象: Prometheus OOM
原因: http.url标签包含用户ID，基数百万
解决:
  使用http.route="/users/:id"代替完整URL
教训: 严格控制标签基数
```

---

## 12. 参考资源

- **故障排查**: <https://opentelemetry.io/docs/collector/troubleshooting/>
- **常见问题**: <https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/troubleshooting.md>
- **社区论坛**: <https://cloud-native.slack.com> (#opentelemetry)

---

**文档状态**: ✅ 完成  
**审核状态**: 待审核  
**相关文档**: [Collector架构](../04_核心组件/02_Collector架构.md), [性能优化](../05_采样与性能/02_性能优化实践.md)
