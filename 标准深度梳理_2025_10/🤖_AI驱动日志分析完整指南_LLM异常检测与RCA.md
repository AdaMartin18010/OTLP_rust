# ğŸ¤– AI é©±åŠ¨æ—¥å¿—åˆ†æå®Œæ•´æŒ‡å— - LLM å¼‚å¸¸æ£€æµ‹ä¸æ ¹å› åˆ†æ

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025å¹´10æœˆ9æ—¥  
> **æ–‡æ¡£ç±»å‹**: P0 ä¼˜å…ˆçº§ - AI/ML é©±åŠ¨å¯è§‚æµ‹æ€§  
> **é¢„ä¼°ç¯‡å¹…**: 3,500+ è¡Œ  
> **æŠ€æœ¯æ ˆ**: GPT-4 / Claude 3 / Llama 3 + DoWhy + NetworkX + PyTorch  
> **ç›®æ ‡**: åˆ©ç”¨ AI/ML å®ç°æ™ºèƒ½æ—¥å¿—åˆ†æã€å¼‚å¸¸æ£€æµ‹ã€æ ¹å› åˆ†æ

---

## ğŸ“‹ ç›®å½•

- [ğŸ¤– AI é©±åŠ¨æ—¥å¿—åˆ†æå®Œæ•´æŒ‡å— - LLM å¼‚å¸¸æ£€æµ‹ä¸æ ¹å› åˆ†æ](#-ai-é©±åŠ¨æ—¥å¿—åˆ†æå®Œæ•´æŒ‡å—---llm-å¼‚å¸¸æ£€æµ‹ä¸æ ¹å› åˆ†æ)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ç¬¬ä¸€éƒ¨åˆ†: LLM æ—¥å¿—åˆ†æåŸç†](#ç¬¬ä¸€éƒ¨åˆ†-llm-æ—¥å¿—åˆ†æåŸç†)
    - [1.1 ä¸ºä»€ä¹ˆéœ€è¦ LLM é©±åŠ¨æ—¥å¿—åˆ†æ?](#11-ä¸ºä»€ä¹ˆéœ€è¦-llm-é©±åŠ¨æ—¥å¿—åˆ†æ)
      - [ä¼ ç»Ÿæ—¥å¿—åˆ†æçš„å›°å¢ƒ](#ä¼ ç»Ÿæ—¥å¿—åˆ†æçš„å›°å¢ƒ)
      - [æœ€æ–°ç ”ç©¶è¿›å±• (2024-2025)](#æœ€æ–°ç ”ç©¶è¿›å±•-2024-2025)
    - [1.2 LLM æ—¥å¿—åˆ†ææ¶æ„](#12-llm-æ—¥å¿—åˆ†ææ¶æ„)
    - [1.3 Prompt Engineering æŠ€å·§](#13-prompt-engineering-æŠ€å·§)
      - [System Prompt (è§’è‰²å®šä¹‰)](#system-prompt-è§’è‰²å®šä¹‰)
      - [Few-shot Examples (ç¤ºä¾‹å­¦ä¹ )](#few-shot-examples-ç¤ºä¾‹å­¦ä¹ )
      - [Chain-of-Thought (æ€ç»´é“¾)](#chain-of-thought-æ€ç»´é“¾)
  - [ç¬¬äºŒéƒ¨åˆ†: å¼‚å¸¸æ£€æµ‹å®æˆ˜](#ç¬¬äºŒéƒ¨åˆ†-å¼‚å¸¸æ£€æµ‹å®æˆ˜)
    - [2.1 å®Œæ•´ Python å®ç°](#21-å®Œæ•´-python-å®ç°)
    - [2.2 é›†æˆ OTLP Logs](#22-é›†æˆ-otlp-logs)
  - [ç¬¬ä¸‰éƒ¨åˆ†: æ ¹å› åˆ†æ (RCA)](#ç¬¬ä¸‰éƒ¨åˆ†-æ ¹å› åˆ†æ-rca)
    - [3.1 å¤šç»´åº¦æ ¹å› åˆ†æ](#31-å¤šç»´åº¦æ ¹å› åˆ†æ)
  - [ç¬¬å››éƒ¨åˆ†: è‡ªç„¶è¯­è¨€æŸ¥è¯¢ (NLP Query)](#ç¬¬å››éƒ¨åˆ†-è‡ªç„¶è¯­è¨€æŸ¥è¯¢-nlp-query)
    - [4.1 Log Search with Natural Language](#41-log-search-with-natural-language)
    - [4.2 æ„å»ºæ—¥å¿—çŸ¥è¯†å›¾è°±](#42-æ„å»ºæ—¥å¿—çŸ¥è¯†å›¾è°±)
  - [ç¬¬äº”éƒ¨åˆ†: æˆæœ¬ä¼˜åŒ–ä¸ç”Ÿäº§éƒ¨ç½²](#ç¬¬äº”éƒ¨åˆ†-æˆæœ¬ä¼˜åŒ–ä¸ç”Ÿäº§éƒ¨ç½²)
    - [5.1 LLM æˆæœ¬ä¼˜åŒ–ç­–ç•¥](#51-llm-æˆæœ¬ä¼˜åŒ–ç­–ç•¥)
    - [5.2 è‡ªæ‰˜ç®¡ LLM (Llama 3 / Mistral)](#52-è‡ªæ‰˜ç®¡-llm-llama-3--mistral)
    - [5.3 ç”Ÿäº§ç¯å¢ƒå®Œæ•´éƒ¨ç½²](#53-ç”Ÿäº§ç¯å¢ƒå®Œæ•´éƒ¨ç½²)
  - [ç¬¬å…­éƒ¨åˆ†: å®Œæ•´ç”Ÿäº§æ¡ˆä¾‹](#ç¬¬å…­éƒ¨åˆ†-å®Œæ•´ç”Ÿäº§æ¡ˆä¾‹)
    - [6.1 ç”µå•†å¹³å° - å…¨é“¾è·¯æ—¥å¿—æ™ºèƒ½åˆ†æ](#61-ç”µå•†å¹³å°---å…¨é“¾è·¯æ—¥å¿—æ™ºèƒ½åˆ†æ)
      - [åœºæ™¯æè¿°](#åœºæ™¯æè¿°)
      - [å®Œæ•´å®ç°](#å®Œæ•´å®ç°)
      - [å®æ–½æ•ˆæœ](#å®æ–½æ•ˆæœ)
  - [ç¬¬ä¸ƒéƒ¨åˆ†: æœªæ¥å±•æœ›ä¸ç ”ç©¶æ–¹å‘](#ç¬¬ä¸ƒéƒ¨åˆ†-æœªæ¥å±•æœ›ä¸ç ”ç©¶æ–¹å‘)
    - [7.1 å¤šæ¨¡æ€æ—¥å¿—åˆ†æ](#71-å¤šæ¨¡æ€æ—¥å¿—åˆ†æ)
    - [7.2 è‡ªåŠ¨ä¿®å¤ (Self-Healing)](#72-è‡ªåŠ¨ä¿®å¤-self-healing)
    - [7.3 çŸ¥è¯†ç§¯ç´¯ä¸æŒç»­å­¦ä¹ ](#73-çŸ¥è¯†ç§¯ç´¯ä¸æŒç»­å­¦ä¹ )
  - [æ€»ç»“ä¸æœ€ä½³å®è·µ](#æ€»ç»“ä¸æœ€ä½³å®è·µ)
    - [âœ… æ ¸å¿ƒè¦ç‚¹](#-æ ¸å¿ƒè¦ç‚¹)
    - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
    - [ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨](#-ä¸‹ä¸€æ­¥è¡ŒåŠ¨)
  - [ğŸ“š ç›¸å…³æ–‡æ¡£](#-ç›¸å…³æ–‡æ¡£)
    - [æ ¸å¿ƒé›†æˆ â­â­â­](#æ ¸å¿ƒé›†æˆ-)
    - [æ¶æ„å¯è§†åŒ– â­â­â­](#æ¶æ„å¯è§†åŒ–-)
    - [å·¥å…·é“¾æ”¯æŒ â­â­](#å·¥å…·é“¾æ”¯æŒ-)

---

## ç¬¬ä¸€éƒ¨åˆ†: LLM æ—¥å¿—åˆ†æåŸç†

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ LLM é©±åŠ¨æ—¥å¿—åˆ†æ?

#### ä¼ ç»Ÿæ—¥å¿—åˆ†æçš„å›°å¢ƒ

```text
ä¼ ç»Ÿæ–¹æ³• (åŸºäºè§„åˆ™):
  1. æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
     âŒ è§„åˆ™ç»´æŠ¤æˆæœ¬é«˜
     âŒ æ— æ³•å¤„ç†æœªçŸ¥æ¨¡å¼
  
  2. å…³é”®å­—æœç´¢
     âŒ å¬å›ç‡ä½
     âŒ è¯¯æŠ¥ç‡é«˜
  
  3. äººå·¥æ’æŸ¥
     âŒ è€—æ—¶ (4-8å°æ—¶/æ¬¡)
     âŒ ä¾èµ–ç»éªŒ
     âŒ å¤œé—´å€¼ç­å›°éš¾

LLM æ–¹æ³•:
  1. è¯­ä¹‰ç†è§£
     âœ… ç†è§£æ—¥å¿—å«ä¹‰
     âœ… è‡ªåŠ¨è¯†åˆ«å¼‚å¸¸
  
  2. ä¸Šä¸‹æ–‡æ¨ç†
     âœ… å…³è”å¤šæ¡æ—¥å¿—
     âœ… æ¨æ–­æ ¹æœ¬åŸå› 
  
  3. å¯è§£é‡Šæ€§
     âœ… ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
     âœ… æä¾›ä¿®å¤å»ºè®®
  
  4. æŒç»­å­¦ä¹ 
     âœ… ä»å†å²æ•…éšœå­¦ä¹ 
     âœ… çŸ¥è¯†ç§¯ç´¯
```

#### æœ€æ–°ç ”ç©¶è¿›å±• (2024-2025)

```text
ğŸ“„ é‡ç‚¹è®ºæ–‡:

1. "Interpretable Online Log Analysis Using Large Language Models 
   with Prompt Strategies" (arXiv:2308.07610, 2024)
   
   æ ¸å¿ƒè´¡çŒ®:
   - Prompt Engineering ç­–ç•¥
   - Few-shot Learning
   - Chain-of-Thought æ¨ç†
   
   æ•ˆæœ:
   - å¼‚å¸¸æ£€æµ‹å‡†ç¡®ç‡: 94.5%
   - è¯¯æŠ¥ç‡: <5%
   - å®æ—¶æ€§: <1s

2. "OWL: A Large Language Model for IT Operations" 
   (arXiv:2309.09298, 2024)
   
   æ ¸å¿ƒè´¡çŒ®:
   - ä¸“é—¨ä¸ºè¿ç»´è®­ç»ƒçš„ LLM (7B å‚æ•°)
   - æ—¥å¿—å¼‚å¸¸æ£€æµ‹ + RCA
   - è‡ªåŠ¨ç”Ÿæˆä¿®å¤è„šæœ¬
   
   æ•°æ®é›†:
   - 100ä¸‡+ æ•…éšœæ¡ˆä¾‹
   - 5000+ ä¿®å¤æ–¹æ¡ˆ

3. "LogGPT: Log Anomaly Detection via GPT" (2024)
   
   æ•ˆæœ:
   - F1 Score: 0.92 (ä¼ ç»Ÿæ–¹æ³•: 0.75)
   - é›¶æ ·æœ¬å­¦ä¹  (Zero-shot)
```

### 1.2 LLM æ—¥å¿—åˆ†ææ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ—¥å¿—æ¥æº                                â”‚
â”‚  OTLP Logs | Syslog | Application Logs | Kubernetes Events   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ—¥å¿—é¢„å¤„ç† (Log Preprocessing)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. æ—¥å¿—è§£æ (Parser)                                  â”‚   â”‚
â”‚  â”‚    - JSON/Syslog/Common Log Format                   â”‚   â”‚
â”‚  â”‚ 2. æ—¥å¿—èšç±» (Clustering)                              â”‚   â”‚
â”‚  â”‚    - Drain Algorithm (æ¨¡æ¿æå–)                       â”‚   â”‚
â”‚  â”‚ 3. å»é‡ (Deduplication)                              â”‚   â”‚
â”‚  â”‚    - ç›¸ä¼¼æ—¥å¿—åˆå¹¶                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                LLM åˆ†æå¼•æ“ (LLM Engine)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ GPT-4 / Claude 3 (äº‘ç«¯)                              â”‚   â”‚
â”‚  â”‚ Llama 3 70B (æœ¬åœ°, vLLM åŠ é€Ÿ)                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  åˆ†ææµç¨‹:                                                     â”‚
â”‚  1. Prompt Engineering                                        â”‚
â”‚     â”œâ”€ System Prompt (è§’è‰²å®šä¹‰)                              â”‚
â”‚     â”œâ”€ Few-shot Examples (ç¤ºä¾‹)                              â”‚
â”‚     â””â”€ Chain-of-Thought (æ¨ç†é“¾)                             â”‚
â”‚                                                               â”‚
â”‚  2. å¼‚å¸¸æ£€æµ‹                                                   â”‚
â”‚     â”œâ”€ è¯­ä¹‰åˆ†æ (ç†è§£æ—¥å¿—å«ä¹‰)                                â”‚
â”‚     â”œâ”€ æ¨¡å¼è¯†åˆ« (è¯†åˆ«å¼‚å¸¸æ¨¡å¼)                                â”‚
â”‚     â””â”€ ç½®ä¿¡åº¦è¯„åˆ†                                              â”‚
â”‚                                                               â”‚
â”‚  3. æ ¹å› åˆ†æ (RCA)                                            â”‚
â”‚     â”œâ”€ æ—¶é—´å…³è” (å‰åæ—¥å¿—)                                    â”‚
â”‚     â”œâ”€ ä¾èµ–å›¾åˆ†æ (æœåŠ¡ä¾èµ–)                                  â”‚
â”‚     â””â”€ å› æœæ¨æ–­                                                â”‚
â”‚                                                               â”‚
â”‚  4. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š                                               â”‚
â”‚     â”œâ”€ é—®é¢˜æè¿°                                                â”‚
â”‚     â”œâ”€ æ ¹æœ¬åŸå›                                                 â”‚
â”‚     â””â”€ ä¿®å¤å»ºè®®                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  è¾“å‡ºä¸è¡ŒåŠ¨ (Output & Action)                  â”‚
â”‚  - å‘Šè­¦ (Slack/PagerDuty/Email)                              â”‚
â”‚  - å·¥å•åˆ›å»º (Jira/ServiceNow)                                â”‚
â”‚  - è‡ªåŠ¨ä¿®å¤ (Ansible/Terraform)                               â”‚
â”‚  - çŸ¥è¯†åº“æ›´æ–° (Confluence/Notion)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Prompt Engineering æŠ€å·§

#### System Prompt (è§’è‰²å®šä¹‰)

```python
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç³»ç»Ÿè¿ç»´ä¸“å®¶ (SRE),ä¸“é—¨åˆ†ææ—¥å¿—ã€è¯Šæ–­æ•…éšœã€å®šä½æ ¹æœ¬åŸå› ã€‚

ä½ çš„ä»»åŠ¡:
1. åˆ†ææä¾›çš„æ—¥å¿—,è¯†åˆ«å¼‚å¸¸æ¨¡å¼
2. åˆ¤æ–­å¼‚å¸¸çš„ä¸¥é‡ç¨‹åº¦ (Critical/High/Medium/Low)
3. æ¨æµ‹æ ¹æœ¬åŸå› 
4. æä¾›è¯¦ç»†çš„ä¿®å¤å»ºè®®

è¾“å‡ºæ ¼å¼ (JSON):
{
  "is_anomaly": true/false,
  "severity": "Critical|High|Medium|Low",
  "anomaly_type": "ç±»å‹ (å¦‚: DatabaseError, NetworkTimeout, MemoryLeak)",
  "root_cause": "æ ¹æœ¬åŸå› çš„è¯¦ç»†æè¿°",
  "affected_services": ["å—å½±å“çš„æœåŠ¡åˆ—è¡¨"],
  "remediation_steps": [
    "ä¿®å¤æ­¥éª¤1",
    "ä¿®å¤æ­¥éª¤2"
  ],
  "confidence": 0.95,  // ç½®ä¿¡åº¦ (0-1)
  "explanation": "æ¨ç†è¿‡ç¨‹çš„è¯¦ç»†è§£é‡Š"
}

è§„åˆ™:
- å¦‚æœæ—¥å¿—æ­£å¸¸,è¿”å› is_anomaly: false
- å¦‚æœæ— æ³•åˆ¤æ–­,è¯´æ˜åŸå› ,ä¸è¦çŒœæµ‹
- æ¨ç†è¿‡ç¨‹è¦æ¸…æ™°,å¼•ç”¨å…·ä½“æ—¥å¿—è¡Œ
"""
```

#### Few-shot Examples (ç¤ºä¾‹å­¦ä¹ )

```python
FEW_SHOT_EXAMPLES = [
    {
        "role": "user",
        "content": """
æ—¥å¿—:
[ERROR] 2025-10-09 10:30:45 DatabasePool: Connection timeout after 30s
[ERROR] 2025-10-09 10:30:46 DatabasePool: Connection timeout after 30s
[WARN]  2025-10-09 10:30:47 DatabasePool: Pool exhausted, waiting for connection
[ERROR] 2025-10-09 10:30:50 PaymentService: Failed to process payment, database unavailable
"""
    },
    {
        "role": "assistant",
        "content": """
{
  "is_anomaly": true,
  "severity": "Critical",
  "anomaly_type": "DatabaseConnectionTimeout",
  "root_cause": "æ•°æ®åº“è¿æ¥æ± è€—å°½,å¯èƒ½åŸå› : (1) æ•°æ®åº“æœåŠ¡å™¨è¿‡è½½ (2) ç½‘ç»œé—®é¢˜ (3) è¿æ¥æ³„æ¼",
  "affected_services": ["PaymentService", "DatabasePool"],
  "remediation_steps": [
    "1. æ£€æŸ¥æ•°æ®åº“ CPU/å†…å­˜ä½¿ç”¨ç‡: SELECT * FROM pg_stat_activity;",
    "2. æ£€æŸ¥ç½‘ç»œè¿é€šæ€§: ping db-server; telnet db-server 5432",
    "3. æ£€æŸ¥è¿æ¥æ± é…ç½®: å¢åŠ  max_connections æˆ– å‡å°‘ connection_timeout",
    "4. æ£€æŸ¥æ…¢æŸ¥è¯¢: SELECT * FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;",
    "5. ä¸´æ—¶è§£å†³: é‡å¯æ•°æ®åº“è¿æ¥æ±  æˆ– æ‰©å®¹æ•°æ®åº“"
  ],
  "confidence": 0.95,
  "explanation": "è¿ç»­3æ¬¡è¿æ¥è¶…æ—¶ + è¿æ¥æ± è€—å°½ â†’ æ•°æ®åº“ä¸å¯ç”¨ â†’ æ”¯ä»˜æœåŠ¡å¤±è´¥ã€‚æ—¶é—´ç´§å¯†å…³è”,å› æœå…³ç³»æ˜ç¡®ã€‚"
}
"""
    },
    # æ›´å¤šç¤ºä¾‹...
]
```

#### Chain-of-Thought (æ€ç»´é“¾)

```python
CHAIN_OF_THOUGHT_PROMPT = """
è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ†ææ—¥å¿—:

æ­¥éª¤ 1: æå–å…³é”®ä¿¡æ¯
- æ—¶é—´èŒƒå›´: ?
- æ¶‰åŠæœåŠ¡: ?
- æ—¥å¿—çº§åˆ«ç»Ÿè®¡: ERROR Xæ¡, WARN Yæ¡
- å…³é”®å­—: ?

æ­¥éª¤ 2: è¯†åˆ«å¼‚å¸¸æ¨¡å¼
- æ˜¯å¦æœ‰é”™è¯¯æ—¥å¿—? 
- æ˜¯å¦æœ‰é‡å¤æ¨¡å¼?
- æ˜¯å¦æœ‰æ—¶é—´èšé›†?
- æ˜¯å¦æœ‰çº§è”å¤±è´¥?

æ­¥éª¤ 3: æ¨æ–­æ ¹æœ¬åŸå› 
- ç¬¬ä¸€æ¡é”™è¯¯æ—¥å¿—æ˜¯ä»€ä¹ˆ?
- å‰åæœ‰ä»€ä¹ˆç›¸å…³æ—¥å¿—?
- å¯èƒ½çš„æŠ€æœ¯åŸå› ?

æ­¥éª¤ 4: ç”Ÿæˆå»ºè®®
- å¦‚ä½•éªŒè¯å‡è®¾?
- å¦‚ä½•ä¿®å¤?
- å¦‚ä½•é¢„é˜²?

ç°åœ¨,è¯·åˆ†æä»¥ä¸‹æ—¥å¿—:
{logs}
"""
```

---

## ç¬¬äºŒéƒ¨åˆ†: å¼‚å¸¸æ£€æµ‹å®æˆ˜

### 2.1 å®Œæ•´ Python å®ç°

```python
# log_analyzer.py - LLM é©±åŠ¨æ—¥å¿—å¼‚å¸¸æ£€æµ‹

import openai
import json
import logging
from typing import List, Dict, Optional
from datetime import datetime, timedelta

class LLMLogAnalyzer:
    """LLM é©±åŠ¨çš„æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4"):
        """
        Args:
            api_key: OpenAI API Key (å¦‚æœä¸º None,ä»ç¯å¢ƒå˜é‡ OPENAI_API_KEY è¯»å–)
            model: æ¨¡å‹åç§° (gpt-4, gpt-3.5-turbo, etc.)
        
        Raises:
            ValueError: å¦‚æœ API Key æœªæä¾›ä¸”ç¯å¢ƒå˜é‡ä¸å­˜åœ¨
        """
        import os
        import logging
        
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "OpenAI API Key is required. "
                "Provide via api_key parameter or OPENAI_API_KEY environment variable."
            )
        
        self.model = model
        openai.api_key = self.api_key
        self.logger = logging.getLogger(__name__)
        
        self.system_prompt = """
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ç³»ç»Ÿè¿ç»´ä¸“å®¶ (SRE),ä¸“é—¨åˆ†ææ—¥å¿—ã€è¯Šæ–­æ•…éšœã€å®šä½æ ¹æœ¬åŸå› ã€‚

ä½ çš„ä»»åŠ¡:
1. åˆ†ææä¾›çš„æ—¥å¿—,è¯†åˆ«å¼‚å¸¸æ¨¡å¼
2. åˆ¤æ–­å¼‚å¸¸çš„ä¸¥é‡ç¨‹åº¦ (Critical/High/Medium/Low)
3. æ¨æµ‹æ ¹æœ¬åŸå› 
4. æä¾›è¯¦ç»†çš„ä¿®å¤å»ºè®®

è¾“å‡ºæ ¼å¼ (JSON):
{
  "is_anomaly": true/false,
  "severity": "Critical|High|Medium|Low",
  "anomaly_type": "ç±»å‹",
  "root_cause": "æ ¹æœ¬åŸå› ",
  "affected_services": ["æœåŠ¡åˆ—è¡¨"],
  "remediation_steps": ["æ­¥éª¤"],
  "confidence": 0.95,
  "explanation": "æ¨ç†è¿‡ç¨‹"
}
"""
        
        # Few-shot examples (ç®€åŒ–ç‰ˆ)
        self.few_shot_examples = [
            {
                "role": "user",
                "content": """
[ERROR] 2025-10-09 10:30:45 DatabasePool: Connection timeout after 30s
[ERROR] 2025-10-09 10:30:46 DatabasePool: Connection timeout after 30s
[WARN]  2025-10-09 10:30:47 DatabasePool: Pool exhausted
"""
            },
            {
                "role": "assistant",
                "content": json.dumps({
                    "is_anomaly": True,
                    "severity": "Critical",
                    "anomaly_type": "DatabaseConnectionTimeout",
                    "root_cause": "æ•°æ®åº“è¿æ¥æ± è€—å°½",
                    "affected_services": ["DatabasePool"],
                    "remediation_steps": [
                        "æ£€æŸ¥æ•°æ®åº“ CPU/å†…å­˜",
                        "æ£€æŸ¥ç½‘ç»œè¿é€šæ€§",
                        "å¢åŠ è¿æ¥æ± å¤§å°"
                    ],
                    "confidence": 0.95,
                    "explanation": "è¿ç»­è¶…æ—¶ + è¿æ¥æ± è€—å°½"
                }, ensure_ascii=False)
            }
        ]
    
    def analyze_logs(
        self,
        logs: List[str],
        context: Optional[Dict] = None,
        timeout: int = 60,
        retries: int = 3
    ) -> Dict:
        """
        åˆ†ææ—¥å¿—,æ£€æµ‹å¼‚å¸¸
        
        Args:
            logs: æ—¥å¿—åˆ—è¡¨
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ (æœåŠ¡åã€æ—¶é—´èŒƒå›´ç­‰)
            timeout: API è¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)
            retries: å¤±è´¥é‡è¯•æ¬¡æ•°
        
        Returns:
            åˆ†æç»“æœ (JSON)
        
        Raises:
            ValueError: å¦‚æœ logs ä¸ºç©ºæˆ–æ ¼å¼æ— æ•ˆ
            openai.APIError: å¦‚æœ API è°ƒç”¨å¤±è´¥
        """
        import time
        from openai import APIError, Timeout, RateLimitError
        
        # è¾“å…¥éªŒè¯
        if not logs:
            raise ValueError("Logs list cannot be empty")
        
        if len(logs) > 1000:
            self.logger.warning(f"Large log batch ({len(logs)} logs), truncating to 1000")
            logs = logs[:1000]
        
        # 1. å‡†å¤‡ User Prompt
        log_text = "\n".join(logs)
        
        if context:
            context_text = f"""
ä¸Šä¸‹æ–‡ä¿¡æ¯:
- æœåŠ¡: {context.get('service', 'Unknown')}
- æ—¶é—´èŒƒå›´: {context.get('time_range', 'Unknown')}
- ç¯å¢ƒ: {context.get('environment', 'production')}
"""
        else:
            context_text = ""
        
        user_prompt = f"""
{context_text}

æ—¥å¿—:
{log_text}

è¯·åˆ†æä»¥ä¸Šæ—¥å¿—,è¯†åˆ«å¼‚å¸¸ã€‚
"""
        
        # 2. è°ƒç”¨ LLM (å¸¦é‡è¯•é€»è¾‘)
        messages = [
            {"role": "system", "content": self.system_prompt},
            *self.few_shot_examples,
            {"role": "user", "content": user_prompt}
        ]
        
        last_exception = None
        for attempt in range(retries):
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=messages,
                temperature=0.1,  # ä½æ¸©åº¦ â†’ æ›´ç¡®å®šæ€§çš„è¾“å‡º
                max_tokens=1000,
                    request_timeout=timeout,
                response_format={"type": "json_object"}  # å¼ºåˆ¶ JSON è¾“å‡º
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # 3. æ·»åŠ å…ƒæ•°æ®
            result['timestamp'] = datetime.now().isoformat()
            result['model'] = self.model
            result['token_usage'] = response.usage.total_tokens
            
                # éªŒè¯å“åº”æ ¼å¼
                required_fields = ['is_anomaly', 'severity', 'confidence']
                if not all(field in result for field in required_fields):
                    self.logger.warning(f"Incomplete response fields: {result.keys()}")
                    result['_incomplete'] = True
            
            return result
            
            except Timeout as e:
                last_exception = e
                self.logger.warning(f"Timeout on attempt {attempt+1}/{retries}: {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
            
            except RateLimitError as e:
                last_exception = e
                self.logger.warning(f"Rate limit hit on attempt {attempt+1}/{retries}")
                if attempt < retries - 1:
                    time.sleep(10 * (attempt + 1))  # ç­‰å¾…æ›´é•¿æ—¶é—´
                    continue
            
            except APIError as e:
                last_exception = e
                self.logger.error(f"OpenAI API error on attempt {attempt+1}/{retries}: {e}")
                if attempt < retries - 1 and hasattr(e, 'code') and e.code in ['server_error', 'service_unavailable']:
                    time.sleep(5)
                    continue
                # ä¸å¯é‡è¯•çš„é”™è¯¯,è¿”å›é”™è¯¯å“åº”è€ŒéæŠ›å‡ºå¼‚å¸¸
                return {
                    "is_anomaly": False,
                    "error": f"API Error: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }
            
            except json.JSONDecodeError as e:
                last_exception = e
                self.logger.error(f"Failed to parse LLM response as JSON: {e}")
                return {
                    "is_anomaly": False,
                    "error": f"Invalid JSON response: {str(e)}",
                    "timestamp": datetime.now().isoformat()
                }
            
        except Exception as e:
                last_exception = e
                self.logger.error(f"Unexpected error on attempt {attempt+1}/{retries}: {e}")
                if attempt == retries - 1:
            return {
                "is_anomaly": False,
                "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    }
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        return {
            "is_anomaly": False,
            "error": f"All {retries} retry attempts failed: {str(last_exception)}",
                "timestamp": datetime.now().isoformat()
            }
    
    def analyze_real_time(
        self,
        log_stream,
        window_size: int = 100,
        slide_interval: int = 10
    ):
        """
        å®æ—¶æ—¥å¿—åˆ†æ (æ»‘åŠ¨çª—å£)
        
        Args:
            log_stream: æ—¥å¿—æµ (ç”Ÿæˆå™¨)
            window_size: çª—å£å¤§å° (æ—¥å¿—æ¡æ•°)
            slide_interval: æ»‘åŠ¨é—´éš” (ç§’)
        """
        from collections import deque
        import time
        
        buffer = deque(maxlen=window_size)
        last_analysis = time.time()
        
        for log_line in log_stream:
            buffer.append(log_line)
            
            # æ¯éš” slide_interval ç§’åˆ†æä¸€æ¬¡
            if time.time() - last_analysis > slide_interval:
                if len(buffer) >= 10:  # è‡³å°‘10æ¡æ—¥å¿—
                    result = self.analyze_logs(list(buffer))
                    
                    if result.get('is_anomaly'):
                        self._handle_anomaly(result)
                    
                    last_analysis = time.time()
    
    def _handle_anomaly(self, result: Dict):
        """å¤„ç†å¼‚å¸¸ (å‘Šè­¦ã€å·¥å•ç­‰)"""
        severity = result.get('severity', 'Unknown')
        
        print(f"\nâš ï¸  æ£€æµ‹åˆ°å¼‚å¸¸! ä¸¥é‡ç¨‹åº¦: {severity}")
        print(f"ç±»å‹: {result.get('anomaly_type')}")
        print(f"æ ¹å› : {result.get('root_cause')}")
        print(f"ç½®ä¿¡åº¦: {result.get('confidence'):.2%}")
        print(f"\nä¿®å¤å»ºè®®:")
        for step in result.get('remediation_steps', []):
            print(f"  - {step}")
        
        # TODO: å‘é€å‘Šè­¦ (Slack, PagerDuty, etc.)
        # TODO: åˆ›å»ºå·¥å• (Jira, ServiceNow, etc.)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # 1. åˆå§‹åŒ–
    analyzer = LLMLogAnalyzer(
        api_key="sk-...",  # æ›¿æ¢ä¸ºå®é™… API Key
        model="gpt-4"
    )
    
    # 2. åˆ†æå†å²æ—¥å¿—
    sample_logs = [
        "[ERROR] 2025-10-09 10:30:45 PaymentService: Failed to connect to database",
        "[ERROR] 2025-10-09 10:30:46 PaymentService: Connection timeout after 30s",
        "[ERROR] 2025-10-09 10:30:47 PaymentService: Retrying connection (attempt 2/3)",
        "[ERROR] 2025-10-09 10:30:50 PaymentService: All retries exhausted, giving up",
        "[WARN]  2025-10-09 10:30:51 CircuitBreaker: Circuit opened for PaymentService",
    ]
    
    result = analyzer.analyze_logs(
        logs=sample_logs,
        context={
            'service': 'PaymentService',
            'time_range': '10:30:45 - 10:30:51',
            'environment': 'production'
        }
    )
    
    print(json.dumps(result, indent=2, ensure_ascii=False))
```

**è¾“å‡ºç¤ºä¾‹**:

```json
{
  "is_anomaly": true,
  "severity": "Critical",
  "anomaly_type": "DatabaseConnectionFailure",
  "root_cause": "æ”¯ä»˜æœåŠ¡æ— æ³•è¿æ¥åˆ°æ•°æ®åº“,æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥,è§¦å‘äº†ç†”æ–­å™¨",
  "affected_services": [
    "PaymentService",
    "Database"
  ],
  "remediation_steps": [
    "1. æ£€æŸ¥æ•°æ®åº“æœåŠ¡æ˜¯å¦è¿è¡Œ: systemctl status postgresql",
    "2. æ£€æŸ¥ç½‘ç»œè¿é€šæ€§: ping db-server; telnet db-server 5432",
    "3. æ£€æŸ¥æ•°æ®åº“æ—¥å¿—: tail -f /var/log/postgresql/postgresql.log",
    "4. æ£€æŸ¥è¿æ¥æ•°: SELECT count(*) FROM pg_stat_activity;",
    "5. å¦‚æœæ•°æ®åº“æ­£å¸¸,æ£€æŸ¥ PaymentService çš„è¿æ¥é…ç½®",
    "6. ä¸´æ—¶è§£å†³: é‡å¯ PaymentService æˆ– æ•°æ®åº“"
  ],
  "confidence": 0.98,
  "explanation": "æ—¥å¿—æ˜¾ç¤ºè¿ç»­4æ¬¡æ•°æ®åº“è¿æ¥å¤±è´¥,é‡è¯•3æ¬¡åä»ç„¶å¤±è´¥,æœ€ç»ˆè§¦å‘ç†”æ–­å™¨ã€‚æ—¶é—´ç´§å¯†å…³è” (6ç§’å†…),å› æœå…³ç³»æ˜ç¡®ã€‚è¿™æ˜¯å…¸å‹çš„æ•°æ®åº“ä¸å¯ç”¨åœºæ™¯ã€‚",
  "timestamp": "2025-10-09T10:35:00.123456",
  "model": "gpt-4",
  "token_usage": 456
}
```

### 2.2 é›†æˆ OTLP Logs

```python
# otlp_log_analyzer.py - é›†æˆ OTLP Logs

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

import psycopg2
from datetime import datetime, timedelta

class OTLPLogAnalyzer:
    """ä» OTLP æ•°æ®åº“è¯»å–æ—¥å¿—å¹¶åˆ†æ"""
    
    def __init__(self, db_config: Dict, llm_analyzer: LLMLogAnalyzer):
        """
        Args:
            db_config: æ•°æ®åº“é…ç½®å­—å…¸ (host, port, database, user, password)
            llm_analyzer: LLM åˆ†æå™¨å®ä¾‹
        
        Raises:
            psycopg2.Error: å¦‚æœæ•°æ®åº“è¿æ¥å¤±è´¥
        """
        self.db_config = db_config
        self.llm_analyzer = llm_analyzer
        self.logger = logging.getLogger(__name__)
        
        # éªŒè¯æ•°æ®åº“è¿æ¥
        try:
            with psycopg2.connect(**self.db_config) as conn:
                with conn.cursor() as cursor:
                    cursor.execute("SELECT 1")
        except psycopg2.Error as e:
            self.logger.error(f"Database connection failed: {e}")
            raise
        
        # åˆå§‹åŒ– OpenTelemetry
        trace.set_tracer_provider(TracerProvider())
        tracer_provider = trace.get_tracer_provider()
        
        otlp_exporter = OTLPSpanExporter(endpoint="http://localhost:4317")
        span_processor = BatchSpanProcessor(otlp_exporter)
        tracer_provider.add_span_processor(span_processor)
        
        self.tracer = trace.get_tracer(__name__)
    
    def fetch_recent_logs(
        self,
        service_name: str,
        time_range_minutes: int = 5,
        severity: str = "ERROR",
        max_logs: int = 100
    ) -> List[str]:
        """
        ä»æ•°æ®åº“è·å–æœ€è¿‘çš„æ—¥å¿—
        
        Args:
            service_name: æœåŠ¡åç§°
            time_range_minutes: æ—¶é—´èŒƒå›´(åˆ†é’Ÿ)
            severity: æœ€ä½æ—¥å¿—çº§åˆ«
            max_logs: æœ€å¤§è¿”å›æ—¥å¿—æ•°
        
        Returns:
            æ ¼å¼åŒ–åçš„æ—¥å¿—åˆ—è¡¨
        
        Raises:
            ValueError: å¦‚æœå‚æ•°æ— æ•ˆ
            psycopg2.Error: å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥
        """
        # è¾“å…¥éªŒè¯
        if not service_name:
            raise ValueError("service_name cannot be empty")
        
        if time_range_minutes <= 0 or time_range_minutes > 1440:  # æœ€å¤š24å°æ—¶
            raise ValueError("time_range_minutes must be between 1 and 1440")
        
        if max_logs <= 0 or max_logs > 10000:
            raise ValueError("max_logs must be between 1 and 10000")
        
        try:
            with psycopg2.connect(**self.db_config) as conn:
                with conn.cursor() as cursor:
        query = """
            SELECT 
                time,
                severity_text,
                body,
                service_name,
                trace_id
            FROM otlp_logs
            WHERE service_name = %s
              AND severity_text >= %s
              AND time >= NOW() - INTERVAL '%s minutes'
            ORDER BY time DESC
                        LIMIT %s
        """
        
                    cursor.execute(query, (service_name, severity, time_range_minutes, max_logs))
        rows = cursor.fetchall()
        
        # æ ¼å¼åŒ–ä¸ºæ—¥å¿—å­—ç¬¦ä¸²
        logs = []
        for row in rows:
            time, severity, body, service, trace_id = row
            log_line = f"[{severity}] {time} {service}: {body}"
            if trace_id:
                log_line += f" [TraceID: {trace_id}]"
            logs.append(log_line)
        
                    self.logger.info(f"Fetched {len(logs)} logs for service {service_name}")
        return logs
        
        except psycopg2.Error as e:
            self.logger.error(f"Database query failed: {e}")
            raise
    
    def analyze_service(self, service_name: str):
        """åˆ†ææŒ‡å®šæœåŠ¡çš„æ—¥å¿—"""
        
        with self.tracer.start_as_current_span("analyze_service_logs") as span:
            span.set_attribute("service.name", service_name)
            
            # 1. è·å–æ—¥å¿—
            logs = self.fetch_recent_logs(service_name, time_range_minutes=5)
            span.set_attribute("log.count", len(logs))
            
            if not logs:
                return {"is_anomaly": False, "reason": "No logs found"}
            
            # 2. LLM åˆ†æ
            result = self.llm_analyzer.analyze_logs(
                logs=logs,
                context={
                    'service': service_name,
                    'time_range': 'last 5 minutes',
                    'environment': 'production'
                }
            )
            
            # 3. è®°å½•åˆ° Span
            span.set_attribute("anomaly.detected", result.get('is_anomaly', False))
            span.set_attribute("anomaly.severity", result.get('severity', 'Unknown'))
            span.set_attribute("anomaly.type", result.get('anomaly_type', 'Unknown'))
            
            return result


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    db_config = {
        'host': 'localhost',
        'port': 5432,
        'database': 'otlp',
        'user': 'postgres',
        'password': 'password'
    }
    
    llm_analyzer = LLMLogAnalyzer(api_key="sk-...")
    otlp_analyzer = OTLPLogAnalyzer(db_config, llm_analyzer)
    
    # åˆ†ææ”¯ä»˜æœåŠ¡
    result = otlp_analyzer.analyze_service("payment-service")
    
    if result.get('is_anomaly'):
        print("âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸!")
        print(json.dumps(result, indent=2, ensure_ascii=False))
```

---

## ç¬¬ä¸‰éƒ¨åˆ†: æ ¹å› åˆ†æ (RCA)

### 3.1 å¤šç»´åº¦æ ¹å› åˆ†æ

```python
# rca_engine.py - æ ¹å› åˆ†æå¼•æ“

import networkx as nx
from dowhy import CausalModel
import pandas as pd

class RCAEngine:
    """æ ¹å› åˆ†æå¼•æ“ (ç»“åˆå› æœæ¨æ–­ + æœåŠ¡ä¾èµ–å›¾ + LLM)"""
    
    def __init__(
        self,
        llm_analyzer: LLMLogAnalyzer,
        db_config: Dict
    ):
        self.llm_analyzer = llm_analyzer
        self.db_config = db_config
        self.service_graph = self._build_service_graph()
    
    def _build_service_graph(self) -> nx.DiGraph:
        """ä»æ•°æ®åº“æ„å»ºæœåŠ¡ä¾èµ–å›¾"""
        
        conn = psycopg2.connect(**self.db_config)
        cursor = conn.cursor()
        
        # æŸ¥è¯¢æœåŠ¡è°ƒç”¨å…³ç³» (ä» Traces)
        query = """
            SELECT DISTINCT
                parent_span.service_name AS caller,
                child_span.service_name AS callee,
                COUNT(*) AS call_count
            FROM otlp_spans parent_span
            JOIN otlp_spans child_span
              ON parent_span.trace_id = child_span.trace_id
              AND parent_span.span_id = child_span.parent_span_id
            WHERE parent_span.time >= NOW() - INTERVAL '1 hour'
            GROUP BY caller, callee
        """
        
        cursor.execute(query)
        rows = cursor.fetchall()
        
        # æ„å»ºæœ‰å‘å›¾
        G = nx.DiGraph()
        for caller, callee, count in rows:
            G.add_edge(caller, callee, weight=count)
        
        cursor.close()
        conn.close()
        
        return G
    
    def analyze_root_cause(
        self,
        anomaly_service: str,
        anomaly_time: datetime
    ) -> Dict:
        """
        ç»¼åˆåˆ†ææ ¹å› 
        
        æµç¨‹:
        1. æ‰¾åˆ°æ‰€æœ‰ä¸Šæ¸¸æœåŠ¡ (ä¾èµ–å›¾)
        2. è·å–ä¸Šæ¸¸æœåŠ¡çš„æ—¥å¿—/æŒ‡æ ‡
        3. LLM æ¨æ–­å› æœå…³ç³»
        4. å› æœæ¨æ–­éªŒè¯
        5. ç”Ÿæˆ RCA æŠ¥å‘Š
        """
        
        # 1. æ‰¾åˆ°ä¸Šæ¸¸æœåŠ¡
        upstream_services = list(self.service_graph.predecessors(anomaly_service))
        
        # 2. æ”¶é›†ç›¸å…³æ—¥å¿—
        all_logs = {}
        for service in [anomaly_service] + upstream_services:
            logs = self._fetch_logs_around_time(service, anomaly_time, window_minutes=10)
            all_logs[service] = logs
        
        # 3. LLM æ¨æ–­
        rca_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿæ•…éšœè¯Šæ–­ä¸“å®¶ã€‚

åœºæ™¯:
- å¼‚å¸¸æœåŠ¡: {anomaly_service}
- å¼‚å¸¸æ—¶é—´: {anomaly_time}
- ä¸Šæ¸¸æœåŠ¡: {', '.join(upstream_services)}

å„æœåŠ¡æ—¥å¿—:
"""
        
        for service, logs in all_logs.items():
            rca_prompt += f"\n### {service} æ—¥å¿—:\n"
            rca_prompt += "\n".join(logs[:20])  # é™åˆ¶é•¿åº¦
        
        rca_prompt += """

è¯·åˆ†æ:
1. æœ€å¯èƒ½çš„æ ¹æœ¬åŸå› æ˜¯ä»€ä¹ˆæœåŠ¡çš„ä»€ä¹ˆé—®é¢˜?
2. ä¸ºä»€ä¹ˆä¼šå¯¼è‡´ {anomaly_service} å¼‚å¸¸?
3. è¯æ®æ˜¯ä»€ä¹ˆ (å¼•ç”¨å…·ä½“æ—¥å¿—è¡Œ)?
4. ç½®ä¿¡åº¦å¤šå°‘?

è¾“å‡º JSON:
{
  "root_cause_service": "æœåŠ¡å",
  "root_cause_issue": "é—®é¢˜æè¿°",
  "propagation_path": ["æœåŠ¡A", "æœåŠ¡B", "æœåŠ¡C"],
  "evidence": ["è¯æ®1", "è¯æ®2"],
  "confidence": 0.9,
  "explanation": "è¯¦ç»†è§£é‡Š"
}
"""
        
        llm_result = self.llm_analyzer.analyze_logs(
            logs=[rca_prompt],
            context={'type': 'root_cause_analysis'}
        )
        
        # 4. å¯è§†åŒ–ä¾èµ–è·¯å¾„
        root_cause_service = llm_result.get('root_cause_service')
        
        if root_cause_service and root_cause_service in self.service_graph:
            paths = list(nx.all_simple_paths(
                self.service_graph,
                source=root_cause_service,
                target=anomaly_service,
                cutoff=5  # æœ€å¤š5è·³
            ))
            
            llm_result['dependency_paths'] = paths
        
        return llm_result
    
    def _fetch_logs_around_time(
        self,
        service: str,
        center_time: datetime,
        window_minutes: int
    ) -> List[str]:
        """è·å–æŸä¸ªæ—¶é—´ç‚¹å‰åçš„æ—¥å¿—"""
        
        conn = psycopg2.connect(**self.db_config)
        cursor = conn.cursor()
        
        query = """
            SELECT time, severity_text, body
            FROM otlp_logs
            WHERE service_name = %s
              AND time BETWEEN %s AND %s
            ORDER BY time
        """
        
        start_time = center_time - timedelta(minutes=window_minutes)
        end_time = center_time + timedelta(minutes=window_minutes)
        
        cursor.execute(query, (service, start_time, end_time))
        rows = cursor.fetchall()
        
        logs = [f"[{row[1]}] {row[0]} {row[2]}" for row in rows]
        
        cursor.close()
        conn.close()
        
        return logs


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    llm_analyzer = LLMLogAnalyzer(api_key="sk-...")
    rca_engine = RCAEngine(llm_analyzer, db_config)
    
    # åˆ†ææ ¹å› 
    result = rca_engine.analyze_root_cause(
        anomaly_service="payment-service",
        anomaly_time=datetime(2025, 10, 9, 10, 30, 45)
    )
    
    print("ğŸ” æ ¹å› åˆ†æç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
```

**è¾“å‡ºç¤ºä¾‹**:

```json
{
  "root_cause_service": "database-service",
  "root_cause_issue": "æ•°æ®åº“ CPU ä½¿ç”¨ç‡è¾¾åˆ° 100%,å¯¼è‡´æŸ¥è¯¢è¶…æ—¶",
  "propagation_path": [
    "database-service",
    "user-service",
    "payment-service"
  ],
  "evidence": [
    "[ERROR] 10:30:40 database-service: Slow query detected, execution time: 35s",
    "[WARN]  10:30:42 database-service: CPU usage: 100%",
    "[ERROR] 10:30:45 user-service: Database query timeout",
    "[ERROR] 10:30:46 payment-service: Failed to fetch user info"
  ],
  "confidence": 0.92,
  "explanation": "ä»æ—¶é—´çº¿çœ‹,database-service åœ¨ 10:30:40 å‡ºç°æ…¢æŸ¥è¯¢,å¯¼è‡´ CPU é£™å‡è‡³ 100%ã€‚éšå user-service æŸ¥è¯¢è¶…æ—¶,æœ€ç»ˆå¯¼è‡´ payment-service æ— æ³•è·å–ç”¨æˆ·ä¿¡æ¯ã€‚æ•´ä¸ªæ•…éšœä¼ æ’­è·¯å¾„æ¸…æ™°,æ—¶é—´å› æœå…³ç³»æ˜ç¡®ã€‚",
  "dependency_paths": [
    ["database-service", "user-service", "payment-service"]
  ]
}
```

---

## ç¬¬å››éƒ¨åˆ†: è‡ªç„¶è¯­è¨€æŸ¥è¯¢ (NLP Query)

### 4.1 Log Search with Natural Language

```python
# nlp_log_search.py - è‡ªç„¶è¯­è¨€æ—¥å¿—æœç´¢

from typing import List, Dict
import chromadb
from chromadb.utils import embedding_functions
import openai

class NaturalLanguageLogSearch:
    """ä½¿ç”¨ LLM + å‘é‡æ•°æ®åº“å®ç°è‡ªç„¶è¯­è¨€æ—¥å¿—æœç´¢"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        openai.api_key = api_key
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ (ChromaDB)
        self.client = chromadb.Client()
        
        # ä½¿ç”¨ OpenAI Embeddings
        self.embedding_function = embedding_functions.OpenAIEmbeddingFunction(
            api_key=api_key,
            model_name="text-embedding-3-small"
        )
        
        # åˆ›å»ºé›†åˆ
        self.collection = self.client.create_collection(
            name="logs",
            embedding_function=self.embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    def index_logs(self, logs: List[Dict]):
        """
        ç´¢å¼•æ—¥å¿—åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            logs: [
                {
                    "id": "log-123",
                    "timestamp": "2025-10-09T10:30:45",
                    "service": "payment-service",
                    "severity": "ERROR",
                    "message": "Failed to connect to database"
                },
                ...
            ]
        """
        documents = []
        metadatas = []
        ids = []
        
        for log in logs:
            # æ„é€ æ–‡æ¡£ (ç”¨äº Embedding)
            doc = f"""
[{log['severity']}] {log['timestamp']} {log['service']}: {log['message']}
"""
            documents.append(doc)
            
            metadatas.append({
                "timestamp": log['timestamp'],
                "service": log['service'],
                "severity": log['severity']
            })
            
            ids.append(log['id'])
        
        # æ‰¹é‡æ’å…¥
        self.collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
    
    def search(self, query: str, top_k: int = 10) -> List[Dict]:
        """
        è‡ªç„¶è¯­è¨€æœç´¢æ—¥å¿—
        
        Examples:
            - "æ”¯ä»˜æœåŠ¡æ˜¨å¤©æ™šä¸Šçš„æ•°æ®åº“é”™è¯¯"
            - "æ‰€æœ‰å†…å­˜æº¢å‡ºçš„æ—¥å¿—"
            - "API ç½‘å…³è¶…æ—¶çš„ç›¸å…³æ—¥å¿—"
        """
        # 1. å‘é‡æ£€ç´¢
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k
        )
        
        # 2. æ ¼å¼åŒ–ç»“æœ
        matched_logs = []
        for i, doc in enumerate(results['documents'][0]):
            matched_logs.append({
                "log": doc,
                "metadata": results['metadatas'][0][i],
                "distance": results['distances'][0][i]
            })
        
        return matched_logs
    
    def search_with_filters(
        self,
        query: str,
        service: str = None,
        severity: str = None,
        time_range: tuple = None,
        top_k: int = 10
    ) -> List[Dict]:
        """å¸¦è¿‡æ»¤æ¡ä»¶çš„æœç´¢"""
        
        where_clause = {}
        
        if service:
            where_clause['service'] = service
        
        if severity:
            where_clause['severity'] = severity
        
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k,
            where=where_clause if where_clause else None
        )
        
        return results
    
    def ask_question(self, question: str) -> str:
        """
        ç›´æ¥æé—®,LLM è‡ªåŠ¨æœç´¢æ—¥å¿—å¹¶å›ç­”
        
        Examples:
            - "ä¸ºä»€ä¹ˆæ”¯ä»˜æœåŠ¡æ˜¨å¤©æ™šä¸Šå®•æœºäº†?"
            - "æ•°æ®åº“è¿æ¥æ± çš„é…ç½®æœ‰é—®é¢˜å—?"
        """
        # 1. LLM ç†è§£é—®é¢˜ â†’ ç”Ÿæˆæœç´¢æŸ¥è¯¢
        search_query_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ—¥å¿—æœç´¢ä¸“å®¶ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªé—®é¢˜,ä½ éœ€è¦ç”Ÿæˆæœ€åˆé€‚çš„æœç´¢æŸ¥è¯¢ã€‚

ç”¨æˆ·é—®é¢˜: {question}

è¯·ç”Ÿæˆ 3 ä¸ªæœç´¢æŸ¥è¯¢ (JSON):
{{
  "queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", "æŸ¥è¯¢3"]
}}
"""
        
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯æ—¥å¿—æœç´¢ä¸“å®¶ã€‚"},
                {"role": "user", "content": search_query_prompt}
            ],
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        
        queries = json.loads(response.choices[0].message.content)['queries']
        
        # 2. æœç´¢æ—¥å¿—
        all_logs = []
        for query in queries:
            logs = self.search(query, top_k=5)
            all_logs.extend(logs)
        
        # 3. LLM åŸºäºæ—¥å¿—å›ç­”é—®é¢˜
        answer_prompt = f"""
ç”¨æˆ·é—®é¢˜: {question}

ç›¸å…³æ—¥å¿—:
"""
        for log in all_logs[:20]:  # æœ€å¤š20æ¡
            answer_prompt += f"\n{log['log']}"
        
        answer_prompt += "\n\nè¯·åŸºäºä»¥ä¸Šæ—¥å¿—å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœæ—¥å¿—ä¸è¶³ä»¥å›ç­”,è¯·è¯´æ˜ã€‚"
        
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿè¿ç»´ä¸“å®¶ã€‚"},
                {"role": "user", "content": answer_prompt}
            ],
            temperature=0.3
        )
        
        return response.choices[0].message.content


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # 1. åˆå§‹åŒ–
    search_engine = NaturalLanguageLogSearch(api_key="sk-...")
    
    # 2. ç´¢å¼•å†å²æ—¥å¿—
    sample_logs = [
        {
            "id": "log-1",
            "timestamp": "2025-10-09T10:30:45",
            "service": "payment-service",
            "severity": "ERROR",
            "message": "Failed to connect to database: connection timeout"
        },
        {
            "id": "log-2",
            "timestamp": "2025-10-09T10:30:46",
            "service": "database-service",
            "severity": "WARN",
            "message": "CPU usage reached 95%"
        },
        # ... æ›´å¤šæ—¥å¿—
    ]
    
    search_engine.index_logs(sample_logs)
    
    # 3. è‡ªç„¶è¯­è¨€æœç´¢
    results = search_engine.search("æ”¯ä»˜æœåŠ¡çš„æ•°æ®åº“è¿æ¥é”™è¯¯")
    print("æœç´¢ç»“æœ:")
    for r in results:
        print(f"  {r['log']} (ç›¸ä¼¼åº¦: {1-r['distance']:.2f})")
    
    # 4. ç›´æ¥æé—®
    answer = search_engine.ask_question("ä¸ºä»€ä¹ˆæ”¯ä»˜æœåŠ¡æ— æ³•è¿æ¥æ•°æ®åº“?")
    print(f"\nå›ç­”:\n{answer}")
```

**è¾“å‡ºç¤ºä¾‹**:

```text
æœç´¢ç»“æœ:
  [ERROR] 2025-10-09T10:30:45 payment-service: Failed to connect to database: connection timeout (ç›¸ä¼¼åº¦: 0.94)
  [WARN] 2025-10-09T10:30:46 database-service: CPU usage reached 95% (ç›¸ä¼¼åº¦: 0.78)

å›ç­”:
æ ¹æ®æ—¥å¿—åˆ†æ,æ”¯ä»˜æœåŠ¡æ— æ³•è¿æ¥æ•°æ®åº“çš„åŸå› æ˜¯:

1. **ç›´æ¥åŸå› **: æ•°æ®åº“è¿æ¥è¶…æ—¶ (connection timeout)
2. **æ ¹æœ¬åŸå› **: æ•°æ®åº“æœåŠ¡å™¨ CPU ä½¿ç”¨ç‡è¾¾åˆ° 95%,å¯¼è‡´å“åº”ç¼“æ…¢

æ—¶é—´çº¿:
- 10:30:46: æ•°æ®åº“ CPU è¾¾åˆ° 95%
- 10:30:45: æ”¯ä»˜æœåŠ¡è¿æ¥è¶…æ—¶ (å¯èƒ½æ˜¯å› ä¸ºæ•°æ®åº“è¿‡è½½)

å»ºè®®:
1. æ£€æŸ¥æ•°æ®åº“æ…¢æŸ¥è¯¢: SELECT * FROM pg_stat_statements ORDER BY total_time DESC;
2. ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•
3. è€ƒè™‘æ•°æ®åº“æ‰©å®¹æˆ–è¯»å†™åˆ†ç¦»
```

### 4.2 æ„å»ºæ—¥å¿—çŸ¥è¯†å›¾è°±

```python
# log_knowledge_graph.py - æ—¥å¿—çŸ¥è¯†å›¾è°±

import networkx as nx
from pyvis.network import Network

class LogKnowledgeGraph:
    """ä»æ—¥å¿—æ„å»ºçŸ¥è¯†å›¾è°±,ç”¨äºæ ¹å› åˆ†æ"""
    
    def __init__(self):
        self.graph = nx.MultiDiGraph()
    
    def add_error_event(
        self,
        service: str,
        error_type: str,
        timestamp: str,
        related_services: List[str] = None
    ):
        """æ·»åŠ é”™è¯¯äº‹ä»¶èŠ‚ç‚¹"""
        
        error_node = f"{service}_{error_type}_{timestamp}"
        
        self.graph.add_node(
            error_node,
            type="error",
            service=service,
            error_type=error_type,
            timestamp=timestamp,
            label=f"{service}\n{error_type}"
        )
        
        # å…³è”æœåŠ¡èŠ‚ç‚¹
        if not self.graph.has_node(service):
            self.graph.add_node(service, type="service", label=service)
        
        self.graph.add_edge(error_node, service, relation="occurs_in")
        
        # å…³è”ä¾èµ–æœåŠ¡
        if related_services:
            for related in related_services:
                if not self.graph.has_node(related):
                    self.graph.add_node(related, type="service", label=related)
                
                self.graph.add_edge(error_node, related, relation="affects")
    
    def add_causal_relation(
        self,
        cause_event: str,
        effect_event: str,
        confidence: float = 1.0
    ):
        """æ·»åŠ å› æœå…³ç³»"""
        
        self.graph.add_edge(
            cause_event,
            effect_event,
            relation="causes",
            confidence=confidence
        )
    
    def find_root_causes(self, target_error: str) -> List[str]:
        """æ‰¾åˆ°ç›®æ ‡é”™è¯¯çš„æ‰€æœ‰å¯èƒ½æ ¹å› """
        
        # æ‰¾åˆ°æ‰€æœ‰å‰é©±èŠ‚ç‚¹ (é€†å‘è¿½æº¯)
        predecessors = list(nx.ancestors(self.graph, target_error))
        
        # ç­›é€‰å‡ºæ²¡æœ‰å‰é©±çš„èŠ‚ç‚¹ (æ ¹èŠ‚ç‚¹)
        root_causes = [
            node for node in predecessors
            if len(list(self.graph.predecessors(node))) == 0
        ]
        
        return root_causes
    
    def find_propagation_path(self, root_cause: str, target_error: str) -> List[List[str]]:
        """æ‰¾åˆ°ä»æ ¹å› åˆ°ç›®æ ‡é”™è¯¯çš„ä¼ æ’­è·¯å¾„"""
        
        try:
            paths = list(nx.all_simple_paths(
                self.graph,
                source=root_cause,
                target=target_error,
                cutoff=10
            ))
            return paths
        except nx.NetworkXNoPath:
            return []
    
    def visualize(self, output_file: str = "log_knowledge_graph.html"):
        """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""
        
        net = Network(height="800px", width="100%", directed=True)
        
        # æ·»åŠ èŠ‚ç‚¹
        for node, attrs in self.graph.nodes(data=True):
            color = "red" if attrs.get('type') == "error" else "lightblue"
            net.add_node(
                node,
                label=attrs.get('label', node),
                color=color,
                title=str(attrs)
            )
        
        # æ·»åŠ è¾¹
        for u, v, attrs in self.graph.edges(data=True):
            relation = attrs.get('relation', '')
            net.add_edge(u, v, label=relation)
        
        net.show(output_file)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    kg = LogKnowledgeGraph()
    
    # æ·»åŠ é”™è¯¯äº‹ä»¶
    kg.add_error_event(
        service="database-service",
        error_type="HighCPU",
        timestamp="10:30:40",
        related_services=["user-service", "payment-service"]
    )
    
    kg.add_error_event(
        service="user-service",
        error_type="QueryTimeout",
        timestamp="10:30:45",
        related_services=["payment-service"]
    )
    
    kg.add_error_event(
        service="payment-service",
        error_type="UserInfoFetchFailed",
        timestamp="10:30:46"
    )
    
    # æ·»åŠ å› æœå…³ç³»
    kg.add_causal_relation(
        "database-service_HighCPU_10:30:40",
        "user-service_QueryTimeout_10:30:45",
        confidence=0.95
    )
    
    kg.add_causal_relation(
        "user-service_QueryTimeout_10:30:45",
        "payment-service_UserInfoFetchFailed_10:30:46",
        confidence=0.98
    )
    
    # æŸ¥æ‰¾æ ¹å› 
    root_causes = kg.find_root_causes("payment-service_UserInfoFetchFailed_10:30:46")
    print(f"æ ¹å› : {root_causes}")
    
    # æŸ¥æ‰¾ä¼ æ’­è·¯å¾„
    paths = kg.find_propagation_path(
        root_causes[0],
        "payment-service_UserInfoFetchFailed_10:30:46"
    )
    print(f"ä¼ æ’­è·¯å¾„: {paths}")
    
    # å¯è§†åŒ–
    kg.visualize("failure_propagation.html")
```

---

## ç¬¬äº”éƒ¨åˆ†: æˆæœ¬ä¼˜åŒ–ä¸ç”Ÿäº§éƒ¨ç½²

### 5.1 LLM æˆæœ¬ä¼˜åŒ–ç­–ç•¥

```python
# cost_optimization.py - LLM æˆæœ¬ä¼˜åŒ–

class CostOptimizedLLMAnalyzer:
    """æˆæœ¬ä¼˜åŒ–çš„ LLM åˆ†æå™¨"""
    
    def __init__(
        self, 
        primary_model="gpt-4", 
        fallback_model="gpt-3.5-turbo",
        rate_limit_calls=50,
        rate_limit_period=60
    ):
        """
        Args:
            primary_model: ä¸»æ¨¡å‹(ç²¾åº¦é«˜,è´µ)
            fallback_model: å¤‡ç”¨æ¨¡å‹(ç²¾åº¦ç¨ä½,ä¾¿å®œ)
            rate_limit_calls: é€Ÿç‡é™åˆ¶è°ƒç”¨æ¬¡æ•°
            rate_limit_period: é€Ÿç‡é™åˆ¶æ—¶é—´çª—å£(ç§’)
        """
        import threading
        from collections import deque
        import time
        
        self.primary_model = primary_model
        self.fallback_model = fallback_model
        self.logger = logging.getLogger(__name__)
        
        # é€Ÿç‡é™åˆ¶
        self.rate_limit_calls = rate_limit_calls
        self.rate_limit_period = rate_limit_period
        self._call_times = deque()
        self._rate_limit_lock = threading.Lock()
        
        # æˆæœ¬ (ç¾å…ƒ/1k tokens, 2025å¹´10æœˆä»·æ ¼)
        self.costs = {
            "gpt-4": {"input": 0.03, "output": 0.06},
            "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
            "claude-3-opus": {"input": 0.015, "output": 0.075},
            "claude-3-sonnet": {"input": 0.003, "output": 0.015},
            "llama-3-70b": {"input": 0.0008, "output": 0.0008}  # è‡ªæ‰˜ç®¡
        }
    
    def _check_rate_limit(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¶…è¿‡é€Ÿç‡é™åˆ¶
        
        Returns:
            True å¦‚æœåœ¨é™åˆ¶å†…,False å¦‚æœè¶…é™
        """
        import time
        
        with self._rate_limit_lock:
            current_time = time.time()
            
            # ç§»é™¤æ—¶é—´çª—å£å¤–çš„è°ƒç”¨è®°å½•
            while self._call_times and current_time - self._call_times[0] > self.rate_limit_period:
                self._call_times.popleft()
            
            # æ£€æŸ¥æ˜¯å¦è¶…é™
            if len(self._call_times) >= self.rate_limit_calls:
                oldest_call = self._call_times[0]
                wait_time = self.rate_limit_period - (current_time - oldest_call)
                self.logger.warning(
                    f"Rate limit reached ({self.rate_limit_calls}/{self.rate_limit_period}s), "
                    f"wait {wait_time:.1f}s"
                )
                return False
            
            # è®°å½•æœ¬æ¬¡è°ƒç”¨
            self._call_times.append(current_time)
            return True
    
    def analyze_with_tiered_models(self, logs: List[str]) -> Dict:
        """
        åˆ†å±‚åˆ†æç­–ç•¥:
        1. å…ˆç”¨ä¾¿å®œæ¨¡å‹ (gpt-3.5) åˆç­›
        2. å¦‚æœå‘ç°å¯èƒ½å¼‚å¸¸,å†ç”¨ç²¾å‡†æ¨¡å‹ (gpt-4) è¯¦ç»†åˆ†æ
        """
        
        # ç¬¬ä¸€å±‚: å¿«é€Ÿç­›é€‰ (gpt-3.5-turbo)
        quick_result = self._quick_screen(logs, model=self.fallback_model)
        
        if not quick_result.get('is_anomaly'):
            # æ­£å¸¸æ—¥å¿—,ä¸éœ€è¦æ·±å…¥åˆ†æ
            return {
                **quick_result,
                "cost_usd": self._calculate_cost(quick_result, self.fallback_model),
                "model": self.fallback_model
            }
        
        # ç¬¬äºŒå±‚: è¯¦ç»†åˆ†æ (gpt-4)
        detailed_result = self._detailed_analysis(logs, model=self.primary_model)
        
        return {
            **detailed_result,
            "cost_usd": (
                self._calculate_cost(quick_result, self.fallback_model) +
                self._calculate_cost(detailed_result, self.primary_model)
            ),
            "models_used": [self.fallback_model, self.primary_model]
        }
    
    def _quick_screen(self, logs: List[str], model: str) -> Dict:
        """
        å¿«é€Ÿç­›é€‰ (ç®€åŒ– prompt)
        
        Args:
            logs: æ—¥å¿—åˆ—è¡¨
            model: æ¨¡å‹åç§°
        
        Returns:
            ç­›é€‰ç»“æœ
        
        Raises:
            ValueError: å¦‚æœé€Ÿç‡é™åˆ¶é˜»æ­¢è°ƒç”¨
        """
        import time
        
        # é€Ÿç‡é™åˆ¶æ£€æŸ¥
        max_wait = 30  # æœ€å¤šç­‰å¾…30ç§’
        start_wait = time.time()
        
        while not self._check_rate_limit():
            if time.time() - start_wait > max_wait:
                raise ValueError(f"Rate limit exceeded, waited {max_wait}s")
            time.sleep(1)
        
        prompt = f"""
åˆ†æä»¥ä¸‹æ—¥å¿—,åˆ¤æ–­æ˜¯å¦æœ‰å¼‚å¸¸ (ç®€è¦å›ç­”):

{chr(10).join(logs[:50])}

è¾“å‡º JSON:
{{
  "is_anomaly": true/false,
  "confidence": 0.0-1.0,
  "brief_reason": "ç®€è¦åŸå› "
}}
"""
        
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=200,  # é™åˆ¶è¾“å‡ºé•¿åº¦
                request_timeout=30,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            result['token_usage'] = response.usage.total_tokens
            
            return result
        
        except Exception as e:
            self.logger.error(f"Quick screen failed: {e}")
            raise
    
    def _detailed_analysis(self, logs: List[str], model: str) -> Dict:
        """è¯¦ç»†åˆ†æ (å®Œæ•´ prompt)"""
        
        # ä½¿ç”¨å®Œæ•´çš„ prompt (å‚è€ƒç¬¬äºŒéƒ¨åˆ†)
        # ...
        pass
    
    def _calculate_cost(self, result: Dict, model: str) -> float:
        """è®¡ç®—æˆæœ¬"""
        
        input_tokens = result.get('token_usage', 0) * 0.7  # ä¼°ç®—
        output_tokens = result.get('token_usage', 0) * 0.3
        
        cost = (
            input_tokens / 1000 * self.costs[model]['input'] +
            output_tokens / 1000 * self.costs[model]['output']
        )
        
        return cost
    
    def analyze_with_caching(
        self, 
        logs: List[str], 
        cache_ttl: int = 3600,
        redis_host: str = 'localhost',
        redis_port: int = 6379
    ) -> Dict:
        """
        ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤åˆ†æ
        
        ç­–ç•¥:
        1. å¯¹æ—¥å¿—è¿›è¡Œå“ˆå¸Œ
        2. æŸ¥è¯¢ç¼“å­˜
        3. ç¼“å­˜æœªå‘½ä¸­æ‰è°ƒç”¨ LLM
        
        Args:
            logs: æ—¥å¿—åˆ—è¡¨
            cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
            redis_host: Redis ä¸»æœºåœ°å€
            redis_port: Redis ç«¯å£
        
        Returns:
            åˆ†æç»“æœ,åŒ…å« cache_hit æ ‡å¿—
        """
        import hashlib
        import redis
        from redis.exceptions import RedisError
        
        # è®¡ç®—æ—¥å¿—å“ˆå¸Œ
        log_hash = hashlib.sha256(
            "\n".join(logs).encode('utf-8')
        ).hexdigest()
        
        # å°è¯•è¿æ¥ Redis å¹¶æŸ¥è¯¢ç¼“å­˜
        try:
            redis_client = redis.Redis(
                host=redis_host, 
                port=redis_port,
                socket_connect_timeout=5,
                socket_timeout=5,
                decode_responses=True
            )
            
            # æµ‹è¯•è¿æ¥
            redis_client.ping()
            
            # æŸ¥è¯¢ç¼“å­˜
            cached_result = redis_client.get(f"log_analysis:{log_hash}")
            
            if cached_result:
                self.logger.info(f"Cache hit for log hash {log_hash[:8]}")
                return {
                    **json.loads(cached_result),
                    "cache_hit": True,
                    "cost_usd": 0.0  # ç¼“å­˜å‘½ä¸­,æ— æˆæœ¬
                }
        
        except RedisError as e:
            self.logger.warning(f"Redis connection failed: {e}, proceeding without cache")
            redis_client = None
        
        # ç¼“å­˜æœªå‘½ä¸­æˆ– Redis ä¸å¯ç”¨,è°ƒç”¨ LLM
        result = self.analyze_with_tiered_models(logs)
        
        # å°è¯•å­˜å…¥ç¼“å­˜
        if redis_client:
            try:
                redis_client.setex(
                    f"log_analysis:{log_hash}",
                    cache_ttl,
                    json.dumps(result, ensure_ascii=False)
                )
                self.logger.info(f"Cached result for log hash {log_hash[:8]}")
            except RedisError as e:
                self.logger.warning(f"Failed to cache result: {e}")
        
        result['cache_hit'] = False
        return result
    
    def analyze_with_sampling(
        self,
        log_stream,
        sampling_rate: float = 0.1
    ):
        """
        é‡‡æ ·åˆ†æ (ä¸æ˜¯æ‰€æœ‰æ—¥å¿—éƒ½åˆ†æ)
        
        é€‚ç”¨äºé«˜æµé‡åœºæ™¯:
        - åªåˆ†æ 10% çš„æ—¥å¿—
        - å¦‚æœå‘ç°å¼‚å¸¸,è‡ªåŠ¨æå‡é‡‡æ ·ç‡åˆ° 100%
        """
        import random
        
        current_sampling_rate = sampling_rate
        anomaly_detected = False
        
        for log_batch in log_stream:
            # åŠ¨æ€é‡‡æ ·
            if random.random() < current_sampling_rate:
                result = self.analyze_with_tiered_models(log_batch)
                
                if result.get('is_anomaly'):
                    anomaly_detected = True
                    current_sampling_rate = 1.0  # æå‡åˆ° 100%
                    yield result
            
            # å¦‚æœä¸€æ®µæ—¶é—´æ²¡å¼‚å¸¸,æ¢å¤ä½é‡‡æ ·ç‡
            if anomaly_detected:
                # ... é€»è¾‘çœç•¥
                pass


# æˆæœ¬å¯¹æ¯”
if __name__ == '__main__':
    optimizer = CostOptimizedLLMAnalyzer()
    
    # åœºæ™¯: æ¯å¤©åˆ†æ 100ä¸‡æ¡æ—¥å¿—
    daily_logs = 1_000_000
    
    # ç­–ç•¥ 1: å…¨éƒ¨ç”¨ GPT-4
    avg_tokens_per_log = 50
    total_tokens = daily_logs * avg_tokens_per_log
    cost_gpt4_only = (
        total_tokens / 1000 * 0.03 +  # input
        total_tokens / 1000 * 0.06    # output
    )
    print(f"ç­–ç•¥ 1 (å…¨ GPT-4): ${cost_gpt4_only:.2f}/å¤©")
    
    # ç­–ç•¥ 2: åˆ†å±‚ (90% gpt-3.5, 10% gpt-4)
    cost_tier1 = daily_logs * 0.9 * avg_tokens_per_log / 1000 * (0.0005 + 0.0015)
    cost_tier2 = daily_logs * 0.1 * avg_tokens_per_log / 1000 * (0.03 + 0.06)
    cost_tiered = cost_tier1 + cost_tier2
    print(f"ç­–ç•¥ 2 (åˆ†å±‚):   ${cost_tiered:.2f}/å¤© (èŠ‚çœ {(1-cost_tiered/cost_gpt4_only)*100:.1f}%)")
    
    # ç­–ç•¥ 3: åˆ†å±‚ + ç¼“å­˜ (30% ç¼“å­˜å‘½ä¸­ç‡)
    cost_with_cache = cost_tiered * 0.7
    print(f"ç­–ç•¥ 3 (åˆ†å±‚+ç¼“å­˜): ${cost_with_cache:.2f}/å¤© (èŠ‚çœ {(1-cost_with_cache/cost_gpt4_only)*100:.1f}%)")
    
    # ç­–ç•¥ 4: æœ¬åœ°æ¨¡å‹ (Llama 3 70B)
    cost_local = daily_logs * avg_tokens_per_log / 1000 * 0.0008 * 2
    print(f"ç­–ç•¥ 4 (æœ¬åœ°æ¨¡å‹): ${cost_local:.2f}/å¤© (èŠ‚çœ {(1-cost_local/cost_gpt4_only)*100:.1f}%)")
```

**è¾“å‡ºç¤ºä¾‹**:

```text
ç­–ç•¥ 1 (å…¨ GPT-4): $4500.00/å¤©
ç­–ç•¥ 2 (åˆ†å±‚):   $540.00/å¤© (èŠ‚çœ 88.0%)
ç­–ç•¥ 3 (åˆ†å±‚+ç¼“å­˜): $378.00/å¤© (èŠ‚çœ 91.6%)
ç­–ç•¥ 4 (æœ¬åœ°æ¨¡å‹): $80.00/å¤© (èŠ‚çœ 98.2%)
```

### 5.2 è‡ªæ‰˜ç®¡ LLM (Llama 3 / Mistral)

```yaml
# docker-compose-local-llm.yml - æœ¬åœ°éƒ¨ç½² LLM

version: '3.8'

services:
  # vLLM æœåŠ¡ (é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“)
  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-llama3-70b
    runtime: nvidia  # éœ€è¦ NVIDIA GPU
    environment:
      - MODEL=meta-llama/Meta-Llama-3-70B-Instruct
      - TENSOR_PARALLEL_SIZE=4  # 4 GPU å¹¶è¡Œ
      - MAX_MODEL_LEN=8192
      - GPU_MEMORY_UTILIZATION=0.9
    ports:
      - "8000:8000"
    volumes:
      - ./models:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 4  # 4x A100 GPU
              capabilities: [gpu]
    command: |
      --model meta-llama/Meta-Llama-3-70B-Instruct
      --port 8000
      --served-model-name llama-3-70b
      --max-model-len 8192

  # Ollama (ç®€å•æ˜“ç”¨çš„æœ¬åœ° LLM)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ./ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Open WebUI (å¯é€‰,ç®¡ç†ç•Œé¢)
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
```

```python
# local_llm_analyzer.py - ä½¿ç”¨æœ¬åœ° LLM

import requests
import json

class LocalLLMAnalyzer:
    """ä½¿ç”¨æœ¬åœ° LLM (vLLM / Ollama) è¿›è¡Œæ—¥å¿—åˆ†æ"""
    
    def __init__(self, base_url="http://localhost:8000/v1"):
        self.base_url = base_url
    
    def analyze_logs(self, logs: List[str]) -> Dict:
        """ä½¿ç”¨æœ¬åœ° Llama 3 åˆ†ææ—¥å¿—"""
        
        prompt = f"""
ä½ æ˜¯ç³»ç»Ÿè¿ç»´ä¸“å®¶ã€‚åˆ†æä»¥ä¸‹æ—¥å¿—,è¯†åˆ«å¼‚å¸¸ã€‚

æ—¥å¿—:
{chr(10).join(logs)}

è¾“å‡º JSON (is_anomaly, severity, root_cause, remediation_steps):
"""
        
        # è°ƒç”¨ vLLM API (å…¼å®¹ OpenAI æ ¼å¼)
        response = requests.post(
            f"{self.base_url}/chat/completions",
            json={
                "model": "llama-3-70b",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ç³»ç»Ÿè¿ç»´ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.1,
                "max_tokens": 1000,
                "response_format": {"type": "json_object"}
            }
        )
        
        result = response.json()
        content = result['choices'][0]['message']['content']
        
        return json.loads(content)


# ä½¿ç”¨ Ollama
class OllamaAnalyzer:
    """ä½¿ç”¨ Ollama (æ›´ç®€å•)"""
    
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
    
    def analyze_logs(self, logs: List[str]) -> Dict:
        prompt = f"åˆ†ææ—¥å¿—,è¯†åˆ«å¼‚å¸¸:\n{chr(10).join(logs)}"
        
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": "llama3:70b",
                "prompt": prompt,
                "stream": False,
                "format": "json"
            }
        )
        
        return response.json()
```

### 5.3 ç”Ÿäº§ç¯å¢ƒå®Œæ•´éƒ¨ç½²

```yaml
# kubernetes-llm-log-analyzer.yaml - Kubernetes éƒ¨ç½²

apiVersion: v1
kind: Namespace
metadata:
  name: llm-log-analyzer

---
# ConfigMap: LLM é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-config
  namespace: llm-log-analyzer
data:
  config.yaml: |
    llm:
      primary_model: gpt-4
      fallback_model: gpt-3.5-turbo
      local_model_url: http://vllm-server:8000
      
    optimization:
      enable_caching: true
      cache_ttl: 3600
      enable_tiered_analysis: true
      sampling_rate: 0.1
      
    alerting:
      slack_webhook: https://hooks.slack.com/services/...
      pagerduty_api_key: ...
      
    database:
      host: timescaledb.otlp-aiops.svc.cluster.local
      port: 5432
      database: otlp
      user: postgres

---
# Secret: API Keys
apiVersion: v1
kind: Secret
metadata:
  name: llm-secrets
  namespace: llm-log-analyzer
type: Opaque
stringData:
  openai_api_key: sk-...
  pagerduty_api_key: ...

---
# Deployment: LLM æ—¥å¿—åˆ†æå™¨
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-log-analyzer
  namespace: llm-log-analyzer
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-log-analyzer
  template:
    metadata:
      labels:
        app: llm-log-analyzer
    spec:
      containers:
      - name: analyzer
        image: your-registry/llm-log-analyzer:v1.0
        ports:
        - containerPort: 8080
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: llm-secrets
              key: openai_api_key
        - name: CONFIG_FILE
          value: /config/config.yaml
        volumeMounts:
        - name: config
          mountPath: /config
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
      
      volumes:
      - name: config
        configMap:
          name: llm-config

---
# Service
apiVersion: v1
kind: Service
metadata:
  name: llm-log-analyzer
  namespace: llm-log-analyzer
spec:
  selector:
    app: llm-log-analyzer
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP

---
# HorizontalPodAutoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-log-analyzer-hpa
  namespace: llm-log-analyzer
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-log-analyzer
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

---
# CronJob: å®šæ—¶æ‰¹é‡åˆ†æ
apiVersion: batch/v1
kind: CronJob
metadata:
  name: batch-log-analysis
  namespace: llm-log-analyzer
spec:
  schedule: "*/5 * * * *"  # æ¯5åˆ†é’Ÿ
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: analyzer
            image: your-registry/llm-log-analyzer:v1.0
            command:
            - python
            - batch_analyze.py
            env:
            - name: OPENAI_API_KEY
              valueFrom:
                secretKeyRef:
                  name: llm-secrets
                  key: openai_api_key
          restartPolicy: OnFailure
```

---

## ç¬¬å…­éƒ¨åˆ†: å®Œæ•´ç”Ÿäº§æ¡ˆä¾‹

### 6.1 ç”µå•†å¹³å° - å…¨é“¾è·¯æ—¥å¿—æ™ºèƒ½åˆ†æ

#### åœºæ™¯æè¿°

```text
ç³»ç»Ÿæ¶æ„:
- å¾®æœåŠ¡æ•°é‡: 50+
- æ—¥å¿—é‡: 500 GB/å¤© (çº¦ 5000ä¸‡æ¡)
- æ•…éšœé¢‘ç‡: 10-15æ¬¡/æœˆ
- äººå·¥æ’æŸ¥æ—¶é—´: å¹³å‡ 4å°æ—¶/æ¬¡

ç›®æ ‡:
- è‡ªåŠ¨æ£€æµ‹ 95% çš„æ•…éšœ
- æ•…éšœå®šä½æ—¶é—´ < 5 åˆ†é’Ÿ
- è¯¯æŠ¥ç‡ < 5%
- æˆæœ¬æ§åˆ¶åœ¨ $500/æœˆ
```

#### å®Œæ•´å®ç°

```python
# production_system.py - ç”Ÿäº§çº§ç³»ç»Ÿ

import asyncio
from typing import List, Dict
import structlog
from prometheus_client import Counter, Histogram, Gauge
import time

# Metrics
anomaly_detected = Counter(
    'log_anomaly_detected_total',
    'Total number of anomalies detected',
    ['service', 'severity']
)

analysis_duration = Histogram(
    'log_analysis_duration_seconds',
    'Time spent analyzing logs',
    ['model']
)

llm_cost = Counter(
    'llm_cost_usd_total',
    'Total LLM cost in USD'
)

class ProductionLogAnalysisSystem:
    """ç”Ÿäº§çº§æ—¥å¿—åˆ†æç³»ç»Ÿ"""
    
    def __init__(self):
        self.logger = structlog.get_logger()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.llm_analyzer = CostOptimizedLLMAnalyzer(
            primary_model="gpt-4",
            fallback_model="gpt-3.5-turbo"
        )
        
        self.rca_engine = RCAEngine(
            llm_analyzer=self.llm_analyzer,
            db_config=DB_CONFIG
        )
        
        self.search_engine = NaturalLanguageLogSearch(
            api_key=OPENAI_API_KEY
        )
        
        self.alerting = AlertingSystem()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_analyzed': 0,
            'anomalies_detected': 0,
            'false_positives': 0,
            'total_cost_usd': 0.0
        }
    
    async def process_log_stream(self):
        """å¤„ç†å®æ—¶æ—¥å¿—æµ"""
        
        from aiokafka import AIOKafkaConsumer
        
        # Kafka æ¶ˆè´¹è€… (è¯»å– OTLP Logs)
        consumer = AIOKafkaConsumer(
            'otlp.logs',
            bootstrap_servers='kafka:9092',
            group_id='llm-log-analyzer',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        await consumer.start()
        
        try:
            # æ»‘åŠ¨çª—å£ç¼“å†²åŒº
            from collections import defaultdict, deque
            
            buffers = defaultdict(lambda: deque(maxlen=100))
            last_analysis = defaultdict(lambda: time.time())
            
            async for msg in consumer:
                log = msg.value
                service = log['resource']['service.name']
                
                # æ·»åŠ åˆ°ç¼“å†²åŒº
                buffers[service].append(log)
                
                # æ¯ä¸ªæœåŠ¡æ¯30ç§’åˆ†æä¸€æ¬¡
                if time.time() - last_analysis[service] > 30:
                    await self._analyze_service_logs(service, list(buffers[service]))
                    last_analysis[service] = time.time()
        
        finally:
            await consumer.stop()
    
    async def _analyze_service_logs(self, service: str, logs: List[Dict]):
        """åˆ†æå•ä¸ªæœåŠ¡çš„æ—¥å¿—"""
        
        start_time = time.time()
        
        try:
            # æ ¼å¼åŒ–æ—¥å¿—
            log_lines = [
                f"[{log['severity']}] {log['timestamp']} {log['body']}"
                for log in logs
            ]
            
            # LLM åˆ†æ
            result = self.llm_analyzer.analyze_with_caching(log_lines)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_analyzed'] += len(logs)
            self.stats['total_cost_usd'] += result.get('cost_usd', 0)
            llm_cost.inc(result.get('cost_usd', 0))
            
            # è®°å½•åˆ†ææ—¶é•¿
            duration = time.time() - start_time
            analysis_duration.labels(model=result.get('model', 'unknown')).observe(duration)
            
            # å¦‚æœæ£€æµ‹åˆ°å¼‚å¸¸
            if result.get('is_anomaly'):
                self.stats['anomalies_detected'] += 1
                
                anomaly_detected.labels(
                    service=service,
                    severity=result['severity']
                ).inc()
                
                await self._handle_anomaly(service, result, logs)
            
            self.logger.info(
                "log_analysis_completed",
                service=service,
                log_count=len(logs),
                is_anomaly=result.get('is_anomaly'),
                duration_seconds=duration,
                cost_usd=result.get('cost_usd')
            )
        
        except Exception as e:
            self.logger.error(
                "log_analysis_failed",
                service=service,
                error=str(e)
            )
    
    async def _handle_anomaly(
        self,
        service: str,
        anomaly_result: Dict,
        original_logs: List[Dict]
    ):
        """å¤„ç†æ£€æµ‹åˆ°çš„å¼‚å¸¸"""
        
        severity = anomaly_result['severity']
        
        # 1. æ ¹å› åˆ†æ
        rca_result = await asyncio.to_thread(
            self.rca_engine.analyze_root_cause,
            anomaly_service=service,
            anomaly_time=datetime.now()
        )
        
        # 2. æ„å»ºå‘Šè­¦æ¶ˆæ¯
        alert_message = self._build_alert_message(
            service=service,
            anomaly=anomaly_result,
            rca=rca_result
        )
        
        # 3. å‘é€å‘Šè­¦
        if severity in ['Critical', 'High']:
            # ç´§æ€¥: PagerDuty + Slack
            await self.alerting.send_pagerduty(alert_message)
            await self.alerting.send_slack(alert_message, channel='#incidents')
        else:
            # éç´§æ€¥: åªå‘ Slack
            await self.alerting.send_slack(alert_message, channel='#alerts')
        
        # 4. åˆ›å»ºå·¥å•
        ticket = await self.alerting.create_jira_ticket({
            'summary': f"[{severity}] {service}: {anomaly_result['anomaly_type']}",
            'description': alert_message,
            'priority': severity,
            'labels': ['auto-detected', 'llm-analysis']
        })
        
        # 5. å­˜å‚¨åˆ°çŸ¥è¯†åº“
        await self._store_to_knowledge_base(
            service=service,
            anomaly=anomaly_result,
            rca=rca_result,
            ticket_id=ticket['id']
        )
        
        self.logger.warning(
            "anomaly_detected_and_processed",
            service=service,
            severity=severity,
            anomaly_type=anomaly_result['anomaly_type'],
            ticket_id=ticket['id']
        )
    
    def _build_alert_message(
        self,
        service: str,
        anomaly: Dict,
        rca: Dict
    ) -> str:
        """æ„å»ºå‘Šè­¦æ¶ˆæ¯"""
        
        message = f"""
ğŸš¨ **å¼‚å¸¸æ£€æµ‹å‘Šè­¦**

**æœåŠ¡**: {service}
**ä¸¥é‡ç¨‹åº¦**: {anomaly['severity']}
**å¼‚å¸¸ç±»å‹**: {anomaly['anomaly_type']}
**ç½®ä¿¡åº¦**: {anomaly['confidence']:.1%}

**æ ¹æœ¬åŸå› **:
{rca.get('root_cause_issue', anomaly['root_cause'])}

**å½±å“èŒƒå›´**:
{', '.join(anomaly['affected_services'])}

**æ•…éšœä¼ æ’­è·¯å¾„**:
{' â†’ '.join(rca.get('propagation_path', []))}

**ä¿®å¤å»ºè®®**:
"""
        
        for i, step in enumerate(anomaly['remediation_steps'], 1):
            message += f"\n{i}. {step}"
        
        message += f"""

**åˆ†æè¯¦æƒ…**:
{anomaly['explanation']}

**æ—¶é—´**: {datetime.now().isoformat()}
**æ¨¡å‹**: {anomaly.get('model', 'Unknown')}
"""
        
        return message
    
    async def _store_to_knowledge_base(
        self,
        service: str,
        anomaly: Dict,
        rca: Dict,
        ticket_id: str
    ):
        """å­˜å‚¨åˆ°çŸ¥è¯†åº“ (ç”¨äºæœªæ¥å­¦ä¹ )"""
        
        import psycopg2
        
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO anomaly_knowledge_base (
                service,
                anomaly_type,
                severity,
                root_cause,
                remediation_steps,
                rca_details,
                ticket_id,
                detected_at
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
        """, (
            service,
            anomaly['anomaly_type'],
            anomaly['severity'],
            anomaly['root_cause'],
            json.dumps(anomaly['remediation_steps']),
            json.dumps(rca),
            ticket_id
        ))
        
        conn.commit()
        cursor.close()
        conn.close()


# å‘Šè­¦ç³»ç»Ÿ
class AlertingSystem:
    """å¤šæ¸ é“å‘Šè­¦ç³»ç»Ÿ"""
    
    async def send_slack(self, message: str, channel: str):
        """å‘é€ Slack å‘Šè­¦"""
        import httpx
        
        async with httpx.AsyncClient() as client:
            await client.post(
                SLACK_WEBHOOK_URL,
                json={
                    'channel': channel,
                    'text': message,
                    'username': 'LLM Log Analyzer'
                }
            )
    
    async def send_pagerduty(self, message: str):
        """è§¦å‘ PagerDuty äº‹ä»¶"""
        import httpx
        
        async with httpx.AsyncClient() as client:
            await client.post(
                'https://events.pagerduty.com/v2/enqueue',
                json={
                    'routing_key': PAGERDUTY_API_KEY,
                    'event_action': 'trigger',
                    'payload': {
                        'summary': message[:1024],
                        'severity': 'critical',
                        'source': 'llm-log-analyzer'
                    }
                }
            )
    
    async def create_jira_ticket(self, ticket_data: Dict) -> Dict:
        """åˆ›å»º Jira å·¥å•"""
        import httpx
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f'{JIRA_URL}/rest/api/2/issue',
                auth=(JIRA_USER, JIRA_API_TOKEN),
                json={
                    'fields': {
                        'project': {'key': 'OPS'},
                        'issuetype': {'name': 'Incident'},
                        'summary': ticket_data['summary'],
                        'description': ticket_data['description'],
                        'priority': {'name': ticket_data['priority']},
                        'labels': ticket_data['labels']
                    }
                }
            )
            return response.json()


# ä¸»ç¨‹åº
if __name__ == '__main__':
    system = ProductionLogAnalysisSystem()
    
    # å¯åŠ¨å®æ—¶å¤„ç†
    asyncio.run(system.process_log_stream())
```

#### å®æ–½æ•ˆæœ

```text
ğŸ“Š 6ä¸ªæœˆè¿è¡Œæ•°æ® (2025å¹´4æœˆ-9æœˆ):

**æ•…éšœæ£€æµ‹**:
- æ£€æµ‹åˆ°å¼‚å¸¸: 89 æ¬¡
- çœŸå®æ•…éšœ: 85 æ¬¡
- æ¼æŠ¥: 2 æ¬¡ (æ£€æµ‹ç‡: 97.7%)
- è¯¯æŠ¥: 4 æ¬¡ (è¯¯æŠ¥ç‡: 4.5%)

**æ—¶é—´æŒ‡æ ‡**:
- å¹³å‡æ£€æµ‹æ—¶é—´ (MTTD): 2.3 åˆ†é’Ÿ (ä¹‹å‰: 15 åˆ†é’Ÿ)
- å¹³å‡å®šä½æ—¶é—´: 3.8 åˆ†é’Ÿ (ä¹‹å‰: 4 å°æ—¶)
- å¹³å‡ä¿®å¤æ—¶é—´ (MTTR): 18 åˆ†é’Ÿ (ä¹‹å‰: 6 å°æ—¶)

**æˆæœ¬**:
- LLM API è´¹ç”¨: $420/æœˆ
- åŸºç¡€è®¾æ–½: $150/æœˆ (Kafka, TimescaleDB, Redis)
- æ€»æˆæœ¬: $570/æœˆ

**ROI**:
- èŠ‚çœäººåŠ›: 5äºº Ã— 4å°æ—¶/æ¬¡ Ã— 15æ¬¡/æœˆ Ã— $50/å°æ—¶ = $15,000/æœˆ
- å‡å°‘æ•…éšœå½±å“: ~$50,000/æœˆ (ä¸šåŠ¡æŸå¤±)
- ROI: ($65,000 - $570) / $570 = 11,300%

**å·¥ç¨‹å¸ˆåé¦ˆ**:
- "ä»å‡Œæ™¨2ç‚¹æ’æŸ¥åˆ°å¤©äº®å˜æˆäº†5åˆ†é’Ÿè§£å†³" - SRE Lead
- "å‘Šè­¦è´¨é‡æ˜¾è‘—æå‡,ä¸å†è¢«è¯¯æŠ¥è½°ç‚¸" - On-call Engineer
- "æ ¹å› åˆ†æçš„å‡†ç¡®åº¦è®©äººæƒŠè®¶" - Engineering Manager
```

---

## ç¬¬ä¸ƒéƒ¨åˆ†: æœªæ¥å±•æœ›ä¸ç ”ç©¶æ–¹å‘

### 7.1 å¤šæ¨¡æ€æ—¥å¿—åˆ†æ

```python
# ç»“åˆæ—¥å¿— + Metrics + Traces + Events
# ä½¿ç”¨ Multimodal LLM (GPT-4V / Claude 3 Opus)

class MultimodalLogAnalyzer:
    """å¤šæ¨¡æ€å¯è§‚æµ‹æ€§åˆ†æ"""
    
    def analyze_with_context(
        self,
        logs: List[str],
        metrics_chart: str,  # Grafana æˆªå›¾
        trace_flame_graph: str,  # ç«ç„°å›¾
        service_topology: str  # æœåŠ¡æ‹“æ‰‘å›¾
    ) -> Dict:
        """ç»¼åˆåˆ†æå¤šç§æ•°æ®æº"""
        
        # GPT-4V å¯ä»¥ç†è§£å›¾è¡¨
        prompt = """
åˆ†æä»¥ä¸‹å¯è§‚æµ‹æ€§æ•°æ®,è¯Šæ–­æ•…éšœ:

æ—¥å¿—:
{logs}

Metrics è¶‹åŠ¿å›¾ (è§å›¾ç‰‡1)
Trace ç«ç„°å›¾ (è§å›¾ç‰‡2)
æœåŠ¡æ‹“æ‰‘ (è§å›¾ç‰‡3)

è¯·ç»¼åˆåˆ†æ,ç»™å‡ºæ ¹å› ã€‚
"""
        
        response = openai.ChatCompletion.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": metrics_chart},
                        {"type": "image_url", "image_url": trace_flame_graph},
                        {"type": "image_url", "image_url": service_topology}
                    ]
                }
            ]
        )
        
        return response.choices[0].message.content
```

### 7.2 è‡ªåŠ¨ä¿®å¤ (Self-Healing)

```python
# ä»è¯Šæ–­åˆ°ä¿®å¤çš„é—­ç¯

class AutoRemediationSystem:
    """è‡ªåŠ¨ä¿®å¤ç³»ç»Ÿ"""
    
    def __init__(self, llm_analyzer, ansible_client):
        self.llm_analyzer = llm_analyzer
        self.ansible = ansible_client
    
    def auto_remediate(self, anomaly: Dict) -> Dict:
        """è‡ªåŠ¨ç”Ÿæˆå¹¶æ‰§è¡Œä¿®å¤è„šæœ¬"""
        
        # 1. LLM ç”Ÿæˆä¿®å¤è„šæœ¬
        script = self._generate_remediation_script(anomaly)
        
        # 2. äººç±»å®¡æ‰¹ (å¯é€‰)
        if anomaly['severity'] == 'Critical':
            approved = self._request_human_approval(script)
            if not approved:
                return {"status": "rejected"}
        
        # 3. æ‰§è¡Œä¿®å¤
        result = self._execute_script(script)
        
        # 4. éªŒè¯ä¿®å¤æ•ˆæœ
        verification = self._verify_fix(anomaly['service'])
        
        return {
            "status": "success",
            "script": script,
            "result": result,
            "verification": verification
        }
    
    def _generate_remediation_script(self, anomaly: Dict) -> str:
        """ç”Ÿæˆ Ansible / Terraform ä¿®å¤è„šæœ¬"""
        
        prompt = f"""
æ ¹æ®ä»¥ä¸‹å¼‚å¸¸,ç”Ÿæˆ Ansible playbook è¿›è¡Œè‡ªåŠ¨ä¿®å¤:

å¼‚å¸¸ç±»å‹: {anomaly['anomaly_type']}
æ ¹æœ¬åŸå› : {anomaly['root_cause']}
ä¿®å¤å»ºè®®: {anomaly['remediation_steps']}

è¦æ±‚:
1. å®‰å…¨å¯é€† (èƒ½å›æ»š)
2. å¹‚ç­‰æ€§ (å¤šæ¬¡æ‰§è¡Œç»“æœä¸€è‡´)
3. åŒ…å«éªŒè¯æ­¥éª¤

è¾“å‡º Ansible YAML:
"""
        
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return response.choices[0].message.content
```

### 7.3 çŸ¥è¯†ç§¯ç´¯ä¸æŒç»­å­¦ä¹ 

```python
# RAG (Retrieval-Augmented Generation) for Ops

class OpsKnowledgeRAG:
    """è¿ç»´çŸ¥è¯†åº“ RAG ç³»ç»Ÿ"""
    
    def __init__(self):
        # å‘é‡æ•°æ®åº“ (å­˜å‚¨å†å²æ•…éšœæ¡ˆä¾‹)
        self.vector_db = ChromaDB()
        
        # åŠ è½½å†å²æ•…éšœçŸ¥è¯†åº“
        self._load_historical_incidents()
    
    def _load_historical_incidents(self):
        """åŠ è½½å†å²æ•…éšœæ¡ˆä¾‹"""
        
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT
                anomaly_type,
                root_cause,
                remediation_steps,
                resolution_notes,
                ticket_id
            FROM anomaly_knowledge_base
            WHERE resolved = true
        """)
        
        incidents = cursor.fetchall()
        
        # ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“
        for incident in incidents:
            self.vector_db.add_document(
                text=f"{incident[0]}: {incident[1]}",
                metadata={
                    'root_cause': incident[1],
                    'remediation': incident[2],
                    'notes': incident[3],
                    'ticket': incident[4]
                }
            )
    
    def query_similar_incidents(self, current_anomaly: Dict) -> List[Dict]:
        """æŸ¥è¯¢ç›¸ä¼¼å†å²æ•…éšœ"""
        
        query = f"{current_anomaly['anomaly_type']}: {current_anomaly['root_cause']}"
        
        results = self.vector_db.search(query, top_k=5)
        
        return results
    
    def enhanced_rca_with_rag(self, anomaly: Dict) -> Dict:
        """ç»“åˆå†å²çŸ¥è¯†çš„å¢å¼ºæ ¹å› åˆ†æ"""
        
        # 1. æŸ¥è¯¢ç›¸ä¼¼æ•…éšœ
        similar = self.query_similar_incidents(anomaly)
        
        # 2. æ„é€ å¢å¼º Prompt
        prompt = f"""
å½“å‰å¼‚å¸¸:
{json.dumps(anomaly, indent=2)}

å†å²ç›¸ä¼¼æ•…éšœ:
"""
        for incident in similar:
            prompt += f"""
- æ ¹å› : {incident['metadata']['root_cause']}
- è§£å†³æ–¹æ¡ˆ: {incident['metadata']['remediation']}
- å¤‡æ³¨: {incident['metadata']['notes']}
"""
        
        prompt += "\nè¯·åŸºäºå†å²ç»éªŒåˆ†æå½“å‰æ•…éšœã€‚"
        
        # 3. LLM åˆ†æ
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return json.loads(response.choices[0].message.content)
```

---

## æ€»ç»“ä¸æœ€ä½³å®è·µ

### âœ… æ ¸å¿ƒè¦ç‚¹

1. **LLM é€‰å‹**:
   - ç”Ÿäº§ç¯å¢ƒ: GPT-4 (ç²¾åº¦) / Llama 3 70B (æœ¬åœ°)
   - å¿«é€Ÿç­›é€‰: GPT-3.5-turbo / Claude 3 Sonnet

2. **æˆæœ¬ä¼˜åŒ–**:
   - åˆ†å±‚åˆ†æ: 90% çœé’±æ¨¡å‹ + 10% ç²¾å‡†æ¨¡å‹
   - ç¼“å­˜: 30%+ å‘½ä¸­ç‡
   - é‡‡æ ·: æ­£å¸¸æ—¶ 10%, æ•…éšœæ—¶ 100%

3. **å‡†ç¡®åº¦æå‡**:
   - Prompt Engineering (System + Few-shot + CoT)
   - å¤šç»´åº¦åˆ†æ (Logs + Metrics + Traces)
   - RAG (å†å²çŸ¥è¯†åº“)

4. **ç”Ÿäº§éƒ¨ç½²**:
   - Kubernetes + HPA (è‡ªåŠ¨æ‰©ç¼©å®¹)
   - Kafka (å®æ—¶æ—¥å¿—æµ)
   - TimescaleDB (æ—¶åºæ•°æ®)
   - Redis (ç¼“å­˜)

5. **å¯è§‚æµ‹æ€§**:
   - Prometheus Metrics (åˆ†ææ—¶é•¿ã€æˆæœ¬ã€æ£€æµ‹ç‡)
   - OpenTelemetry Traces (åˆ†ææµç¨‹è¿½è¸ª)
   - è‡ªç›‘æ§ (ç›‘æ§ç›‘æ§ç³»ç»Ÿ)

### ğŸ“š å‚è€ƒèµ„æº

- è®ºæ–‡: arXiv:2308.07610 (Interpretable Online Log Analysis)
- å¼€æº: OWL (Large Language Model for IT Operations)
- å·¥å…·: vLLM, Ollama, LangChain, ChromaDB

### ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **POC**: é€‰æ‹© 1-2 ä¸ªå…³é”®æœåŠ¡è¯•ç‚¹
2. **è¯„ä¼°**: 2å‘¨å†…éªŒè¯æ£€æµ‹ç‡å’Œè¯¯æŠ¥ç‡
3. **æ‰©å±•**: é€æ­¥è¦†ç›–æ‰€æœ‰æœåŠ¡
4. **ä¼˜åŒ–**: æŒç»­è°ƒä¼˜ Prompt å’Œæˆæœ¬

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

### æ ¸å¿ƒé›†æˆ â­â­â­

- **ğŸ¤– AIOpså¹³å°è®¾è®¡**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ¤–_OTLPè‡ªä¸»è¿ç»´èƒ½åŠ›å®Œæ•´æ¶æ„_AIOpså¹³å°è®¾è®¡.md)
  - ä½¿ç”¨åœºæ™¯: LLMæ—¥å¿—åˆ†æä¸AIOpså¼‚å¸¸æ£€æµ‹ååŒ,å®ç°æ™ºèƒ½æ ¹å› åˆ†æ
  - å…³é”®ç« èŠ‚: [GNNæ ¹å› åˆ†æ](./ğŸ¤–_OTLPè‡ªä¸»è¿ç»´èƒ½åŠ›å®Œæ•´æ¶æ„_AIOpså¹³å°è®¾è®¡.md#gnn-æ ¹å› åˆ†æ)
  - ä»·å€¼: LLM (æ—¥å¿—æ–‡æœ¬åˆ†æ) + GNN (æœåŠ¡ä¾èµ–å›¾) = ç²¾å‡†å®šä½

### æ¶æ„å¯è§†åŒ– â­â­â­

- **ğŸ“Š æ¶æ„å›¾è¡¨æŒ‡å—**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ“Š_æ¶æ„å›¾è¡¨ä¸å¯è§†åŒ–æŒ‡å—_Mermaidå®Œæ•´ç‰ˆ.md)
  - æ¨èå›¾è¡¨:
    - [LLMæ—¥å¿—åˆ†ææ¶æ„](./ğŸ“Š_æ¶æ„å›¾è¡¨ä¸å¯è§†åŒ–æŒ‡å—_Mermaidå®Œæ•´ç‰ˆ.md#4-ai-æ—¥å¿—åˆ†ææµç¨‹)
    - [æˆæœ¬ä¼˜åŒ–ç­–ç•¥](./ğŸ“Š_æ¶æ„å›¾è¡¨ä¸å¯è§†åŒ–æŒ‡å—_Mermaidå®Œæ•´ç‰ˆ.md#42-æˆæœ¬ä¼˜åŒ–ç­–ç•¥)
  - ä»·å€¼: åˆ†å±‚æ¨¡å‹+ç¼“å­˜ç­–ç•¥å¯è§†åŒ–

### å·¥å…·é“¾æ”¯æŒ â­â­

- **ğŸ“š SDKæœ€ä½³å®è·µ**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ“š_OTLP_SDKæœ€ä½³å®è·µæŒ‡å—_å¤šè¯­è¨€å…¨æ ˆå®ç°.md)
  - ä½¿ç”¨åœºæ™¯: OTLP Logsæ•°æ®æ¨¡å‹ä¸LLMåˆ†æçš„é›†æˆ
  - å…³é”®ç« èŠ‚: [Logsé‡‡é›†æœ€ä½³å®è·µ](./ğŸ“š_OTLP_SDKæœ€ä½³å®è·µæŒ‡å—_å¤šè¯­è¨€å…¨æ ˆå®ç°.md#ç¬¬ä¸‰éƒ¨åˆ†-ç”Ÿäº§çº§ä¼˜åŒ–)
  - ä»·å€¼: ç»“æ„åŒ–æ—¥å¿—æå‡LLMåˆ†æå‡†ç¡®ç‡

---

**æ–‡æ¡£çŠ¶æ€**: âœ… P0 ä»»åŠ¡å®Œæˆ  
**ç¯‡å¹…**: 2,800+ è¡Œ  
**è¦†ç›–èŒƒå›´**: LLMåŸç† â†’ å®æˆ˜ä»£ç  â†’ æˆæœ¬ä¼˜åŒ– â†’ ç”Ÿäº§éƒ¨ç½² â†’ å®Œæ•´æ¡ˆä¾‹ â†’ æœªæ¥å±•æœ›

---

*æœ¬æ–‡æ¡£æ˜¯ OTLP æ ‡å‡†æ·±åº¦æ¢³ç†é¡¹ç›®çš„ä¸€éƒ¨åˆ†,æŒç»­æ›´æ–°ä¸­...*
