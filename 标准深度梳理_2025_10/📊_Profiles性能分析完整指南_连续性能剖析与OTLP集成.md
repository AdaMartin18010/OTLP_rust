# 📊 Profiles 性能分析完整指南 - 连续性能剖析与 OTLP 集成

> **文档版本**: v1.0  
> **创建日期**: 2025年10月9日  
> **文档类型**: P1 优先级 - Profiles 信号深度指南  
> **预估篇幅**: 2,500+ 行  
> **目标**: 掌握 Continuous Profiling 与 OTLP Profiles 信号

---

## 📋 目录

- [📊 Profiles 性能分析完整指南 - 连续性能剖析与 OTLP 集成](#-profiles-性能分析完整指南---连续性能剖析与-otlp-集成)
  - [📋 目录](#-目录)
  - [第一部分: Profiling 基础](#第一部分-profiling-基础)
    - [1.1 什么是 Profiling?](#11-什么是-profiling)
      - [传统 Profiling vs Continuous Profiling](#传统-profiling-vs-continuous-profiling)
    - [1.2 Profiling 核心概念](#12-profiling-核心概念)
    - [1.3 Profiling 工具生态](#13-profiling-工具生态)
  - [第二部分: CPU Profiling 深度实战](#第二部分-cpu-profiling-深度实战)
    - [2.1 Go pprof 实战](#21-go-pprof-实战)
      - [基础 CPU Profiling](#基础-cpu-profiling)
      - [生产环境持续 Profiling](#生产环境持续-profiling)
    - [2.2 Java JFR (Java Flight Recorder)](#22-java-jfr-java-flight-recorder)
      - [JFR 基础使用](#jfr-基础使用)
      - [JFR 高级配置](#jfr-高级配置)
    - [2.3 Python py-spy](#23-python-py-spy)
      - [py-spy 使用](#py-spy-使用)
  - [第三部分: 内存 Profiling](#第三部分-内存-profiling)
    - [3.1 Go Heap Profiling](#31-go-heap-profiling)
      - [Heap Profile 分析](#heap-profile-分析)
      - [内存泄漏检测](#内存泄漏检测)
    - [3.2 Java Heap Dump 分析](#32-java-heap-dump-分析)
    - [3.3 Allocation Profiling (分配追踪)](#33-allocation-profiling-分配追踪)
  - [第四部分: Continuous Profiling 平台](#第四部分-continuous-profiling-平台)
    - [4.1 Parca - 开源 Continuous Profiling](#41-parca---开源-continuous-profiling)
      - [Parca 架构](#parca-架构)
      - [部署 Parca](#部署-parca)
      - [集成应用](#集成应用)
    - [4.2 Pyroscope - 多语言支持](#42-pyroscope---多语言支持)
      - [Pyroscope 架构](#pyroscope-架构)
      - [部署 Pyroscope](#部署-pyroscope)
      - [多语言集成](#多语言集成)
    - [4.3 Grafana Phlare (现已合并到 Pyroscope)](#43-grafana-phlare-现已合并到-pyroscope)
  - [第五部分: OTLP Profiles 信号](#第五部分-otlp-profiles-信号)
    - [5.1 OTLP Profiles 数据模型](#51-otlp-profiles-数据模型)
      - [Profile 数据结构](#profile-数据结构)
    - [5.2 OTLP Profiles Exporter](#52-otlp-profiles-exporter)
      - [Go OTLP Profiles Exporter](#go-otlp-profiles-exporter)
    - [5.3 OTLP Collector Profiles 接收](#53-otlp-collector-profiles-接收)
  - [第六部分: 实战案例 - 性能优化](#第六部分-实战案例---性能优化)
    - [6.1 CPU 热点优化](#61-cpu-热点优化)
      - [案例: Go Web 服务 CPU 优化](#案例-go-web-服务-cpu-优化)
    - [6.2 内存泄漏排查](#62-内存泄漏排查)
      - [案例: Java 微服务内存泄漏](#案例-java-微服务内存泄漏)
    - [6.3 Goroutine 泄漏检测](#63-goroutine-泄漏检测)
  - [第七部分: 生产环境最佳实践](#第七部分-生产环境最佳实践)
    - [7.1 采样策略](#71-采样策略)
    - [7.2 性能影响评估](#72-性能影响评估)
    - [7.3 数据存储与查询](#73-数据存储与查询)
    - [7.4 告警与异常检测](#74-告警与异常检测)
  - [第八部分: Profiles + Traces + Metrics 关联](#第八部分-profiles--traces--metrics-关联)
    - [8.1 Exemplars - 从 Metrics 到 Profiles](#81-exemplars---从-metrics-到-profiles)
    - [8.2 SpanID 关联 - 从 Traces 到 Profiles](#82-spanid-关联---从-traces-到-profiles)
    - [8.3 统一可观测性平台](#83-统一可观测性平台)
  - [第九部分: 高级话题](#第九部分-高级话题)
    - [9.1 eBPF-based Profiling](#91-ebpf-based-profiling)
      - [Parca Agent eBPF 实现](#parca-agent-ebpf-实现)
    - [9.2 差分分析 (Diff Analysis)](#92-差分分析-diff-analysis)
    - [9.3 多维度标签 (Multi-dimensional Labels)](#93-多维度标签-multi-dimensional-labels)
  - [第十部分: 完整生产案例](#第十部分-完整生产案例)
    - [10.1 案例: 电商平台性能优化](#101-案例-电商平台性能优化)
      - [系统背景](#系统背景)
      - [实施方案](#实施方案)
      - [优化成果](#优化成果)
  - [总结](#总结)
    - [Continuous Profiling 核心价值](#continuous-profiling-核心价值)
    - [适用场景](#适用场景)
    - [参考资源](#参考资源)
  - [📚 相关文档](#-相关文档)
    - [核心集成 ⭐⭐⭐](#核心集成-)
    - [架构可视化 ⭐⭐⭐](#架构可视化-)
    - [工具链支持 ⭐⭐](#工具链支持-)

---

## 第一部分: Profiling 基础

### 1.1 什么是 Profiling?

```text
Profiling (性能剖析) 是分析程序运行时行为的技术:

📊 主要类型:
1. CPU Profiling - 分析 CPU 时间消耗
2. Memory Profiling - 分析内存分配和使用
3. Goroutine/Thread Profiling - 分析并发行为
4. Block Profiling - 分析阻塞事件
5. Mutex Profiling - 分析锁竞争

🎯 核心问题:
- 哪些函数消耗了最多 CPU?
- 内存分配在哪里?
- 是否存在内存泄漏?
- Goroutine 是否泄漏?
- 锁竞争在哪里?
```

#### 传统 Profiling vs Continuous Profiling

```text
传统 Profiling (Ad-hoc):
❌ 只在出现问题时手动采集
❌ 难以复现生产环境问题
❌ 历史数据缺失
❌ 影响生产环境 (开销大)

Continuous Profiling (连续性能剖析):
✅ 7x24 小时自动采集
✅ 历史数据可追溯 (时间维度)
✅ 低开销 (<1% CPU)
✅ 实时发现性能退化
✅ 多维度标签 (service, version, region)
✅ 与 Traces/Metrics 关联

典型工具:
- Google Cloud Profiler (商业)
- Datadog Continuous Profiler (商业)
- Parca (开源)
- Pyroscope (开源)
- Grafana Phlare → Pyroscope (开源)
```

### 1.2 Profiling 核心概念

```text
1. 采样 (Sampling):
   - 定期采集程序调用栈
   - 通常 10-100 Hz (每秒 10-100 次)
   - 降低性能影响

2. 火焰图 (Flame Graph):
   - 可视化调用栈
   - X 轴: 样本数 (宽度表示 CPU 消耗)
   - Y 轴: 调用深度
   - 颜色: 函数/包分类

3. Profile 类型:
   - CPU Profile: on-CPU time
   - Off-CPU Profile: blocked time (I/O, lock, sleep)
   - Heap Profile: 堆内存分配
   - Allocation Profile: 所有内存分配 (包括栈)
   - Goroutine Profile: goroutine 数量和状态
   - Mutex Profile: 锁竞争
   - Block Profile: 阻塞事件

4. pprof 格式:
   - Google 开发的 profile 数据格式
   - Protocol Buffers 序列化
   - 包含 location, function, sample 等信息
```

### 1.3 Profiling 工具生态

```text
语言工具:
- Go: pprof (内置), runtime/pprof
- Java: JFR (Java Flight Recorder), JProfiler, YourKit
- Python: cProfile, py-spy, austin
- Rust: pprof-rs, cargo-flamegraph
- Node.js: V8 profiler, clinic.js
- C/C++: perf, gprof, Valgrind

平台工具:
- Parca: 开源, eBPF-based, 多语言
- Pyroscope: 开源, 多语言, Grafana 集成
- Elastic Profiling (原 Prodfiler): 商业, eBPF
- Google Cloud Profiler: 商业, 多语言
- Datadog Profiler: 商业, APM 集成

可视化:
- FlameGraph (Brendan Gregg)
- pprof web UI
- Grafana
- Speedscope
```

---

## 第二部分: CPU Profiling 深度实战

### 2.1 Go pprof 实战

#### 基础 CPU Profiling

```go
// main.go - 启用 pprof HTTP 端点
package main

import (
    "net/http"
    _ "net/http/pprof"  // 自动注册 /debug/pprof/*
    "time"
)

func main() {
    // 启动 pprof HTTP server
    go func() {
        http.ListenAndServe("localhost:6060", nil)
    }()

    // 你的业务逻辑
    runBusinessLogic()
}

func runBusinessLogic() {
    for {
        // 模拟 CPU 密集型任务
        computeHeavy()
        time.Sleep(100 * time.Millisecond)
    }
}

func computeHeavy() {
    // 示例: 计算质数
    for i := 0; i < 1000000; i++ {
        _ = isPrime(i)
    }
}

func isPrime(n int) bool {
    if n <= 1 {
        return false
    }
    for i := 2; i*i <= n; i++ {
        if n%i == 0 {
            return false
        }
    }
    return true
}
```

**采集 CPU Profile**:

```bash
# 方式 1: 使用 curl (采集 30 秒)
curl -o cpu.prof http://localhost:6060/debug/pprof/profile?seconds=30

# 方式 2: 使用 go tool pprof (交互式)
go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30

# 等待 30 秒后进入交互式界面
(pprof) top10        # 查看 CPU 消耗前 10 的函数
(pprof) list isPrime # 查看 isPrime 函数详细代码
(pprof) web          # 生成 SVG 火焰图 (需要 graphviz)
(pprof) quit

# 方式 3: 生成火焰图 (FlameGraph)
go tool pprof -http=:8080 cpu.prof
# 打开浏览器: http://localhost:8080
```

**输出示例**:

```text
(pprof) top10
Showing nodes accounting for 2.50s, 95.42% of 2.62s total
Dropped 15 nodes (cum <= 0.01s)
Showing top 10 nodes out of 45
      flat  flat%   sum%        cum   cum%
     1.80s 68.70% 68.70%      1.80s 68.70%  main.isPrime
     0.40s 15.27% 83.97%      2.20s 83.97%  main.computeHeavy
     0.20s  7.63% 91.60%      0.20s  7.63%  runtime.pthread_cond_signal
     0.05s  1.91% 93.51%      0.05s  1.91%  runtime.usleep
     0.03s  1.15% 94.66%      0.03s  1.15%  runtime.nanotime
     0.02s  0.76% 95.42%      0.02s  0.76%  runtime.lock2

解读:
- isPrime 函数消耗了 68.70% 的 CPU 时间
- computeHeavy 调用 isPrime,累计 83.97%
- 优化建议: 优化 isPrime 算法 (如 Sieve of Eratosthenes)
```

#### 生产环境持续 Profiling

```go
// continuous_profiler.go - 定期采集并上传 Profile
package profiler

import (
    "bytes"
    "context"
    "fmt"
    "io"
    "net/http"
    "runtime"
    "runtime/pprof"
    "time"
)

type ContinuousProfiler struct {
    serviceNamenstring
    version      string
    uploadURL    string
    interval     time.Duration
    client       *http.Client
}

func NewContinuousProfiler(serviceName, version, uploadURL string) *ContinuousProfiler {
    return &ContinuousProfiler{
        serviceName: serviceName,
        version:     version,
        uploadURL:   uploadURL,
        interval:    10 * time.Second,  // 每 10 秒采集一次
        client:      &http.Client{Timeout: 30 * time.Second},
    }
}

func (cp *ContinuousProfiler) Start(ctx context.Context) {
    ticker := time.NewTicker(cp.interval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            // 采集 CPU Profile
            if err := cp.collectAndUploadCPU(); err != nil {
                fmt.Printf("Failed to upload CPU profile: %v\n", err)
            }

            // 采集 Heap Profile
            if err := cp.collectAndUploadHeap(); err != nil {
                fmt.Printf("Failed to upload heap profile: %v\n", err)
            }

        case <-ctx.Done():
            return
        }
    }
}

func (cp *ContinuousProfiler) collectAndUploadCPU() error {
    var buf bytes.Buffer

    // 采集 10 秒的 CPU profile
    if err := pprof.StartCPUProfile(&buf); err != nil {
        return fmt.Errorf("start cpu profile: %w", err)
    }
    time.Sleep(10 * time.Second)
    pprof.StopCPUProfile()

    // 上传到 Profiling 平台
    return cp.upload("cpu", &buf)
}

func (cp *ContinuousProfiler) collectAndUploadHeap() error {
    var buf bytes.Buffer

    // 采集 Heap profile
    runtime.GC()  // 触发 GC 以获取最新堆快照
    if err := pprof.WriteHeapProfile(&buf); err != nil {
        return fmt.Errorf("write heap profile: %w", err)
    }

    return cp.upload("heap", &buf)
}

func (cp *ContinuousProfiler) upload(profileType string, data io.Reader) error {
    req, err := http.NewRequest("POST", cp.uploadURL, data)
    if err != nil {
        return err
    }

    // 添加标签 (用于多维度查询)
    req.Header.Set("X-Service-Name", cp.serviceName)
    req.Header.Set("X-Version", cp.version)
    req.Header.Set("X-Profile-Type", profileType)
    req.Header.Set("Content-Type", "application/octet-stream")

    resp, err := cp.client.Do(req)
    if err != nil {
        return err
    }
    defer resp.Body.Close()

    if resp.StatusCode != http.StatusOK {
        return fmt.Errorf("upload failed: %s", resp.Status)
    }

    return nil
}
```

**使用示例**:

```go
// main.go
func main() {
    profiler := profiler.NewContinuousProfiler(
        "my-service",
        "v1.2.3",
        "http://parca-server:7070/upload",
    )

    ctx := context.Background()
    go profiler.Start(ctx)

    // 你的业务逻辑
    runBusinessLogic()
}
```

### 2.2 Java JFR (Java Flight Recorder)

#### JFR 基础使用

```bash
# 启动 Java 应用时启用 JFR
java -XX:StartFlightRecording=duration=60s,filename=recording.jfr \
     -jar myapp.jar

# 或在运行时动态启动 (使用 jcmd)
# 1. 找到 Java 进程 PID
jps -l

# 2. 启动 JFR 录制
jcmd <PID> JFR.start name=my-recording duration=60s filename=recording.jfr

# 3. 停止录制
jcmd <PID> JFR.stop name=my-recording

# 4. 转换为火焰图 (使用 async-profiler)
java -cp converter.jar jfr2flame recording.jfr flamegraph.html
```

#### JFR 高级配置

```java
// CustomJFRConfiguration.java - 自定义 JFR 配置
import jdk.jfr.Configuration;
import jdk.jfr.Recording;

public class CustomJFRConfiguration {
    public static void startProfiling() throws Exception {
        // 加载配置 (profile 或 default)
        Configuration config = Configuration.getConfiguration("profile");

        // 创建录制
        Recording recording = new Recording(config);

        // 自定义设置
        recording.setMaxSize(100 * 1024 * 1024);  // 最大 100MB
        recording.setMaxAge(java.time.Duration.ofHours(2));  // 保留 2 小时

        // 启用特定事件
        recording.enable("jdk.ObjectAllocationInNewTLAB")
                 .withThreshold(java.time.Duration.ofMillis(10));  // 阈值 10ms

        recording.enable("jdk.ObjectAllocationOutsideTLAB");

        // 开始录制
        recording.start();

        // 定期转储 (每 10 分钟)
        ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
        scheduler.scheduleAtFixedRate(() -> {
            try {
                Path path = Paths.get("recording-" + System.currentTimeMillis() + ".jfr");
                recording.dump(path);
                System.out.println("Dumped recording to " + path);
            } catch (IOException e) {
                e.printStackTrace();
            }
        }, 10, 10, TimeUnit.MINUTES);
    }
}
```

### 2.3 Python py-spy

#### py-spy 使用

```bash
# 安装 py-spy
pip install py-spy

# 采集运行中的 Python 进程
py-spy record -o profile.svg --pid <PID>

# 采集 60 秒
py-spy record -o profile.svg --duration 60 --pid <PID>

# 顶部视图 (类似 top)
py-spy top --pid <PID>

# 生成火焰图 (HTML)
py-spy record -o profile.html --format speedscope --pid <PID>
```

**Python 内置 cProfile**:

```python
# profile_example.py
import cProfile
import pstats
from io import StringIO

def heavy_computation():
    result = 0
    for i in range(1000000):
        result += i ** 2
    return result

def main():
    for _ in range(10):
        heavy_computation()

if __name__ == "__main__":
    # 启用 profiling
    profiler = cProfile.Profile()
    profiler.enable()

    main()

    profiler.disable()

    # 输出统计
    s = StringIO()
    ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
    ps.print_stats(20)  # 前 20 个函数
    print(s.getvalue())

    # 保存到文件
    profiler.dump_stats('profile.prof')

    # 可视化: snakeviz profile.prof
```

---

## 第三部分: 内存 Profiling

### 3.1 Go Heap Profiling

#### Heap Profile 分析

```go
// heap_profiling.go
package main

import (
    "fmt"
    "net/http"
    _ "net/http/pprof"
    "runtime"
    "time"
)

type LargeStruct struct {
    Data [1024 * 1024]byte  // 1MB
}

var globalLeaks []*LargeStruct

func main() {
    go func() {
        http.ListenAndServe("localhost:6060", nil)
    }()

    // 模拟内存泄漏
    go leakMemory()

    select {}
}

func leakMemory() {
    ticker := time.NewTicker(1 * time.Second)
    for range ticker.C {
        // 每秒分配 1MB 且不释放
        obj := &LargeStruct{}
        globalLeaks = append(globalLeaks, obj)

        var m runtime.MemStats
        runtime.ReadMemStats(&m)
        fmt.Printf("Alloc = %v MiB, TotalAlloc = %v MiB, Sys = %v MiB, NumGC = %v\n",
            m.Alloc/1024/1024, m.TotalAlloc/1024/1024, m.Sys/1024/1024, m.NumGC)
    }
}
```

**采集 Heap Profile**:

```bash
# 采集 heap profile
curl -o heap.prof http://localhost:6060/debug/pprof/heap

# 分析
go tool pprof heap.prof

(pprof) top10
Showing nodes accounting for 512MB, 100% of 512MB total
      flat  flat%   sum%        cum   cum%
    512MB   100%   100%     512MB   100%  main.leakMemory

(pprof) list leakMemory
# 查看详细代码

(pprof) web  # 可视化
```

#### 内存泄漏检测

```bash
# 对比两个时间点的 heap profile
curl -o heap1.prof http://localhost:6060/debug/pprof/heap
# 等待一段时间
sleep 60
curl -o heap2.prof http://localhost:6060/debug/pprof/heap

# 对比 (找出增长的部分)
go tool pprof -base heap1.prof heap2.prof

(pprof) top10
# 显示差异最大的函数
```

### 3.2 Java Heap Dump 分析

```bash
# 生成 heap dump
jcmd <PID> GC.heap_dump heapdump.hprof

# 使用 jhat 分析 (简单)
jhat heapdump.hprof
# 浏览器打开: http://localhost:7000

# 使用 Eclipse MAT (推荐, 强大的内存泄漏检测)
# 下载: https://www.eclipse.org/mat/
# 打开 heapdump.hprof
# 运行 "Leak Suspects Report"
```

**Java 内存泄漏示例**:

```java
// MemoryLeakExample.java
import java.util.ArrayList;
import java.util.List;

public class MemoryLeakExample {
    private static List<byte[]> leak = new ArrayList<>();

    public static void main(String[] args) throws InterruptedException {
        while (true) {
            // 每次分配 1MB 且不释放
            byte[] data = new byte[1024 * 1024];
            leak.add(data);

            System.out.println("Allocated 1MB, total: " + leak.size() + "MB");
            Thread.sleep(1000);
        }
    }
}
```

### 3.3 Allocation Profiling (分配追踪)

```go
// allocation_profiling.go
package main

import (
    "net/http"
    _ "net/http/pprof"
    "runtime"
)

func main() {
    // 启用 allocation profiling (默认每 512KB 采样一次)
    runtime.MemProfileRate = 1  // 每次分配都记录 (仅用于测试, 生产环境开销大)

    go func() {
        http.ListenAndServe("localhost:6060", nil)
    }()

    // 业务逻辑
    for {
        allocateMemory()
    }
}

func allocateMemory() {
    // 短生命周期的分配
    data := make([]byte, 1024)
    _ = data
}
```

**采集 Allocation Profile**:

```bash
# 采集 allocs profile
curl -o allocs.prof http://localhost:6060/debug/pprof/allocs

go tool pprof allocs.prof

(pprof) top10
# 显示分配次数最多的函数
```

---

## 第四部分: Continuous Profiling 平台

### 4.1 Parca - 开源 Continuous Profiling

#### Parca 架构

```text
Parca 架构:

┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│ Application │────▶│ Parca Agent │────▶│ Parca Server│
│   (pprof)   │     │   (eBPF)    │     │  (Storage)  │
└─────────────┘     └─────────────┘     └─────────────┘
                                              │
                                              ▼
                                        ┌───────────┐
                                        │ Grafana   │
                                        │ Dashboard │
                                        └───────────┘

特性:
1. eBPF-based: 无需修改应用代码
2. 多语言支持: Go, Java, Python, C/C++, Rust
3. 低开销: <1% CPU
4. 历史数据: 长期存储 (压缩后仅几 MB/天)
5. 多维度标签: service, version, region, pod
6. Diff 分析: 对比不同时间/版本的 profile
```

#### 部署 Parca

```yaml
# parca-deployment.yaml - Kubernetes 部署
apiVersion: v1
kind: Namespace
metadata:
  name: parca

---
# Parca Server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: parca-server
  namespace: parca
spec:
  replicas: 1
  selector:
    matchLabels:
      app: parca-server
  template:
    metadata:
      labels:
        app: parca-server
    spec:
      containers:
      - name: parca
        image: ghcr.io/parca-dev/parca:v0.21.0
        args:
        - /parca
        - --http-address=:7070
        - --storage-path=/var/lib/parca
        - --storage-active-memory=2GB
        ports:
        - containerPort: 7070
          name: http
        volumeMounts:
        - name: storage
          mountPath: /var/lib/parca
        resources:
          requests:
            memory: "2Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2"
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: parca-storage

---
apiVersion: v1
kind: Service
metadata:
  name: parca-server
  namespace: parca
spec:
  selector:
    app: parca-server
  ports:
  - port: 7070
    targetPort: 7070
  type: LoadBalancer

---
# Parca Agent (DaemonSet)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: parca-agent
  namespace: parca
spec:
  selector:
    matchLabels:
      app: parca-agent
  template:
    metadata:
      labels:
        app: parca-agent
    spec:
      hostPID: true  # 访问宿主机进程
      containers:
      - name: parca-agent
        image: ghcr.io/parca-dev/parca-agent:v0.27.0
        args:
        - /bin/parca-agent
        - --node=$(NODE_NAME)
        - --remote-store-address=parca-server.parca.svc.cluster.local:7070
        - --remote-store-insecure
        - --log-level=info
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          privileged: true  # eBPF 需要
        volumeMounts:
        - name: sys
          mountPath: /sys
        - name: modules
          mountPath: /lib/modules
        - name: debugfs
          mountPath: /sys/kernel/debug
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      volumes:
      - name: sys
        hostPath:
          path: /sys
      - name: modules
        hostPath:
          path: /lib/modules
      - name: debugfs
        hostPath:
          path: /sys/kernel/debug

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: parca-storage
  namespace: parca
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
```

**部署**:

```bash
kubectl apply -f parca-deployment.yaml

# 查看状态
kubectl get pods -n parca

# 访问 Web UI
kubectl port-forward -n parca svc/parca-server 7070:7070
# 浏览器打开: http://localhost:7070
```

#### 集成应用

**Go 应用集成** (如果不使用 eBPF Agent):

```go
// main.go - 主动上传 profile 到 Parca
package main

import (
    "bytes"
    "context"
    "fmt"
    "net/http"
    "runtime/pprof"
    "time"

    "google.golang.org/grpc"
    profilestorepb "github.com/parca-dev/parca/gen/proto/go/parca/profilestore/v1alpha1"
)

func main() {
    // 连接 Parca Server
    conn, err := grpc.Dial("parca-server:7070", grpc.WithInsecure())
    if err != nil {
        panic(err)
    }
    defer conn.Close()

    client := profilestorepb.NewProfileStoreServiceClient(conn)

    // 定期采集并上传
    ticker := time.NewTicker(10 * time.Second)
    for range ticker.C {
        uploadCPUProfile(client)
    }
}

func uploadCPUProfile(client profilestorepb.ProfileStoreServiceClient) {
    var buf bytes.Buffer

    // 采集 CPU profile
    if err := pprof.StartCPUProfile(&buf); err != nil {
        fmt.Printf("Failed to start CPU profile: %v\n", err)
        return
    }
    time.Sleep(10 * time.Second)
    pprof.StopCPUProfile()

    // 上传到 Parca
    _, err := client.WriteRaw(context.Background(), &profilestorepb.WriteRawRequest{
        Series: []*profilestorepb.RawProfileSeries{
            {
                Labels: &profilestorepb.LabelSet{
                    Labels: []*profilestorepb.Label{
                        {Name: "service_name", Value: "my-service"},
                        {Name: "version", Value: "v1.2.3"},
                        {Name: "__name__", Value: "cpu"},
                    },
                },
                Samples: []*profilestorepb.RawSample{
                    {
                        RawProfile: buf.Bytes(),
                    },
                },
            },
        },
    })

    if err != nil {
        fmt.Printf("Failed to upload profile: %v\n", err)
    }
}
```

### 4.2 Pyroscope - 多语言支持

#### Pyroscope 架构

```text
Pyroscope 架构:

┌──────────────┐     ┌──────────────┐     ┌──────────────┐
│ Application  │────▶│  Pyroscope   │────▶│  Pyroscope   │
│ (Push/Pull)  │     │    Agent     │     │    Server    │
└──────────────┘     └──────────────┘     └──────────────┘
                                                 │
                                                 ▼
                                           ┌──────────┐
                                           │ Grafana  │
                                           └──────────┘

特性:
1. 多语言 SDK: Go, Java, Python, .NET, Ruby, Node.js
2. Pull 模式: 从 pprof 端点拉取 (类似 Prometheus)
3. Push 模式: SDK 主动推送
4. 标签支持: 多维度查询
5. Diff 分析: 版本对比
6. Grafana 集成: 原生数据源
```

#### 部署 Pyroscope

```yaml
# pyroscope-deployment.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: pyroscope

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pyroscope-server
  namespace: pyroscope
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pyroscope-server
  template:
    metadata:
      labels:
        app: pyroscope-server
    spec:
      containers:
      - name: pyroscope
        image: grafana/pyroscope:latest
        args:
        - server
        ports:
        - containerPort: 4040
          name: http
        volumeMounts:
        - name: storage
          mountPath: /var/lib/pyroscope
        env:
        - name: PYROSCOPE_LOG_LEVEL
          value: "info"
        - name: PYROSCOPE_STORAGE_PATH
          value: "/var/lib/pyroscope"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1"
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: pyroscope-storage

---
apiVersion: v1
kind: Service
metadata:
  name: pyroscope-server
  namespace: pyroscope
spec:
  selector:
    app: pyroscope-server
  ports:
  - port: 4040
    targetPort: 4040
  type: LoadBalancer

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pyroscope-storage
  namespace: pyroscope
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
```

#### 多语言集成

**Go 集成**:

```go
// main.go - Pyroscope Go SDK
package main

import (
    "github.com/grafana/pyroscope-go"
)

func main() {
    // 启动 Pyroscope profiler
    pyroscope.Start(pyroscope.Config{
        ApplicationName: "my-go-service",
        ServerAddress:   "http://pyroscope-server:4040",
        Logger:          nil,  // 可选: 自定义日志

        // 采集的 profile 类型
        ProfileTypes: []pyroscope.ProfileType{
            pyroscope.ProfileCPU,
            pyroscope.ProfileAllocObjects,
            pyroscope.ProfileAllocSpace,
            pyroscope.ProfileInuseObjects,
            pyroscope.ProfileInuseSpace,
        },

        // 标签 (多维度查询)
        Tags: map[string]string{
            "version":  "v1.2.3",
            "region":   "us-east-1",
            "hostname": "pod-abc123",
        },
    })

    // 你的业务逻辑
    runBusinessLogic()
}
```

**Java 集成**:

```java
// Main.java - Pyroscope Java Agent
// 启动时添加 JVM 参数:
// java -javaagent:pyroscope.jar \
//      -Dpyroscope.application.name=my-java-service \
//      -Dpyroscope.server.address=http://pyroscope-server:4040 \
//      -Dpyroscope.format=jfr \
//      -jar myapp.jar

// 或在代码中启动:
import io.pyroscope.javaagent.PyroscopeAgent;
import io.pyroscope.javaagent.config.Config;

public class Main {
    public static void main(String[] args) {
        PyroscopeAgent.start(
            new Config.Builder()
                .setApplicationName("my-java-service")
                .setServerAddress("http://pyroscope-server:4040")
                .setFormat(Format.JFR)
                .build()
        );

        // 你的业务逻辑
        runBusinessLogic();
    }
}
```

**Python 集成**:

```python
# main.py - Pyroscope Python SDK
import pyroscope

pyroscope.configure(
    application_name="my-python-service",
    server_address="http://pyroscope-server:4040",
    tags={
        "version": "v1.2.3",
        "region": "us-east-1",
    }
)

# 你的业务逻辑
def main():
    while True:
        heavy_computation()

if __name__ == "__main__":
    main()
```

### 4.3 Grafana Phlare (现已合并到 Pyroscope)

```text
Grafana Phlare 已于 2024 年合并到 Pyroscope:
- Phlare 项目停止独立开发
- Pyroscope 继承了 Phlare 的特性
- Grafana 原生支持 Pyroscope 数据源

使用 Grafana 查询 Pyroscope 数据:
1. 安装 Pyroscope 数据源插件
2. 配置数据源: http://pyroscope-server:4040
3. 创建 Dashboard:
   - 火焰图面板
   - CPU 使用趋势
   - 内存分配趋势
   - 服务对比
```

---

## 第五部分: OTLP Profiles 信号

### 5.1 OTLP Profiles 数据模型

#### Profile 数据结构

```protobuf
// opentelemetry/proto/profiles/v1/profiles.proto

message ProfilesData {
  repeated ResourceProfiles resource_profiles = 1;
}

message ResourceProfiles {
  // 资源信息 (service, host, etc.)
  opentelemetry.proto.resource.v1.Resource resource = 1;

  // 作用域 (instrumentation library)
  repeated ScopeProfiles scope_profiles = 2;

  // Schema URL
  string schema_url = 3;
}

message ScopeProfiles {
  opentelemetry.proto.common.v1.InstrumentationScope scope = 1;
  repeated Profile profiles = 2;
  string schema_url = 3;
}

message Profile {
  // Profile ID (唯一标识)
  bytes profile_id = 1;

  // 开始时间
  fixed64 start_time_unix_nano = 2;

  // 结束时间
  fixed64 end_time_unix_nano = 3;

  // 属性 (标签)
  repeated opentelemetry.proto.common.v1.KeyValue attributes = 4;

  // Profile 类型 (cpu, heap, allocs, etc.)
  string profile_type = 5;

  // pprof 格式的二进制数据
  bytes pprof_data = 6;

  // 关联的 Trace ID (可选)
  bytes trace_id = 7;

  // 关联的 Span ID (可选)
  bytes span_id = 8;
}
```

### 5.2 OTLP Profiles Exporter

#### Go OTLP Profiles Exporter

```go
// otlp_profiles_exporter.go
package profiler

import (
    "bytes"
    "context"
    "fmt"
    "runtime/pprof"
    "time"

    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/sdk/resource"
    profilesv1 "go.opentelemetry.io/proto/otlp/profiles/v1"
    "google.golang.org/grpc"
)

type OTLPProfilesExporter struct {
    serviceName string
    version     string
    client      profilesv1.ProfilesServiceClient
}

func NewOTLPProfilesExporter(serviceName, version, endpoint string) (*OTLPProfilesExporter, error) {
    conn, err := grpc.Dial(endpoint, grpc.WithInsecure())
    if err != nil {
        return nil, err
    }

    return &OTLPProfilesExporter{
        serviceName: serviceName,
        version:     version,
        client:      profilesv1.NewProfilesServiceClient(conn),
    }, nil
}

func (e *OTLPProfilesExporter) ExportCPUProfile(ctx context.Context, duration time.Duration) error {
    // 采集 CPU profile
    var buf bytes.Buffer
    if err := pprof.StartCPUProfile(&buf); err != nil {
        return fmt.Errorf("start cpu profile: %w", err)
    }
    time.Sleep(duration)
    pprof.StopCPUProfile()

    // 构建 OTLP Profiles 数据
    now := time.Now()
    profilesData := &profilesv1.ProfilesData{
        ResourceProfiles: []*profilesv1.ResourceProfiles{
            {
                Resource: &resource.Resource{
                    Attributes: []attribute.KeyValue{
                        attribute.String("service.name", e.serviceName),
                        attribute.String("service.version", e.version),
                    },
                },
                ScopeProfiles: []*profilesv1.ScopeProfiles{
                    {
                        Profiles: []*profilesv1.Profile{
                            {
                                ProfileId:         generateProfileID(),
                                StartTimeUnixNano: uint64(now.Add(-duration).UnixNano()),
                                EndTimeUnixNano:   uint64(now.UnixNano()),
                                ProfileType:       "cpu",
                                PprofData:         buf.Bytes(),
                                Attributes: []*profilesv1.KeyValue{
                                    {Key: "profile.type", Value: &profilesv1.AnyValue{Value: &profilesv1.AnyValue_StringValue{StringValue: "cpu"}}},
                                    {Key: "sample.type", Value: &profilesv1.AnyValue{Value: &profilesv1.AnyValue_StringValue{StringValue: "cpu-time"}}},
                                },
                            },
                        },
                    },
                },
            },
        },
    }

    // 导出到 OTLP Collector
    req := &profilesv1.ExportProfilesServiceRequest{
        ResourceProfiles: profilesData.ResourceProfiles,
    }

    _, err := e.client.Export(ctx, req)
    return err
}

func generateProfileID() []byte {
    // 生成唯一 Profile ID (16 字节)
    id := make([]byte, 16)
    // TODO: 使用随机数或时间戳生成
    return id
}
```

**使用示例**:

```go
// main.go
func main() {
    exporter, err := profiler.NewOTLPProfilesExporter(
        "my-service",
        "v1.2.3",
        "otel-collector:4317",  // OTLP gRPC endpoint
    )
    if err != nil {
        panic(err)
    }

    // 定期导出 CPU profile
    ticker := time.NewTicker(10 * time.Second)
    for range ticker.C {
        if err := exporter.ExportCPUProfile(context.Background(), 10*time.Second); err != nil {
            fmt.Printf("Failed to export profile: %v\n", err)
        }
    }
}
```

### 5.3 OTLP Collector Profiles 接收

```yaml
# otel-collector-config.yaml - 配置 Profiles 接收和导出

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  # pprof 接收器 (从 pprof HTTP 端点拉取)
  pprof:
    endpoint: 0.0.0.0:1777
    profiles:
      - targets:
        - endpoint: http://my-service:6060/debug/pprof/profile?seconds=10
          labels:
            service_name: my-service
            version: v1.2.3

processors:
  batch:
    timeout: 10s
    send_batch_size: 1024

  # 添加资源属性
  resource:
    attributes:
      - key: deployment.environment
        value: production
        action: upsert

exporters:
  # 导出到 Pyroscope
  pyroscope:
    endpoint: http://pyroscope-server:4040

  # 导出到 Parca
  otlphttp:
    endpoint: http://parca-server:7070/otlp

  # 调试输出
  logging:
    loglevel: info

service:
  pipelines:
    profiles:
      receivers: [otlp, pprof]
      processors: [batch, resource]
      exporters: [pyroscope, otlphttp, logging]
```

**部署 OTLP Collector (支持 Profiles)**:

```bash
# 使用官方 Collector Contrib 版本 (包含 pprof receiver)
docker run -v $(pwd)/otel-collector-config.yaml:/etc/otelcol/config.yaml \
    -p 4317:4317 -p 4318:4318 -p 1777:1777 \
    otel/opentelemetry-collector-contrib:latest
```

---

## 第六部分: 实战案例 - 性能优化

### 6.1 CPU 热点优化

#### 案例: Go Web 服务 CPU 优化

**问题**: 一个 Go Web API 在高负载下 CPU 使用率达到 80%+

**步骤 1: 采集 CPU Profile**:

```bash
# 生产环境采集 30 秒 profile
curl -o cpu-before.prof http://api-server:6060/debug/pprof/profile?seconds=30
```

**步骤 2: 分析火焰图**:

```bash
go tool pprof -http=:8080 cpu-before.prof
# 打开 http://localhost:8080
```

**发现**:

```text
(pprof) top10
      flat  flat%   sum%        cum   cum%
     3.20s 40.00% 40.00%      3.20s 40.00%  encoding/json.(*encodeState).string
     1.60s 20.00% 60.00%      4.80s 60.00%  encoding/json.Marshal
     0.80s 10.00% 70.00%      0.80s 10.00%  reflect.Value.NumField
     0.60s  7.50% 77.50%      0.60s  7.50%  runtime.mallocgc
     ...

发现: JSON 序列化消耗了 60% 的 CPU 时间!
```

**步骤 3: 优化**:

```go
// before.go - 优化前
func (h *Handler) GetUsers(w http.ResponseWriter, r *http.Request) {
    users := h.db.GetUsers()

    // 每次请求都序列化 (慢)
    data, _ := json.Marshal(users)
    w.Write(data)
}

// after.go - 优化后
import "github.com/json-iterator/go"  // 更快的 JSON 库

var jsonAPI = jsoniter.ConfigCompatibleWithStandardLibrary

func (h *Handler) GetUsers(w http.ResponseWriter, r *http.Request) {
    users := h.db.GetUsers()

    // 使用 jsoniter (快 2-3 倍)
    data, _ := jsonAPI.Marshal(users)
    w.Write(data)
}

// 或使用 easyjson (代码生成, 快 4-5 倍)
//go:generate easyjson -all users.go
```

**步骤 4: 验证优化效果**:

```bash
# 再次采集 profile
curl -o cpu-after.prof http://api-server:6060/debug/pprof/profile?seconds=30

# 对比
go tool pprof -http=:8080 -base cpu-before.prof cpu-after.prof

结果:
- JSON 序列化时间减少 70%
- 整体 CPU 使用率从 80% 降至 35%
- QPS 从 1000 提升至 2500
```

### 6.2 内存泄漏排查

#### 案例: Java 微服务内存泄漏

**问题**: Java 服务运行几天后 OOM (Out of Memory)

**步骤 1: 采集 Heap Dump**:

```bash
# 当内存使用率高时采集
jcmd <PID> GC.heap_dump before-gc.hprof

# 触发 Full GC
jcmd <PID> GC.run

# 再次采集
jcmd <PID> GC.heap_dump after-gc.hprof
```

**步骤 2: 使用 Eclipse MAT 分析**:

```text
打开 after-gc.hprof
运行 "Leak Suspects Report"

发现:
- java.util.HashMap 占用 2.5GB 内存
- Dominated by: com.example.CacheManager
- Retained Size: 2.5GB (85% of heap)

详细分析:
CacheManager 持有一个 HashMap,
不断添加数据但从不清理!
```

**步骤 3: 修复代码**:

```java
// before.java - 有泄漏
public class CacheManager {
    private Map<String, User> cache = new HashMap<>();

    public void addUser(User user) {
        cache.put(user.getId(), user);  // 永不删除!
    }
}

// after.java - 修复后
import com.google.common.cache.Cache;
import com.google.common.cache.CacheBuilder;
import java.util.concurrent.TimeUnit;

public class CacheManager {
    // 使用 Guava Cache (自动过期)
    private Cache<String, User> cache = CacheBuilder.newBuilder()
        .maximumSize(10000)  // 最多 10000 条
        .expireAfterWrite(1, TimeUnit.HOURS)  // 1 小时过期
        .build();

    public void addUser(User user) {
        cache.put(user.getId(), user);
    }
}
```

**步骤 4: 验证**:

```bash
# 重新部署并监控
# 内存使用率稳定在 30% 以下
# 不再 OOM
```

### 6.3 Goroutine 泄漏检测

```go
// goroutine_leak.go - 有泄漏的代码
package main

import (
    "net/http"
    _ "net/http/pprof"
    "time"
)

func main() {
    go func() {
        http.ListenAndServe("localhost:6060", nil)
    }()

    // 模拟 goroutine 泄漏
    for {
        go leakyGoroutine()
        time.Sleep(100 * time.Millisecond)
    }
}

func leakyGoroutine() {
    // 这个 goroutine 会阻塞永远不退出
    ch := make(chan int)  // unbuffered channel
    <-ch  // 永远阻塞!
}
```

**检测泄漏**:

```bash
# 查看 goroutine 数量
curl http://localhost:6060/debug/pprof/goroutine?debug=1

goroutine profile: total 1234
1234 @ 0x...
#   0x... main.leakyGoroutine+0x40    /path/to/goroutine_leak.go:23

发现: 有 1234 个 goroutine 阻塞在 leakyGoroutine!
```

**修复**:

```go
// fixed.go
func fixedGoroutine(ctx context.Context) {
    ch := make(chan int)

    select {
    case <-ch:
        // 正常处理
    case <-ctx.Done():
        // 超时或取消,goroutine 退出
        return
    }
}

func main() {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    for {
        go fixedGoroutine(ctx)
        time.Sleep(100 * time.Millisecond)
    }
}
```

---

## 第七部分: 生产环境最佳实践

### 7.1 采样策略

```text
采样策略权衡:

1. 采样率 (Sampling Rate):
   - 10 Hz (每秒 10 次): 低开销, 可能丢失短生命周期函数
   - 100 Hz: 平衡 (推荐生产环境)
   - 1000 Hz: 高开销, 仅用于调试

2. 采集时长:
   - CPU Profile: 10-30 秒 (推荐 10 秒)
   - Heap Profile: 瞬时快照
   - Allocation Profile: 30-60 秒

3. 采集频率:
   - Continuous Profiling: 每 10-60 秒
   - 按需采集: 仅在性能问题时

4. 数据保留:
   - 原始 Profile: 7-30 天
   - 聚合数据: 永久
```

**Go 采样配置**:

```go
// sampling_config.go
import "runtime"

func init() {
    // CPU Profile 采样率 (默认 100 Hz)
    runtime.SetCPUProfileRate(100)

    // Memory Profile 采样率 (默认每 512KB 采样一次)
    runtime.MemProfileRate = 512 * 1024  // 512KB

    // Block Profile 采样率 (默认 0, 即禁用)
    runtime.SetBlockProfileRate(1000000)  // 1ms 以上的阻塞才记录

    // Mutex Profile 采样率 (默认 0, 即禁用)
    runtime.SetMutexProfileFraction(1000)  // 1/1000 的锁竞争被记录
}
```

### 7.2 性能影响评估

```text
Profiling 性能开销:

方式                    CPU 开销    内存开销    生产可用
─────────────────────  ─────────  ─────────  ────────
Go pprof (100 Hz)       <1%         ~10MB      ✅
Java JFR (default)      <1%         ~20MB      ✅
Python cProfile         10-30%      ~50MB      ❌ (仅开发)
py-spy (sampling)       <1%         ~5MB       ✅
eBPF (Parca/Pyroscope)  <0.5%       ~5MB       ✅
Instrumentation         5-20%       高         ❌ (仅测试)

推荐生产环境:
1. eBPF-based (最低开销)
2. Go pprof + Continuous Profiling
3. Java JFR
```

### 7.3 数据存储与查询

```text
Profile 数据存储:

1. 数据量估算:
   - 单个 CPU Profile: ~500KB (未压缩)
   - 压缩后: ~50KB (gzip)
   - 聚合后: ~5KB/天 (仅保留 top N 函数)

   示例: 100 个服务, 每个每 10 秒采集一次
   - 每天: 100 * 8640 * 50KB = 43GB (未压缩)
   - 压缩后: 4.3GB/天
   - 聚合后: 0.5GB/天

2. 存储方案:
   - Parca: 自有格式, 高度压缩
   - Pyroscope: Parquet 格式
   - 对象存储: S3/GCS (长期存储)

3. 查询优化:
   - 标签索引 (service, version, region)
   - 时间分片 (按小时/天)
   - 预聚合 (top functions)
```

### 7.4 告警与异常检测

```yaml
# prometheus-rules.yaml - Profile 相关告警

groups:
- name: profiling_alerts
  rules:
  # CPU 热点函数占比异常增加
  - alert: CPUHotspotIncreased
    expr: |
      (
        sum by (service, function) (rate(cpu_profile_samples[5m]))
        /
        sum by (service) (rate(cpu_profile_samples[5m]))
      ) > 0.3  # 单个函数占比超过 30%
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "CPU hotspot detected in {{ $labels.service }}"
      description: "Function {{ $labels.function }} is using {{ $value | humanizePercentage }} of CPU time"

  # 内存分配速率异常
  - alert: HighAllocationRate
    expr: rate(go_memstats_alloc_bytes_total[5m]) > 100 * 1024 * 1024  # 100MB/s
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory allocation rate in {{ $labels.service }}"
      description: "Allocation rate: {{ $value | humanize }}B/s"

  # Goroutine 泄漏
  - alert: GoroutineLeakSuspected
    expr: go_goroutines > 10000 and rate(go_goroutines[30m]) > 100
    for: 15m
    labels:
      severity: critical
    annotations:
      summary: "Possible goroutine leak in {{ $labels.service }}"
      description: "Goroutine count: {{ $value }}, increasing rapidly"
```

---

## 第八部分: Profiles + Traces + Metrics 关联

### 8.1 Exemplars - 从 Metrics 到 Profiles

```text
Exemplars 是 Prometheus 的特性,
可以将 Metrics 与 Traces/Profiles 关联:

Metrics (聚合数据)  ───┐
                       ├──▶ Exemplar ──▶ Trace/Profile (详细数据)
Histogram/Summary  ───┘

示例:
http_request_duration_seconds_bucket{le="0.5"} 1000
# 其中某个请求的 exemplar:
http_request_duration_seconds_bucket{le="0.5"} 1000 # {trace_id="abc123",profile_id="xyz789"} 0.123 1633024800
```

**Go 实现 (Prometheus + Exemplars)**:

```go
// exemplars.go
package main

import (
    "context"
    "math/rand"
    "net/http"
    "time"

    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
    "github.com/prometheus/client_golang/prometheus/promhttp"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/trace"
)

var (
    // 支持 Exemplars 的 Histogram
    httpDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "http_request_duration_seconds",
            Help:    "HTTP request duration",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "path"},
    )
)

func handler(w http.ResponseWriter, r *http.Request) {
    start := time.Now()

    // 开始 Trace
    ctx := r.Context()
    tracer := otel.Tracer("my-service")
    ctx, span := tracer.Start(ctx, "handleRequest")
    defer span.End()

    // 业务逻辑
    time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)

    // 记录 Metrics + Exemplar (包含 trace_id)
    duration := time.Since(start).Seconds()
    traceID := span.SpanContext().TraceID().String()

    httpDuration.WithLabelValues("GET", "/api").
        ObserveWithExemplar(duration, prometheus.Labels{
            "trace_id": traceID,
        })

    w.WriteHeader(http.StatusOK)
}

func main() {
    http.HandleFunc("/api", handler)
    http.Handle("/metrics", promhttp.Handler())
    http.ListenAndServe(":8080", nil)
}
```

**Grafana 查询 (从 Metrics 跳转到 Trace)**:

```promql
# 查询慢请求的 trace_id
topk(10, histogram_quantile(0.99,
  rate(http_request_duration_seconds_bucket[5m])
)) by (trace_id)

# 点击 Exemplar 图标 → 自动跳转到 Jaeger/Tempo
```

### 8.2 SpanID 关联 - 从 Traces 到 Profiles

```go
// span_profile_link.go
package main

import (
    "bytes"
    "context"
    "runtime/pprof"
    "time"

    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/trace"
)

func slowOperation(ctx context.Context) {
    tracer := otel.Tracer("my-service")
    ctx, span := tracer.Start(ctx, "slowOperation")
    defer span.End()

    // 采集这个 Span 的 CPU Profile
    var buf bytes.Buffer
    pprof.StartCPUProfile(&buf)
    
    // 执行耗时操作
    time.Sleep(2 * time.Second)
    heavyComputation()
    
    pprof.StopCPUProfile()

    // 将 Profile 与 Span 关联
    profileID := uploadProfile(buf.Bytes())
    span.SetAttributes(attribute.String("profile.id", profileID))

    // 在 Trace UI 中可以看到 profile.id 属性
    // 点击可跳转到 Profile 查看器
}

func uploadProfile(data []byte) string {
    // 上传 Profile 到 Parca/Pyroscope
    // 返回 Profile ID
    return "profile-abc123"
}
```

### 8.3 统一可观测性平台

```text
完整的可观测性关联:

┌─────────────────────────────────────────────────────┐
│                   Grafana Dashboard                 │
└───────────┬─────────────────────────────────────────┘
            │
    ┌───────┼───────┬──────────────┬─────────────┐
    │       │       │              │             │
    ▼       ▼       ▼              ▼             ▼
┌────────┐ ┌────┐ ┌─────────┐ ┌────────┐ ┌──────────┐
│Metrics │ │Logs│ │ Traces  │ │Profiles│ │ Events   │
│Prometheus Loki │ │ Tempo   │ │Pyroscope│ │Alertmgr │
└────────┘ └────┘ └─────────┘ └────────┘ └──────────┘
    │       │          │            │          │
    └───────┴──────────┴────────────┴──────────┘
                       │
                 Correlation
              (trace_id, span_id,
               profile_id, labels)

工作流示例:
1. Metrics 显示 P99 延迟增加 → 点击 Exemplar → 跳转到 Trace
2. Trace 显示某个 Span 慢 → 查看 profile.id → 跳转到 Profile
3. Profile 显示 CPU 热点函数 → 查看函数代码 → 优化
4. 优化后 → Metrics 显示延迟降低 → 验证成功
```

---

## 第九部分: 高级话题

### 9.1 eBPF-based Profiling

#### Parca Agent eBPF 实现

```text
eBPF-based Profiling 优势:
1. 无需修改应用代码
2. 无需重启应用
3. 支持所有语言 (包括 native code)
4. 极低开销 (<0.5% CPU)
5. 内核级采样,更精确

工作原理:
1. eBPF 程序 attach 到 perf_event (timer)
2. 每 10ms 采集一次 CPU 调用栈
3. 解析符号表 (DWARF, symbol table)
4. 聚合并上传到 Parca Server

挑战:
1. 符号解析 (需要 debug symbols 或 DWARF)
2. 容器化环境 (需要访问宿主机 /proc)
3. JIT 代码 (Java, Node.js 需要额外支持)
```

**Parca Agent 核心 eBPF 程序** (简化版):

```c
// parca_agent.bpf.c
#include <linux/bpf.h>
#include <bpf/bpf_helpers.h>

#define MAX_STACK_DEPTH 127

struct {
    __uint(type, BPF_MAP_TYPE_STACK_TRACE);
    __uint(max_entries, 10000);
    __uint(key_size, sizeof(__u32));
    __uint(value_size, MAX_STACK_DEPTH * sizeof(__u64));
} stack_traces SEC(".maps");

struct {
    __uint(type, BPF_MAP_TYPE_HASH);
    __uint(max_entries, 10000);
    __type(key, __u32);  // stack_id
    __type(value, __u64);  // count
} counts SEC(".maps");

SEC("perf_event")
int profile_cpu(struct bpf_perf_event_data *ctx) {
    __u32 pid = bpf_get_current_pid_tgid() >> 32;

    // 采集调用栈
    __u32 stack_id = bpf_get_stackid(ctx, &stack_traces, 0);
    if (stack_id < 0)
        return 0;

    // 增加计数
    __u64 *count = bpf_map_lookup_elem(&counts, &stack_id);
    if (count) {
        __sync_fetch_and_add(count, 1);
    } else {
        __u64 one = 1;
        bpf_map_update_elem(&counts, &stack_id, &one, BPF_NOEXIST);
    }

    return 0;
}

char LICENSE[] SEC("license") = "GPL";
```

**用户态程序** (Go):

```go
// parca_agent.go
package main

import (
    "fmt"
    "time"

    "github.com/cilium/ebpf"
    "github.com/cilium/ebpf/link"
    "github.com/cilium/ebpf/perf"
)

func main() {
    // 加载 eBPF 程序
    spec, err := ebpf.LoadCollectionSpec("parca_agent.bpf.o")
    if err != nil {
        panic(err)
    }

    coll, err := ebpf.NewCollection(spec)
    if err != nil {
        panic(err)
    }
    defer coll.Close()

    // Attach 到 perf_event (CPU clock, 100 Hz)
    prog := coll.Programs["profile_cpu"]
    link, err := link.AttachPerfEvent(link.PerfEventOptions{
        Program:  prog,
        PerfType: perf.EventTypeHardware,
        Config:   perf.ConfigHardwareCPUCycles,
        SamplePeriod: 10000000,  // 100 Hz
    })
    if err != nil {
        panic(err)
    }
    defer link.Close()

    // 定期读取 counts map
    ticker := time.NewTicker(10 * time.Second)
    for range ticker.C {
        processProfiles(coll)
    }
}

func processProfiles(coll *ebpf.Collection) {
    stackTraces := coll.Maps["stack_traces"]
    counts := coll.Maps["counts"]

    // 遍历 counts
    var stackID uint32
    var count uint64
    iter := counts.Iterate()
    for iter.Next(&stackID, &count) {
        // 获取调用栈
        var stack []uint64
        if err := stackTraces.Lookup(stackID, &stack); err != nil {
            continue
        }

        // 解析符号 (简化, 实际需要读取 /proc/[pid]/maps 和 ELF)
        for _, addr := range stack {
            symbol := resolveSymbol(addr)
            fmt.Printf("  %s\n", symbol)
        }
        fmt.Printf("  count: %d\n\n", count)
    }

    // 清空 counts (开始新的采样周期)
    counts.Close()
}

func resolveSymbol(addr uint64) string {
    // TODO: 解析符号表
    return fmt.Sprintf("0x%x", addr)
}
```

### 9.2 差分分析 (Diff Analysis)

```bash
# 对比两个版本的 Profile (找出性能退化)

# 采集 v1.0 的 profile
curl -o v1.0.prof http://service-v1:6060/debug/pprof/profile?seconds=30

# 部署 v1.1
# 采集 v1.1 的 profile
curl -o v1.1.prof http://service-v1:6060/debug/pprof/profile?seconds=30

# 对比
go tool pprof -http=:8080 -base v1.0.prof v1.1.prof

# 或使用 Parca/Pyroscope Web UI:
# 1. 选择 v1.0 作为 baseline
# 2. 选择 v1.1 作为 comparison
# 3. 查看差异火焰图 (Diff Flame Graph)
#    - 红色: 性能变差 (CPU 时间增加)
#    - 绿色: 性能改善 (CPU 时间减少)
#    - 灰色: 无变化
```

**Parca Web UI Diff 查询**:

```text
# Parca Query Language (PQL)

# 查询 v1.1 相比 v1.0 的差异
profile_cpu{service="my-service", version="v1.1"}
-
profile_cpu{service="my-service", version="v1.0"}

# 结果: 显示每个函数的 CPU 时间变化
main.processRequest: +50ms (+25%)  # 性能退化
main.parseJSON: -20ms (-40%)       # 性能改善
```

### 9.3 多维度标签 (Multi-dimensional Labels)

```go
// multi_dimensional_labels.go
package main

import (
    "github.com/grafana/pyroscope-go"
)

func main() {
    pyroscope.Start(pyroscope.Config{
        ApplicationName: "my-service",
        ServerAddress:   "http://pyroscope:4040",

        // 静态标签 (在进程启动时确定)
        Tags: map[string]string{
            "service":  "my-service",
            "version":  "v1.2.3",
            "region":   "us-east-1",
            "env":      "production",
            "hostname": "pod-abc123",
        },
    })

    // 动态标签 (在运行时根据上下文添加)
    pyroscope.TagWrapper(context.Background(), pyroscope.Labels(
        "endpoint", "/api/users",
        "user_tier", "premium",
    ), func(ctx context.Context) {
        // 这个代码块的 profile 会包含上述标签
        handleRequest(ctx)
    })
}

func handleRequest(ctx context.Context) {
    // 根据用户类型添加不同标签
    userID := getUserID(ctx)
    userTier := getUserTier(userID)

    pyroscope.TagWrapper(ctx, pyroscope.Labels(
        "user_id", userID,
        "user_tier", userTier,
    ), func(ctx context.Context) {
        // 实际业务逻辑
        processUserRequest(ctx)
    })
}
```

**查询多维度 Profile**:

```text
# Pyroscope Query

# 查询所有 Premium 用户的 CPU profile
profile_cpu{service="my-service", user_tier="premium"}

# 查询特定 endpoint 的 profile
profile_cpu{service="my-service", endpoint="/api/users"}

# 对比不同 region 的性能
profile_cpu{service="my-service", region="us-east-1"}
vs
profile_cpu{service="my-service", region="eu-west-1"}
```

---

## 第十部分: 完整生产案例

### 10.1 案例: 电商平台性能优化

#### 系统背景

```text
系统: 大型电商平台订单服务
规模: 10,000 QPS, 100+ 微服务
问题: P99 延迟从 200ms 增加到 1.5s (过去 2 周)
影响: 用户投诉增加, 订单转化率下降 5%
```

#### 实施方案

**步骤 1: 部署 Continuous Profiling**:

```yaml
# 部署 Parca
kubectl apply -f parca-deployment.yaml

# 为所有服务启用 pprof (Go 服务)
# main.go
import _ "net/http/pprof"

func main() {
    go func() {
        http.ListenAndServe(":6060", nil)
    }()
    // ...
}

# 配置 Parca Agent 拉取 profiles
# parca-agent-config.yaml
scrape_configs:
  - job_name: "order-service"
    scrape_interval: "10s"
    static_configs:
      - targets:
        - "order-service-1:6060"
        - "order-service-2:6060"
        - "order-service-3:6060"
        labels:
          service: "order-service"
          version: "v2.3.5"
```

**步骤 2: 分析性能退化**:

```bash
# 对比 2 周前的 profile
# Parca UI: 选择时间范围对比

发现:
1. database.(*Client).Query 从 10% → 45% (CPU 时间)
2. 新增的函数 validateOrder 占用 15%
3. json.Marshal 占用 20% (之前 5%)
```

**步骤 3: 深入分析**:

```text
1. database.(*Client).Query 增加:
   - 查看代码 git blame
   - 发现 2 周前添加了新的查询: SELECT * FROM order_items WHERE order_id = ?
   - 问题: N+1 查询! 每个订单都查询一次
   
2. validateOrder 函数:
   - 新增的业务逻辑
   - 包含多次数据库查询和外部 API 调用
   - 优化空间大

3. json.Marshal 增加:
   - 响应体变大 (新增了大量字段)
   - 没有复用序列化结果
```

**步骤 4: 优化**:

```go
// 优化 1: 修复 N+1 查询
// before.go
func GetOrder(orderID string) (*Order, error) {
    order := &Order{}
    db.Get(order, "SELECT * FROM orders WHERE id = ?", orderID)

    // N+1 查询!
    for _, item := range order.Items {
        db.Get(&item, "SELECT * FROM order_items WHERE order_id = ?", orderID)
    }
    return order, nil
}

// after.go
func GetOrder(orderID string) (*Order, error) {
    order := &Order{}
    db.Get(order, "SELECT * FROM orders WHERE id = ?", orderID)

    // 批量查询
    db.Select(&order.Items, "SELECT * FROM order_items WHERE order_id = ?", orderID)
    return order, nil
}

// 优化 2: 并行化 validateOrder
// before.go
func validateOrder(order *Order) error {
    if err := checkInventory(order); err != nil {  // 500ms
        return err
    }
    if err := checkCredit(order.UserID); err != nil {  // 300ms
        return err
    }
    if err := checkAddress(order.Address); err != nil {  // 200ms
        return err
    }
    return nil  // 总计: 1000ms
}

// after.go
func validateOrder(order *Order) error {
    var g errgroup.Group

    g.Go(func() error {
        return checkInventory(order)  // 并行
    })
    g.Go(func() error {
        return checkCredit(order.UserID)  // 并行
    })
    g.Go(func() error {
        return checkAddress(order.Address)  // 并行
    })

    return g.Wait()  // 总计: 500ms (最慢的那个)
}

// 优化 3: 缓存 JSON 序列化结果
var jsonCache = cache.New(5*time.Minute, 10*time.Minute)

func serializeOrder(order *Order) ([]byte, error) {
    cacheKey := fmt.Sprintf("order_json_%s_%d", order.ID, order.UpdatedAt.Unix())

    if cached, found := jsonCache.Get(cacheKey); found {
        return cached.([]byte), nil  // 命中缓存
    }

    data, err := json.Marshal(order)
    if err != nil {
        return nil, err
    }

    jsonCache.Set(cacheKey, data, cache.DefaultExpiration)
    return data, nil
}
```

**步骤 5: 验证优化效果**:

```bash
# 部署优化后的版本 v2.3.6
kubectl set image deployment/order-service order-service=order-service:v2.3.6

# 对比 Profile
# Parca UI: 选择 v2.3.5 vs v2.3.6

结果:
- database.(*Client).Query: 45% → 8% (减少 82%)
- validateOrder: 15% → 5% (减少 67%)
- json.Marshal: 20% → 3% (减少 85%)
```

#### 优化成果

```text
性能指标:
- P50 延迟: 50ms → 45ms (改善 10%)
- P99 延迟: 1.5s → 180ms (改善 88%)
- P999 延迟: 3s → 500ms (改善 83%)
- QPS: 10,000 → 15,000 (提升 50%)

资源使用:
- CPU 使用率: 75% → 35% (减少 53%)
- 内存使用: 8GB → 6GB (减少 25%)
- 数据库连接数: 500 → 100 (减少 80%)

业务指标:
- 订单转化率: 恢复并提升 8%
- 用户投诉: 减少 90%
- 成本节约: 减少 15 台服务器 (节省 $5,000/月)

ROI:
- 优化投入: 40 工时 (约 $8,000)
- 月度收益: $5,000 (成本) + $50,000 (业务增长)
- ROI: 687% (首月)
```

---

## 总结

### Continuous Profiling 核心价值

✅ **提前发现性能问题**: 在用户投诉之前发现  
✅ **快速定位根因**: 从 Metrics → Traces → Profiles → 代码  
✅ **历史数据追溯**: 回答"什么时候开始变慢的"  
✅ **优化效果验证**: 量化优化前后的性能差异  
✅ **成本优化**: 发现资源浪费 (CPU, 内存, 网络)  
✅ **低开销**: <1% CPU, 可 7x24 运行

### 适用场景

1. **微服务架构** - 定位跨服务性能瓶颈
2. **高并发系统** - CPU/内存优化
3. **SaaS 平台** - 多租户性能隔离
4. **金融交易** - 低延迟优化
5. **大数据处理** - 计算密集型优化

### 参考资源

- 📚 [Google Cloud Profiler 文档](https://cloud.google.com/profiler/docs)
- 📚 [Parca 官方网站](https://www.parca.dev/)
- 📚 [Pyroscope 文档](https://grafana.com/docs/pyroscope/latest/)
- 📚 [Go pprof 文档](https://pkg.go.dev/net/http/pprof)
- 📚 [Java Flight Recorder 指南](https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm)
- 📚 [Brendan Gregg's Flame Graphs](https://www.brendangregg.com/flamegraphs.html)
- 📚 [eBPF Profiling 原理](https://www.polarsignals.com/blog/posts/2022/11/29/profiling-at-the-speed-of-light-with-ebpf/)
- 📄 [论文: Continuous Profiling at Google](https://research.google/pubs/pub36575/)

---

## 📚 相关文档

### 核心集成 ⭐⭐⭐

- **🤖 AIOps平台设计**: [查看文档](./🤖_OTLP自主运维能力完整架构_AIOps平台设计.md)
  - 使用场景: Profiling数据接入AIOps,自动检测性能异常
  - 关键章节: [性能瓶颈检测](./🤖_OTLP自主运维能力完整架构_AIOps平台设计.md#第三部分-机器学习模型)
  - 价值: 性能问题发现时间从3天降至30分钟

- **🐝 eBPF零侵入追踪**: [查看文档](./🐝_eBPF可观测性深度技术指南_零侵入式追踪.md)
  - 使用场景: 基于eBPF的持续性能剖析,无需修改应用
  - 关键章节: [eBPF Profiling](./🐝_eBPF可观测性深度技术指南_零侵入式追踪.md#性能分析)
  - 价值: 零侵入,开销<1%,生产环境7x24运行

- **🕸️ Service Mesh集成**: [查看文档](./🕸️_服务网格可观测性完整指南_Istio_Linkerd深度集成.md)
  - 使用场景: Envoy Sidecar性能剖析,优化Service Mesh开销
  - 关键章节: [Envoy性能优化](./🕸️_服务网格可观测性完整指南_Istio_Linkerd深度集成.md#第五部分-生产实战案例)
  - 价值: Sidecar开销从15%降至3%

### 架构可视化 ⭐⭐⭐

- **📊 架构图表指南**: [查看文档](./📊_架构图表与可视化指南_Mermaid完整版.md)
  - 推荐图表:
    - [Profiling架构](./📊_架构图表与可视化指南_Mermaid完整版.md#6-continuous-profiling)
    - [Profiling与Tracing关联](./📊_架构图表与可视化指南_Mermaid完整版.md#62-profiling-与-tracing-关联)
  - 价值: Trace → Profile跳转流程一目了然

### 工具链支持 ⭐⭐

- **📚 SDK最佳实践**: [查看文档](./📚_OTLP_SDK最佳实践指南_多语言全栈实现.md)
  - 使用场景: SDK集成Profiling,实现Trace-Profile关联
  - 关键章节: [Exemplars集成](./📚_OTLP_SDK最佳实践指南_多语言全栈实现.md#第三部分-生产级优化)
  - 价值: 从慢请求直接跳转到火焰图

---

**文档完成时间**: 2025年10月9日  
**文档状态**: 完整版 (2,500+ 行)  
**推荐学习时长**: 3-5 天 (含实践)

---

*"Performance is not a sprint, it's a marathon with Continuous Profiling!"*-
