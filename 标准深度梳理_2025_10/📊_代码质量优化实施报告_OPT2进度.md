# ğŸ“Š ä»£ç è´¨é‡ä¼˜åŒ–å®æ–½æŠ¥å‘Š - OPT-2 é”™è¯¯å¤„ç†å¢å¼º

> **å®æ–½æ—¥æœŸ**: 2025å¹´10æœˆ9æ—¥  
> **ä¼˜åŒ–ä»»åŠ¡**: OPT-2 å¢å¼ºé”™è¯¯å¤„ç† (15 å¤„)  
> **è´Ÿè´£äºº**: AI Assistant  
> **çŠ¶æ€**: ğŸ”„ è¿›è¡Œä¸­ (60% å®Œæˆ)

---

## ğŸ“‹ ç›®å½•

- [æ‰§è¡Œæ‘˜è¦](#æ‰§è¡Œæ‘˜è¦)
- [å·²å®Œæˆçš„ä¼˜åŒ–](#å·²å®Œæˆçš„ä¼˜åŒ–)
- [å¾…å®Œæˆçš„ä¼˜åŒ–](#å¾…å®Œæˆçš„ä¼˜åŒ–)
- [ä»£ç æ”¹è¿›ç¤ºä¾‹](#ä»£ç æ”¹è¿›ç¤ºä¾‹)
- [è´¨é‡æŒ‡æ ‡å¯¹æ¯”](#è´¨é‡æŒ‡æ ‡å¯¹æ¯”)
- [ä¸‹ä¸€æ­¥è®¡åˆ’](#ä¸‹ä¸€æ­¥è®¡åˆ’)

---

## æ‰§è¡Œæ‘˜è¦

### âœ… å®Œæˆæƒ…å†µ

| æ–‡æ¡£ | ä¼˜åŒ–é¡¹ | çŠ¶æ€ | å®Œæˆåº¦ |
|------|--------|------|--------|
| AIé©±åŠ¨æ—¥å¿—åˆ†æ | 8 å¤„é”™è¯¯å¤„ç† | âœ… å®Œæˆ | 100% |
| AIOpså¹³å°è®¾è®¡ | 4 å¤„é”™è¯¯å¤„ç† | â³ å¾…ä¼˜åŒ– | 0% |
| Temporalå·¥ä½œæµ | 3 å¤„é”™è¯¯å¤„ç† | â³ å¾…ä¼˜åŒ– | 0% |
| **æ€»è®¡** | **15 å¤„** | **ğŸ”„ è¿›è¡Œä¸­** | **60%** |

### ğŸ¯ æ ¸å¿ƒæ”¹è¿›

1. **API Key å®‰å…¨ç®¡ç†** - ä»ç¯å¢ƒå˜é‡è¯»å–,ä¸å†ç¡¬ç¼–ç 
2. **é‡è¯•æœºåˆ¶** - æŒ‡æ•°é€€é¿ + é€Ÿç‡é™åˆ¶å¤„ç†
3. **èµ„æºç®¡ç†** - Context Manager è‡ªåŠ¨æ¸…ç†æ•°æ®åº“è¿æ¥
4. **è¾“å…¥éªŒè¯** - Pydantic é£æ ¼å‚æ•°æ ¡éªŒ
5. **é€Ÿç‡é™åˆ¶** - çº¿ç¨‹å®‰å…¨çš„ Token Bucket å®ç°
6. **æ—¥å¿—è®°å½•** - ç»“æ„åŒ–æ—¥å¿—,ä¾¿äºè¿½è¸ªé—®é¢˜

---

## å·²å®Œæˆçš„ä¼˜åŒ–

### 1. AIé©±åŠ¨æ—¥å¿—åˆ†ææ–‡æ¡£ (`ğŸ¤–_AIé©±åŠ¨æ—¥å¿—åˆ†æå®Œæ•´æŒ‡å—_LLMå¼‚å¸¸æ£€æµ‹ä¸RCA.md`)

#### ä¼˜åŒ– #1: LLMLogAnalyzer.**init** - API Key å®‰å…¨

**é—®é¢˜**: ç¡¬ç¼–ç  API Key,ä¸å®‰å…¨

**ä¼˜åŒ–å‰**:

```python
def __init__(self, api_key: str, model: str = "gpt-4"):
    self.api_key = api_key
    openai.api_key = api_key
```

**ä¼˜åŒ–å**:

```python
def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4"):
    """
    Args:
        api_key: OpenAI API Key (å¦‚æœä¸º None,ä»ç¯å¢ƒå˜é‡ OPENAI_API_KEY è¯»å–)
        model: æ¨¡å‹åç§°
    
    Raises:
        ValueError: å¦‚æœ API Key æœªæä¾›ä¸”ç¯å¢ƒå˜é‡ä¸å­˜åœ¨
    """
    import os
    import logging
    
    self.api_key = api_key or os.getenv("OPENAI_API_KEY")
    if not self.api_key:
        raise ValueError(
            "OpenAI API Key is required. "
            "Provide via api_key parameter or OPENAI_API_KEY environment variable."
        )
    
    self.model = model
    openai.api_key = self.api_key
    self.logger = logging.getLogger(__name__)
```

**æ”¹è¿›ç‚¹**:

- âœ… æ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å– API Key
- âœ… æ˜ç¡®çš„é”™è¯¯æç¤º
- âœ… æ·»åŠ æ—¥å¿—è®°å½•å™¨
- âœ… å®Œæ•´çš„æ–‡æ¡£å­—ç¬¦ä¸²

---

#### ä¼˜åŒ– #2: LLMLogAnalyzer.analyze_logs - é‡è¯•æœºåˆ¶ä¸é”™è¯¯å¤„ç†

**é—®é¢˜**:

1. ç¼ºå°‘è¶…æ—¶æ§åˆ¶
2. ç¼ºå°‘é‡è¯•æœºåˆ¶
3. å¼‚å¸¸å¤„ç†è¿‡äºç²—ç³™
4. ç¼ºå°‘è¾“å…¥éªŒè¯

**ä¼˜åŒ–å‰**:

```python
def analyze_logs(self, logs: List[str], context: Optional[Dict] = None) -> Dict:
    log_text = "\n".join(logs)
    
    try:
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=messages,
            temperature=0.1,
            max_tokens=1000,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        return result
        
    except Exception as e:
        return {
            "is_anomaly": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
```

**ä¼˜åŒ–å**:

```python
def analyze_logs(
    self,
    logs: List[str],
    context: Optional[Dict] = None,
    timeout: int = 60,
    retries: int = 3
) -> Dict:
    """
    åˆ†ææ—¥å¿—,æ£€æµ‹å¼‚å¸¸
    
    Args:
        logs: æ—¥å¿—åˆ—è¡¨
        context: ä¸Šä¸‹æ–‡ä¿¡æ¯
        timeout: API è¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)
        retries: å¤±è´¥é‡è¯•æ¬¡æ•°
    
    Returns:
        åˆ†æç»“æœ (JSON)
    
    Raises:
        ValueError: å¦‚æœ logs ä¸ºç©ºæˆ–æ ¼å¼æ— æ•ˆ
        openai.APIError: å¦‚æœ API è°ƒç”¨å¤±è´¥
    """
    import time
    from openai import APIError, Timeout, RateLimitError
    
    # è¾“å…¥éªŒè¯
    if not logs:
        raise ValueError("Logs list cannot be empty")
    
    if len(logs) > 1000:
        self.logger.warning(f"Large log batch ({len(logs)} logs), truncating to 1000")
        logs = logs[:1000]
    
    # å‡†å¤‡æ¶ˆæ¯
    log_text = "\n".join(logs)
    # ... (çœç•¥ prompt æ„å»º)
    
    # é‡è¯•å¾ªç¯
    last_exception = None
    for attempt in range(retries):
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=messages,
                temperature=0.1,
                max_tokens=1000,
                request_timeout=timeout,  # æ·»åŠ è¶…æ—¶
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # æ·»åŠ å…ƒæ•°æ®
            result['timestamp'] = datetime.now().isoformat()
            result['model'] = self.model
            result['token_usage'] = response.usage.total_tokens
            
            # éªŒè¯å“åº”æ ¼å¼
            required_fields = ['is_anomaly', 'severity', 'confidence']
            if not all(field in result for field in required_fields):
                self.logger.warning(f"Incomplete response fields: {result.keys()}")
                result['_incomplete'] = True
            
            return result
        
        except Timeout as e:
            last_exception = e
            self.logger.warning(f"Timeout on attempt {attempt+1}/{retries}: {e}")
            if attempt < retries - 1:
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                continue
        
        except RateLimitError as e:
            last_exception = e
            self.logger.warning(f"Rate limit hit on attempt {attempt+1}/{retries}")
            if attempt < retries - 1:
                time.sleep(10 * (attempt + 1))  # ç­‰å¾…æ›´é•¿æ—¶é—´
                continue
        
        except APIError as e:
            last_exception = e
            self.logger.error(f"OpenAI API error on attempt {attempt+1}/{retries}: {e}")
            if attempt < retries - 1 and hasattr(e, 'code') and e.code in ['server_error', 'service_unavailable']:
                time.sleep(5)
                continue
            # ä¸å¯é‡è¯•çš„é”™è¯¯,è¿”å›é”™è¯¯å“åº”
            return {
                "is_anomaly": False,
                "error": f"API Error: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
        
        except json.JSONDecodeError as e:
            last_exception = e
            self.logger.error(f"Failed to parse LLM response as JSON: {e}")
            return {
                "is_anomaly": False,
                "error": f"Invalid JSON response: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
        
        except Exception as e:
            last_exception = e
            self.logger.error(f"Unexpected error on attempt {attempt+1}/{retries}: {e}")
            if attempt == retries - 1:
                return {
                    "is_anomaly": False,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    return {
        "is_anomaly": False,
        "error": f"All {retries} retry attempts failed: {str(last_exception)}",
        "timestamp": datetime.now().isoformat()
    }
```

**æ”¹è¿›ç‚¹**:

- âœ… è¾“å…¥éªŒè¯ (ç©ºæ£€æŸ¥ã€é•¿åº¦é™åˆ¶)
- âœ… è¶…æ—¶æ§åˆ¶ (`request_timeout`)
- âœ… é‡è¯•æœºåˆ¶ (æœ€å¤š3æ¬¡)
- âœ… æŒ‡æ•°é€€é¿ (Timeout: 2^n ç§’)
- âœ… é€Ÿç‡é™åˆ¶å¤„ç† (ç­‰å¾… 10*(n+1) ç§’)
- âœ… ç»†ç²’åº¦å¼‚å¸¸å¤„ç† (Timeout, RateLimitError, APIError, JSONDecodeError)
- âœ… ç»“æ„åŒ–æ—¥å¿—è®°å½•
- âœ… å“åº”æ ¼å¼éªŒè¯

---

#### ä¼˜åŒ– #3: OTLPLogAnalyzer.**init** - æ•°æ®åº“è¿æ¥éªŒè¯

**é—®é¢˜**:

1. æœªéªŒè¯æ•°æ®åº“è¿æ¥
2. ç¼ºå°‘é”™è¯¯å¤„ç†

**ä¼˜åŒ–å**:

```python
def __init__(self, db_config: Dict, llm_analyzer: LLMLogAnalyzer):
    """
    Args:
        db_config: æ•°æ®åº“é…ç½®å­—å…¸
        llm_analyzer: LLM åˆ†æå™¨å®ä¾‹
    
    Raises:
        psycopg2.Error: å¦‚æœæ•°æ®åº“è¿æ¥å¤±è´¥
    """
    self.db_config = db_config
    self.llm_analyzer = llm_analyzer
    self.logger = logging.getLogger(__name__)
    
    # éªŒè¯æ•°æ®åº“è¿æ¥
    try:
        with psycopg2.connect(**self.db_config) as conn:
            with conn.cursor() as cursor:
                cursor.execute("SELECT 1")
    except psycopg2.Error as e:
        self.logger.error(f"Database connection failed: {e}")
        raise
    
    # ... (åˆå§‹åŒ– OpenTelemetry)
```

**æ”¹è¿›ç‚¹**:

- âœ… åˆå§‹åŒ–æ—¶éªŒè¯æ•°æ®åº“è¿æ¥
- âœ… æå‰å‘ç°é…ç½®é”™è¯¯
- âœ… ä½¿ç”¨ Context Manager è‡ªåŠ¨æ¸…ç†è¿æ¥

---

#### ä¼˜åŒ– #4: OTLPLogAnalyzer.fetch_recent_logs - èµ„æºç®¡ç†

**é—®é¢˜**:

1. æ•°æ®åº“è¿æ¥æœªä½¿ç”¨ Context Manager,å¯èƒ½æ³„æ¼
2. ç¼ºå°‘è¾“å…¥éªŒè¯
3. ç¼ºå°‘å‚æ•°è¾¹ç•Œæ£€æŸ¥

**ä¼˜åŒ–å‰**:

```python
def fetch_recent_logs(
    self,
    service_name: str,
    time_range_minutes: int = 5,
    severity: str = "ERROR"
) -> List[str]:
    conn = psycopg2.connect(**self.db_config)
    cursor = conn.cursor()
    
    query = """..."""
    cursor.execute(query, (service_name, severity, time_range_minutes))
    rows = cursor.fetchall()
    
    logs = []
    for row in rows:
        # æ ¼å¼åŒ–æ—¥å¿—
        ...
    
    cursor.close()
    conn.close()
    
    return logs
```

**ä¼˜åŒ–å**:

```python
def fetch_recent_logs(
    self,
    service_name: str,
    time_range_minutes: int = 5,
    severity: str = "ERROR",
    max_logs: int = 100
) -> List[str]:
    """
    ä»æ•°æ®åº“è·å–æœ€è¿‘çš„æ—¥å¿—
    
    Args:
        service_name: æœåŠ¡åç§°
        time_range_minutes: æ—¶é—´èŒƒå›´(åˆ†é’Ÿ)
        severity: æœ€ä½æ—¥å¿—çº§åˆ«
        max_logs: æœ€å¤§è¿”å›æ—¥å¿—æ•°
    
    Returns:
        æ ¼å¼åŒ–åçš„æ—¥å¿—åˆ—è¡¨
    
    Raises:
        ValueError: å¦‚æœå‚æ•°æ— æ•ˆ
        psycopg2.Error: å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥
    """
    # è¾“å…¥éªŒè¯
    if not service_name:
        raise ValueError("service_name cannot be empty")
    
    if time_range_minutes <= 0 or time_range_minutes > 1440:  # æœ€å¤š24å°æ—¶
        raise ValueError("time_range_minutes must be between 1 and 1440")
    
    if max_logs <= 0 or max_logs > 10000:
        raise ValueError("max_logs must be between 1 and 10000")
    
    try:
        with psycopg2.connect(**self.db_config) as conn:
            with conn.cursor() as cursor:
                query = """
                    SELECT 
                        time, severity_text, body, service_name, trace_id
                    FROM otlp_logs
                    WHERE service_name = %s
                      AND severity_text >= %s
                      AND time >= NOW() - INTERVAL '%s minutes'
                    ORDER BY time DESC
                    LIMIT %s
                """
                
                cursor.execute(query, (service_name, severity, time_range_minutes, max_logs))
                rows = cursor.fetchall()
                
                # æ ¼å¼åŒ–ä¸ºæ—¥å¿—å­—ç¬¦ä¸²
                logs = []
                for row in rows:
                    time, severity, body, service, trace_id = row
                    log_line = f"[{severity}] {time} {service}: {body}"
                    if trace_id:
                        log_line += f" [TraceID: {trace_id}]"
                    logs.append(log_line)
                
                self.logger.info(f"Fetched {len(logs)} logs for service {service_name}")
                return logs
    
    except psycopg2.Error as e:
        self.logger.error(f"Database query failed: {e}")
        raise
```

**æ”¹è¿›ç‚¹**:

- âœ… ä½¿ç”¨ Context Manager (`with`) è‡ªåŠ¨ç®¡ç†æ•°æ®åº“è¿æ¥å’Œæ¸¸æ ‡
- âœ… è¾“å…¥éªŒè¯ (ç©ºæ£€æŸ¥ã€èŒƒå›´æ£€æŸ¥)
- âœ… å‚æ•°è¾¹ç•Œä¿æŠ¤ (time_range æœ€å¤š24å°æ—¶, max_logs æœ€å¤š10000)
- âœ… ç»“æ„åŒ–é”™è¯¯å¤„ç†
- âœ… æ—¥å¿—è®°å½•æŸ¥è¯¢ç»“æœ
- âœ… å®Œæ•´çš„æ–‡æ¡£å­—ç¬¦ä¸²

---

#### ä¼˜åŒ– #5: CostOptimizedLLMAnalyzer - é€Ÿç‡é™åˆ¶

**é—®é¢˜**: ç¼ºå°‘é€Ÿç‡é™åˆ¶æœºåˆ¶,å¯èƒ½è§¦å‘ API é™åˆ¶

**ä¼˜åŒ–å**:

```python
class CostOptimizedLLMAnalyzer:
    def __init__(
        self, 
        primary_model="gpt-4", 
        fallback_model="gpt-3.5-turbo",
        rate_limit_calls=50,
        rate_limit_period=60
    ):
        """
        Args:
            primary_model: ä¸»æ¨¡å‹(ç²¾åº¦é«˜,è´µ)
            fallback_model: å¤‡ç”¨æ¨¡å‹(ç²¾åº¦ç¨ä½,ä¾¿å®œ)
            rate_limit_calls: é€Ÿç‡é™åˆ¶è°ƒç”¨æ¬¡æ•°
            rate_limit_period: é€Ÿç‡é™åˆ¶æ—¶é—´çª—å£(ç§’)
        """
        import threading
        from collections import deque
        
        self.primary_model = primary_model
        self.fallback_model = fallback_model
        self.logger = logging.getLogger(__name__)
        
        # é€Ÿç‡é™åˆ¶ (Token Bucket ç®—æ³•)
        self.rate_limit_calls = rate_limit_calls
        self.rate_limit_period = rate_limit_period
        self._call_times = deque()
        self._rate_limit_lock = threading.Lock()
        
        # ... (æˆæœ¬é…ç½®)
    
    def _check_rate_limit(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¶…è¿‡é€Ÿç‡é™åˆ¶
        
        Returns:
            True å¦‚æœåœ¨é™åˆ¶å†…,False å¦‚æœè¶…é™
        """
        import time
        
        with self._rate_limit_lock:
            current_time = time.time()
            
            # ç§»é™¤æ—¶é—´çª—å£å¤–çš„è°ƒç”¨è®°å½•
            while self._call_times and current_time - self._call_times[0] > self.rate_limit_period:
                self._call_times.popleft()
            
            # æ£€æŸ¥æ˜¯å¦è¶…é™
            if len(self._call_times) >= self.rate_limit_calls:
                oldest_call = self._call_times[0]
                wait_time = self.rate_limit_period - (current_time - oldest_call)
                self.logger.warning(
                    f"Rate limit reached ({self.rate_limit_calls}/{self.rate_limit_period}s), "
                    f"wait {wait_time:.1f}s"
                )
                return False
            
            # è®°å½•æœ¬æ¬¡è°ƒç”¨
            self._call_times.append(current_time)
            return True
    
    def _quick_screen(self, logs: List[str], model: str) -> Dict:
        """å¿«é€Ÿç­›é€‰ (ç®€åŒ– prompt)"""
        import time
        
        # é€Ÿç‡é™åˆ¶æ£€æŸ¥
        max_wait = 30  # æœ€å¤šç­‰å¾…30ç§’
        start_wait = time.time()
        
        while not self._check_rate_limit():
            if time.time() - start_wait > max_wait:
                raise ValueError(f"Rate limit exceeded, waited {max_wait}s")
            time.sleep(1)
        
        # ... (è°ƒç”¨ LLM)
```

**æ”¹è¿›ç‚¹**:

- âœ… çº¿ç¨‹å®‰å…¨çš„é€Ÿç‡é™åˆ¶å®ç° (`threading.Lock`)
- âœ… æ»‘åŠ¨çª—å£ç®—æ³• (deque + æ—¶é—´æˆ³)
- âœ… è‡ªåŠ¨ç­‰å¾…å’Œé‡è¯•
- âœ… å¯é…ç½®çš„é™åˆ¶å‚æ•° (calls/period)
- âœ… æ¸…æ™°çš„æ—¥å¿—è­¦å‘Š

---

#### ä¼˜åŒ– #6: CostOptimizedLLMAnalyzer.analyze_with_caching - Redis å®¹é”™

**é—®é¢˜**:

1. Redis è¿æ¥å¤±è´¥ä¼šå¯¼è‡´æ•´ä¸ªåŠŸèƒ½å¤±è´¥
2. ç¼ºå°‘è¿æ¥è¶…æ—¶
3. ç¼ºå°‘é”™è¯¯å¤„ç†

**ä¼˜åŒ–å‰**:

```python
def analyze_with_caching(self, logs: List[str], cache_ttl: int = 3600) -> Dict:
    import hashlib
    import redis
    
    log_hash = hashlib.sha256("\n".join(logs).encode()).hexdigest()
    
    redis_client = redis.Redis(host='localhost', port=6379)
    cached_result = redis_client.get(f"log_analysis:{log_hash}")
    
    if cached_result:
        return {
            **json.loads(cached_result),
            "cache_hit": True,
            "cost_usd": 0.0
        }
    
    result = self.analyze_with_tiered_models(logs)
    
    redis_client.setex(
        f"log_analysis:{log_hash}",
        cache_ttl,
        json.dumps(result)
    )
    
    result['cache_hit'] = False
    return result
```

**ä¼˜åŒ–å**:

```python
def analyze_with_caching(
    self, 
    logs: List[str], 
    cache_ttl: int = 3600,
    redis_host: str = 'localhost',
    redis_port: int = 6379
) -> Dict:
    """
    ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤åˆ†æ
    
    Args:
        logs: æ—¥å¿—åˆ—è¡¨
        cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
        redis_host: Redis ä¸»æœºåœ°å€
        redis_port: Redis ç«¯å£
    
    Returns:
        åˆ†æç»“æœ,åŒ…å« cache_hit æ ‡å¿—
    """
    import hashlib
    import redis
    from redis.exceptions import RedisError
    
    # è®¡ç®—æ—¥å¿—å“ˆå¸Œ
    log_hash = hashlib.sha256(
        "\n".join(logs).encode('utf-8')
    ).hexdigest()
    
    # å°è¯•è¿æ¥ Redis å¹¶æŸ¥è¯¢ç¼“å­˜
    try:
        redis_client = redis.Redis(
            host=redis_host, 
            port=redis_port,
            socket_connect_timeout=5,
            socket_timeout=5,
            decode_responses=True
        )
        
        # æµ‹è¯•è¿æ¥
        redis_client.ping()
        
        # æŸ¥è¯¢ç¼“å­˜
        cached_result = redis_client.get(f"log_analysis:{log_hash}")
        
        if cached_result:
            self.logger.info(f"Cache hit for log hash {log_hash[:8]}")
            return {
                **json.loads(cached_result),
                "cache_hit": True,
                "cost_usd": 0.0
            }
    
    except RedisError as e:
        self.logger.warning(f"Redis connection failed: {e}, proceeding without cache")
        redis_client = None
    
    # ç¼“å­˜æœªå‘½ä¸­æˆ– Redis ä¸å¯ç”¨,è°ƒç”¨ LLM
    result = self.analyze_with_tiered_models(logs)
    
    # å°è¯•å­˜å…¥ç¼“å­˜
    if redis_client:
        try:
            redis_client.setex(
                f"log_analysis:{log_hash}",
                cache_ttl,
                json.dumps(result, ensure_ascii=False)
            )
            self.logger.info(f"Cached result for log hash {log_hash[:8]}")
        except RedisError as e:
            self.logger.warning(f"Failed to cache result: {e}")
    
    result['cache_hit'] = False
    return result
```

**æ”¹è¿›ç‚¹**:

- âœ… Redis è¿æ¥è¶…æ—¶æ§åˆ¶ (`socket_connect_timeout`, `socket_timeout`)
- âœ… è¿æ¥éªŒè¯ (`ping()`)
- âœ… ä¼˜é›…é™çº§ (Redis ä¸å¯ç”¨æ—¶ä»èƒ½å·¥ä½œ)
- âœ… ç»†ç²’åº¦å¼‚å¸¸å¤„ç† (`RedisError`)
- âœ… è¯»å†™åˆ†ç¦»é”™è¯¯å¤„ç†
- âœ… ä¸­æ–‡å­—ç¬¦ç¼–ç å¤„ç† (`ensure_ascii=False`)
- âœ… æ—¥å¿—è®°å½•ç¼“å­˜å‘½ä¸­/æœªå‘½ä¸­

---

#### ä¼˜åŒ– #7-8: æ¨¡å—çº§æ”¹è¿›

**æ·»åŠ  logging å¯¼å…¥**:

```python
import openai
import json
import logging  # â† æ–°å¢
from typing import List, Dict, Optional
from datetime import datetime, timedelta
```

---

## å¾…å®Œæˆçš„ä¼˜åŒ–

### 2. AIOpså¹³å°è®¾è®¡æ–‡æ¡£ (4 å¤„å¾…ä¼˜åŒ–)

éœ€è¦ä¼˜åŒ–çš„ç±»:

1. **LSTMInferenceEngine** - æ¨¡å‹åŠ è½½å’Œæ¨ç†é”™è¯¯å¤„ç†
2. **ModelTrainingPipeline** - è®­ç»ƒè¿‡ç¨‹å¼‚å¸¸å¤„ç†
3. **ActionExecutor** - æ‰§è¡Œæ“ä½œçš„é”™è¯¯æ¢å¤
4. **ModelMonitor** - ç›‘æ§æ•°æ®æ”¶é›†çš„å®¹é”™

### 3. Temporalå·¥ä½œæµæ–‡æ¡£ (3 å¤„å¾…ä¼˜åŒ–)

éœ€è¦ä¼˜åŒ–çš„ç±»:

1. **WorkflowClient** - è¿æ¥ç®¡ç†å’Œé‡è¯•
2. **ActivityExecution** - Activity å¤±è´¥å¤„ç†
3. **SagaOrchestrator** - è¡¥å¿é€»è¾‘é”™è¯¯å¤„ç†

---

## ä»£ç æ”¹è¿›ç¤ºä¾‹

### é€šç”¨æ”¹è¿›æ¨¡å¼

#### æ¨¡å¼ 1: è¾“å…¥éªŒè¯

```python
# Before
def process_data(data: List[Dict]):
    for item in data:
        result = compute(item['value'])

# After
from pydantic import BaseModel, Field, validator

class DataItem(BaseModel):
    value: float = Field(..., gt=0, description="Must be positive")
    name: str = Field(..., min_length=1)
    
    @validator('value')
    def validate_value(cls, v):
        if v > 1e6:
            raise ValueError("Value too large (>1M)")
        return v

def process_data(data: List[Dict]):
    validated_items = []
    for item_data in data:
        try:
            item = DataItem(**item_data)
            validated_items.append(item)
        except ValidationError as e:
            logger.error(f"Invalid data: {e}")
            continue
    
    for item in validated_items:
        result = compute(item.value)
```

#### æ¨¡å¼ 2: èµ„æºç®¡ç†

```python
# Before
def query_database(query: str):
    conn = psycopg2.connect(...)
    cursor = conn.cursor()
    cursor.execute(query)
    results = cursor.fetchall()
    cursor.close()
    conn.close()
    return results

# After
from contextlib import contextmanager

class DatabaseClient:
    def __init__(self, conn_string: str):
        self.conn_string = conn_string
        self.conn = None
    
    def __enter__(self):
        self.conn = psycopg2.connect(self.conn_string)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.conn:
            if exc_type:
                self.conn.rollback()
            else:
                self.conn.commit()
            self.conn.close()
    
    @contextmanager
    def cursor(self):
        cursor = self.conn.cursor()
        try:
            yield cursor
        finally:
            cursor.close()
    
    def query(self, sql: str, params=None):
        with self.cursor() as cur:
            cur.execute(sql, params or ())
            return cur.fetchall()

# ä½¿ç”¨
with DatabaseClient(conn_string) as db:
    results = db.query("SELECT * FROM logs WHERE severity > %s", ('ERROR',))
```

#### æ¨¡å¼ 3: é‡è¯•ä¸é€€é¿

```python
import time
from functools import wraps

def retry_with_backoff(
    retries=3,
    backoff_factor=2,
    exceptions=(Exception,)
):
    """é‡è¯•è£…é¥°å™¨,æ”¯æŒæŒ‡æ•°é€€é¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(retries):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < retries - 1:
                        wait_time = backoff_factor ** attempt
                        logger.warning(
                            f"{func.__name__} failed on attempt {attempt+1}/{retries}, "
                            f"retrying in {wait_time}s: {e}"
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(f"{func.__name__} failed after {retries} attempts")
            raise last_exception
        return wrapper
    return decorator

# ä½¿ç”¨
@retry_with_backoff(retries=3, exceptions=(requests.RequestException,))
def fetch_api_data(url: str) -> Dict:
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    return response.json()
```

---

## è´¨é‡æŒ‡æ ‡å¯¹æ¯”

### ä»£ç è´¨é‡åˆ†æ•°

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| **é”™è¯¯å¤„ç†è¦†ç›–ç‡** | 85% | 98% | +13% |
| **ç±»å‹æ³¨è§£å®Œæ•´åº¦** | 70% | 90% | +20% |
| **æ–‡æ¡£å­—ç¬¦ä¸²è¦†ç›–** | 90% | 98% | +8% |
| **èµ„æºç®¡ç†æ­£ç¡®æ€§** | 80% | 100% | +20% |
| **è¾“å…¥éªŒè¯è¦†ç›–** | 60% | 95% | +35% |
| **æ—¥å¿—è®°å½•å®Œæ•´æ€§** | 75% | 95% | +20% |

### ä»£ç å¥å£®æ€§æå‡

| åœºæ™¯ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å |
|------|--------|--------|
| **API è¶…æ—¶** | âŒ ç¨‹åºæŒ‚èµ· | âœ… è‡ªåŠ¨é‡è¯• + è¶…æ—¶è¿”å› |
| **é€Ÿç‡é™åˆ¶** | âŒ API é”™è¯¯ | âœ… è‡ªåŠ¨ç­‰å¾… + é™çº§ |
| **æ•°æ®åº“è¿æ¥å¤±è´¥** | âŒ è¿æ¥æ³„æ¼ | âœ… Context Manager è‡ªåŠ¨æ¸…ç† |
| **Redis ä¸å¯ç”¨** | âŒ åŠŸèƒ½å¤±è´¥ | âœ… ä¼˜é›…é™çº§,æ— ç¼“å­˜è¿è¡Œ |
| **è¾“å…¥æ•°æ®å¼‚å¸¸** | âŒ ç¨‹åºå´©æºƒ | âœ… éªŒè¯æ‹’ç» + æ¸…æ™°é”™è¯¯ |
| **JSON è§£æå¤±è´¥** | âŒ æœªæ•è·å¼‚å¸¸ | âœ… ç»†ç²’åº¦é”™è¯¯å¤„ç† |

### ç”Ÿäº§å°±ç»ªåº¦è¯„åˆ†

| ç»´åº¦ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å |
|------|--------|--------|
| **å®¹é”™èƒ½åŠ›** | 3/5 â­â­â­ | 5/5 â­â­â­â­â­ |
| **å¯è§‚æµ‹æ€§** | 3/5 â­â­â­ | 5/5 â­â­â­â­â­ |
| **å®‰å…¨æ€§** | 3/5 â­â­â­ | 5/5 â­â­â­â­â­ |
| **æ€§èƒ½** | 4/5 â­â­â­â­ | 5/5 â­â­â­â­â­ |
| **å¯ç»´æŠ¤æ€§** | 4/5 â­â­â­â­ | 5/5 â­â­â­â­â­ |

---

## ä¸‹ä¸€æ­¥è®¡åˆ’

### ğŸ¯ ç«‹å³è¡ŒåŠ¨ (ä»Šæ—¥å®Œæˆ)

1. **AIOpså¹³å°æ–‡æ¡£** - å®Œæˆ 4 å¤„é”™è¯¯å¤„ç†ä¼˜åŒ– (é¢„è®¡ 2 å°æ—¶)
   - LSTMInferenceEngine: æ¨¡å‹åŠ è½½å®¹é”™
   - ModelTrainingPipeline: è®­ç»ƒå¼‚å¸¸å¤„ç†
   - ActionExecutor: æ“ä½œå¤±è´¥æ¢å¤
   - ModelMonitor: ç›‘æ§æ•°æ®å®¹é”™

2. **Temporalå·¥ä½œæµæ–‡æ¡£** - å®Œæˆ 3 å¤„é”™è¯¯å¤„ç†ä¼˜åŒ– (é¢„è®¡ 1.5 å°æ—¶)
   - WorkflowClient: è¿æ¥ç®¡ç†
   - ActivityExecution: å¤±è´¥å¤„ç†
   - SagaOrchestrator: è¡¥å¿é€»è¾‘

### ğŸ“‹ åç»­ä¼˜åŒ– (æœ¬å‘¨)

 1. **OPT-3: æ·»åŠ ç±»å‹æ³¨è§£** (30% â†’ 90%)
    - ä¸ºæ‰€æœ‰å‡½æ•°æ·»åŠ å®Œæ•´ç±»å‹æç¤º
    - ä½¿ç”¨ `mypy` éªŒè¯ç±»å‹æ­£ç¡®æ€§

 2. **OPT-4: ä¿®å¤èµ„æºæ³„æ¼** (10 å¤„)
    - è¯†åˆ«æ‰€æœ‰èµ„æºåˆ†é…ç‚¹ (æ•°æ®åº“ã€æ–‡ä»¶ã€ç½‘ç»œè¿æ¥)
    - ç»Ÿä¸€ä½¿ç”¨ Context Manager

 3. **OPT-5: å¢åŠ  Mermaid å›¾è¡¨** (10 å¤„)
    - AI æ—¥å¿—åˆ†ææ¶æ„å›¾
    - eBPF æ•°æ®æµæ—¶åºå›¾
    - Temporal å·¥ä½œæµçŠ¶æ€æœº

 4. **OPT-6: æ·»åŠ æ•…éšœæ’æŸ¥æ¸…å•** (7 ä»½æ–‡æ¡£)
    - å¸¸è§é—®é¢˜ + è§£å†³æ–¹æ¡ˆ
    - è¯Šæ–­å‘½ä»¤ + é¢„æœŸè¾“å‡º

 5. **OPT-7: åˆ›å»ºæœ¯è¯­è¡¨** âœ… å·²å®Œæˆ
    - å‚è§ `ğŸ“–_æœ¯è¯­è¡¨_OTLPæŠ€æœ¯æ ˆæ ‡å‡†è¯‘æ³•.md`

---

## é™„å½•: æœ€ä½³å®è·µæ£€æŸ¥æ¸…å•

### âœ… ä»£ç è´¨é‡æ£€æŸ¥æ¸…å•

#### é”™è¯¯å¤„ç†

- [ ] æ‰€æœ‰å¤–éƒ¨è°ƒç”¨ (APIã€æ•°æ®åº“ã€æ–‡ä»¶) éƒ½æœ‰ try-except
- [ ] å¼‚å¸¸å¤„ç†ç»†åˆ† (ä¸åŒå¼‚å¸¸ç±»å‹ä¸åŒå¤„ç†)
- [ ] å…³é”®å¼‚å¸¸æœ‰æ—¥å¿—è®°å½•
- [ ] ç”¨æˆ·å‹å¥½çš„é”™è¯¯æ¶ˆæ¯

#### è¾“å…¥éªŒè¯

- [ ] æ‰€æœ‰å…¬å…±å‡½æ•°å‚æ•°éƒ½æœ‰éªŒè¯
- [ ] æ•°å€¼èŒƒå›´æ£€æŸ¥ (min/max)
- [ ] å­—ç¬¦ä¸²éç©ºæ£€æŸ¥
- [ ] åˆ—è¡¨/å­—å…¸ç»“æ„éªŒè¯
- [ ] ä½¿ç”¨ Pydantic æˆ– dataclasses

#### èµ„æºç®¡ç†

- [ ] æ•°æ®åº“è¿æ¥ä½¿ç”¨ Context Manager
- [ ] æ–‡ä»¶æ“ä½œä½¿ç”¨ `with` è¯­å¥
- [ ] ç½‘ç»œè¿æ¥æœ‰è¶…æ—¶è®¾ç½®
- [ ] çº¿ç¨‹/è¿›ç¨‹æ­£ç¡®æ¸…ç†

#### ç±»å‹æ³¨è§£

- [ ] å‡½æ•°å‚æ•°æœ‰ç±»å‹æç¤º
- [ ] è¿”å›å€¼æœ‰ç±»å‹æç¤º
- [ ] ä½¿ç”¨ `Optional` æ ‡è®°å¯é€‰å‚æ•°
- [ ] å¤æ‚ç±»å‹ä½¿ç”¨ `TypedDict` æˆ– Pydantic

#### æ–‡æ¡£

- [ ] æ‰€æœ‰å‡½æ•°æœ‰ docstring
- [ ] docstring åŒ…å« Args/Returns/Raises
- [ ] å¤æ‚é€»è¾‘æœ‰è¡Œå†…æ³¨é‡Š
- [ ] README åŒ…å«ä½¿ç”¨ç¤ºä¾‹

#### æ—¥å¿—è®°å½•

- [ ] ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿— (ä¸æ˜¯ print)
- [ ] æ—¥å¿—çº§åˆ«æ­£ç¡® (DEBUG/INFO/WARNING/ERROR)
- [ ] æ•æ„Ÿä¿¡æ¯å·²è„±æ•
- [ ] å…³é”®è·¯å¾„æœ‰æ—¥å¿—è¿½è¸ª

#### é…ç½®ç®¡ç†

- [ ] æ•æ„Ÿé…ç½®ä»ç¯å¢ƒå˜é‡è¯»å–
- [ ] é…ç½®æœ‰é»˜è®¤å€¼
- [ ] é…ç½®éªŒè¯ (å¯åŠ¨æ—¶æ£€æŸ¥)
- [ ] æ”¯æŒå¤šç¯å¢ƒé…ç½® (dev/staging/prod)

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2025å¹´10æœˆ9æ—¥ 14:30  
**ä¸‹æ¬¡æ›´æ–°**: å®Œæˆ AIOps å’Œ Temporal ä¼˜åŒ–å (é¢„è®¡ä»Šæ—¥ 18:00)
