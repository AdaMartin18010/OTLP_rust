# æ—¶åºæ•°æ®åº“é›†æˆ - Rust å®Œæ•´å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **Rust ç‰ˆæœ¬**: 1.90+  
> **æœ€åæ›´æ–°**: 2025-10-10

---

## ğŸ“‹ ç›®å½•

- [æ—¶åºæ•°æ®åº“é›†æˆ - Rust å®Œæ•´å®ç°](#æ—¶åºæ•°æ®åº“é›†æˆ---rust-å®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
    - [æ”¯æŒçš„æ—¶åºæ•°æ®åº“](#æ”¯æŒçš„æ—¶åºæ•°æ®åº“)
    - [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
  - [ğŸ”§ InfluxDB é›†æˆ](#-influxdb-é›†æˆ)
    - [1. InfluxDB å®¢æˆ·ç«¯å°è£…](#1-influxdb-å®¢æˆ·ç«¯å°è£…)
    - [2. OTLP åˆ° InfluxDB è½¬æ¢å™¨](#2-otlp-åˆ°-influxdb-è½¬æ¢å™¨)
  - [ğŸ“Š TimescaleDB é›†æˆ](#-timescaledb-é›†æˆ)
    - [1. TimescaleDB å®¢æˆ·ç«¯](#1-timescaledb-å®¢æˆ·ç«¯)
    - [2. Hypertable ç®¡ç†](#2-hypertable-ç®¡ç†)
  - [âš¡ Prometheus é›†æˆ](#-prometheus-é›†æˆ)
    - [1. Prometheus è¿œç¨‹å†™å…¥](#1-prometheus-è¿œç¨‹å†™å…¥)
  - [ğŸ” VictoriaMetrics é›†æˆ](#-victoriametrics-é›†æˆ)
    - [1. VictoriaMetrics å®¢æˆ·ç«¯](#1-victoriametrics-å®¢æˆ·ç«¯)
  - [ğŸ’¾ åˆ—å¼å­˜å‚¨å¼•æ“](#-åˆ—å¼å­˜å‚¨å¼•æ“)
    - [1. Apache Parquet é›†æˆ](#1-apache-parquet-é›†æˆ)
    - [2. åˆ—å¼æ•°æ®å†™å…¥å™¨](#2-åˆ—å¼æ•°æ®å†™å…¥å™¨)
  - [ğŸ—œï¸ æ•°æ®å‹ç¼©](#ï¸-æ•°æ®å‹ç¼©)
    - [1. æ—¶åºå‹ç¼©ç®—æ³•](#1-æ—¶åºå‹ç¼©ç®—æ³•)
    - [2. Gorilla å‹ç¼©](#2-gorilla-å‹ç¼©)
    - [3. Delta-of-Delta ç¼–ç ](#3-delta-of-delta-ç¼–ç )
  - [ğŸ“ˆ æ•°æ®ä¿ç•™ç­–ç•¥](#-æ•°æ®ä¿ç•™ç­–ç•¥)
    - [1. TTL ç®¡ç†å™¨](#1-ttl-ç®¡ç†å™¨)
    - [2. æ•°æ®é™é‡‡æ ·](#2-æ•°æ®é™é‡‡æ ·)
  - [ğŸ”„ ç»Ÿä¸€å­˜å‚¨æŠ½è±¡](#-ç»Ÿä¸€å­˜å‚¨æŠ½è±¡)
    - [1. å­˜å‚¨åç«¯ Trait](#1-å­˜å‚¨åç«¯-trait)
  - [ğŸ“Š å®Œæ•´ç¤ºä¾‹ï¼šå¤šåç«¯å­˜å‚¨ç³»ç»Ÿ](#-å®Œæ•´ç¤ºä¾‹å¤šåç«¯å­˜å‚¨ç³»ç»Ÿ)
  - [ğŸ¯ æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)
    - [æ•°æ®å»ºæ¨¡](#æ•°æ®å»ºæ¨¡)
    - [å†™å…¥ä¼˜åŒ–](#å†™å…¥ä¼˜åŒ–)
    - [æŸ¥è¯¢ä¼˜åŒ–](#æŸ¥è¯¢ä¼˜åŒ–)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## ğŸ¯ æ¦‚è¿°

OTLP æ•°æ®æœ¬è´¨ä¸Šæ˜¯æ—¶åºæ•°æ®ï¼Œé€‰æ‹©åˆé€‚çš„æ—¶åºæ•°æ®åº“å¯¹ç³»ç»Ÿæ€§èƒ½è‡³å…³é‡è¦ã€‚
æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•é›†æˆä¸»æµæ—¶åºæ•°æ®åº“å¹¶å®ç°é«˜æ•ˆçš„æ•°æ®å‹ç¼©ã€‚

### æ”¯æŒçš„æ—¶åºæ•°æ®åº“

- âœ… **InfluxDB**: ä¸“ä¸šæ—¶åºæ•°æ®åº“
- âœ… **TimescaleDB**: PostgreSQL æ‰©å±•
- âœ… **Prometheus**: ç›‘æ§ç³»ç»Ÿ
- âœ… **VictoriaMetrics**: é«˜æ€§èƒ½æ—¶åºæ•°æ®åº“
- âœ… **ClickHouse**: åˆ—å¼ OLAP æ•°æ®åº“
- âœ… **Apache Parquet**: åˆ—å¼æ–‡ä»¶æ ¼å¼

### æ ¸å¿ƒç‰¹æ€§

- ğŸ”§ **ç»Ÿä¸€æ¥å£**: æŠ½è±¡ä¸åŒæ•°æ®åº“çš„å·®å¼‚
- âš¡ **æ‰¹é‡å†™å…¥**: æå‡å†™å…¥æ€§èƒ½
- ğŸ—œï¸ **æ™ºèƒ½å‹ç¼©**: å‡å°‘å­˜å‚¨æˆæœ¬
- ğŸ“Š **è‡ªåŠ¨é™é‡‡æ ·**: é•¿æœŸæ•°æ®å½’æ¡£
- ğŸ” **é«˜æ•ˆæŸ¥è¯¢**: æ—¶é—´èŒƒå›´æŸ¥è¯¢ä¼˜åŒ–

---

## ğŸ”§ InfluxDB é›†æˆ

### 1. InfluxDB å®¢æˆ·ç«¯å°è£…

```rust
use serde::{Serialize, Deserialize};
use reqwest::Client;
use std::collections::HashMap;
use chrono::{DateTime, Utc};

/// InfluxDB å®¢æˆ·ç«¯
pub struct InfluxDbClient {
    base_url: String,
    token: String,
    org: String,
    bucket: String,
    http_client: Client,
}

impl InfluxDbClient {
    pub fn new(base_url: String, token: String, org: String, bucket: String) -> Self {
        Self {
            base_url,
            token,
            org,
            bucket,
            http_client: Client::new(),
        }
    }
    
    /// å†™å…¥æ•°æ®ç‚¹
    pub async fn write_points(&self, points: Vec<InfluxPoint>) -> Result<(), Box<dyn std::error::Error>> {
        let line_protocol = self.to_line_protocol(&points);
        
        let url = format!("{}/api/v2/write?org={}&bucket={}", 
            self.base_url, self.org, self.bucket);
        
        let response = self.http_client
            .post(&url)
            .header("Authorization", format!("Token {}", self.token))
            .header("Content-Type", "text/plain; charset=utf-8")
            .body(line_protocol)
            .send()
            .await?;
        
        if !response.status().is_success() {
            let error_text = response.text().await?;
            return Err(format!("InfluxDB write failed: {}", error_text).into());
        }
        
        Ok(())
    }
    
    /// æ‰¹é‡å†™å…¥
    pub async fn write_batch(&self, points: Vec<InfluxPoint>, batch_size: usize) -> Result<(), Box<dyn std::error::Error>> {
        for chunk in points.chunks(batch_size) {
            self.write_points(chunk.to_vec()).await?;
        }
        
        Ok(())
    }
    
    /// è½¬æ¢ä¸º Line Protocol
    fn to_line_protocol(&self, points: &[InfluxPoint]) -> String {
        points
            .iter()
            .map(|p| p.to_line_protocol())
            .collect::<Vec<_>>()
            .join("\n")
    }
    
    /// æŸ¥è¯¢æ•°æ®
    pub async fn query(&self, flux_query: &str) -> Result<Vec<HashMap<String, serde_json::Value>>, Box<dyn std::error::Error>> {
        let url = format!("{}/api/v2/query?org={}", self.base_url, self.org);
        
        let request_body = serde_json::json!({
            "query": flux_query,
            "type": "flux"
        });
        
        let response = self.http_client
            .post(&url)
            .header("Authorization", format!("Token {}", self.token))
            .header("Content-Type", "application/json")
            .json(&request_body)
            .send()
            .await?;
        
        let csv_text = response.text().await?;
        let results = self.parse_csv_result(&csv_text)?;
        
        Ok(results)
    }
    
    fn parse_csv_result(&self, csv: &str) -> Result<Vec<HashMap<String, serde_json::Value>>, Box<dyn std::error::Error>> {
        // ç®€åŒ–å®ç°ï¼šè§£æ CSV æ ¼å¼çš„æŸ¥è¯¢ç»“æœ
        let mut results = Vec::new();
        
        let lines: Vec<&str> = csv.lines().collect();
        if lines.len() < 2 {
            return Ok(results);
        }
        
        // ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´
        let headers: Vec<&str> = lines[0].split(',').collect();
        
        // è§£ææ•°æ®è¡Œ
        for line in lines.iter().skip(1) {
            let values: Vec<&str> = line.split(',').collect();
            
            if values.len() == headers.len() {
                let mut row = HashMap::new();
                
                for (header, value) in headers.iter().zip(values.iter()) {
                    row.insert(header.to_string(), serde_json::Value::String(value.to_string()));
                }
                
                results.push(row);
            }
        }
        
        Ok(results)
    }
}

/// InfluxDB æ•°æ®ç‚¹
#[derive(Debug, Clone)]
pub struct InfluxPoint {
    pub measurement: String,
    pub tags: HashMap<String, String>,
    pub fields: HashMap<String, InfluxValue>,
    pub timestamp: Option<DateTime<Utc>>,
}

#[derive(Debug, Clone)]
pub enum InfluxValue {
    Float(f64),
    Integer(i64),
    String(String),
    Boolean(bool),
}

impl InfluxPoint {
    /// è½¬æ¢ä¸º Line Protocol
    pub fn to_line_protocol(&self) -> String {
        let mut result = self.measurement.clone();
        
        // æ·»åŠ æ ‡ç­¾
        if !self.tags.is_empty() {
            result.push(',');
            let tags: Vec<String> = self.tags
                .iter()
                .map(|(k, v)| format!("{}={}", Self::escape_key(k), Self::escape_tag_value(v)))
                .collect();
            result.push_str(&tags.join(","));
        }
        
        // æ·»åŠ å­—æ®µ
        result.push(' ');
        let fields: Vec<String> = self.fields
            .iter()
            .map(|(k, v)| format!("{}={}", Self::escape_key(k), v.to_string()))
            .collect();
        result.push_str(&fields.join(","));
        
        // æ·»åŠ æ—¶é—´æˆ³
        if let Some(ts) = self.timestamp {
            result.push(' ');
            result.push_str(&ts.timestamp_nanos_opt().unwrap_or(0).to_string());
        }
        
        result
    }
    
    fn escape_key(s: &str) -> String {
        s.replace(',', "\\,")
            .replace('=', "\\=")
            .replace(' ', "\\ ")
    }
    
    fn escape_tag_value(s: &str) -> String {
        s.replace(',', "\\,")
            .replace('=', "\\=")
            .replace(' ', "\\ ")
    }
}

impl InfluxValue {
    fn to_string(&self) -> String {
        match self {
            Self::Float(f) => f.to_string(),
            Self::Integer(i) => format!("{}i", i),
            Self::String(s) => format!("\"{}\"", s.replace('"', "\\\"")),
            Self::Boolean(b) => b.to_string(),
        }
    }
}
```

---

### 2. OTLP åˆ° InfluxDB è½¬æ¢å™¨

```rust
/// OTLP Metric åˆ° InfluxDB è½¬æ¢å™¨
pub struct OtlpToInfluxConverter {
    // é…ç½®
}

impl OtlpToInfluxConverter {
    pub fn new() -> Self {
        Self {}
    }
    
    /// è½¬æ¢ Metric åˆ° InfluxDB æ•°æ®ç‚¹
    pub fn convert_metric(&self, metric: &crate::DistributedMetric) -> Vec<InfluxPoint> {
        let mut points = Vec::new();
        
        for data_point in &metric.data_points {
            let mut tags = HashMap::new();
            tags.insert("service_name".to_string(), metric.resource.service_name.clone());
            
            if let Some(ns) = &metric.resource.service_namespace {
                tags.insert("service_namespace".to_string(), ns.clone());
            }
            
            // æ·»åŠ æ•°æ®ç‚¹çš„å±æ€§ä½œä¸ºæ ‡ç­¾
            for (k, v) in &data_point.attributes {
                tags.insert(k.clone(), v.clone());
            }
            
            let mut fields = HashMap::new();
            
            match &data_point.value {
                crate::MetricValue::Int(i) => {
                    fields.insert("value".to_string(), InfluxValue::Integer(*i));
                }
                crate::MetricValue::Double(d) => {
                    fields.insert("value".to_string(), InfluxValue::Float(*d));
                }
                crate::MetricValue::Histogram { count, sum, buckets } => {
                    fields.insert("count".to_string(), InfluxValue::Integer(*count as i64));
                    fields.insert("sum".to_string(), InfluxValue::Float(*sum));
                    
                    // ä¸ºæ¯ä¸ªæ¡¶åˆ›å»ºå•ç‹¬çš„å­—æ®µ
                    for (i, bucket) in buckets.iter().enumerate() {
                        fields.insert(
                            format!("bucket_{}", i),
                            InfluxValue::Integer(bucket.count as i64)
                        );
                    }
                }
                _ => {
                    // å…¶ä»–ç±»å‹...
                }
            }
            
            points.push(InfluxPoint {
                measurement: metric.name.clone(),
                tags,
                fields,
                timestamp: Some(data_point.timestamp),
            });
        }
        
        points
    }
    
    /// è½¬æ¢ Trace åˆ° InfluxDB æ•°æ®ç‚¹ï¼ˆç”¨äºè¿½è¸ªç»Ÿè®¡ï¼‰
    pub fn convert_trace_stats(&self, trace: &crate::DistributedTrace) -> Vec<InfluxPoint> {
        let mut points = Vec::new();
        
        for span in &trace.spans {
            let mut tags = HashMap::new();
            tags.insert("service_name".to_string(), trace.resource.service_name.clone());
            tags.insert("span_name".to_string(), span.name.clone());
            
            let duration_ms = (span.end_time - span.start_time).num_milliseconds();
            
            let mut fields = HashMap::new();
            fields.insert("duration_ms".to_string(), InfluxValue::Integer(duration_ms));
            
            points.push(InfluxPoint {
                measurement: "span_duration".to_string(),
                tags,
                fields,
                timestamp: Some(span.start_time),
            });
        }
        
        points
    }
}
```

---

## ğŸ“Š TimescaleDB é›†æˆ

### 1. TimescaleDB å®¢æˆ·ç«¯

```rust
use tokio_postgres::{Client, NoTls, Error};

/// TimescaleDB å®¢æˆ·ç«¯
pub struct TimescaleDbClient {
    client: Client,
}

impl TimescaleDbClient {
    /// è¿æ¥ TimescaleDB
    pub async fn connect(connection_string: &str) -> Result<Self, Error> {
        let (client, connection) = tokio_postgres::connect(connection_string, NoTls).await?;
        
        // åœ¨åå°å¤„ç†è¿æ¥
        tokio::spawn(async move {
            if let Err(e) = connection.await {
                eprintln!("TimescaleDB connection error: {}", e);
            }
        });
        
        Ok(Self { client })
    }
    
    /// åˆå§‹åŒ–è¡¨ç»“æ„
    pub async fn initialize_schema(&self) -> Result<(), Error> {
        // åˆ›å»º Metrics è¡¨
        self.client.execute(
            "CREATE TABLE IF NOT EXISTS metrics (
                time TIMESTAMPTZ NOT NULL,
                metric_name TEXT NOT NULL,
                service_name TEXT NOT NULL,
                service_namespace TEXT,
                value DOUBLE PRECISION,
                tags JSONB,
                PRIMARY KEY (time, metric_name, service_name)
            )",
            &[],
        ).await?;
        
        // åˆ›å»º Hypertable
        self.client.execute(
            "SELECT create_hypertable('metrics', 'time', if_not_exists => TRUE)",
            &[],
        ).await?;
        
        // åˆ›å»ºç´¢å¼•
        self.client.execute(
            "CREATE INDEX IF NOT EXISTS idx_metrics_service ON metrics (service_name, time DESC)",
            &[],
        ).await?;
        
        self.client.execute(
            "CREATE INDEX IF NOT EXISTS idx_metrics_tags ON metrics USING GIN (tags)",
            &[],
        ).await?;
        
        tracing::info!("TimescaleDB schema initialized");
        
        Ok(())
    }
    
    /// æ’å…¥ Metric æ•°æ®
    pub async fn insert_metric(
        &self,
        timestamp: DateTime<Utc>,
        metric_name: &str,
        service_name: &str,
        service_namespace: Option<&str>,
        value: f64,
        tags: &HashMap<String, String>,
    ) -> Result<(), Error> {
        let tags_json = serde_json::to_value(tags).unwrap();
        
        self.client.execute(
            "INSERT INTO metrics (time, metric_name, service_name, service_namespace, value, tags)
             VALUES ($1, $2, $3, $4, $5, $6)",
            &[&timestamp, &metric_name, &service_name, &service_namespace, &value, &tags_json],
        ).await?;
        
        Ok(())
    }
    
    /// æ‰¹é‡æ’å…¥
    pub async fn insert_batch(&self, points: Vec<TimescalePoint>) -> Result<(), Error> {
        let mut transaction = self.client.transaction().await?;
        
        let statement = transaction.prepare(
            "INSERT INTO metrics (time, metric_name, service_name, service_namespace, value, tags)
             VALUES ($1, $2, $3, $4, $5, $6)"
        ).await?;
        
        for point in points {
            let tags_json = serde_json::to_value(&point.tags).unwrap();
            
            transaction.execute(
                &statement,
                &[
                    &point.timestamp,
                    &point.metric_name,
                    &point.service_name,
                    &point.service_namespace,
                    &point.value,
                    &tags_json,
                ],
            ).await?;
        }
        
        transaction.commit().await?;
        
        Ok(())
    }
    
    /// æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
    pub async fn query_range(
        &self,
        metric_name: &str,
        service_name: &str,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    ) -> Result<Vec<TimescalePoint>, Error> {
        let rows = self.client.query(
            "SELECT time, metric_name, service_name, service_namespace, value, tags
             FROM metrics
             WHERE metric_name = $1 AND service_name = $2 AND time >= $3 AND time <= $4
             ORDER BY time ASC",
            &[&metric_name, &service_name, &start, &end],
        ).await?;
        
        let mut results = Vec::new();
        
        for row in rows {
            let tags_json: serde_json::Value = row.get(5);
            let tags: HashMap<String, String> = serde_json::from_value(tags_json).unwrap_or_default();
            
            results.push(TimescalePoint {
                timestamp: row.get(0),
                metric_name: row.get(1),
                service_name: row.get(2),
                service_namespace: row.get(3),
                value: row.get(4),
                tags,
            });
        }
        
        Ok(results)
    }
}

#[derive(Debug, Clone)]
pub struct TimescalePoint {
    pub timestamp: DateTime<Utc>,
    pub metric_name: String,
    pub service_name: String,
    pub service_namespace: Option<String>,
    pub value: f64,
    pub tags: HashMap<String, String>,
}
```

---

### 2. Hypertable ç®¡ç†

```rust
/// Hypertable ç®¡ç†å™¨
pub struct HypertableManager {
    client: TimescaleDbClient,
}

impl HypertableManager {
    pub fn new(client: TimescaleDbClient) -> Self {
        Self { client }
    }
    
    /// è®¾ç½®æ•°æ®ä¿ç•™ç­–ç•¥
    pub async fn set_retention_policy(
        &self,
        table_name: &str,
        retention_days: i32,
    ) -> Result<(), Error> {
        self.client.client.execute(
            &format!(
                "SELECT add_retention_policy('{}', INTERVAL '{} days', if_not_exists => TRUE)",
                table_name, retention_days
            ),
            &[],
        ).await?;
        
        tracing::info!("Set retention policy: {} days for table {}", retention_days, table_name);
        
        Ok(())
    }
    
    /// è®¾ç½®å‹ç¼©ç­–ç•¥
    pub async fn set_compression_policy(
        &self,
        table_name: &str,
        compress_after_days: i32,
    ) -> Result<(), Error> {
        // å¯ç”¨å‹ç¼©
        self.client.client.execute(
            &format!(
                "ALTER TABLE {} SET (timescaledb.compress, timescaledb.compress_segmentby = 'service_name')",
                table_name
            ),
            &[],
        ).await?;
        
        // è®¾ç½®å‹ç¼©ç­–ç•¥
        self.client.client.execute(
            &format!(
                "SELECT add_compression_policy('{}', INTERVAL '{} days', if_not_exists => TRUE)",
                table_name, compress_after_days
            ),
            &[],
        ).await?;
        
        tracing::info!("Set compression policy: {} days for table {}", compress_after_days, table_name);
        
        Ok(())
    }
    
    /// åˆ›å»ºè¿ç»­èšåˆï¼ˆContinuous Aggregateï¼‰
    pub async fn create_continuous_aggregate(
        &self,
        aggregate_name: &str,
        source_table: &str,
        bucket_width: &str, // e.g., "1 hour"
    ) -> Result<(), Error> {
        self.client.client.execute(
            &format!(
                "CREATE MATERIALIZED VIEW {} WITH (timescaledb.continuous) AS
                 SELECT time_bucket('{}', time) AS bucket,
                        metric_name,
                        service_name,
                        AVG(value) as avg_value,
                        MAX(value) as max_value,
                        MIN(value) as min_value,
                        COUNT(*) as count
                 FROM {}
                 GROUP BY bucket, metric_name, service_name",
                aggregate_name, bucket_width, source_table
            ),
            &[],
        ).await?;
        
        // è®¾ç½®åˆ·æ–°ç­–ç•¥
        self.client.client.execute(
            &format!(
                "SELECT add_continuous_aggregate_policy('{}',
                    start_offset => INTERVAL '1 month',
                    end_offset => INTERVAL '1 hour',
                    schedule_interval => INTERVAL '1 hour')",
                aggregate_name
            ),
            &[],
        ).await?;
        
        tracing::info!("Created continuous aggregate: {}", aggregate_name);
        
        Ok(())
    }
}
```

---

## âš¡ Prometheus é›†æˆ

### 1. Prometheus è¿œç¨‹å†™å…¥

```rust
use prost::Message;

/// Prometheus è¿œç¨‹å†™å…¥å®¢æˆ·ç«¯
pub struct PrometheusRemoteWriteClient {
    remote_write_url: String,
    http_client: Client,
}

impl PrometheusRemoteWriteClient {
    pub fn new(remote_write_url: String) -> Self {
        Self {
            remote_write_url,
            http_client: Client::new(),
        }
    }
    
    /// å†™å…¥æ—¶é—´åºåˆ—
    pub async fn write_timeseries(
        &self,
        timeseries: Vec<PrometheusTimeSeries>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        // æ„å»º Prometheus Remote Write è¯·æ±‚
        let write_request = prometheus::WriteRequest {
            timeseries: timeseries.into_iter().map(|ts| ts.into()).collect(),
            metadata: vec![],
        };
        
        // åºåˆ—åŒ–ä¸º Protobuf
        let mut buf = Vec::new();
        write_request.encode(&mut buf)?;
        
        // Snappy å‹ç¼©
        let compressed = snap::raw::Encoder::new().compress_vec(&buf)?;
        
        // å‘é€è¯·æ±‚
        let response = self.http_client
            .post(&self.remote_write_url)
            .header("Content-Encoding", "snappy")
            .header("Content-Type", "application/x-protobuf")
            .header("X-Prometheus-Remote-Write-Version", "0.1.0")
            .body(compressed)
            .send()
            .await?;
        
        if !response.status().is_success() {
            let error_text = response.text().await?;
            return Err(format!("Prometheus write failed: {}", error_text).into());
        }
        
        Ok(())
    }
}

/// Prometheus æ—¶é—´åºåˆ—
#[derive(Debug, Clone)]
pub struct PrometheusTimeSeries {
    pub labels: Vec<(String, String)>,
    pub samples: Vec<PrometheusSample>,
}

#[derive(Debug, Clone)]
pub struct PrometheusSample {
    pub timestamp_ms: i64,
    pub value: f64,
}

// Protobuf å®šä¹‰ï¼ˆç®€åŒ–ï¼‰
mod prometheus {
    use prost::Message;
    
    #[derive(Clone, PartialEq, Message)]
    pub struct WriteRequest {
        #[prost(message, repeated, tag = "1")]
        pub timeseries: Vec<TimeSeries>,
        
        #[prost(message, repeated, tag = "3")]
        pub metadata: Vec<MetricMetadata>,
    }
    
    #[derive(Clone, PartialEq, Message)]
    pub struct TimeSeries {
        #[prost(message, repeated, tag = "1")]
        pub labels: Vec<Label>,
        
        #[prost(message, repeated, tag = "2")]
        pub samples: Vec<Sample>,
    }
    
    #[derive(Clone, PartialEq, Message)]
    pub struct Label {
        #[prost(string, tag = "1")]
        pub name: String,
        
        #[prost(string, tag = "2")]
        pub value: String,
    }
    
    #[derive(Clone, PartialEq, Message)]
    pub struct Sample {
        #[prost(double, tag = "1")]
        pub value: f64,
        
        #[prost(int64, tag = "2")]
        pub timestamp: i64,
    }
    
    #[derive(Clone, PartialEq, Message)]
    pub struct MetricMetadata {
        #[prost(enumeration = "MetricType", tag = "1")]
        pub r#type: i32,
        
        #[prost(string, tag = "2")]
        pub metric_family_name: String,
    }
    
    #[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, PartialOrd, Ord)]
    #[repr(i32)]
    pub enum MetricType {
        Counter = 0,
        Gauge = 1,
        Histogram = 2,
        Summary = 4,
    }
}

impl From<PrometheusTimeSeries> for prometheus::TimeSeries {
    fn from(ts: PrometheusTimeSeries) -> Self {
        prometheus::TimeSeries {
            labels: ts.labels
                .into_iter()
                .map(|(name, value)| prometheus::Label { name, value })
                .collect(),
            samples: ts.samples
                .into_iter()
                .map(|s| prometheus::Sample {
                    value: s.value,
                    timestamp: s.timestamp_ms,
                })
                .collect(),
        }
    }
}
```

---

## ğŸ” VictoriaMetrics é›†æˆ

### 1. VictoriaMetrics å®¢æˆ·ç«¯

```rust
/// VictoriaMetrics å®¢æˆ·ç«¯ï¼ˆå…¼å®¹ Prometheus APIï¼‰
pub struct VictoriaMetricsClient {
    base_url: String,
    http_client: Client,
}

impl VictoriaMetricsClient {
    pub fn new(base_url: String) -> Self {
        Self {
            base_url,
            http_client: Client::new(),
        }
    }
    
    /// å†™å…¥æ•°æ®ï¼ˆä½¿ç”¨ Prometheus Remote Write åè®®ï¼‰
    pub async fn write(&self, timeseries: Vec<PrometheusTimeSeries>) -> Result<(), Box<dyn std::error::Error>> {
        let prometheus_client = PrometheusRemoteWriteClient::new(
            format!("{}/api/v1/write", self.base_url)
        );
        
        prometheus_client.write_timeseries(timeseries).await
    }
    
    /// ä½¿ç”¨ InfluxDB Line Protocol å†™å…¥
    pub async fn write_influx_line_protocol(&self, lines: Vec<String>) -> Result<(), Box<dyn std::error::Error>> {
        let url = format!("{}/write", self.base_url);
        let body = lines.join("\n");
        
        let response = self.http_client
            .post(&url)
            .header("Content-Type", "text/plain")
            .body(body)
            .send()
            .await?;
        
        if !response.status().is_success() {
            let error_text = response.text().await?;
            return Err(format!("VictoriaMetrics write failed: {}", error_text).into());
        }
        
        Ok(())
    }
    
    /// æŸ¥è¯¢æ•°æ®ï¼ˆPromQLï¼‰
    pub async fn query(
        &self,
        promql: &str,
        time: Option<DateTime<Utc>>,
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        let mut url = format!("{}/api/v1/query?query={}", self.base_url, urlencoding::encode(promql));
        
        if let Some(t) = time {
            url.push_str(&format!("&time={}", t.timestamp()));
        }
        
        let response = self.http_client
            .get(&url)
            .send()
            .await?;
        
        let result = response.json::<serde_json::Value>().await?;
        
        Ok(result)
    }
    
    /// èŒƒå›´æŸ¥è¯¢ï¼ˆPromQLï¼‰
    pub async fn query_range(
        &self,
        promql: &str,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
        step: &str, // e.g., "15s"
    ) -> Result<serde_json::Value, Box<dyn std::error::Error>> {
        let url = format!(
            "{}/api/v1/query_range?query={}&start={}&end={}&step={}",
            self.base_url,
            urlencoding::encode(promql),
            start.timestamp(),
            end.timestamp(),
            step
        );
        
        let response = self.http_client
            .get(&url)
            .send()
            .await?;
        
        let result = response.json::<serde_json::Value>().await?;
        
        Ok(result)
    }
}
```

---

## ğŸ’¾ åˆ—å¼å­˜å‚¨å¼•æ“

### 1. Apache Parquet é›†æˆ

```rust
use parquet::{
    file::{
        properties::WriterProperties,
        writer::SerializedFileWriter,
    },
    schema::parser::parse_message_type,
};
use arrow::{
    array::{ArrayRef, Float64Array, Int64Array, StringArray, TimestampMillisecondArray},
    datatypes::{DataType, Field, Schema, TimeUnit},
    record_batch::RecordBatch,
};
use std::sync::Arc;

/// Parquet å†™å…¥å™¨
pub struct ParquetWriter {
    output_path: std::path::PathBuf,
}

impl ParquetWriter {
    pub fn new(output_path: std::path::PathBuf) -> Self {
        Self { output_path }
    }
    
    /// å†™å…¥ Metric æ•°æ®åˆ° Parquet æ–‡ä»¶
    pub async fn write_metrics(
        &self,
        metrics: Vec<MetricRecord>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        // å®šä¹‰ Schema
        let schema = Arc::new(Schema::new(vec![
            Field::new("timestamp", DataType::Timestamp(TimeUnit::Millisecond, None), false),
            Field::new("metric_name", DataType::Utf8, false),
            Field::new("service_name", DataType::Utf8, false),
            Field::new("value", DataType::Float64, false),
        ]));
        
        // æ„å»ºåˆ—æ•°ç»„
        let timestamps: Vec<i64> = metrics.iter().map(|m| m.timestamp.timestamp_millis()).collect();
        let metric_names: Vec<String> = metrics.iter().map(|m| m.metric_name.clone()).collect();
        let service_names: Vec<String> = metrics.iter().map(|m| m.service_name.clone()).collect();
        let values: Vec<f64> = metrics.iter().map(|m| m.value).collect();
        
        let timestamp_array: ArrayRef = Arc::new(TimestampMillisecondArray::from(timestamps));
        let metric_name_array: ArrayRef = Arc::new(StringArray::from(metric_names));
        let service_name_array: ArrayRef = Arc::new(StringArray::from(service_names));
        let value_array: ArrayRef = Arc::new(Float64Array::from(values));
        
        // åˆ›å»º RecordBatch
        let batch = RecordBatch::try_new(
            schema.clone(),
            vec![timestamp_array, metric_name_array, service_name_array, value_array],
        )?;
        
        // å†™å…¥ Parquet æ–‡ä»¶
        let file = std::fs::File::create(&self.output_path)?;
        let props = WriterProperties::builder()
            .set_compression(parquet::basic::Compression::SNAPPY)
            .build();
        
        let mut writer = SerializedFileWriter::new(file, schema, Arc::new(props))?;
        
        let mut arrow_writer = parquet::arrow::ArrowWriter::try_new(
            &mut writer,
            batch.schema(),
            None,
        )?;
        
        arrow_writer.write(&batch)?;
        arrow_writer.close()?;
        
        tracing::info!("Wrote {} metrics to Parquet file: {:?}", metrics.len(), self.output_path);
        
        Ok(())
    }
}

#[derive(Debug, Clone)]
pub struct MetricRecord {
    pub timestamp: DateTime<Utc>,
    pub metric_name: String,
    pub service_name: String,
    pub value: f64,
}
```

---

### 2. åˆ—å¼æ•°æ®å†™å…¥å™¨

```rust
/// åˆ—å¼æ‰¹é‡å†™å…¥å™¨
pub struct ColumnarBatchWriter {
    buffer: Vec<MetricRecord>,
    batch_size: usize,
    parquet_writer: ParquetWriter,
}

impl ColumnarBatchWriter {
    pub fn new(output_path: std::path::PathBuf, batch_size: usize) -> Self {
        Self {
            buffer: Vec::with_capacity(batch_size),
            batch_size,
            parquet_writer: ParquetWriter::new(output_path),
        }
    }
    
    /// æ·»åŠ è®°å½•åˆ°ç¼“å†²åŒº
    pub async fn add(&mut self, record: MetricRecord) -> Result<(), Box<dyn std::error::Error>> {
        self.buffer.push(record);
        
        if self.buffer.len() >= self.batch_size {
            self.flush().await?;
        }
        
        Ok(())
    }
    
    /// åˆ·æ–°ç¼“å†²åŒº
    pub async fn flush(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        if self.buffer.is_empty() {
            return Ok(());
        }
        
        self.parquet_writer.write_metrics(self.buffer.clone()).await?;
        self.buffer.clear();
        
        Ok(())
    }
}
```

---

## ğŸ—œï¸ æ•°æ®å‹ç¼©

### 1. æ—¶åºå‹ç¼©ç®—æ³•

```rust
/// æ—¶åºæ•°æ®å‹ç¼©å™¨
pub struct TimeSeriesCompressor {
    // é…ç½®
}

impl TimeSeriesCompressor {
    /// Delta Encodingï¼ˆå·®åˆ†ç¼–ç ï¼‰
    pub fn delta_encode(values: &[i64]) -> Vec<i64> {
        if values.is_empty() {
            return Vec::new();
        }
        
        let mut result = Vec::with_capacity(values.len());
        result.push(values[0]); // ç¬¬ä¸€ä¸ªå€¼ä¿æŒä¸å˜
        
        for i in 1..values.len() {
            result.push(values[i] - values[i - 1]);
        }
        
        result
    }
    
    /// Delta Decodingï¼ˆå·®åˆ†è§£ç ï¼‰
    pub fn delta_decode(deltas: &[i64]) -> Vec<i64> {
        if deltas.is_empty() {
            return Vec::new();
        }
        
        let mut result = Vec::with_capacity(deltas.len());
        result.push(deltas[0]);
        
        for i in 1..deltas.len() {
            result.push(result[i - 1] + deltas[i]);
        }
        
        result
    }
    
    /// Delta-of-Delta Encoding
    pub fn delta_of_delta_encode(values: &[i64]) -> Vec<i64> {
        let deltas = Self::delta_encode(values);
        Self::delta_encode(&deltas)
    }
    
    /// Delta-of-Delta Decoding
    pub fn delta_of_delta_decode(dods: &[i64]) -> Vec<i64> {
        let deltas = Self::delta_decode(dods);
        Self::delta_decode(&deltas)
    }
}
```

---

### 2. Gorilla å‹ç¼©

```rust
/// Gorilla æ—¶é—´æˆ³å‹ç¼©ï¼ˆFacebook ç®—æ³•ï¼‰
pub struct GorillaTimestampCompressor {
    // çŠ¶æ€
}

impl GorillaTimestampCompressor {
    /// å‹ç¼©æ—¶é—´æˆ³åºåˆ—
    pub fn compress(&self, timestamps: &[i64]) -> Vec<u8> {
        if timestamps.is_empty() {
            return Vec::new();
        }
        
        let mut compressed = Vec::new();
        
        // ç¬¬ä¸€ä¸ªæ—¶é—´æˆ³å®Œæ•´å­˜å‚¨ï¼ˆ64ä½ï¼‰
        compressed.extend_from_slice(&timestamps[0].to_be_bytes());
        
        if timestamps.len() == 1 {
            return compressed;
        }
        
        // ç¬¬äºŒä¸ªæ—¶é—´æˆ³å­˜å‚¨ä¸ºå·®å€¼ï¼ˆ64ä½ï¼‰
        let delta1 = timestamps[1] - timestamps[0];
        compressed.extend_from_slice(&delta1.to_be_bytes());
        
        // åç»­æ—¶é—´æˆ³ä½¿ç”¨ Delta-of-Delta
        let mut prev_delta = delta1;
        
        for i in 2..timestamps.len() {
            let delta = timestamps[i] - timestamps[i - 1];
            let dod = delta - prev_delta;
            
            // æ ¹æ® DoD çš„å¤§å°é€‰æ‹©ç¼–ç æ–¹å¼
            if dod == 0 {
                // 0 ä½ï¼š'0'
                compressed.push(0);
            } else if dod >= -63 && dod <= 64 {
                // 7 ä½ï¼š'10' + 7ä½æœ‰ç¬¦å·æ•´æ•°
                compressed.push(0x80 | ((dod & 0x7F) as u8));
            } else if dod >= -255 && dod <= 256 {
                // 9 ä½ï¼š'110' + 9ä½æœ‰ç¬¦å·æ•´æ•°
                compressed.push(0xC0 | ((dod >> 8) & 0x3) as u8);
                compressed.push((dod & 0xFF) as u8);
            } else {
                // 12 ä½æˆ–æ›´å¤š...
                compressed.extend_from_slice(&dod.to_be_bytes());
            }
            
            prev_delta = delta;
        }
        
        compressed
    }
}
```

---

### 3. Delta-of-Delta ç¼–ç 

```rust
/// XOR æµ®ç‚¹æ•°å‹ç¼©ï¼ˆGorilla ç®—æ³•ï¼‰
pub struct GorillaFloatCompressor {
    // çŠ¶æ€
}

impl GorillaFloatCompressor {
    /// å‹ç¼©æµ®ç‚¹æ•°åºåˆ—
    pub fn compress(&self, values: &[f64]) -> Vec<u8> {
        if values.is_empty() {
            return Vec::new();
        }
        
        let mut compressed = Vec::new();
        
        // ç¬¬ä¸€ä¸ªå€¼å®Œæ•´å­˜å‚¨
        compressed.extend_from_slice(&values[0].to_bits().to_be_bytes());
        
        if values.len() == 1 {
            return compressed;
        }
        
        let mut prev_bits = values[0].to_bits();
        let mut prev_leading_zeros = 0;
        let mut prev_trailing_zeros = 0;
        
        for &value in values.iter().skip(1) {
            let current_bits = value.to_bits();
            let xor = prev_bits ^ current_bits;
            
            if xor == 0 {
                // å€¼ç›¸åŒï¼Œå†™å…¥ '0' ä½
                compressed.push(0);
            } else {
                let leading_zeros = xor.leading_zeros();
                let trailing_zeros = xor.trailing_zeros();
                
                if leading_zeros >= prev_leading_zeros && trailing_zeros >= prev_trailing_zeros {
                    // ä½¿ç”¨æ§åˆ¶ä½ '10' + ä¸­é—´ä½
                    compressed.push(0x80);
                    let meaningful_bits = 64 - leading_zeros - trailing_zeros;
                    let value_to_store = (xor >> trailing_zeros) & ((1 << meaningful_bits) - 1);
                    compressed.extend_from_slice(&value_to_store.to_be_bytes());
                } else {
                    // ä½¿ç”¨æ§åˆ¶ä½ '11' + leading zeros + meaningful bits length + meaningful bits
                    compressed.push(0xC0);
                    compressed.push(leading_zeros as u8);
                    let meaningful_bits = 64 - leading_zeros - trailing_zeros;
                    compressed.push(meaningful_bits as u8);
                    let value_to_store = (xor >> trailing_zeros) & ((1 << meaningful_bits) - 1);
                    compressed.extend_from_slice(&value_to_store.to_be_bytes());
                    
                    prev_leading_zeros = leading_zeros;
                    prev_trailing_zeros = trailing_zeros;
                }
            }
            
            prev_bits = current_bits;
        }
        
        compressed
    }
}
```

---

## ğŸ“ˆ æ•°æ®ä¿ç•™ç­–ç•¥

### 1. TTL ç®¡ç†å™¨

```rust
/// TTLï¼ˆTime-To-Liveï¼‰ç®¡ç†å™¨
pub struct TtlManager {
    retention_policies: HashMap<String, chrono::Duration>,
}

impl TtlManager {
    pub fn new() -> Self {
        Self {
            retention_policies: HashMap::new(),
        }
    }
    
    /// è®¾ç½®æ•°æ®ä¿ç•™ç­–ç•¥
    pub fn set_policy(&mut self, table_name: String, retention: chrono::Duration) {
        self.retention_policies.insert(table_name, retention);
    }
    
    /// æ¸…ç†è¿‡æœŸæ•°æ®
    pub async fn cleanup_expired_data(&self, storage: &dyn StorageBackend) -> Result<(), Box<dyn std::error::Error>> {
        let now = Utc::now();
        
        for (table_name, retention) in &self.retention_policies {
            let cutoff_time = now - *retention;
            
            tracing::info!("Cleaning up data older than {} in table {}", cutoff_time, table_name);
            
            storage.delete_before(table_name, cutoff_time).await?;
        }
        
        Ok(())
    }
    
    /// å®šæœŸæ¸…ç†ä»»åŠ¡
    pub async fn start_cleanup_task(&self, storage: std::sync::Arc<dyn StorageBackend>, interval: std::time::Duration) {
        let mut interval_timer = tokio::time::interval(interval);
        
        loop {
            interval_timer.tick().await;
            
            if let Err(e) = self.cleanup_expired_data(storage.as_ref()).await {
                tracing::error!("TTL cleanup failed: {}", e);
            }
        }
    }
}
```

---

### 2. æ•°æ®é™é‡‡æ ·

```rust
/// æ•°æ®é™é‡‡æ ·å™¨
pub struct Downsampler {
    // é…ç½®
}

impl Downsampler {
    /// é™é‡‡æ ·åˆ°æŒ‡å®šæ—¶é—´ç²’åº¦
    pub fn downsample(
        &self,
        data: Vec<MetricRecord>,
        bucket_size: chrono::Duration,
    ) -> Vec<MetricRecord> {
        if data.is_empty() {
            return Vec::new();
        }
        
        // æŒ‰æ—¶é—´æ¡¶åˆ†ç»„
        let mut buckets: HashMap<i64, Vec<MetricRecord>> = HashMap::new();
        
        for record in data {
            let bucket = record.timestamp.timestamp() / bucket_size.num_seconds();
            buckets.entry(bucket).or_insert_with(Vec::new).push(record);
        }
        
        // å¯¹æ¯ä¸ªæ¡¶è¿›è¡Œèšåˆ
        let mut result = Vec::new();
        
        for (bucket, records) in buckets {
            if let Some(first) = records.first() {
                let avg_value = records.iter().map(|r| r.value).sum::<f64>() / records.len() as f64;
                
                result.push(MetricRecord {
                    timestamp: DateTime::from_timestamp(bucket * bucket_size.num_seconds(), 0).unwrap(),
                    metric_name: first.metric_name.clone(),
                    service_name: first.service_name.clone(),
                    value: avg_value,
                });
            }
        }
        
        result.sort_by_key(|r| r.timestamp);
        
        result
    }
}
```

---

## ğŸ”„ ç»Ÿä¸€å­˜å‚¨æŠ½è±¡

### 1. å­˜å‚¨åç«¯ Trait

```rust
use async_trait::async_trait;

/// å­˜å‚¨åç«¯ Trait
#[async_trait]
pub trait StorageBackend: Send + Sync {
    /// å†™å…¥å•ä¸ª Metric
    async fn write_metric(&self, metric: &crate::DistributedMetric) -> Result<(), Box<dyn std::error::Error>>;
    
    /// æ‰¹é‡å†™å…¥
    async fn write_batch(&self, metrics: Vec<crate::DistributedMetric>) -> Result<(), Box<dyn std::error::Error>>;
    
    /// æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
    async fn query_range(
        &self,
        metric_name: &str,
        service_name: &str,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    ) -> Result<Vec<MetricRecord>, Box<dyn std::error::Error>>;
    
    /// åˆ é™¤æŒ‡å®šæ—¶é—´ä¹‹å‰çš„æ•°æ®
    async fn delete_before(&self, table_name: &str, cutoff: DateTime<Utc>) -> Result<(), Box<dyn std::error::Error>>;
    
    /// å¥åº·æ£€æŸ¥
    async fn health_check(&self) -> Result<bool, Box<dyn std::error::Error>>;
}

/// InfluxDB åç«¯å®ç°
pub struct InfluxDbBackend {
    client: InfluxDbClient,
    converter: OtlpToInfluxConverter,
}

#[async_trait]
impl StorageBackend for InfluxDbBackend {
    async fn write_metric(&self, metric: &crate::DistributedMetric) -> Result<(), Box<dyn std::error::Error>> {
        let points = self.converter.convert_metric(metric);
        self.client.write_points(points).await
    }
    
    async fn write_batch(&self, metrics: Vec<crate::DistributedMetric>) -> Result<(), Box<dyn std::error::Error>> {
        let mut all_points = Vec::new();
        
        for metric in &metrics {
            all_points.extend(self.converter.convert_metric(metric));
        }
        
        self.client.write_batch(all_points, 5000).await
    }
    
    async fn query_range(
        &self,
        metric_name: &str,
        service_name: &str,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    ) -> Result<Vec<MetricRecord>, Box<dyn std::error::Error>> {
        let flux_query = format!(
            r#"from(bucket: "otlp")
               |> range(start: {}, stop: {})
               |> filter(fn: (r) => r._measurement == "{}" and r.service_name == "{}")"#,
            start.to_rfc3339(),
            end.to_rfc3339(),
            metric_name,
            service_name
        );
        
        let results = self.client.query(&flux_query).await?;
        
        // è½¬æ¢ä¸º MetricRecord
        let mut records = Vec::new();
        
        for row in results {
            if let (Some(time), Some(value)) = (row.get("_time"), row.get("_value")) {
                // ç®€åŒ–è½¬æ¢
                records.push(MetricRecord {
                    timestamp: Utc::now(), // TODO: è§£ææ—¶é—´
                    metric_name: metric_name.to_string(),
                    service_name: service_name.to_string(),
                    value: 0.0, // TODO: è§£æå€¼
                });
            }
        }
        
        Ok(records)
    }
    
    async fn delete_before(&self, _table_name: &str, _cutoff: DateTime<Utc>) -> Result<(), Box<dyn std::error::Error>> {
        // InfluxDB ä½¿ç”¨ Retention Policy è‡ªåŠ¨åˆ é™¤
        Ok(())
    }
    
    async fn health_check(&self) -> Result<bool, Box<dyn std::error::Error>> {
        // ç®€åŒ–å®ç°
        Ok(true)
    }
}
```

---

## ğŸ“Š å®Œæ•´ç¤ºä¾‹ï¼šå¤šåç«¯å­˜å‚¨ç³»ç»Ÿ

```rust
/// å¤šåç«¯å­˜å‚¨ç³»ç»Ÿ
pub struct MultiBackendStorage {
    backends: Vec<Box<dyn StorageBackend>>,
    write_strategy: WriteStrategy,
}

#[derive(Debug, Clone, Copy)]
pub enum WriteStrategy {
    /// å†™å…¥æ‰€æœ‰åç«¯
    All,
    
    /// å†™å…¥ä¸»åç«¯ï¼Œå¼‚æ­¥å¤åˆ¶åˆ°å…¶ä»–åç«¯
    PrimaryWithAsyncReplication,
    
    /// ä»…å†™å…¥ä¸»åç«¯
    PrimaryOnly,
}

impl MultiBackendStorage {
    pub fn new(backends: Vec<Box<dyn StorageBackend>>, write_strategy: WriteStrategy) -> Self {
        Self {
            backends,
            write_strategy,
        }
    }
    
    /// å†™å…¥ Metric
    pub async fn write_metric(&self, metric: crate::DistributedMetric) -> Result<(), Box<dyn std::error::Error>> {
        match self.write_strategy {
            WriteStrategy::All => {
                let mut handles = Vec::new();
                
                for backend in &self.backends {
                    let backend_clone = backend as *const dyn StorageBackend;
                    let metric_clone = metric.clone();
                    
                    let handle = tokio::spawn(async move {
                        unsafe { (*backend_clone).write_metric(&metric_clone).await }
                    });
                    
                    handles.push(handle);
                }
                
                for handle in handles {
                    handle.await??;
                }
            }
            
            WriteStrategy::PrimaryWithAsyncReplication => {
                // åŒæ­¥å†™å…¥ä¸»åç«¯
                if let Some(primary) = self.backends.first() {
                    primary.write_metric(&metric).await?;
                }
                
                // å¼‚æ­¥å¤åˆ¶åˆ°å…¶ä»–åç«¯
                for backend in self.backends.iter().skip(1) {
                    let backend_clone = backend as *const dyn StorageBackend;
                    let metric_clone = metric.clone();
                    
                    tokio::spawn(async move {
                        if let Err(e) = unsafe { (*backend_clone).write_metric(&metric_clone).await } {
                            tracing::error!("Async replication failed: {}", e);
                        }
                    });
                }
            }
            
            WriteStrategy::PrimaryOnly => {
                if let Some(primary) = self.backends.first() {
                    primary.write_metric(&metric).await?;
                }
            }
        }
        
        Ok(())
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt::init();
    
    // åˆå§‹åŒ–å¤šä¸ªå­˜å‚¨åç«¯
    let influxdb_backend = Box::new(InfluxDbBackend {
        client: InfluxDbClient::new(
            "http://localhost:8086".to_string(),
            "my-token".to_string(),
            "my-org".to_string(),
            "otlp".to_string(),
        ),
        converter: OtlpToInfluxConverter::new(),
    });
    
    let backends: Vec<Box<dyn StorageBackend>> = vec![influxdb_backend];
    
    let storage = MultiBackendStorage::new(backends, WriteStrategy::All);
    
    println!("Multi-backend storage system initialized!");
    
    Ok(())
}
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### æ•°æ®å»ºæ¨¡

- ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºä¸»åˆ†åŒºé”®
- é«˜åŸºæ•°æ ‡ç­¾ä½¿ç”¨å“ˆå¸Œ
- é¿å…è¿‡å¤šçš„æ ‡ç­¾ç»„åˆ

### å†™å…¥ä¼˜åŒ–

- æ‰¹é‡å†™å…¥ï¼ˆ5K-10K æ•°æ®ç‚¹ï¼‰
- ä½¿ç”¨å¼‚æ­¥å†™å…¥
- å¯ç”¨å‹ç¼©

### æŸ¥è¯¢ä¼˜åŒ–

- ä½¿ç”¨æ—¶é—´èŒƒå›´è¿‡æ»¤
- ä½¿ç”¨è¿ç»­èšåˆ
- ç¼“å­˜çƒ­ç‚¹æŸ¥è¯¢

---

## ğŸ“š å‚è€ƒèµ„æº

- [InfluxDB Documentation](https://docs.influxdata.com/)
- [TimescaleDB Best Practices](https://docs.timescale.com/)
- [Prometheus Storage](https://prometheus.io/docs/prometheus/latest/storage/)
- [Gorilla: A Fast, Scalable, In-Memory Time Series Database](https://www.vldb.org/pvldb/vol8/p1816-teller.pdf)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-10  
**ä½œè€…**: OTLP Rust é¡¹ç›®ç»„
