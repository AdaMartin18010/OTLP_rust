# OTLPæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæ•´å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025å¹´10æœˆ8æ—¥  
> **Rustç‰ˆæœ¬**: 1.90  
> **OpenTelemetryç‰ˆæœ¬**: 0.31.0  
> **æ–‡æ¡£ç±»å‹**: Performance Benchmarking Implementation

---

## ğŸ“‹ ç›®å½•

- [OTLPæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæ•´å®ç°](#otlpæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [æ¦‚è¿°](#æ¦‚è¿°)
    - [ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Ÿ](#ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½åŸºå‡†æµ‹è¯•)
    - [æ€§èƒ½åŸºå‡†æµ‹è¯•ç»´åº¦](#æ€§èƒ½åŸºå‡†æµ‹è¯•ç»´åº¦)
  - [åŸºå‡†æµ‹è¯•æ¡†æ¶è®¾è®¡](#åŸºå‡†æµ‹è¯•æ¡†æ¶è®¾è®¡)
    - [1. ä¾èµ–é…ç½®](#1-ä¾èµ–é…ç½®)
    - [2. åŸºå‡†æµ‹è¯•åŸºç¡€è®¾æ–½](#2-åŸºå‡†æµ‹è¯•åŸºç¡€è®¾æ–½)
  - [Tracesæ€§èƒ½åŸºå‡†æµ‹è¯•](#tracesæ€§èƒ½åŸºå‡†æµ‹è¯•)
    - [1. Spanåˆ›å»ºæ€§èƒ½æµ‹è¯•](#1-spanåˆ›å»ºæ€§èƒ½æµ‹è¯•)
    - [2. Spanå¯¼å‡ºæ€§èƒ½æµ‹è¯•](#2-spanå¯¼å‡ºæ€§èƒ½æµ‹è¯•)
  - [Metricsæ€§èƒ½åŸºå‡†æµ‹è¯•](#metricsæ€§èƒ½åŸºå‡†æµ‹è¯•)
    - [1. Metricä»ªè¡¨åˆ›å»ºä¸è®°å½•æ€§èƒ½](#1-metricä»ªè¡¨åˆ›å»ºä¸è®°å½•æ€§èƒ½)
    - [2. é«˜åŸºæ•°æ ‡ç­¾æ€§èƒ½æµ‹è¯•](#2-é«˜åŸºæ•°æ ‡ç­¾æ€§èƒ½æµ‹è¯•)
  - [Logsæ€§èƒ½åŸºå‡†æµ‹è¯•](#logsæ€§èƒ½åŸºå‡†æµ‹è¯•)
    - [1. Logè®°å½•æ€§èƒ½æµ‹è¯•](#1-logè®°å½•æ€§èƒ½æµ‹è¯•)
  - [ååé‡æµ‹è¯•](#ååé‡æµ‹è¯•)
    - [1. ç»¼åˆååé‡æµ‹è¯•](#1-ç»¼åˆååé‡æµ‹è¯•)
    - [2. å³°å€¼ååé‡å‹åŠ›æµ‹è¯•](#2-å³°å€¼ååé‡å‹åŠ›æµ‹è¯•)
  - [å»¶è¿Ÿæµ‹è¯•](#å»¶è¿Ÿæµ‹è¯•)
    - [1. ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•](#1-ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•)
  - [èµ„æºæ¶ˆè€—æµ‹è¯•](#èµ„æºæ¶ˆè€—æµ‹è¯•)
    - [1. CPUå’Œå†…å­˜æ¶ˆè€—æµ‹è¯•](#1-cpuå’Œå†…å­˜æ¶ˆè€—æµ‹è¯•)
    - [2. å†…å­˜æ³„æ¼æ£€æµ‹](#2-å†…å­˜æ³„æ¼æ£€æµ‹)
  - [æ€§èƒ½å¯¹æ¯”åˆ†æ](#æ€§èƒ½å¯¹æ¯”åˆ†æ)
    - [1. ä¸å…¶ä»–æ–¹æ¡ˆå¯¹æ¯”](#1-ä¸å…¶ä»–æ–¹æ¡ˆå¯¹æ¯”)
    - [2. æ€§èƒ½å¼€é”€åˆ†æ](#2-æ€§èƒ½å¼€é”€åˆ†æ)
  - [å®Œæ•´åŸºå‡†æµ‹è¯•å¥—ä»¶](#å®Œæ•´åŸºå‡†æµ‹è¯•å¥—ä»¶)
    - [è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•](#è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•)
    - [æŒç»­é›†æˆä¸­çš„åŸºå‡†æµ‹è¯•](#æŒç»­é›†æˆä¸­çš„åŸºå‡†æµ‹è¯•)
    - [åŸºå‡†æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿](#åŸºå‡†æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿)
  - [æ€»ç»“ä¸æœ€ä½³å®è·µ](#æ€»ç»“ä¸æœ€ä½³å®è·µ)
    - [åŸºå‡†æµ‹è¯•æœ€ä½³å®è·µ](#åŸºå‡†æµ‹è¯•æœ€ä½³å®è·µ)
    - [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](#æ€§èƒ½ä¼˜åŒ–æŒ‡å—)
    - [æ€§èƒ½ç›®æ ‡å‚è€ƒ](#æ€§èƒ½ç›®æ ‡å‚è€ƒ)
    - [å…³é”®è¦ç‚¹](#å…³é”®è¦ç‚¹)
  - [å‚è€ƒèµ„æº](#å‚è€ƒèµ„æº)
    - [å®˜æ–¹æ–‡æ¡£](#å®˜æ–¹æ–‡æ¡£)
    - [ç›¸å…³å·¥å…·](#ç›¸å…³å·¥å…·)
    - [ç¤¾åŒºèµ„æº](#ç¤¾åŒºèµ„æº)

---

## æ¦‚è¿°

### ä¸ºä»€ä¹ˆéœ€è¦æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Ÿ

æ€§èƒ½åŸºå‡†æµ‹è¯•å¯¹äºOTLPå®ç°è‡³å…³é‡è¦ï¼š

1. **æ€§èƒ½éªŒè¯**: ç¡®ä¿OTLPå®ç°æ»¡è¶³ç”Ÿäº§ç¯å¢ƒæ€§èƒ½è¦æ±‚
2. **å›å½’æ£€æµ‹**: åŠæ—¶å‘ç°æ€§èƒ½é€€åŒ–
3. **ä¼˜åŒ–æŒ‡å¯¼**: ä¸ºæ€§èƒ½ä¼˜åŒ–æä¾›é‡åŒ–æŒ‡æ ‡
4. **å®¹é‡è§„åˆ’**: ä¸ºç³»ç»Ÿå®¹é‡è§„åˆ’æä¾›æ•°æ®æ”¯æŒ
5. **å¯¹æ¯”åˆ†æ**: ä¸å…¶ä»–å®ç°æ–¹æ¡ˆè¿›è¡Œæ€§èƒ½å¯¹æ¯”

### æ€§èƒ½åŸºå‡†æµ‹è¯•ç»´åº¦

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           OTLP Performance Testing              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  1. Throughput (ååé‡)                         â”‚
â”‚     - Traces/sec                                â”‚
â”‚     - Metrics/sec                               â”‚
â”‚     - Logs/sec                                  â”‚
â”‚                                                 â”‚
â”‚  2. Latency (å»¶è¿Ÿ)                              â”‚
â”‚     - P50, P90, P95, P99, P999                  â”‚
â”‚     - Export latency                            â”‚
â”‚     - Processing latency                        â”‚
â”‚                                                 â”‚
â”‚  3. Resource Consumption (èµ„æºæ¶ˆè€—)             â”‚
â”‚     - CPU usage                                 â”‚
â”‚     - Memory usage                              â”‚
â”‚     - Network bandwidth                         â”‚
â”‚                                                 â”‚
â”‚  4. Scalability (å¯æ‰©å±•æ€§)                      â”‚
â”‚     - Vertical scaling                          â”‚
â”‚     - Horizontal scaling                        â”‚
â”‚                                                 â”‚
â”‚  5. Reliability (å¯é æ€§)                        â”‚
â”‚     - Data loss rate                            â”‚
â”‚     - Error rate                                â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## åŸºå‡†æµ‹è¯•æ¡†æ¶è®¾è®¡

### 1. ä¾èµ–é…ç½®

é¦–å…ˆåœ¨`Cargo.toml`ä¸­æ·»åŠ åŸºå‡†æµ‹è¯•ä¾èµ–ï¼š

```toml
[package]
name = "otlp-benchmarks"
version = "0.1.0"
edition = "2021"

[dependencies]
opentelemetry = "0.31.0"
opentelemetry_sdk = { version = "0.31.0", features = ["rt-tokio"] }
opentelemetry-otlp = { version = "0.24.0", features = ["tonic", "metrics", "logs"] }
tokio = { version = "1.47.1", features = ["full", "time"] }
tonic = "0.12"
tracing = "0.1"
tracing-subscriber = "0.3"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
anyhow = "1.0"
rand = "0.8"

# åŸºå‡†æµ‹è¯•æ¡†æ¶
criterion = { version = "0.5", features = ["async_tokio", "html_reports"] }

# å†…å­˜åˆ†æ
dhat = "0.3"

# ç³»ç»Ÿç›‘æ§
sysinfo = "0.30"

[dev-dependencies]
proptest = "1.4"

[[bench]]
name = "traces_benchmark"
harness = false

[[bench]]
name = "metrics_benchmark"
harness = false

[[bench]]
name = "logs_benchmark"
harness = false

[[bench]]
name = "throughput_benchmark"
harness = false

[[bench]]
name = "latency_benchmark"
harness = false
```

### 2. åŸºå‡†æµ‹è¯•åŸºç¡€è®¾æ–½

```rust
// benches/common/mod.rs
use opentelemetry::{
    global, trace::{Tracer, TracerProvider as _}, 
    metrics::{Meter, MeterProvider as _},
    KeyValue,
};
use opentelemetry_sdk::{
    trace::{TracerProvider, Config as TraceConfig},
    metrics::{MeterProvider, PeriodicReader, SdkMeterProvider},
    logs::{LoggerProvider, Config as LogsConfig},
    Resource,
};
use opentelemetry_otlp::{Protocol, WithExportConfig};
use std::time::Duration;
use sysinfo::{System, SystemExt, ProcessExt, Pid};

/// åŸºå‡†æµ‹è¯•é…ç½®
#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    pub endpoint: String,
    pub protocol: Protocol,
    pub batch_size: usize,
    pub export_timeout: Duration,
    pub max_queue_size: usize,
}

impl Default for BenchmarkConfig {
    fn default() -> Self {
        Self {
            endpoint: "http://localhost:4317".to_string(),
            protocol: Protocol::Grpc,
            batch_size: 512,
            export_timeout: Duration::from_secs(30),
            max_queue_size: 2048,
        }
    }
}

/// åˆå§‹åŒ–OTLP Tracerï¼ˆç”¨äºåŸºå‡†æµ‹è¯•ï¼‰
pub fn init_tracer_for_benchmark(config: &BenchmarkConfig) -> TracerProvider {
    let exporter = opentelemetry_otlp::SpanExporter::builder()
        .with_tonic()
        .with_endpoint(&config.endpoint)
        .with_timeout(config.export_timeout)
        .build()
        .expect("Failed to create OTLP exporter");

    let provider = TracerProvider::builder()
        .with_batch_exporter(exporter, opentelemetry_sdk::runtime::Tokio)
        .with_config(TraceConfig::default().with_resource(Resource::new(vec![
            KeyValue::new("service.name", "benchmark-service"),
            KeyValue::new("benchmark.type", "traces"),
        ])))
        .build();

    global::set_tracer_provider(provider.clone());
    provider
}

/// åˆå§‹åŒ–OTLP Meterï¼ˆç”¨äºåŸºå‡†æµ‹è¯•ï¼‰
pub fn init_meter_for_benchmark(config: &BenchmarkConfig) -> SdkMeterProvider {
    let exporter = opentelemetry_otlp::MetricExporter::builder()
        .with_tonic()
        .with_endpoint(&config.endpoint)
        .with_timeout(config.export_timeout)
        .build()
        .expect("Failed to create OTLP metric exporter");

    let reader = PeriodicReader::builder(exporter, opentelemetry_sdk::runtime::Tokio)
        .with_interval(Duration::from_secs(1))
        .build();

    let provider = SdkMeterProvider::builder()
        .with_reader(reader)
        .with_resource(Resource::new(vec![
            KeyValue::new("service.name", "benchmark-service"),
            KeyValue::new("benchmark.type", "metrics"),
        ]))
        .build();

    global::set_meter_provider(provider.clone());
    provider
}

/// åˆå§‹åŒ–OTLP Loggerï¼ˆç”¨äºåŸºå‡†æµ‹è¯•ï¼‰
pub fn init_logger_for_benchmark(config: &BenchmarkConfig) -> LoggerProvider {
    let exporter = opentelemetry_otlp::LogExporter::builder()
        .with_tonic()
        .with_endpoint(&config.endpoint)
        .with_timeout(config.export_timeout)
        .build()
        .expect("Failed to create OTLP log exporter");

    let provider = LoggerProvider::builder()
        .with_batch_exporter(exporter, opentelemetry_sdk::runtime::Tokio)
        .with_config(LogsConfig::default().with_resource(Resource::new(vec![
            KeyValue::new("service.name", "benchmark-service"),
            KeyValue::new("benchmark.type", "logs"),
        ])))
        .build();

    provider
}

/// ç³»ç»Ÿèµ„æºç›‘æ§å™¨
pub struct ResourceMonitor {
    system: System,
    process_pid: Pid,
}

impl ResourceMonitor {
    pub fn new() -> Self {
        let mut system = System::new_all();
        system.refresh_all();
        
        let process_pid = sysinfo::get_current_pid()
            .expect("Failed to get current process PID");
        
        Self {
            system,
            process_pid,
        }
    }
    
    /// è·å–CPUä½¿ç”¨ç‡
    pub fn cpu_usage(&mut self) -> f32 {
        self.system.refresh_process(self.process_pid);
        if let Some(process) = self.system.process(self.process_pid) {
            process.cpu_usage()
        } else {
            0.0
        }
    }
    
    /// è·å–å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰
    pub fn memory_usage_mb(&mut self) -> f64 {
        self.system.refresh_process(self.process_pid);
        if let Some(process) = self.system.process(self.process_pid) {
            process.memory() as f64 / 1024.0 / 1024.0
        } else {
            0.0
        }
    }
    
    /// è·å–ç³»ç»Ÿæ€»å†…å­˜ï¼ˆGBï¼‰
    pub fn total_memory_gb(&self) -> f64 {
        self.system.total_memory() as f64 / 1024.0 / 1024.0 / 1024.0
    }
}

/// åŸºå‡†æµ‹è¯•ç»“æœç»Ÿè®¡
#[derive(Debug, Clone)]
pub struct BenchmarkStats {
    pub total_operations: u64,
    pub duration_secs: f64,
    pub throughput: f64,  // operations/sec
    pub avg_latency_us: f64,
    pub p50_latency_us: f64,
    pub p90_latency_us: f64,
    pub p95_latency_us: f64,
    pub p99_latency_us: f64,
    pub p999_latency_us: f64,
    pub avg_cpu_usage: f32,
    pub avg_memory_mb: f64,
}

impl BenchmarkStats {
    pub fn report(&self, name: &str) {
        println!("\n{}", "=".repeat(60));
        println!("ğŸ“Š Benchmark Report: {}", name);
        println!("{}", "=".repeat(60));
        println!("Total Operations:    {:>12}", self.total_operations);
        println!("Duration:            {:>12.2}s", self.duration_secs);
        println!("Throughput:          {:>12.2} ops/sec", self.throughput);
        println!("Avg Latency:         {:>12.2} Âµs", self.avg_latency_us);
        println!("P50 Latency:         {:>12.2} Âµs", self.p50_latency_us);
        println!("P90 Latency:         {:>12.2} Âµs", self.p90_latency_us);
        println!("P95 Latency:         {:>12.2} Âµs", self.p95_latency_us);
        println!("P99 Latency:         {:>12.2} Âµs", self.p99_latency_us);
        println!("P999 Latency:        {:>12.2} Âµs", self.p999_latency_us);
        println!("Avg CPU Usage:       {:>12.2}%", self.avg_cpu_usage);
        println!("Avg Memory:          {:>12.2} MB", self.avg_memory_mb);
        println!("{}", "=".repeat(60));
    }
}

/// è®¡ç®—ç™¾åˆ†ä½æ•°
pub fn calculate_percentile(sorted_data: &[u64], percentile: f64) -> f64 {
    if sorted_data.is_empty() {
        return 0.0;
    }
    
    let index = (percentile / 100.0 * sorted_data.len() as f64) as usize;
    let index = index.min(sorted_data.len() - 1);
    sorted_data[index] as f64
}
```

---

## Tracesæ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. Spanåˆ›å»ºæ€§èƒ½æµ‹è¯•

```rust
// benches/traces_benchmark.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use opentelemetry::{global, trace::{Tracer, Span, Status, SpanKind}, KeyValue};
use tokio::runtime::Runtime;
use std::time::{Duration, Instant};

mod common;
use common::*;

/// Spanåˆ›å»ºåŸºå‡†æµ‹è¯•
fn span_creation_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        // é¢„çƒ­
        for _ in 0..1000 {
            let _span = tracer.span_builder("warmup").start(&tracer);
        }
        
        let mut group = c.benchmark_group("span_creation");
        
        // ç®€å•Span
        group.bench_function("simple_span", |b| {
            b.iter(|| {
                let _span = tracer.span_builder("simple").start(&tracer);
            });
        });
        
        // å¸¦å±æ€§çš„Span
        group.bench_function("span_with_attributes", |b| {
            b.iter(|| {
                let _span = tracer.span_builder("with_attrs")
                    .with_attributes(vec![
                        KeyValue::new("key1", "value1"),
                        KeyValue::new("key2", "value2"),
                        KeyValue::new("key3", "value3"),
                    ])
                    .start(&tracer);
            });
        });
        
        // åµŒå¥—Span
        group.bench_function("nested_span", |b| {
            b.iter(|| {
                let parent = tracer.span_builder("parent").start(&tracer);
                let _cx = opentelemetry::Context::current_with_span(parent);
                let _child = tracer.span_builder("child").start(&tracer);
            });
        });
        
        group.finish();
    });
}

/// Spanå±æ€§è®¾ç½®æ€§èƒ½æµ‹è¯•
fn span_attributes_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        let mut group = c.benchmark_group("span_attributes");
        
        for attr_count in [1, 5, 10, 20, 50].iter() {
            group.bench_with_input(
                BenchmarkId::from_parameter(attr_count),
                attr_count,
                |b, &count| {
                    b.iter(|| {
                        let mut span = tracer.span_builder("test").start(&tracer);
                        for i in 0..count {
                            span.set_attribute(KeyValue::new(
                                format!("key{}", i),
                                format!("value{}", i),
                            ));
                        }
                    });
                },
            );
        }
        
        group.finish();
    });
}

/// Spanäº‹ä»¶æ·»åŠ æ€§èƒ½æµ‹è¯•
fn span_events_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        let mut group = c.benchmark_group("span_events");
        
        // æ·»åŠ å•ä¸ªäº‹ä»¶
        group.bench_function("add_event", |b| {
            b.iter(|| {
                let mut span = tracer.span_builder("test").start(&tracer);
                span.add_event("test_event", vec![]);
            });
        });
        
        // æ·»åŠ å¸¦å±æ€§çš„äº‹ä»¶
        group.bench_function("add_event_with_attrs", |b| {
            b.iter(|| {
                let mut span = tracer.span_builder("test").start(&tracer);
                span.add_event("test_event", vec![
                    KeyValue::new("event.key1", "value1"),
                    KeyValue::new("event.key2", "value2"),
                ]);
            });
        });
        
        group.finish();
    });
}

/// Trace Contextä¼ æ’­æ€§èƒ½æµ‹è¯•
fn context_propagation_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        let mut group = c.benchmark_group("context_propagation");
        
        // W3C Trace Contextæ³¨å…¥
        group.bench_function("inject_context", |b| {
            b.iter(|| {
                let span = tracer.span_builder("test").start(&tracer);
                let cx = opentelemetry::Context::current_with_span(span);
                let mut headers = std::collections::HashMap::new();
                
                use opentelemetry::propagation::{TextMapPropagator, Injector};
                use opentelemetry_sdk::propagation::TraceContextPropagator;
                
                struct HashMapInjector<'a>(&'a mut std::collections::HashMap<String, String>);
                impl<'a> Injector for HashMapInjector<'a> {
                    fn set(&mut self, key: &str, value: String) {
                        self.0.insert(key.to_string(), value);
                    }
                }
                
                let propagator = TraceContextPropagator::new();
                propagator.inject_context(&cx, &mut HashMapInjector(&mut headers));
            });
        });
        
        // W3C Trace Contextæå–
        group.bench_function("extract_context", |b| {
            let mut headers = std::collections::HashMap::new();
            headers.insert(
                "traceparent".to_string(),
                "00-0af7651916cd43dd8448eb211c80319c-b7ad6b7169203331-01".to_string(),
            );
            
            b.iter(|| {
                use opentelemetry::propagation::{TextMapPropagator, Extractor};
                use opentelemetry_sdk::propagation::TraceContextPropagator;
                
                struct HashMapExtractor<'a>(&'a std::collections::HashMap<String, String>);
                impl<'a> Extractor for HashMapExtractor<'a> {
                    fn get(&self, key: &str) -> Option<&str> {
                        self.0.get(key).map(|s| s.as_str())
                    }
                    fn keys(&self) -> Vec<&str> {
                        self.0.keys().map(|s| s.as_str()).collect()
                    }
                }
                
                let propagator = TraceContextPropagator::new();
                let _cx = propagator.extract(&HashMapExtractor(&headers));
            });
        });
        
        group.finish();
    });
}

criterion_group!(
    traces_benches,
    span_creation_benchmark,
    span_attributes_benchmark,
    span_events_benchmark,
    context_propagation_benchmark
);
criterion_main!(traces_benches);
```

### 2. Spanå¯¼å‡ºæ€§èƒ½æµ‹è¯•

```rust
// benches/traces_export_benchmark.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use opentelemetry::{global, trace::Tracer, KeyValue};
use tokio::runtime::Runtime;
use std::time::Instant;

mod common;
use common::*;

/// Spanæ‰¹é‡å¯¼å‡ºæ€§èƒ½æµ‹è¯•
fn span_export_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        let mut group = c.benchmark_group("span_export");
        group.sample_size(50);
        
        for batch_size in [10, 100, 500, 1000].iter() {
            group.bench_with_input(
                criterion::BenchmarkId::from_parameter(batch_size),
                batch_size,
                |b, &size| {
                    b.iter(|| {
                        for i in 0..size {
                            let mut span = tracer.span_builder(format!("span-{}", i))
                                .with_attributes(vec![
                                    KeyValue::new("iteration", i as i64),
                                    KeyValue::new("batch_size", size as i64),
                                ])
                                .start(&tracer);
                            
                            span.set_attribute(KeyValue::new("status", "completed"));
                            span.end();
                        }
                        
                        // å¼ºåˆ¶åˆ·æ–°
                        provider.force_flush();
                    });
                },
            );
        }
        
        group.finish();
    });
}

criterion_group!(traces_export_benches, span_export_benchmark);
criterion_main!(traces_export_benches);
```

---

## Metricsæ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. Metricä»ªè¡¨åˆ›å»ºä¸è®°å½•æ€§èƒ½

```rust
// benches/metrics_benchmark.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use opentelemetry::{global, KeyValue};
use tokio::runtime::Runtime;

mod common;
use common::*;

/// Counteræ€§èƒ½æµ‹è¯•
fn counter_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_meter_for_benchmark(&config);
        let meter = global::meter("benchmark");
        
        let counter = meter
            .u64_counter("test_counter")
            .with_description("Test counter for benchmarking")
            .build();
        
        let mut group = c.benchmark_group("counter");
        
        // ç®€å•è®¡æ•°
        group.bench_function("simple_add", |b| {
            b.iter(|| {
                counter.add(1, &[]);
            });
        });
        
        // å¸¦æ ‡ç­¾è®¡æ•°
        group.bench_function("add_with_labels", |b| {
            b.iter(|| {
                counter.add(1, &[
                    KeyValue::new("label1", "value1"),
                    KeyValue::new("label2", "value2"),
                ]);
            });
        });
        
        // å¤šä¸ªæ ‡ç­¾è®¡æ•°
        for label_count in [1, 3, 5, 10].iter() {
            group.bench_with_input(
                BenchmarkId::new("add_with_n_labels", label_count),
                label_count,
                |b, &count| {
                    b.iter(|| {
                        let labels: Vec<KeyValue> = (0..count)
                            .map(|i| KeyValue::new(format!("label{}", i), format!("value{}", i)))
                            .collect();
                        counter.add(1, &labels);
                    });
                },
            );
        }
        
        group.finish();
    });
}

/// Histogramæ€§èƒ½æµ‹è¯•
fn histogram_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_meter_for_benchmark(&config);
        let meter = global::meter("benchmark");
        
        let histogram = meter
            .f64_histogram("test_histogram")
            .with_description("Test histogram for benchmarking")
            .build();
        
        let mut group = c.benchmark_group("histogram");
        
        // ç®€å•è®°å½•
        group.bench_function("simple_record", |b| {
            b.iter(|| {
                histogram.record(123.45, &[]);
            });
        });
        
        // å¸¦æ ‡ç­¾è®°å½•
        group.bench_function("record_with_labels", |b| {
            b.iter(|| {
                histogram.record(123.45, &[
                    KeyValue::new("method", "GET"),
                    KeyValue::new("status", "200"),
                ]);
            });
        });
        
        group.finish();
    });
}

/// Gaugeæ€§èƒ½æµ‹è¯•
fn gauge_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_meter_for_benchmark(&config);
        let meter = global::meter("benchmark");
        
        let gauge = meter
            .f64_observable_gauge("test_gauge")
            .with_description("Test gauge for benchmarking")
            .build();
        
        let mut group = c.benchmark_group("gauge");
        
        // Observable Gaugeå›è°ƒ
        group.bench_function("observable_callback", |b| {
            b.iter(|| {
                // Observable Gaugeé€šè¿‡å›è°ƒè‡ªåŠ¨è®°å½•
                // è¿™é‡Œæµ‹è¯•å›è°ƒå‡½æ•°çš„æ€§èƒ½
                let value = 42.0;
                black_box(value);
            });
        });
        
        group.finish();
    });
}

/// UpDownCounteræ€§èƒ½æµ‹è¯•
fn updown_counter_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_meter_for_benchmark(&config);
        let meter = global::meter("benchmark");
        
        let updown_counter = meter
            .i64_up_down_counter("test_updown_counter")
            .with_description("Test up-down counter for benchmarking")
            .build();
        
        let mut group = c.benchmark_group("updown_counter");
        
        // å¢åŠ 
        group.bench_function("add", |b| {
            b.iter(|| {
                updown_counter.add(1, &[]);
            });
        });
        
        // å‡å°‘
        group.bench_function("subtract", |b| {
            b.iter(|| {
                updown_counter.add(-1, &[]);
            });
        });
        
        group.finish();
    });
}

criterion_group!(
    metrics_benches,
    counter_benchmark,
    histogram_benchmark,
    gauge_benchmark,
    updown_counter_benchmark
);
criterion_main!(metrics_benches);
```

### 2. é«˜åŸºæ•°æ ‡ç­¾æ€§èƒ½æµ‹è¯•

```rust
// benches/metrics_cardinality_benchmark.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use opentelemetry::{global, KeyValue};
use tokio::runtime::Runtime;
use rand::{Rng, thread_rng};

mod common;
use common::*;

/// é«˜åŸºæ•°æ ‡ç­¾æ€§èƒ½æµ‹è¯•
fn high_cardinality_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_meter_for_benchmark(&config);
        let meter = global::meter("benchmark");
        
        let counter = meter
            .u64_counter("high_cardinality_counter")
            .build();
        
        let mut group = c.benchmark_group("high_cardinality");
        
        // æµ‹è¯•ä¸åŒåŸºæ•°
        for cardinality in [10, 100, 1000, 10000].iter() {
            group.bench_with_input(
                BenchmarkId::from_parameter(cardinality),
                cardinality,
                |b, &card| {
                    let mut rng = thread_rng();
                    b.iter(|| {
                        let user_id = rng.gen_range(0..card);
                        counter.add(1, &[
                            KeyValue::new("user_id", user_id as i64),
                            KeyValue::new("action", "click"),
                        ]);
                    });
                },
            );
        }
        
        group.finish();
    });
}

criterion_group!(cardinality_benches, high_cardinality_benchmark);
criterion_main!(cardinality_benches);
```

---

## Logsæ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. Logè®°å½•æ€§èƒ½æµ‹è¯•

```rust
// benches/logs_benchmark.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use opentelemetry::{logs::{Logger, LoggerProvider as _, Severity}, KeyValue};
use tokio::runtime::Runtime;

mod common;
use common::*;

/// æ—¥å¿—è®°å½•æ€§èƒ½æµ‹è¯•
fn log_emission_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let provider = init_logger_for_benchmark(&config);
        let logger = provider.logger("benchmark");
        
        let mut group = c.benchmark_group("log_emission");
        
        // ç®€å•æ—¥å¿—
        group.bench_function("simple_log", |b| {
            b.iter(|| {
                logger.emit(
                    opentelemetry::logs::LogRecord::builder()
                        .with_severity_number(Severity::Info)
                        .with_body("Simple log message".into())
                        .build(),
                );
            });
        });
        
        // å¸¦å±æ€§çš„æ—¥å¿—
        group.bench_function("log_with_attributes", |b| {
            b.iter(|| {
                logger.emit(
                    opentelemetry::logs::LogRecord::builder()
                        .with_severity_number(Severity::Info)
                        .with_body("Log with attributes".into())
                        .with_attribute(KeyValue::new("key1", "value1"))
                        .with_attribute(KeyValue::new("key2", "value2"))
                        .build(),
                );
            });
        });
        
        // ä¸åŒä¸¥é‡çº§åˆ«
        for severity in [
            Severity::Debug,
            Severity::Info,
            Severity::Warn,
            Severity::Error,
        ].iter() {
            group.bench_with_input(
                BenchmarkId::new("severity", format!("{:?}", severity)),
                severity,
                |b, &sev| {
                    b.iter(|| {
                        logger.emit(
                            opentelemetry::logs::LogRecord::builder()
                                .with_severity_number(sev)
                                .with_body("Test log".into())
                                .build(),
                        );
                    });
                },
            );
        }
        
        group.finish();
    });
}

criterion_group!(logs_benches, log_emission_benchmark);
criterion_main!(logs_benches);
```

---

## ååé‡æµ‹è¯•

### 1. ç»¼åˆååé‡æµ‹è¯•

```rust
// benches/throughput_benchmark.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use opentelemetry::{global, trace::Tracer, KeyValue};
use tokio::runtime::Runtime;
use std::time::{Duration, Instant};

mod common;
use common::*;

/// æµ‹é‡Tracesååé‡
fn traces_throughput_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        let mut group = c.benchmark_group("traces_throughput");
        group.sample_size(20);
        group.measurement_time(Duration::from_secs(10));
        
        // æµ‹è¯•ä¸åŒå·¥ä½œè´Ÿè½½
        for spans_per_sec in [1000, 5000, 10000, 50000].iter() {
            group.bench_with_input(
                BenchmarkId::from_parameter(spans_per_sec),
                spans_per_sec,
                |b, &target_rate| {
                    b.iter(|| {
                        let start = Instant::now();
                        let duration = Duration::from_secs(1);
                        let mut count = 0;
                        
                        while start.elapsed() < duration {
                            let mut span = tracer.span_builder("throughput_test")
                                .with_attributes(vec![
                                    KeyValue::new("iteration", count),
                                ])
                                .start(&tracer);
                            span.end();
                            count += 1;
                            
                            // è¾¾åˆ°ç›®æ ‡é€Ÿç‡åæš‚åœ
                            if count >= target_rate {
                                break;
                            }
                        }
                        
                        black_box(count);
                    });
                },
            );
        }
        
        group.finish();
        
        provider.shutdown().unwrap();
    });
}

/// æµ‹é‡Metricsååé‡
fn metrics_throughput_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let provider = init_meter_for_benchmark(&config);
        let meter = global::meter("benchmark");
        
        let counter = meter.u64_counter("throughput_counter").build();
        
        let mut group = c.benchmark_group("metrics_throughput");
        group.sample_size(20);
        group.measurement_time(Duration::from_secs(10));
        
        for metrics_per_sec in [10000, 50000, 100000, 500000].iter() {
            group.bench_with_input(
                BenchmarkId::from_parameter(metrics_per_sec),
                metrics_per_sec,
                |b, &target_rate| {
                    b.iter(|| {
                        let start = Instant::now();
                        let duration = Duration::from_secs(1);
                        let mut count = 0;
                        
                        while start.elapsed() < duration && count < target_rate {
                            counter.add(1, &[KeyValue::new("test", "throughput")]);
                            count += 1;
                        }
                        
                        black_box(count);
                    });
                },
            );
        }
        
        group.finish();
        
        provider.shutdown().unwrap();
    });
}

/// å¹¶å‘ååé‡æµ‹è¯•
fn concurrent_throughput_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        let mut group = c.benchmark_group("concurrent_throughput");
        group.sample_size(10);
        group.measurement_time(Duration::from_secs(15));
        
        for num_tasks in [1, 2, 4, 8, 16].iter() {
            group.bench_with_input(
                BenchmarkId::from_parameter(num_tasks),
                num_tasks,
                |b, &tasks| {
                    b.to_async(&rt).iter(|| async {
                        let mut handles = vec![];
                        
                        for task_id in 0..tasks {
                            let tracer = tracer.clone();
                            let handle = tokio::spawn(async move {
                                for i in 0..1000 {
                                    let mut span = tracer.span_builder(format!("task-{}-span-{}", task_id, i))
                                        .start(&tracer);
                                    span.end();
                                }
                            });
                            handles.push(handle);
                        }
                        
                        for handle in handles {
                            handle.await.unwrap();
                        }
                    });
                },
            );
        }
        
        group.finish();
        
        provider.shutdown().unwrap();
    });
}

criterion_group!(
    throughput_benches,
    traces_throughput_benchmark,
    metrics_throughput_benchmark,
    concurrent_throughput_benchmark
);
criterion_main!(throughput_benches);
```

### 2. å³°å€¼ååé‡å‹åŠ›æµ‹è¯•

```rust
// benches/stress_throughput.rs
use std::sync::Arc;
use std::sync::atomic::{AtomicU64, Ordering};
use std::time::{Duration, Instant};
use tokio::runtime::Runtime;
use opentelemetry::{global, trace::Tracer, KeyValue};

mod common;
use common::*;

/// å³°å€¼ååé‡å‹åŠ›æµ‹è¯•
fn main() {
    let rt = Runtime::new().unwrap();
    
    rt.block_on(async {
        let config = BenchmarkConfig::default();
        let provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("stress_test");
        
        let total_operations = Arc::new(AtomicU64::new(0));
        let test_duration = Duration::from_secs(30);
        let num_workers = num_cpus::get();
        
        println!("\nğŸ”¥ Starting stress test...");
        println!("Duration: {:?}", test_duration);
        println!("Workers: {}", num_workers);
        
        let start = Instant::now();
        let mut handles = vec![];
        
        for worker_id in 0..num_workers {
            let tracer = tracer.clone();
            let counter = Arc::clone(&total_operations);
            let end_time = start + test_duration;
            
            let handle = tokio::spawn(async move {
                let mut local_count = 0u64;
                
                while Instant::now() < end_time {
                    // æ‰¹é‡åˆ›å»ºspans
                    for _ in 0..100 {
                        let mut span = tracer.span_builder(format!("stress-{}-{}", worker_id, local_count))
                            .with_attributes(vec![
                                KeyValue::new("worker", worker_id as i64),
                                KeyValue::new("iteration", local_count as i64),
                            ])
                            .start(&tracer);
                        span.end();
                        local_count += 1;
                    }
                }
                
                counter.fetch_add(local_count, Ordering::Relaxed);
            });
            
            handles.push(handle);
        }
        
        // ç­‰å¾…æ‰€æœ‰workerå®Œæˆ
        for handle in handles {
            handle.await.unwrap();
        }
        
        let elapsed = start.elapsed();
        let total = total_operations.load(Ordering::Relaxed);
        let throughput = total as f64 / elapsed.as_secs_f64();
        
        println!("\nğŸ“Š Stress Test Results:");
        println!("Total Operations: {}", total);
        println!("Duration: {:.2}s", elapsed.as_secs_f64());
        println!("Throughput: {:.2} ops/sec", throughput);
        println!("Per-worker: {:.2} ops/sec", throughput / num_workers as f64);
        
        provider.shutdown().unwrap();
    });
}
```

---

## å»¶è¿Ÿæµ‹è¯•

### 1. ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•

```rust
// benches/latency_benchmark.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion, BenchmarkId};
use opentelemetry::{global, trace::Tracer, KeyValue};
use tokio::runtime::Runtime;
use std::time::{Duration, Instant};

mod common;
use common::*;

/// Spanåˆ›å»ºå»¶è¿Ÿæµ‹è¯•
fn span_creation_latency(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let _provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        let mut group = c.benchmark_group("span_latency");
        
        // æµ‹é‡åˆ›å»ºå»¶è¿Ÿ
        group.bench_function("create_latency", |b| {
            b.iter_custom(|iters| {
                let start = Instant::now();
                for i in 0..iters {
                    let _span = tracer.span_builder(format!("span-{}", i))
                        .start(&tracer);
                }
                start.elapsed()
            });
        });
        
        // æµ‹é‡åˆ›å»º+ç»“æŸå»¶è¿Ÿ
        group.bench_function("create_and_end_latency", |b| {
            b.iter_custom(|iters| {
                let start = Instant::now();
                for i in 0..iters {
                    let mut span = tracer.span_builder(format!("span-{}", i))
                        .start(&tracer);
                    span.end();
                }
                start.elapsed()
            });
        });
        
        group.finish();
    });
}

/// Exportå»¶è¿Ÿæµ‹è¯•
fn export_latency_benchmark(c: &mut Criterion) {
    let rt = Runtime::new().unwrap();
    let config = BenchmarkConfig::default();
    
    rt.block_on(async {
        let provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        let mut group = c.benchmark_group("export_latency");
        group.sample_size(20);
        
        for batch_size in [10, 100, 500].iter() {
            group.bench_with_input(
                BenchmarkId::from_parameter(batch_size),
                batch_size,
                |b, &size| {
                    b.iter_custom(|iters| {
                        let mut total_duration = Duration::ZERO;
                        
                        for _ in 0..iters {
                            // åˆ›å»ºspans
                            for i in 0..size {
                                let mut span = tracer.span_builder(format!("span-{}", i))
                                    .start(&tracer);
                                span.end();
                            }
                            
                            // æµ‹é‡flushå»¶è¿Ÿ
                            let start = Instant::now();
                            provider.force_flush();
                            total_duration += start.elapsed();
                        }
                        
                        total_duration
                    });
                },
            );
        }
        
        group.finish();
    });
}

/// è¯¦ç»†å»¶è¿Ÿåˆ†å¸ƒæµ‹è¯•
fn latency_distribution_test() {
    let rt = Runtime::new().unwrap();
    
    rt.block_on(async {
        let config = BenchmarkConfig::default();
        let _provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("benchmark");
        
        let num_samples = 10000;
        let mut latencies = Vec::with_capacity(num_samples);
        
        println!("\nğŸ” Collecting {} latency samples...", num_samples);
        
        for i in 0..num_samples {
            let start = Instant::now();
            
            let mut span = tracer.span_builder(format!("latency-test-{}", i))
                .with_attributes(vec![
                    KeyValue::new("iteration", i as i64),
                ])
                .start(&tracer);
            span.end();
            
            let elapsed = start.elapsed();
            latencies.push(elapsed.as_micros() as u64);
        }
        
        // è®¡ç®—ç»Ÿè®¡æ•°æ®
        latencies.sort_unstable();
        
        let avg = latencies.iter().sum::<u64>() as f64 / latencies.len() as f64;
        let p50 = calculate_percentile(&latencies, 50.0);
        let p90 = calculate_percentile(&latencies, 90.0);
        let p95 = calculate_percentile(&latencies, 95.0);
        let p99 = calculate_percentile(&latencies, 99.0);
        let p999 = calculate_percentile(&latencies, 99.9);
        let max = *latencies.last().unwrap() as f64;
        
        println!("\nğŸ“Š Latency Distribution:");
        println!("Samples:  {}", num_samples);
        println!("Average:  {:.2} Âµs", avg);
        println!("P50:      {:.2} Âµs", p50);
        println!("P90:      {:.2} Âµs", p90);
        println!("P95:      {:.2} Âµs", p95);
        println!("P99:      {:.2} Âµs", p99);
        println!("P999:     {:.2} Âµs", p999);
        println!("Max:      {:.2} Âµs", max);
    });
}

criterion_group!(
    latency_benches,
    span_creation_latency,
    export_latency_benchmark
);
criterion_main!(latency_benches);
```

---

## èµ„æºæ¶ˆè€—æµ‹è¯•

### 1. CPUå’Œå†…å­˜æ¶ˆè€—æµ‹è¯•

```rust
// benches/resource_consumption.rs
use std::sync::Arc;
use std::sync::atomic::{AtomicBool, Ordering};
use std::time::{Duration, Instant};
use std::thread;
use tokio::runtime::Runtime;
use opentelemetry::{global, trace::Tracer, KeyValue};

mod common;
use common::*;

/// CPUå’Œå†…å­˜æ¶ˆè€—æµ‹è¯•
fn main() {
    let rt = Runtime::new().unwrap();
    
    rt.block_on(async {
        let config = BenchmarkConfig::default();
        let provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("resource_test");
        
        let mut monitor = ResourceMonitor::new();
        let running = Arc::new(AtomicBool::new(true));
        let running_clone = Arc::clone(&running);
        
        println!("\nğŸ“Š Starting resource consumption test...");
        println!("Duration: 60 seconds");
        println!("Initial memory: {:.2} MB", monitor.memory_usage_mb());
        
        // ç›‘æ§çº¿ç¨‹
        let monitor_handle = thread::spawn(move || {
            let mut cpu_samples = vec![];
            let mut mem_samples = vec![];
            
            while running_clone.load(Ordering::Relaxed) {
                let cpu = monitor.cpu_usage();
                let mem = monitor.memory_usage_mb();
                
                cpu_samples.push(cpu);
                mem_samples.push(mem);
                
                thread::sleep(Duration::from_secs(1));
            }
            
            (cpu_samples, mem_samples)
        });
        
        // å·¥ä½œè´Ÿè½½
        let start = Instant::now();
        let test_duration = Duration::from_secs(60);
        let mut total_spans = 0u64;
        
        while start.elapsed() < test_duration {
            // ä»¥å›ºå®šé€Ÿç‡åˆ›å»ºspans
            for i in 0..1000 {
                let mut span = tracer.span_builder(format!("resource-test-{}", i))
                    .with_attributes(vec![
                        KeyValue::new("iteration", i as i64),
                        KeyValue::new("timestamp", start.elapsed().as_secs() as i64),
                    ])
                    .start(&tracer);
                span.end();
                total_spans += 1;
            }
            
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
        
        // åœæ­¢ç›‘æ§
        running.store(false, Ordering::Relaxed);
        let (cpu_samples, mem_samples) = monitor_handle.join().unwrap();
        
        // è®¡ç®—ç»Ÿè®¡æ•°æ®
        let avg_cpu = cpu_samples.iter().sum::<f32>() / cpu_samples.len() as f32;
        let max_cpu = cpu_samples.iter().cloned().fold(0.0f32, f32::max);
        
        let avg_mem = mem_samples.iter().sum::<f64>() / mem_samples.len() as f64;
        let max_mem = mem_samples.iter().cloned().fold(0.0f64, f64::max);
        let min_mem = mem_samples.iter().cloned().fold(f64::MAX, f64::min);
        
        let throughput = total_spans as f64 / start.elapsed().as_secs_f64();
        
        println!("\nğŸ“ˆ Resource Consumption Results:");
        println!("Total Spans:     {}", total_spans);
        println!("Throughput:      {:.2} spans/sec", throughput);
        println!("\nCPU Usage:");
        println!("  Average:       {:.2}%", avg_cpu);
        println!("  Max:           {:.2}%", max_cpu);
        println!("\nMemory Usage:");
        println!("  Average:       {:.2} MB", avg_mem);
        println!("  Min:           {:.2} MB", min_mem);
        println!("  Max:           {:.2} MB", max_mem);
        println!("  Growth:        {:.2} MB", max_mem - min_mem);
        
        provider.shutdown().unwrap();
    });
}
```

### 2. å†…å­˜æ³„æ¼æ£€æµ‹

```rust
// benches/memory_leak_test.rs
use std::time::{Duration, Instant};
use tokio::runtime::Runtime;
use opentelemetry::{global, trace::Tracer, KeyValue};

mod common;
use common::*;

/// å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•
fn main() {
    let rt = Runtime::new().unwrap();
    
    rt.block_on(async {
        let config = BenchmarkConfig::default();
        let provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("leak_test");
        
        let mut monitor = ResourceMonitor::new();
        
        println!("\nğŸ” Memory Leak Detection Test");
        println!("Running 10 cycles, 100k spans per cycle");
        
        let mut cycle_memories = vec![];
        
        for cycle in 0..10 {
            let cycle_start = Instant::now();
            let mem_before = monitor.memory_usage_mb();
            
            // åˆ›å»ºå¤§é‡spans
            for i in 0..100_000 {
                let mut span = tracer.span_builder(format!("leak-test-{}-{}", cycle, i))
                    .with_attributes(vec![
                        KeyValue::new("cycle", cycle as i64),
                        KeyValue::new("iteration", i as i64),
                    ])
                    .start(&tracer);
                span.end();
            }
            
            // å¼ºåˆ¶åˆ·æ–°
            provider.force_flush();
            
            // ç»™GCä¸€äº›æ—¶é—´
            tokio::time::sleep(Duration::from_secs(2)).await;
            
            let mem_after = monitor.memory_usage_mb();
            let mem_growth = mem_after - mem_before;
            let elapsed = cycle_start.elapsed();
            
            cycle_memories.push(mem_after);
            
            println!(
                "Cycle {}: {:.2}s, Mem before: {:.2} MB, Mem after: {:.2} MB, Growth: {:.2} MB",
                cycle, elapsed.as_secs_f64(), mem_before, mem_after, mem_growth
            );
        }
        
        // åˆ†æå†…å­˜å¢é•¿è¶‹åŠ¿
        let first_mem = cycle_memories[0];
        let last_mem = cycle_memories[cycle_memories.len() - 1];
        let total_growth = last_mem - first_mem;
        let avg_growth_per_cycle = total_growth / (cycle_memories.len() - 1) as f64;
        
        println!("\nğŸ“Š Memory Leak Analysis:");
        println!("Initial memory:    {:.2} MB", first_mem);
        println!("Final memory:      {:.2} MB", last_mem);
        println!("Total growth:      {:.2} MB", total_growth);
        println!("Avg growth/cycle:  {:.2} MB", avg_growth_per_cycle);
        
        if avg_growth_per_cycle > 5.0 {
            println!("\nâš ï¸  WARNING: Possible memory leak detected!");
        } else {
            println!("\nâœ… No significant memory leak detected");
        }
        
        provider.shutdown().unwrap();
    });
}
```

---

## æ€§èƒ½å¯¹æ¯”åˆ†æ

### 1. ä¸å…¶ä»–æ–¹æ¡ˆå¯¹æ¯”

```rust
// benches/comparison_benchmark.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use std::time::Instant;

/// å¯¹æ¯”ï¼šç›´æ¥å‡½æ•°è°ƒç”¨ vs OTLPè¿½è¸ª
fn overhead_comparison(c: &mut Criterion) {
    let mut group = c.benchmark_group("overhead_comparison");
    
    // Baseline: æ— è¿½è¸ª
    group.bench_function("no_tracing", |b| {
        b.iter(|| {
            let result = expensive_operation(100);
            black_box(result);
        });
    });
    
    // With OTLP tracing
    use opentelemetry::{global, trace::Tracer};
    use tokio::runtime::Runtime;
    
    let rt = Runtime::new().unwrap();
    rt.block_on(async {
        mod common;
        use common::*;
        
        let config = BenchmarkConfig::default();
        let _provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("comparison");
        
        group.bench_function("with_otlp_tracing", |b| {
            b.iter(|| {
                let mut span = tracer.span_builder("expensive_operation")
                    .start(&tracer);
                let result = expensive_operation(100);
                span.end();
                black_box(result);
            });
        });
    });
    
    group.finish();
}

fn expensive_operation(iterations: u64) -> u64 {
    let mut sum = 0u64;
    for i in 0..iterations {
        sum = sum.wrapping_add(i * i);
    }
    sum
}

criterion_group!(comparison_benches, overhead_comparison);
criterion_main!(comparison_benches);
```

### 2. æ€§èƒ½å¼€é”€åˆ†æ

```rust
// benches/overhead_analysis.rs
use std::time::{Duration, Instant};
use tokio::runtime::Runtime;
use opentelemetry::{global, trace::Tracer, KeyValue};

mod common;
use common::*;

/// åˆ†æOTLPè¿½è¸ªçš„æ€§èƒ½å¼€é”€
fn main() {
    let rt = Runtime::new().unwrap();
    
    println!("\nğŸ“Š OTLP Tracing Overhead Analysis\n");
    
    rt.block_on(async {
        let num_iterations = 100_000;
        
        // 1. Baseline (æ— è¿½è¸ª)
        println!("Running baseline (no tracing)...");
        let baseline_start = Instant::now();
        for i in 0..num_iterations {
            simulate_work(i);
        }
        let baseline_duration = baseline_start.elapsed();
        let baseline_per_op = baseline_duration.as_nanos() as f64 / num_iterations as f64;
        
        println!("Baseline: {:?} ({:.2} ns/op)", baseline_duration, baseline_per_op);
        
        // 2. With OTLP tracing
        println!("\nRunning with OTLP tracing...");
        let config = BenchmarkConfig::default();
        let provider = init_tracer_for_benchmark(&config);
        let tracer = global::tracer("overhead_analysis");
        
        let traced_start = Instant::now();
        for i in 0..num_iterations {
            let mut span = tracer.span_builder(format!("work-{}", i))
                .with_attributes(vec![
                    KeyValue::new("iteration", i as i64),
                ])
                .start(&tracer);
            simulate_work(i);
            span.end();
        }
        let traced_duration = traced_start.elapsed();
        let traced_per_op = traced_duration.as_nanos() as f64 / num_iterations as f64;
        
        println!("Traced: {:?} ({:.2} ns/op)", traced_duration, traced_per_op);
        
        // 3. è®¡ç®—å¼€é”€
        let overhead = traced_duration - baseline_duration;
        let overhead_per_op = traced_per_op - baseline_per_op;
        let overhead_percent = (overhead_per_op / baseline_per_op) * 100.0;
        
        println!("\nğŸ“ˆ Overhead Analysis:");
        println!("Total overhead:    {:?}", overhead);
        println!("Per-op overhead:   {:.2} ns", overhead_per_op);
        println!("Overhead percent:  {:.2}%", overhead_percent);
        
        provider.shutdown().unwrap();
    });
}

fn simulate_work(n: u64) {
    let mut sum = 0u64;
    for i in 0..10 {
        sum = sum.wrapping_add(n.wrapping_mul(i));
    }
    std::hint::black_box(sum);
}
```

---

## å®Œæ•´åŸºå‡†æµ‹è¯•å¥—ä»¶

### è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•

```bash
# 1. è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
cargo bench

# 2. è¿è¡Œç‰¹å®šåŸºå‡†æµ‹è¯•
cargo bench --bench traces_benchmark
cargo bench --bench metrics_benchmark
cargo bench --bench throughput_benchmark
cargo bench --bench latency_benchmark

# 3. ç”ŸæˆHTMLæŠ¥å‘Š
cargo bench -- --save-baseline main

# 4. å¯¹æ¯”ä¸¤ä¸ªåŸºçº¿
cargo bench -- --baseline main --baseline feature-x

# 5. è¿è¡Œèµ„æºæ¶ˆè€—æµ‹è¯•
cargo run --release --bin resource_consumption
cargo run --release --bin memory_leak_test
cargo run --release --bin overhead_analysis
```

### æŒç»­é›†æˆä¸­çš„åŸºå‡†æµ‹è¯•

```yaml
# .github/workflows/benchmark.yml
name: Benchmark

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true
      
      - name: Setup OTLP Collector
        run: |
          docker run -d --name otel-collector \
            -p 4317:4317 \
            -p 4318:4318 \
            otel/opentelemetry-collector-contrib:0.95.0
      
      - name: Run benchmarks
        run: cargo bench --no-fail-fast
      
      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'cargo'
          output-file-path: target/criterion/*/new/estimates.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
```

### åŸºå‡†æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿

```markdown
    # OTLP Performance Benchmark Report

    ## Test Environment

    - **Date**: 2025-10-08
    - **Rust Version**: 1.90
    - **OpenTelemetry SDK**: 0.31.0
    - **OS**: Ubuntu 22.04
    - **CPU**: Intel Xeon 8 cores @ 2.4GHz
    - **RAM**: 16GB
    - **Collector**: OpenTelemetry Collector 0.95.0

    ## Summary

    ### Traces Performance

    | Metric | Value | Target | Status |
    |--------|-------|--------|--------|
    | Throughput | 50,000 spans/sec | 10,000 | âœ… Pass |
    | P99 Latency | 120 Âµs | < 1ms | âœ… Pass |
    | CPU Usage | 15% | < 30% | âœ… Pass |
    | Memory | 125 MB | < 200 MB | âœ… Pass |

    ### Metrics Performance

    | Metric | Value | Target | Status |
    |--------|-------|--------|--------|
    | Throughput | 500,000 metrics/sec | 100,000 | âœ… Pass |
    | P99 Latency | 50 Âµs | < 500Âµs | âœ… Pass |
    | CPU Usage | 12% | < 30% | âœ… Pass |
    | Memory | 95 MB | < 200 MB | âœ… Pass |

    ### Logs Performance

    | Metric | Value | Target | Status |
    |--------|-------|--------|--------|
    | Throughput | 100,000 logs/sec | 50,000 | âœ… Pass |
    | P99 Latency | 80 Âµs | < 1ms | âœ… Pass |
    | CPU Usage | 18% | < 30% | âœ… Pass |
    | Memory | 110 MB | < 200 MB | âœ… Pass |

    ## Detailed Results

    ### Latency Distribution

    ```text

    Span Creation Latency:
    P50:  15 Âµs
    P90:  45 Âµs
    P95:  68 Âµs
    P99:  120 Âµs
    P999: 250 Âµs
    Max:  580 Âµs

    ```

    ### Resource Consumption Over Time

    [Graph showing CPU and Memory usage over 60-second test period]

    ### Comparison with Previous Version

    | Metric | Previous | Current | Change |
    |--------|----------|---------|--------|
    | Throughput | 45,000/sec | 50,000/sec | +11% â¬†ï¸ |
    | P99 Latency | 150 Âµs | 120 Âµs | -20% â¬‡ï¸ |
    | Memory | 140 MB | 125 MB | -10% â¬‡ï¸ |

    ## Recommendations

    1. âœ… Performance meets all target requirements
    2. âœ… No memory leaks detected
    3. âœ… Overhead is acceptable (< 5% compared to baseline)
    4. ğŸ”„ Consider increasing batch size for higher throughput
    5. ğŸ”„ Monitor P999 latency in production

    ## Conclusion

    The OTLP implementation demonstrates excellent performance characteristics suitable for production deployment.

```

---

## æ€»ç»“ä¸æœ€ä½³å®è·µ

### åŸºå‡†æµ‹è¯•æœ€ä½³å®è·µ

1. **æŒç»­è¿è¡Œ**: å°†åŸºå‡†æµ‹è¯•é›†æˆåˆ°CI/CDæµç¨‹
2. **å¤šç»´åº¦æµ‹è¯•**: è¦†ç›–ååé‡ã€å»¶è¿Ÿã€èµ„æºæ¶ˆè€—
3. **çœŸå®è´Ÿè½½**: ä½¿ç”¨æ¥è¿‘ç”Ÿäº§ç¯å¢ƒçš„æµ‹è¯•åœºæ™¯
4. **åŸºçº¿å¯¹æ¯”**: å»ºç«‹æ€§èƒ½åŸºçº¿ï¼Œç›‘æ§å›å½’
5. **æ–‡æ¡£åŒ–**: è¯¦ç»†è®°å½•æµ‹è¯•ç¯å¢ƒå’Œç»“æœ

### æ€§èƒ½ä¼˜åŒ–æŒ‡å—

```rust
/// æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•
const PERFORMANCE_CHECKLIST: &[&str] = &[
    "âœ… ä½¿ç”¨æ‰¹é‡å¯¼å‡ºè€ŒéåŒæ­¥å¯¼å‡º",
    "âœ… åˆç†é…ç½®æ‰¹å¤„ç†å‚æ•° (batch_size, schedule_delay)",
    "âœ… ä½¿ç”¨é€‚å½“çš„é‡‡æ ·ç­–ç•¥",
    "âœ… é¿å…é«˜åŸºæ•°æ ‡ç­¾",
    "âœ… å¤ç”¨Tracer/Meterå®ä¾‹",
    "âœ… å¼‚æ­¥å¯¼å‡ºï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹",
    "âœ… ç›‘æ§é˜Ÿåˆ—å¤§å°ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º",
    "âœ… è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´",
    "âœ… ä½¿ç”¨è¿æ¥æ± å¤ç”¨ç½‘ç»œè¿æ¥",
    "âœ… å®šæœŸè¿›è¡Œæ€§èƒ½å›å½’æµ‹è¯•",
];
```

### æ€§èƒ½ç›®æ ‡å‚è€ƒ

| ç±»å‹ | ååé‡ç›®æ ‡ | å»¶è¿Ÿç›®æ ‡ (P99) | èµ„æºç›®æ ‡ |
|------|------------|----------------|----------|
| Traces | 10,000+ spans/sec | < 1ms | < 200MB |
| Metrics | 100,000+ metrics/sec | < 500Âµs | < 200MB |
| Logs | 50,000+ logs/sec | < 1ms | < 200MB |

### å…³é”®è¦ç‚¹

1. **åŸºå‡†æµ‹è¯•æ˜¯å¿…éœ€çš„**: ä¸ºæ€§èƒ½ä¼˜åŒ–æä¾›é‡åŒ–ä¾æ®
2. **å¤šç»´åº¦è¯„ä¼°**: ä¸ä»…å…³æ³¨ååé‡ï¼Œä¹Ÿå…³æ³¨å»¶è¿Ÿå’Œèµ„æºæ¶ˆè€—
3. **çœŸå®åœºæ™¯**: æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒè´Ÿè½½ç‰¹å¾
4. **æŒç»­ç›‘æ§**: åŠæ—¶å‘ç°æ€§èƒ½é€€åŒ–
5. **ä¼˜åŒ–å¹³è¡¡**: åœ¨æ€§èƒ½ã€å¯é æ€§å’Œå¯ç»´æŠ¤æ€§ä¹‹é—´å–å¾—å¹³è¡¡

---

## å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- [Criterion.rs Documentation](https://bheisler.github.io/criterion.rs/book/)
- [OpenTelemetry Performance](https://opentelemetry.io/docs/specs/otel/performance/)

### ç›¸å…³å·¥å…·

- **Criterion**: RuståŸºå‡†æµ‹è¯•æ¡†æ¶
- **pprof**: CPU profiling
- **valgrind**: å†…å­˜åˆ†æ
- **perf**: Linuxæ€§èƒ½åˆ†æå·¥å…·

### ç¤¾åŒºèµ„æº

- [OpenTelemetry Benchmark Suite](https://github.com/open-telemetry/opentelemetry-benchmark)
- [Rust Performance Book](https://nnethercote.github.io/perf-book/)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´10æœˆ8æ—¥  
**çŠ¶æ€**: âœ… å®Œæˆ  
**é¢„è®¡è¡Œæ•°**: 3,600+ è¡Œ

---

**#OTLP #Rust #Performance #Benchmarking #Criterion #Throughput #Latency #Profiling**-
