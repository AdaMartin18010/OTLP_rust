# ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–å®æˆ˜

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025å¹´10æœˆ8æ—¥  
> **Rustç‰ˆæœ¬**: 1.90  
> **OpenTelemetryç‰ˆæœ¬**: 0.31.0  
> **æ–‡æ¡£ç±»å‹**: Production Performance Optimization

---

## ğŸ“‹ ç›®å½•

- [ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–å®æˆ˜](#ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–å®æˆ˜)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [æ¦‚è¿°](#æ¦‚è¿°)
    - [ä¸ºä»€ä¹ˆéœ€è¦ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–ï¼Ÿ](#ä¸ºä»€ä¹ˆéœ€è¦ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–)
    - [æ€§èƒ½ä¼˜åŒ–æ–¹æ³•è®º](#æ€§èƒ½ä¼˜åŒ–æ–¹æ³•è®º)
  - [æ€§èƒ½ç“¶é¢ˆè¯†åˆ«](#æ€§èƒ½ç“¶é¢ˆè¯†åˆ«)
    - [1. ç³»ç»Ÿçº§æ€§èƒ½åˆ†æå·¥å…·](#1-ç³»ç»Ÿçº§æ€§èƒ½åˆ†æå·¥å…·)
    - [2. ä½¿ç”¨pprofè¿›è¡ŒCPUæ€§èƒ½åˆ†æ](#2-ä½¿ç”¨pprofè¿›è¡Œcpuæ€§èƒ½åˆ†æ)
    - [3. å†…å­˜åˆ†æå·¥å…·](#3-å†…å­˜åˆ†æå·¥å…·)
    - [4. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«æ¸…å•](#4-æ€§èƒ½ç“¶é¢ˆè¯†åˆ«æ¸…å•)
  - [CPUä¼˜åŒ–](#cpuä¼˜åŒ–)
    - [1. å‡å°‘Spanåˆ›å»ºå¼€é”€](#1-å‡å°‘spanåˆ›å»ºå¼€é”€)
    - [2. ä¼˜åŒ–Contextä¼ æ’­](#2-ä¼˜åŒ–contextä¼ æ’­)
    - [3. å‡å°‘å±æ€§åºåˆ—åŒ–å¼€é”€](#3-å‡å°‘å±æ€§åºåˆ—åŒ–å¼€é”€)
  - [å†…å­˜ä¼˜åŒ–](#å†…å­˜ä¼˜åŒ–)
    - [1. ä¼˜åŒ–æ‰¹å¤„ç†é˜Ÿåˆ—å¤§å°](#1-ä¼˜åŒ–æ‰¹å¤„ç†é˜Ÿåˆ—å¤§å°)
    - [2. å®ç°å¯¹è±¡æ± ](#2-å®ç°å¯¹è±¡æ± )
    - [3. å†…å­˜æ³„æ¼æ£€æµ‹ä¸ä¿®å¤](#3-å†…å­˜æ³„æ¼æ£€æµ‹ä¸ä¿®å¤)
  - [ç½‘ç»œä¼˜åŒ–](#ç½‘ç»œä¼˜åŒ–)
    - [1. è¿æ¥æ± ä¼˜åŒ–](#1-è¿æ¥æ± ä¼˜åŒ–)
    - [2. å‹ç¼©ä¼˜åŒ–](#2-å‹ç¼©ä¼˜åŒ–)
    - [3. é‡è¯•ç­–ç•¥ä¼˜åŒ–](#3-é‡è¯•ç­–ç•¥ä¼˜åŒ–)
  - [æ‰¹å¤„ç†ä¼˜åŒ–](#æ‰¹å¤„ç†ä¼˜åŒ–)
    - [1. åŠ¨æ€æ‰¹å¤„ç†å¤§å°è°ƒæ•´](#1-åŠ¨æ€æ‰¹å¤„ç†å¤§å°è°ƒæ•´)
    - [2. æ™ºèƒ½æ‰¹å¤„ç†è°ƒåº¦](#2-æ™ºèƒ½æ‰¹å¤„ç†è°ƒåº¦)
  - [é‡‡æ ·ç­–ç•¥ä¼˜åŒ–](#é‡‡æ ·ç­–ç•¥ä¼˜åŒ–)
    - [1. æ™ºèƒ½é‡‡æ ·å™¨](#1-æ™ºèƒ½é‡‡æ ·å™¨)
  - [ç”Ÿäº§ç¯å¢ƒå®æˆ˜æ¡ˆä¾‹](#ç”Ÿäº§ç¯å¢ƒå®æˆ˜æ¡ˆä¾‹)
    - [æ¡ˆä¾‹1: é«˜æµé‡WebæœåŠ¡ä¼˜åŒ–](#æ¡ˆä¾‹1-é«˜æµé‡webæœåŠ¡ä¼˜åŒ–)
    - [æ¡ˆä¾‹2: å¾®æœåŠ¡é“¾è·¯è¿½è¸ªä¼˜åŒ–](#æ¡ˆä¾‹2-å¾®æœåŠ¡é“¾è·¯è¿½è¸ªä¼˜åŒ–)
    - [æ¡ˆä¾‹3: Batch Exportè¶…æ—¶ä¼˜åŒ–](#æ¡ˆä¾‹3-batch-exportè¶…æ—¶ä¼˜åŒ–)
  - [æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦](#æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦)
    - [1. å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆKPIsï¼‰](#1-å…³é”®æ€§èƒ½æŒ‡æ ‡kpis)
    - [2. æ€§èƒ½å‘Šè­¦è§„åˆ™](#2-æ€§èƒ½å‘Šè­¦è§„åˆ™)
    - [3. æ€§èƒ½Dashboard](#3-æ€§èƒ½dashboard)
  - [æ€»ç»“ä¸æœ€ä½³å®è·µ](#æ€»ç»“ä¸æœ€ä½³å®è·µ)
    - [æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒåŸåˆ™](#æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒåŸåˆ™)
    - [ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•](#ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•)
    - [å…³é”®è¦ç‚¹æ€»ç»“](#å…³é”®è¦ç‚¹æ€»ç»“)
  - [å‚è€ƒèµ„æº](#å‚è€ƒèµ„æº)
    - [å·¥å…·å’Œæ¡†æ¶](#å·¥å…·å’Œæ¡†æ¶)
    - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)

---

## æ¦‚è¿°

### ä¸ºä»€ä¹ˆéœ€è¦ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–ï¼Ÿ

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒOTLPå®ç°çš„æ€§èƒ½ç›´æ¥å½±å“åˆ°ï¼š

1. **åº”ç”¨æ€§èƒ½**: è¿½è¸ªå¼€é”€åº”è¯¥æœ€å°åŒ–
2. **æˆæœ¬æ•ˆç›Š**: èµ„æºä½¿ç”¨ç›´æ¥å½±å“è¿è¥æˆæœ¬
3. **ç³»ç»Ÿç¨³å®šæ€§**: ä¸å½“çš„é…ç½®å¯èƒ½å¯¼è‡´ç³»ç»Ÿé—®é¢˜
4. **å¯è§‚æµ‹æ€§è´¨é‡**: æ€§èƒ½é—®é¢˜å¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±

### æ€§èƒ½ä¼˜åŒ–æ–¹æ³•è®º

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Performance Optimization Methodology       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  1. Measure (æµ‹é‡)                              â”‚
â”‚     â””â”€ å»ºç«‹æ€§èƒ½åŸºçº¿                             â”‚
â”‚     â””â”€ è¯†åˆ«ç“¶é¢ˆ                                 â”‚
â”‚                                                 â”‚
â”‚  2. Analyze (åˆ†æ)                              â”‚
â”‚     â””â”€ æ·±å…¥åˆ†ææ ¹å›                              â”‚
â”‚     â””â”€ è¯„ä¼°ä¼˜åŒ–æ–¹æ¡ˆ                             â”‚
â”‚                                                 â”‚
â”‚  3. Optimize (ä¼˜åŒ–)                             â”‚
â”‚     â””â”€ å®æ–½ä¼˜åŒ–æªæ–½                             â”‚
â”‚     â””â”€ éªŒè¯æ•ˆæœ                                 â”‚
â”‚                                                 â”‚
â”‚  4. Monitor (ç›‘æ§)                              â”‚
â”‚     â””â”€ æŒç»­ç›‘æ§æ€§èƒ½                             â”‚
â”‚     â””â”€ é˜²æ­¢æ€§èƒ½é€€åŒ–                             â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æ€§èƒ½ç“¶é¢ˆè¯†åˆ«

### 1. ç³»ç»Ÿçº§æ€§èƒ½åˆ†æå·¥å…·

```rust
// src/performance/profiler.rs
use std::time::{Duration, Instant};
use std::sync::Arc;
use std::sync::atomic::{AtomicU64, Ordering};
use sysinfo::{System, SystemExt, ProcessExt};

/// æ€§èƒ½åˆ†æå™¨
pub struct PerformanceProfiler {
    system: System,
    start_time: Instant,
    spans_created: Arc<AtomicU64>,
    spans_exported: Arc<AtomicU64>,
    export_errors: Arc<AtomicU64>,
}

impl PerformanceProfiler {
    pub fn new() -> Self {
        Self {
            system: System::new_all(),
            start_time: Instant::now(),
            spans_created: Arc::new(AtomicU64::new(0)),
            spans_exported: Arc::new(AtomicU64::new(0)),
            export_errors: Arc::new(AtomicU64::new(0)),
        }
    }
    
    /// è®°å½•Spanåˆ›å»º
    pub fn record_span_created(&self) {
        self.spans_created.fetch_add(1, Ordering::Relaxed);
    }
    
    /// è®°å½•Spanå¯¼å‡º
    pub fn record_span_exported(&self, count: u64) {
        self.spans_exported.fetch_add(count, Ordering::Relaxed);
    }
    
    /// è®°å½•å¯¼å‡ºé”™è¯¯
    pub fn record_export_error(&self) {
        self.export_errors.fetch_add(1, Ordering::Relaxed);
    }
    
    /// è·å–æ€§èƒ½æŠ¥å‘Š
    pub fn report(&mut self) -> PerformanceReport {
        self.system.refresh_all();
        
        let elapsed = self.start_time.elapsed();
        let spans_created = self.spans_created.load(Ordering::Relaxed);
        let spans_exported = self.spans_exported.load(Ordering::Relaxed);
        let export_errors = self.export_errors.load(Ordering::Relaxed);
        
        let process = self.system.process(sysinfo::get_current_pid().unwrap())
            .expect("Failed to get process");
        
        PerformanceReport {
            elapsed,
            spans_created,
            spans_exported,
            export_errors,
            cpu_usage: process.cpu_usage(),
            memory_mb: process.memory() as f64 / 1024.0 / 1024.0,
            throughput: spans_created as f64 / elapsed.as_secs_f64(),
        }
    }
}

#[derive(Debug, Clone)]
pub struct PerformanceReport {
    pub elapsed: Duration,
    pub spans_created: u64,
    pub spans_exported: u64,
    pub export_errors: u64,
    pub cpu_usage: f32,
    pub memory_mb: f64,
    pub throughput: f64,
}

impl PerformanceReport {
    pub fn print(&self) {
        println!("\nğŸ“Š Performance Report");
        println!("========================");
        println!("Duration:         {:.2}s", self.elapsed.as_secs_f64());
        println!("Spans Created:    {}", self.spans_created);
        println!("Spans Exported:   {}", self.spans_exported);
        println!("Export Errors:    {}", self.export_errors);
        println!("Throughput:       {:.2} spans/sec", self.throughput);
        println!("CPU Usage:        {:.2}%", self.cpu_usage);
        println!("Memory Usage:     {:.2} MB", self.memory_mb);
        println!("========================\n");
    }
}
```

### 2. ä½¿ç”¨pprofè¿›è¡ŒCPUæ€§èƒ½åˆ†æ

```rust
// Cargo.toml
[dependencies]
pprof = { version = "0.13", features = ["flamegraph", "criterion"] }

// src/performance/cpu_profiling.rs
use pprof::ProfilerGuard;
use std::fs::File;
use std::io::Write;

/// CPUæ€§èƒ½åˆ†æ
pub fn profile_cpu_intensive_operation() -> Result<(), Box<dyn std::error::Error>> {
    // å¯åŠ¨CPU profiler
    let guard = pprof::ProfilerGuardBuilder::default()
        .frequency(1000) // é‡‡æ ·é¢‘ç‡: 1000 Hz
        .blocklist(&["libc", "libpthread"])
        .build()?;
    
    // æ‰§è¡Œéœ€è¦åˆ†æçš„æ“ä½œ
    perform_traced_operations();
    
    // ç”ŸæˆæŠ¥å‘Š
    if let Ok(report) = guard.report().build() {
        // ç”Ÿæˆflamegraph
        let file = File::create("flamegraph.svg")?;
        report.flamegraph(file)?;
        
        // æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        println!("CPU Profile:");
        for (name, count) in report.data.iter().take(20) {
            println!("  {}: {} samples", name, count);
        }
    }
    
    Ok(())
}

fn perform_traced_operations() {
    use opentelemetry::{global, trace::Tracer, KeyValue};
    
    let tracer = global::tracer("profiling");
    
    for i in 0..10_000 {
        let mut span = tracer.span_builder(format!("operation-{}", i))
            .with_attributes(vec![
                KeyValue::new("iteration", i as i64),
            ])
            .start(&tracer);
        
        // æ¨¡æ‹Ÿä¸šåŠ¡é€»è¾‘
        simulate_work(100);
        
        span.end();
    }
}

fn simulate_work(iterations: u64) {
    let mut sum = 0u64;
    for i in 0..iterations {
        sum = sum.wrapping_add(i * i);
    }
    std::hint::black_box(sum);
}
```

### 3. å†…å­˜åˆ†æå·¥å…·

```rust
// ä½¿ç”¨dhatè¿›è¡Œå †åˆ†æ
// Cargo.toml
[dependencies]
dhat = "0.3"

// src/main.rs
#[global_allocator]
static ALLOC: dhat::Alloc = dhat::Alloc;

fn main() {
    let _profiler = dhat::Profiler::new_heap();
    
    // åº”ç”¨ä»£ç 
    run_application();
    
    // dhatä¼šåœ¨ç¨‹åºé€€å‡ºæ—¶ç”Ÿæˆdhat-heap.json
}

// åˆ†æå†…å­˜åˆ†é…çƒ­ç‚¹
pub fn analyze_memory_hotspots() {
    use opentelemetry::{global, trace::Tracer, KeyValue};
    
    let tracer = global::tracer("memory_analysis");
    
    // åˆ›å»ºå¤§é‡spans
    let mut spans = Vec::new();
    for i in 0..100_000 {
        let span = tracer.span_builder(format!("span-{}", i))
            .with_attributes(vec![
                KeyValue::new("id", i as i64),
                KeyValue::new("type", "analysis"),
            ])
            .start(&tracer);
        
        spans.push(span);
        
        // æ¯1000ä¸ªspansï¼Œæ¸…ç©ºä¸€æ¬¡
        if spans.len() >= 1000 {
            spans.clear();
        }
    }
}
```

### 4. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«æ¸…å•

```rust
/// æ€§èƒ½ç“¶é¢ˆè¯†åˆ«æ£€æŸ¥æ¸…å•
pub struct BottleneckChecklist {
    pub items: Vec<BottleneckCheck>,
}

#[derive(Debug)]
pub struct BottleneckCheck {
    pub category: String,
    pub description: String,
    pub how_to_check: String,
}

impl BottleneckChecklist {
    pub fn new() -> Self {
        Self {
            items: vec![
                BottleneckCheck {
                    category: "CPU".to_string(),
                    description: "Spanåˆ›å»ºå’Œå¤„ç†æ¶ˆè€—å¤§é‡CPU".to_string(),
                    how_to_check: "ä½¿ç”¨pprofç”Ÿæˆflamegraphï¼ŒæŸ¥çœ‹çƒ­ç‚¹å‡½æ•°".to_string(),
                },
                BottleneckCheck {
                    category: "Memory".to_string(),
                    description: "å†…å­˜ä½¿ç”¨æŒç»­å¢é•¿".to_string(),
                    how_to_check: "ä½¿ç”¨dhatåˆ†æå †åˆ†é…ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜æ³„æ¼".to_string(),
                },
                BottleneckCheck {
                    category: "Network".to_string(),
                    description: "å¯¼å‡ºå»¶è¿Ÿè¿‡é«˜".to_string(),
                    how_to_check: "ç›‘æ§export_durationæŒ‡æ ‡ï¼Œæ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ".to_string(),
                },
                BottleneckCheck {
                    category: "Batching".to_string(),
                    description: "æ‰¹å¤„ç†é…ç½®ä¸åˆç†".to_string(),
                    how_to_check: "æ£€æŸ¥batch_sizeå’Œschedule_delayé…ç½®".to_string(),
                },
                BottleneckCheck {
                    category: "Sampling".to_string(),
                    description: "é‡‡æ ·ç‡è¿‡é«˜å¯¼è‡´å¼€é”€å¤§".to_string(),
                    how_to_check: "æ£€æŸ¥é‡‡æ ·é…ç½®ï¼Œè¯„ä¼°é‡‡æ ·ç‡".to_string(),
                },
                BottleneckCheck {
                    category: "Queue".to_string(),
                    description: "é˜Ÿåˆ—ç§¯å‹å¯¼è‡´å†…å­˜å¢é•¿".to_string(),
                    how_to_check: "ç›‘æ§queue_sizeæŒ‡æ ‡".to_string(),
                },
                BottleneckCheck {
                    category: "Cardinality".to_string(),
                    description: "é«˜åŸºæ•°æ ‡ç­¾å¯¼è‡´å†…å­˜å’ŒCPUå¼€é”€".to_string(),
                    how_to_check: "å®¡æŸ¥æ ‡ç­¾ä½¿ç”¨ï¼Œè¯†åˆ«é«˜åŸºæ•°æ ‡ç­¾".to_string(),
                },
            ],
        }
    }
    
    pub fn print(&self) {
        println!("\nğŸ” Performance Bottleneck Checklist\n");
        for check in &self.items {
            println!("ğŸ“Œ {}", check.category);
            println!("   Issue: {}", check.description);
            println!("   Check: {}", check.how_to_check);
            println!();
        }
    }
}
```

---

## CPUä¼˜åŒ–

### 1. å‡å°‘Spanåˆ›å»ºå¼€é”€

```rust
// src/optimization/span_optimization.rs
use opentelemetry::{global, trace::{Tracer, Span}, KeyValue};

/// ä¼˜åŒ–å‰ï¼šæ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„å±æ€§Vec
pub fn create_span_unoptimized(tracer: &dyn Tracer, name: &str, user_id: i64) {
    let mut span = tracer.span_builder(name)
        .with_attributes(vec![
            KeyValue::new("service.name", "my-service"),
            KeyValue::new("user.id", user_id),
        ])
        .start(tracer);
    span.end();
}

/// ä¼˜åŒ–åï¼šå¤ç”¨é€šç”¨å±æ€§
pub struct SpanFactory {
    common_attributes: Vec<KeyValue>,
}

impl SpanFactory {
    pub fn new(service_name: &str) -> Self {
        Self {
            common_attributes: vec![
                KeyValue::new("service.name", service_name.to_string()),
            ],
        }
    }
    
    pub fn create_span(&self, tracer: &dyn Tracer, name: &str, user_id: i64) -> impl Span {
        let mut attrs = self.common_attributes.clone();
        attrs.push(KeyValue::new("user.id", user_id));
        
        tracer.span_builder(name)
            .with_attributes(attrs)
            .start(tracer)
    }
}

/// è¿›ä¸€æ­¥ä¼˜åŒ–ï¼šä½¿ç”¨Arcå…±äº«å±æ€§
use std::sync::Arc;

pub struct OptimizedSpanFactory {
    common_attributes: Arc<Vec<KeyValue>>,
}

impl OptimizedSpanFactory {
    pub fn new(service_name: &str) -> Self {
        Self {
            common_attributes: Arc::new(vec![
                KeyValue::new("service.name", service_name.to_string()),
            ]),
        }
    }
    
    pub fn create_span(&self, tracer: &dyn Tracer, name: &str, user_id: i64) -> impl Span {
        let mut attrs = (*self.common_attributes).clone();
        attrs.push(KeyValue::new("user.id", user_id));
        
        tracer.span_builder(name)
            .with_attributes(attrs)
            .start(tracer)
    }
}
```

### 2. ä¼˜åŒ–Contextä¼ æ’­

```rust
// src/optimization/context_propagation.rs
use opentelemetry::{global, Context};
use opentelemetry::propagation::{TextMapPropagator, Injector, Extractor};
use opentelemetry_sdk::propagation::TraceContextPropagator;
use std::collections::HashMap;

/// ä¼˜åŒ–å‰ï¼šæ¯æ¬¡éƒ½åˆ›å»ºæ–°çš„propagator
pub fn inject_context_unoptimized(cx: &Context, headers: &mut HashMap<String, String>) {
    let propagator = TraceContextPropagator::new();
    
    struct HashMapInjector<'a>(&'a mut HashMap<String, String>);
    impl<'a> Injector for HashMapInjector<'a> {
        fn set(&mut self, key: &str, value: String) {
            self.0.insert(key.to_string(), value);
        }
    }
    
    propagator.inject_context(cx, &mut HashMapInjector(headers));
}

/// ä¼˜åŒ–åï¼šä½¿ç”¨å…¨å±€propagator
pub fn inject_context_optimized(cx: &Context, headers: &mut HashMap<String, String>) {
    struct HashMapInjector<'a>(&'a mut HashMap<String, String>);
    impl<'a> Injector for HashMapInjector<'a> {
        fn set(&mut self, key: &str, value: String) {
            self.0.insert(key.to_string(), value);
        }
    }
    
    // ä½¿ç”¨global propagatorï¼Œé¿å…æ¯æ¬¡åˆ›å»º
    global::get_text_map_propagator(|propagator| {
        propagator.inject_context(cx, &mut HashMapInjector(headers));
    });
}

/// è¿›ä¸€æ­¥ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†
pub struct ContextPropagationBatcher {
    pending_contexts: Vec<(Context, HashMap<String, String>)>,
}

impl ContextPropagationBatcher {
    pub fn new() -> Self {
        Self {
            pending_contexts: Vec::with_capacity(100),
        }
    }
    
    pub fn add(&mut self, cx: Context, headers: HashMap<String, String>) {
        self.pending_contexts.push((cx, headers));
        
        if self.pending_contexts.len() >= 100 {
            self.flush();
        }
    }
    
    pub fn flush(&mut self) {
        struct HashMapInjector<'a>(&'a mut HashMap<String, String>);
        impl<'a> Injector for HashMapInjector<'a> {
            fn set(&mut self, key: &str, value: String) {
                self.0.insert(key.to_string(), value);
            }
        }
        
        global::get_text_map_propagator(|propagator| {
            for (cx, mut headers) in self.pending_contexts.drain(..) {
                propagator.inject_context(&cx, &mut HashMapInjector(&mut headers));
            }
        });
    }
}
```

### 3. å‡å°‘å±æ€§åºåˆ—åŒ–å¼€é”€

```rust
// src/optimization/attribute_optimization.rs
use opentelemetry::KeyValue;
use std::sync::Arc;

/// å±æ€§æ± ï¼šå¤ç”¨å¸¸è§å±æ€§
pub struct AttributePool {
    common_attrs: HashMap<String, Arc<KeyValue>>,
}

impl AttributePool {
    pub fn new() -> Self {
        let mut pool = Self {
            common_attrs: HashMap::new(),
        };
        
        // é¢„åˆ›å»ºå¸¸è§å±æ€§
        pool.register("http.method", "GET");
        pool.register("http.method", "POST");
        pool.register("http.status", "200");
        pool.register("http.status", "404");
        pool.register("http.status", "500");
        
        pool
    }
    
    fn register(&mut self, key: &str, value: &str) {
        let attr_key = format!("{}={}", key, value);
        self.common_attrs.insert(
            attr_key,
            Arc::new(KeyValue::new(key.to_string(), value.to_string())),
        );
    }
    
    pub fn get(&self, key: &str, value: &str) -> Option<Arc<KeyValue>> {
        let attr_key = format!("{}={}", key, value);
        self.common_attrs.get(&attr_key).cloned()
    }
}

use std::collections::HashMap;
use once_cell::sync::Lazy;

static ATTRIBUTE_POOL: Lazy<AttributePool> = Lazy::new(|| AttributePool::new());

/// ä½¿ç”¨å±æ€§æ± 
pub fn create_optimized_attributes(method: &str, status: &str) -> Vec<KeyValue> {
    let mut attrs = Vec::new();
    
    if let Some(attr) = ATTRIBUTE_POOL.get("http.method", method) {
        attrs.push((*attr).clone());
    } else {
        attrs.push(KeyValue::new("http.method", method.to_string()));
    }
    
    if let Some(attr) = ATTRIBUTE_POOL.get("http.status", status) {
        attrs.push((*attr).clone());
    } else {
        attrs.push(KeyValue::new("http.status", status.to_string()));
    }
    
    attrs
}
```

---

## å†…å­˜ä¼˜åŒ–

### 1. ä¼˜åŒ–æ‰¹å¤„ç†é˜Ÿåˆ—å¤§å°

```rust
// src/optimization/batch_optimization.rs
use opentelemetry_sdk::trace::{BatchConfig, TracerProvider};
use std::time::Duration;

/// æ ¹æ®ç³»ç»Ÿèµ„æºåŠ¨æ€é…ç½®æ‰¹å¤„ç†å‚æ•°
pub fn create_optimized_batch_config() -> BatchConfig {
    use sysinfo::{System, SystemExt};
    
    let mut sys = System::new_all();
    sys.refresh_all();
    
    let total_memory_gb = sys.total_memory() as f64 / 1024.0 / 1024.0 / 1024.0;
    
    // æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´é…ç½®
    let (max_queue_size, max_export_batch_size) = if total_memory_gb >= 16.0 {
        // å¤§å†…å­˜ç³»ç»Ÿï¼šä½¿ç”¨æ›´å¤§çš„é˜Ÿåˆ—
        (8192, 1024)
    } else if total_memory_gb >= 8.0 {
        // ä¸­ç­‰å†…å­˜ç³»ç»Ÿ
        (4096, 512)
    } else {
        // å°å†…å­˜ç³»ç»Ÿï¼šä½¿ç”¨è¾ƒå°çš„é˜Ÿåˆ—
        (2048, 256)
    };
    
    BatchConfig::default()
        .with_max_queue_size(max_queue_size)
        .with_max_export_batch_size(max_export_batch_size)
        .with_scheduled_delay(Duration::from_millis(500))
        .with_max_export_timeout(Duration::from_secs(30))
}

/// ç›‘æ§é˜Ÿåˆ—ä½¿ç”¨æƒ…å†µ
pub struct QueueMonitor {
    current_size: std::sync::atomic::AtomicUsize,
    max_size: usize,
    high_watermark: usize,
}

impl QueueMonitor {
    pub fn new(max_size: usize) -> Self {
        Self {
            current_size: std::sync::atomic::AtomicUsize::new(0),
            max_size,
            high_watermark: (max_size as f64 * 0.8) as usize,
        }
    }
    
    pub fn record_enqueue(&self) {
        use std::sync::atomic::Ordering;
        let size = self.current_size.fetch_add(1, Ordering::Relaxed);
        
        if size >= self.high_watermark {
            tracing::warn!(
                "Queue approaching capacity: {}/{} ({:.1}%)",
                size,
                self.max_size,
                (size as f64 / self.max_size as f64) * 100.0
            );
        }
    }
    
    pub fn record_dequeue(&self, count: usize) {
        use std::sync::atomic::Ordering;
        self.current_size.fetch_sub(count, Ordering::Relaxed);
    }
    
    pub fn current_utilization(&self) -> f64 {
        use std::sync::atomic::Ordering;
        let size = self.current_size.load(Ordering::Relaxed);
        (size as f64 / self.max_size as f64) * 100.0
    }
}
```

### 2. å®ç°å¯¹è±¡æ± 

```rust
// src/optimization/object_pool.rs
use std::sync::{Arc, Mutex};

/// Spanæ„å»ºå™¨å¯¹è±¡æ± 
pub struct SpanBuilderPool {
    pool: Arc<Mutex<Vec<SpanBuilderState>>>,
    max_size: usize,
}

#[derive(Default)]
struct SpanBuilderState {
    // å¤ç”¨çš„æ„å»ºå™¨çŠ¶æ€
    attributes: Vec<KeyValue>,
}

impl SpanBuilderPool {
    pub fn new(max_size: usize) -> Self {
        let mut pool = Vec::with_capacity(max_size);
        for _ in 0..max_size {
            pool.push(SpanBuilderState::default());
        }
        
        Self {
            pool: Arc::new(Mutex::new(pool)),
            max_size,
        }
    }
    
    pub fn acquire(&self) -> Option<SpanBuilderState> {
        self.pool.lock().unwrap().pop()
    }
    
    pub fn release(&self, mut state: SpanBuilderState) {
        // æ¸…ç©ºçŠ¶æ€
        state.attributes.clear();
        
        let mut pool = self.pool.lock().unwrap();
        if pool.len() < self.max_size {
            pool.push(state);
        }
    }
}

use opentelemetry::KeyValue;

/// ä½¿ç”¨å¯¹è±¡æ± çš„ç¤ºä¾‹
pub fn create_span_with_pool(pool: &SpanBuilderPool, name: &str) {
    if let Some(mut state) = pool.acquire() {
        // ä½¿ç”¨å¤ç”¨çš„çŠ¶æ€
        state.attributes.push(KeyValue::new("span.name", name.to_string()));
        
        // ... åˆ›å»ºspan
        
        // å½’è¿˜åˆ°æ± ä¸­
        pool.release(state);
    } else {
        // æ± è€—å°½ï¼Œåˆ›å»ºæ–°å¯¹è±¡
        let state = SpanBuilderState::default();
        // ... åˆ›å»ºspan
    }
}
```

### 3. å†…å­˜æ³„æ¼æ£€æµ‹ä¸ä¿®å¤

```rust
// src/optimization/leak_detection.rs
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::time::{Duration, Instant};

/// å†…å­˜æ³„æ¼æ£€æµ‹å™¨
pub struct LeakDetector {
    baseline_memory: Arc<AtomicUsize>,
    last_check: Arc<Mutex<Instant>>,
    check_interval: Duration,
}

use std::sync::Mutex;

impl LeakDetector {
    pub fn new(check_interval: Duration) -> Self {
        use sysinfo::{System, SystemExt, ProcessExt};
        
        let mut sys = System::new_all();
        sys.refresh_all();
        
        let process = sys.process(sysinfo::get_current_pid().unwrap())
            .expect("Failed to get process");
        
        let baseline = process.memory() as usize;
        
        Self {
            baseline_memory: Arc::new(AtomicUsize::new(baseline)),
            last_check: Arc::new(Mutex::new(Instant::now())),
            check_interval,
        }
    }
    
    pub fn check(&self) -> Option<LeakReport> {
        let mut last_check = self.last_check.lock().unwrap();
        
        if last_check.elapsed() < self.check_interval {
            return None;
        }
        
        use sysinfo::{System, SystemExt, ProcessExt};
        
        let mut sys = System::new_all();
        sys.refresh_all();
        
        let process = sys.process(sysinfo::get_current_pid().unwrap())
            .expect("Failed to get process");
        
        let current_memory = process.memory() as usize;
        let baseline = self.baseline_memory.load(Ordering::Relaxed);
        let growth = current_memory as i64 - baseline as i64;
        let growth_rate = growth as f64 / baseline as f64;
        
        *last_check = Instant::now();
        
        Some(LeakReport {
            current_memory_mb: current_memory as f64 / 1024.0 / 1024.0,
            baseline_memory_mb: baseline as f64 / 1024.0 / 1024.0,
            growth_mb: growth as f64 / 1024.0 / 1024.0,
            growth_rate,
            potential_leak: growth_rate > 0.5, // å¢é•¿è¶…è¿‡50%è®¤ä¸ºå¯èƒ½æ³„æ¼
        })
    }
}

#[derive(Debug)]
pub struct LeakReport {
    pub current_memory_mb: f64,
    pub baseline_memory_mb: f64,
    pub growth_mb: f64,
    pub growth_rate: f64,
    pub potential_leak: bool,
}

impl LeakReport {
    pub fn print(&self) {
        println!("\nğŸ” Memory Leak Detection Report");
        println!("Current Memory:   {:.2} MB", self.current_memory_mb);
        println!("Baseline Memory:  {:.2} MB", self.baseline_memory_mb);
        println!("Growth:           {:.2} MB ({:.1}%)", self.growth_mb, self.growth_rate * 100.0);
        
        if self.potential_leak {
            println!("âš ï¸  WARNING: Potential memory leak detected!");
        } else {
            println!("âœ… Memory usage within normal range");
        }
    }
}
```

---

## ç½‘ç»œä¼˜åŒ–

### 1. è¿æ¥æ± ä¼˜åŒ–

```rust
// src/optimization/connection_pool.rs
use tonic::transport::{Channel, Endpoint};
use std::time::Duration;

/// ä¼˜åŒ–OTLP gRPCè¿æ¥é…ç½®
pub async fn create_optimized_channel(endpoint: &str) -> Result<Channel, Box<dyn std::error::Error>> {
    let channel = Endpoint::from_shared(endpoint.to_string())?
        // å¯ç”¨HTTP/2 keepalive
        .http2_keep_alive_interval(Duration::from_secs(30))
        .keep_alive_timeout(Duration::from_secs(10))
        .keep_alive_while_idle(true)
        // è¿æ¥è¶…æ—¶
        .connect_timeout(Duration::from_secs(5))
        // TCPç›¸å…³ä¼˜åŒ–
        .tcp_nodelay(true)
        .tcp_keepalive(Some(Duration::from_secs(30)))
        // å¹¶å‘æµæ•°é‡
        .initial_connection_window_size(1024 * 1024) // 1MB
        .initial_stream_window_size(512 * 1024)      // 512KB
        // è¿æ¥æ± å¤§å°ï¼ˆé€šè¿‡é‡ç”¨è¿æ¥å®ç°ï¼‰
        .connect_lazy();
    
    Ok(channel.connect().await?)
}

/// è¿æ¥å¥åº·æ£€æŸ¥
pub struct ConnectionHealthChecker {
    channel: Channel,
    last_check: Mutex<Instant>,
    check_interval: Duration,
}

use std::sync::Mutex;
use std::time::Instant;

impl ConnectionHealthChecker {
    pub fn new(channel: Channel, check_interval: Duration) -> Self {
        Self {
            channel,
            last_check: Mutex::new(Instant::now()),
            check_interval,
        }
    }
    
    pub async fn check_health(&self) -> Result<(), Box<dyn std::error::Error>> {
        let mut last_check = self.last_check.lock().unwrap();
        
        if last_check.elapsed() < self.check_interval {
            return Ok(());
        }
        
        // å‘é€å¥åº·æ£€æŸ¥è¯·æ±‚
        // è¿™é‡Œå‡è®¾ä½¿ç”¨grpc health checking protocol
        // å®é™…å®ç°å–å†³äºCollectorçš„å¥åº·æ£€æŸ¥ç«¯ç‚¹
        
        *last_check = Instant::now();
        Ok(())
    }
}
```

### 2. å‹ç¼©ä¼˜åŒ–

```rust
// src/optimization/compression.rs
use opentelemetry_otlp::{SpanExporter, WithExportConfig};
use tonic::metadata::MetadataValue;

/// é…ç½®gRPCå‹ç¼©
pub fn create_compressed_exporter(endpoint: &str) -> Result<SpanExporter, Box<dyn std::error::Error>> {
    let mut exporter_builder = opentelemetry_otlp::SpanExporter::builder()
        .with_tonic()
        .with_endpoint(endpoint);
    
    // å¯ç”¨gzipå‹ç¼©
    // æ³¨æ„ï¼šéœ€è¦Collectorç«¯æ”¯æŒ
    let exporter = exporter_builder.build()?;
    
    Ok(exporter)
}

/// è¯„ä¼°å‹ç¼©æ•ˆæœ
pub struct CompressionAnalyzer {
    uncompressed_bytes: Arc<AtomicU64>,
    compressed_bytes: Arc<AtomicU64>,
}

use std::sync::Arc;
use std::sync::atomic::{AtomicU64, Ordering};

impl CompressionAnalyzer {
    pub fn new() -> Self {
        Self {
            uncompressed_bytes: Arc::new(AtomicU64::new(0)),
            compressed_bytes: Arc::new(AtomicU64::new(0)),
        }
    }
    
    pub fn record(&self, uncompressed: u64, compressed: u64) {
        self.uncompressed_bytes.fetch_add(uncompressed, Ordering::Relaxed);
        self.compressed_bytes.fetch_add(compressed, Ordering::Relaxed);
    }
    
    pub fn compression_ratio(&self) -> f64 {
        let uncompressed = self.uncompressed_bytes.load(Ordering::Relaxed);
        let compressed = self.compressed_bytes.load(Ordering::Relaxed);
        
        if uncompressed == 0 {
            return 1.0;
        }
        
        compressed as f64 / uncompressed as f64
    }
    
    pub fn report(&self) {
        let uncompressed = self.uncompressed_bytes.load(Ordering::Relaxed);
        let compressed = self.compressed_bytes.load(Ordering::Relaxed);
        let ratio = self.compression_ratio();
        let savings = ((1.0 - ratio) * 100.0).max(0.0);
        
        println!("\nğŸ“Š Compression Analysis");
        println!("Uncompressed: {} bytes", uncompressed);
        println!("Compressed:   {} bytes", compressed);
        println!("Ratio:        {:.2}x", 1.0 / ratio);
        println!("Savings:      {:.1}%", savings);
    }
}
```

### 3. é‡è¯•ç­–ç•¥ä¼˜åŒ–

```rust
// src/optimization/retry_strategy.rs
use std::time::Duration;
use tokio::time::sleep;

/// æ™ºèƒ½é‡è¯•ç­–ç•¥
pub struct AdaptiveRetryStrategy {
    max_attempts: u32,
    base_delay: Duration,
    max_delay: Duration,
    success_count: Arc<AtomicU32>,
    failure_count: Arc<AtomicU32>,
}

use std::sync::atomic::{AtomicU32, Ordering};

impl AdaptiveRetryStrategy {
    pub fn new(max_attempts: u32) -> Self {
        Self {
            max_attempts,
            base_delay: Duration::from_millis(100),
            max_delay: Duration::from_secs(60),
            success_count: Arc::new(AtomicU32::new(0)),
            failure_count: Arc::new(AtomicU32::new(0)),
        }
    }
    
    /// è®¡ç®—ä¸‹ä¸€æ¬¡é‡è¯•å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ + æŠ–åŠ¨ï¼‰
    fn calculate_delay(&self, attempt: u32) -> Duration {
        use rand::Rng;
        
        // æŒ‡æ•°é€€é¿
        let exponential = self.base_delay.as_millis() as u64 * 2u64.pow(attempt);
        let capped = exponential.min(self.max_delay.as_millis() as u64);
        
        // æ·»åŠ æŠ–åŠ¨ (Â±25%)
        let mut rng = rand::thread_rng();
        let jitter = rng.gen_range(0.75..=1.25);
        let with_jitter = (capped as f64 * jitter) as u64;
        
        Duration::from_millis(with_jitter)
    }
    
    /// æ‰§è¡Œå¸¦é‡è¯•çš„æ“ä½œ
    pub async fn execute<F, Fut, T, E>(&self, mut operation: F) -> Result<T, E>
    where
        F: FnMut() -> Fut,
        Fut: std::future::Future<Output = Result<T, E>>,
    {
        let mut attempt = 0;
        
        loop {
            match operation().await {
                Ok(result) => {
                    self.success_count.fetch_add(1, Ordering::Relaxed);
                    return Ok(result);
                }
                Err(err) if attempt < self.max_attempts - 1 => {
                    self.failure_count.fetch_add(1, Ordering::Relaxed);
                    let delay = self.calculate_delay(attempt);
                    
                    tracing::warn!(
                        "Operation failed (attempt {}/{}), retrying in {:?}",
                        attempt + 1,
                        self.max_attempts,
                        delay
                    );
                    
                    sleep(delay).await;
                    attempt += 1;
                }
                Err(err) => {
                    self.failure_count.fetch_add(1, Ordering::Relaxed);
                    return Err(err);
                }
            }
        }
    }
    
    pub fn success_rate(&self) -> f64 {
        let success = self.success_count.load(Ordering::Relaxed);
        let failure = self.failure_count.load(Ordering::Relaxed);
        let total = success + failure;
        
        if total == 0 {
            return 1.0;
        }
        
        success as f64 / total as f64
    }
}
```

---

## æ‰¹å¤„ç†ä¼˜åŒ–

### 1. åŠ¨æ€æ‰¹å¤„ç†å¤§å°è°ƒæ•´

```rust
// src/optimization/dynamic_batching.rs
use std::sync::Arc;
use std::sync::atomic::{AtomicUsize, Ordering};
use std::time::{Duration, Instant};

/// åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–å™¨
pub struct DynamicBatchOptimizer {
    current_batch_size: Arc<AtomicUsize>,
    min_batch_size: usize,
    max_batch_size: usize,
    target_latency_ms: u64,
    recent_latencies: Arc<Mutex<Vec<Duration>>>,
}

impl DynamicBatchOptimizer {
    pub fn new(initial_size: usize, min: usize, max: usize, target_latency_ms: u64) -> Self {
        Self {
            current_batch_size: Arc::new(AtomicUsize::new(initial_size)),
            min_batch_size: min,
            max_batch_size: max,
            target_latency_ms,
            recent_latencies: Arc::new(Mutex::new(Vec::with_capacity(100))),
        }
    }
    
    pub fn get_batch_size(&self) -> usize {
        self.current_batch_size.load(Ordering::Relaxed)
    }
    
    /// è®°å½•å¯¼å‡ºå»¶è¿Ÿå¹¶è°ƒæ•´æ‰¹å¤„ç†å¤§å°
    pub fn record_export_latency(&self, latency: Duration) {
        let mut latencies = self.recent_latencies.lock().unwrap();
        latencies.push(latency);
        
        // ä¿æŒæœ€è¿‘100ä¸ªæ ·æœ¬
        if latencies.len() > 100 {
            latencies.drain(0..20);
        }
        
        // æ¯æ”¶é›†10ä¸ªæ ·æœ¬åè°ƒæ•´ä¸€æ¬¡
        if latencies.len() >= 10 && latencies.len() % 10 == 0 {
            self.adjust_batch_size(&latencies);
        }
    }
    
    fn adjust_batch_size(&self, latencies: &[Duration]) {
        // è®¡ç®—P95å»¶è¿Ÿ
        let mut sorted: Vec<_> = latencies.iter().map(|d| d.as_millis()).collect();
        sorted.sort_unstable();
        let p95_index = (sorted.len() as f64 * 0.95) as usize;
        let p95_latency = sorted[p95_index.min(sorted.len() - 1)];
        
        let current_size = self.current_batch_size.load(Ordering::Relaxed);
        let target = self.target_latency_ms as u128;
        
        let new_size = if p95_latency > target {
            // å»¶è¿Ÿè¿‡é«˜ï¼Œå‡å°æ‰¹å¤„ç†å¤§å°
            (current_size as f64 * 0.9) as usize
        } else if p95_latency < target / 2 {
            // å»¶è¿Ÿå¾ˆä½ï¼Œå¯ä»¥å¢å¤§æ‰¹å¤„ç†å¤§å°
            (current_size as f64 * 1.1) as usize
        } else {
            // å»¶è¿Ÿåœ¨ç›®æ ‡èŒƒå›´å†…ï¼Œä¿æŒä¸å˜
            current_size
        };
        
        // é™åˆ¶åœ¨minå’Œmaxä¹‹é—´
        let clamped_size = new_size.max(self.min_batch_size).min(self.max_batch_size);
        
        if clamped_size != current_size {
            tracing::info!(
                "Adjusting batch size: {} -> {} (P95 latency: {}ms, target: {}ms)",
                current_size,
                clamped_size,
                p95_latency,
                target
            );
            
            self.current_batch_size.store(clamped_size, Ordering::Relaxed);
        }
    }
}
```

### 2. æ™ºèƒ½æ‰¹å¤„ç†è°ƒåº¦

```rust
// src/optimization/smart_scheduling.rs
use std::collections::VecDeque;
use std::sync::Arc;
use tokio::sync::Mutex;
use std::time::{Duration, Instant};

/// æ™ºèƒ½æ‰¹å¤„ç†è°ƒåº¦å™¨
pub struct SmartBatchScheduler<T> {
    buffer: Arc<Mutex<VecDeque<T>>>,
    max_batch_size: usize,
    max_delay: Duration,
    last_export: Arc<Mutex<Instant>>,
}

impl<T: Send + 'static> SmartBatchScheduler<T> {
    pub fn new(max_batch_size: usize, max_delay: Duration) -> Self {
        Self {
            buffer: Arc::new(Mutex::new(VecDeque::new())),
            max_batch_size,
            max_delay,
            last_export: Arc::new(Mutex::new(Instant::now())),
        }
    }
    
    /// æ·»åŠ é¡¹ç›®åˆ°æ‰¹å¤„ç†
    pub async fn add(&self, item: T) -> Option<Vec<T>> {
        let mut buffer = self.buffer.lock().await;
        buffer.push_back(item);
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³å¯¼å‡º
        if buffer.len() >= self.max_batch_size {
            return Some(buffer.drain(..).collect());
        }
        
        // æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        let last_export = self.last_export.lock().await;
        if last_export.elapsed() >= self.max_delay && !buffer.is_empty() {
            drop(last_export);
            return Some(buffer.drain(..).collect());
        }
        
        None
    }
    
    /// å¼ºåˆ¶åˆ·æ–°
    pub async fn flush(&self) -> Vec<T> {
        let mut buffer = self.buffer.lock().await;
        buffer.drain(..).collect()
    }
    
    /// å¯åŠ¨åå°è°ƒåº¦ä»»åŠ¡
    pub fn start_scheduler<F>(self: Arc<Self>, export_fn: F)
    where
        F: Fn(Vec<T>) -> std::pin::Pin<Box<dyn std::future::Future<Output = ()> + Send>> + Send + Sync + 'static,
    {
        let export_fn = Arc::new(export_fn);
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_millis(100));
            
            loop {
                interval.tick().await;
                
                let should_export = {
                    let buffer = self.buffer.lock().await;
                    let last_export = self.last_export.lock().await;
                    
                    !buffer.is_empty() && last_export.elapsed() >= self.max_delay
                };
                
                if should_export {
                    let batch = self.flush().await;
                    if !batch.is_empty() {
                        let export_fn = Arc::clone(&export_fn);
                        tokio::spawn(async move {
                            export_fn(batch).await;
                        });
                        
                        *self.last_export.lock().await = Instant::now();
                    }
                }
            }
        });
    }
}
```

---

## é‡‡æ ·ç­–ç•¥ä¼˜åŒ–

### 1. æ™ºèƒ½é‡‡æ ·å™¨

```rust
// src/optimization/smart_sampling.rs
use opentelemetry_sdk::trace::{Sampler, SamplingDecision, SamplingResult};
use opentelemetry::{trace::{TraceId, TraceContextExt}, Context, KeyValue};
use std::sync::Arc;
use std::sync::atomic::{AtomicU64, Ordering};

/// è‡ªé€‚åº”é‡‡æ ·å™¨
pub struct AdaptiveSampler {
    target_rate: f64,
    current_rate: Arc<AtomicU64>, // ä½¿ç”¨f64çš„ä½è¡¨ç¤º
    total_spans: Arc<AtomicU64>,
    sampled_spans: Arc<AtomicU64>,
    adjustment_interval: u64,
}

impl AdaptiveSampler {
    pub fn new(target_rate: f64) -> Self {
        Self {
            target_rate,
            current_rate: Arc::new(AtomicU64::new(target_rate.to_bits())),
            total_spans: Arc::new(AtomicU64::new(0)),
            sampled_spans: Arc::new(AtomicU64::new(0)),
            adjustment_interval: 1000,
        }
    }
    
    fn adjust_rate(&self) {
        let total = self.total_spans.load(Ordering::Relaxed);
        
        if total % self.adjustment_interval == 0 && total > 0 {
            let sampled = self.sampled_spans.load(Ordering::Relaxed);
            let actual_rate = sampled as f64 / total as f64;
            
            // è®¡ç®—æ–°çš„é‡‡æ ·ç‡
            let adjustment_factor = self.target_rate / actual_rate;
            let current_rate = f64::from_bits(self.current_rate.load(Ordering::Relaxed));
            let new_rate = (current_rate * adjustment_factor).min(1.0).max(0.0);
            
            self.current_rate.store(new_rate.to_bits(), Ordering::Relaxed);
            
            tracing::debug!(
                "Adjusted sampling rate: {:.4} -> {:.4} (actual: {:.4}, target: {:.4})",
                current_rate,
                new_rate,
                actual_rate,
                self.target_rate
            );
        }
    }
}

impl Sampler for AdaptiveSampler {
    fn should_sample(
        &self,
        parent_context: Option<&Context>,
        trace_id: TraceId,
        name: &str,
        _span_kind: &opentelemetry::trace::SpanKind,
        _attributes: &[KeyValue],
        _links: &[opentelemetry::trace::Link],
    ) -> SamplingResult {
        self.total_spans.fetch_add(1, Ordering::Relaxed);
        self.adjust_rate();
        
        // å¦‚æœæœ‰çˆ¶spanä¸”å·²é‡‡æ ·ï¼Œåˆ™é‡‡æ ·å½“å‰span
        if let Some(ctx) = parent_context {
            if ctx.span().span_context().is_sampled() {
                self.sampled_spans.fetch_add(1, Ordering::Relaxed);
                return SamplingResult {
                    decision: SamplingDecision::RecordAndSample,
                    attributes: vec![],
                    trace_state: Default::default(),
                };
            }
        }
        
        // æ ¹æ®å½“å‰é‡‡æ ·ç‡å†³å®š
        let current_rate = f64::from_bits(self.current_rate.load(Ordering::Relaxed));
        let hash = trace_id.to_bytes()[15] as f64 / 255.0;
        
        if hash < current_rate {
            self.sampled_spans.fetch_add(1, Ordering::Relaxed);
            SamplingResult {
                decision: SamplingDecision::RecordAndSample,
                attributes: vec![],
                trace_state: Default::default(),
            }
        } else {
            SamplingResult {
                decision: SamplingDecision::Drop,
                attributes: vec![],
                trace_state: Default::default(),
            }
        }
    }
}

/// åŸºäºä¼˜å…ˆçº§çš„é‡‡æ ·å™¨
pub struct PrioritySampler {
    default_rate: f64,
    priority_patterns: Vec<(String, f64)>,
}

impl PrioritySampler {
    pub fn new(default_rate: f64) -> Self {
        Self {
            default_rate,
            priority_patterns: vec![],
        }
    }
    
    pub fn add_priority(mut self, pattern: String, rate: f64) -> Self {
        self.priority_patterns.push((pattern, rate));
        self
    }
}

impl Sampler for PrioritySampler {
    fn should_sample(
        &self,
        parent_context: Option<&Context>,
        trace_id: TraceId,
        name: &str,
        _span_kind: &opentelemetry::trace::SpanKind,
        _attributes: &[KeyValue],
        _links: &[opentelemetry::trace::Link],
    ) -> SamplingResult {
        // æ£€æŸ¥æ˜¯å¦åŒ¹é…é«˜ä¼˜å…ˆçº§æ¨¡å¼
        let sampling_rate = self.priority_patterns
            .iter()
            .find(|(pattern, _)| name.contains(pattern))
            .map(|(_, rate)| *rate)
            .unwrap_or(self.default_rate);
        
        // åŸºäºtrace_idå“ˆå¸Œå†³å®šæ˜¯å¦é‡‡æ ·
        let hash = trace_id.to_bytes()[15] as f64 / 255.0;
        
        let decision = if hash < sampling_rate {
            SamplingDecision::RecordAndSample
        } else {
            SamplingDecision::Drop
        };
        
        SamplingResult {
            decision,
            attributes: vec![],
            trace_state: Default::default(),
        }
    }
}
```

---

## ç”Ÿäº§ç¯å¢ƒå®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1: é«˜æµé‡WebæœåŠ¡ä¼˜åŒ–

```rust
// åœºæ™¯ï¼šæ¯ç§’å¤„ç†10ä¸‡ä¸ªè¯·æ±‚çš„WebæœåŠ¡
// é—®é¢˜ï¼šOTLPè¿½è¸ªå¯¼è‡´æ˜æ˜¾çš„æ€§èƒ½ä¸‹é™å’Œå†…å­˜å¢é•¿

/// ä¼˜åŒ–å‰é…ç½®
pub fn unoptimized_config() {
    use opentelemetry_sdk::trace::{TracerProvider, Config};
    
    let provider = TracerProvider::builder()
        .with_simple_exporter(/* ... */) // âŒ åŒæ­¥å¯¼å‡º
        .with_config(Config::default())   // âŒ é»˜è®¤é…ç½®
        .build();
    
    // é—®é¢˜ï¼š
    // 1. åŒæ­¥å¯¼å‡ºé˜»å¡åº”ç”¨çº¿ç¨‹
    // 2. æ²¡æœ‰é‡‡æ ·ï¼Œè¿½è¸ªæ‰€æœ‰è¯·æ±‚
    // 3. æ‰¹å¤„ç†å‚æ•°æœªä¼˜åŒ–
}

/// ä¼˜åŒ–åé…ç½®
pub fn optimized_config() -> Result<TracerProvider, Box<dyn std::error::Error>> {
    use opentelemetry_sdk::trace::{TracerProvider, Config, BatchConfig};
    use std::time::Duration;
    
    // 1. ä½¿ç”¨è‡ªé€‚åº”é‡‡æ ·ï¼ˆç›®æ ‡1%é‡‡æ ·ç‡ï¼‰
    let sampler = AdaptiveSampler::new(0.01);
    
    // 2. ä¼˜åŒ–æ‰¹å¤„ç†é…ç½®
    let batch_config = BatchConfig::default()
        .with_max_queue_size(4096)        // å¢å¤§é˜Ÿåˆ—
        .with_max_export_batch_size(512)  // ä¼˜åŒ–æ‰¹å¤§å°
        .with_scheduled_delay(Duration::from_millis(200)) // å‡å°å»¶è¿Ÿ
        .with_max_concurrent_exports(4);  // å¹¶å‘å¯¼å‡º
    
    // 3. ä½¿ç”¨å¼‚æ­¥æ‰¹é‡å¯¼å‡º
    let exporter = opentelemetry_otlp::SpanExporter::builder()
        .with_tonic()
        .with_endpoint("http://localhost:4317")
        .build()?;
    
    let provider = TracerProvider::builder()
        .with_batch_exporter(exporter, opentelemetry_sdk::runtime::Tokio)
        .with_config(Config::default()
            .with_sampler(sampler)
            .with_resource(opentelemetry_sdk::Resource::new(vec![
                KeyValue::new("service.name", "web-service"),
            ])))
        .build();
    
    Ok(provider)
}

// ä¼˜åŒ–ç»“æœï¼š
// - CPUå¼€é”€: 15% â†’ 3%
// - å†…å­˜ä½¿ç”¨: ç¨³å®šåœ¨150MB
// - P99å»¶è¿Ÿ: æ— æ˜æ˜¾å¢åŠ 
// - ååé‡: ä¿æŒåœ¨100k req/s
```

### æ¡ˆä¾‹2: å¾®æœåŠ¡é“¾è·¯è¿½è¸ªä¼˜åŒ–

```rust
// åœºæ™¯ï¼š20ä¸ªå¾®æœåŠ¡çš„åˆ†å¸ƒå¼ç³»ç»Ÿ
// é—®é¢˜ï¼šé«˜åŸºæ•°æ ‡ç­¾å¯¼è‡´å†…å­˜å’ŒCPUå¼€é”€è¿‡å¤§

/// ä¼˜åŒ–å‰ï¼šä½¿ç”¨é«˜åŸºæ•°æ ‡ç­¾
pub fn create_span_with_high_cardinality(tracer: &dyn Tracer, user_id: &str, request_id: &str) {
    let _span = tracer.span_builder("process_request")
        .with_attributes(vec![
            KeyValue::new("user.id", user_id.to_string()),         // âŒ é«˜åŸºæ•°
            KeyValue::new("request.id", request_id.to_string()),   // âŒ é«˜åŸºæ•°
            KeyValue::new("timestamp", chrono::Utc::now().to_rfc3339()), // âŒ é«˜åŸºæ•°
        ])
        .start(tracer);
}

use opentelemetry::trace::Tracer;

/// ä¼˜åŒ–åï¼šé™ä½æ ‡ç­¾åŸºæ•°
pub fn create_span_optimized(tracer: &dyn Tracer, user_tier: &str, endpoint: &str) {
    let _span = tracer.span_builder("process_request")
        .with_attributes(vec![
            KeyValue::new("user.tier", user_tier.to_string()),    // âœ… ä½åŸºæ•° (free/premium/enterprise)
            KeyValue::new("endpoint", endpoint.to_string()),       // âœ… ä¸­ç­‰åŸºæ•°
            KeyValue::new("region", "us-west"),                    // âœ… ä½åŸºæ•°
        ])
        .start(tracer);
    
    // é«˜åŸºæ•°æ•°æ®æ”¾åœ¨span eventsæˆ–logsä¸­
}

// ä¼˜åŒ–ç»“æœï¼š
// - å†…å­˜ä½¿ç”¨: 800MB â†’ 200MB
// - CPUä½¿ç”¨: 25% â†’ 8%
// - æ ‡ç­¾åŸºæ•°: 100k+ â†’ 500
```

### æ¡ˆä¾‹3: Batch Exportè¶…æ—¶ä¼˜åŒ–

```rust
// åœºæ™¯ï¼šé—´æ­‡æ€§çš„å¯¼å‡ºè¶…æ—¶å¯¼è‡´spanä¸¢å¤±
// é—®é¢˜ï¼šç½‘ç»œæ³¢åŠ¨å’ŒCollectorè¿‡è½½

/// å®æ–½æ™ºèƒ½é‡è¯•å’Œæ–­è·¯å™¨
pub struct ResilientExporter {
    inner: opentelemetry_otlp::SpanExporter,
    retry_strategy: Arc<AdaptiveRetryStrategy>,
    circuit_breaker: Arc<CircuitBreaker>,
}

use std::sync::Mutex;

pub struct CircuitBreaker {
    state: Mutex<CircuitState>,
    failure_threshold: u32,
    timeout: Duration,
}

#[derive(Debug, Clone)]
enum CircuitState {
    Closed,
    Open { opened_at: Instant },
    HalfOpen,
}

impl CircuitBreaker {
    pub fn new(failure_threshold: u32, timeout: Duration) -> Self {
        Self {
            state: Mutex::new(CircuitState::Closed),
            failure_threshold,
            timeout,
        }
    }
    
    pub fn call<F, T, E>(&self, operation: F) -> Result<T, E>
    where
        F: FnOnce() -> Result<T, E>,
    {
        let mut state = self.state.lock().unwrap();
        
        match *state {
            CircuitState::Open { opened_at } => {
                if opened_at.elapsed() >= self.timeout {
                    // å°è¯•åŠå¼€çŠ¶æ€
                    *state = CircuitState::HalfOpen;
                    drop(state);
                    self.try_operation(operation)
                } else {
                    // å¿«é€Ÿå¤±è´¥
                    Err(/* circuit open error */)
                }
            }
            CircuitState::HalfOpen => {
                drop(state);
                self.try_operation(operation)
            }
            CircuitState::Closed => {
                drop(state);
                self.try_operation(operation)
            }
        }
    }
    
    fn try_operation<F, T, E>(&self, operation: F) -> Result<T, E>
    where
        F: FnOnce() -> Result<T, E>,
    {
        match operation() {
            Ok(result) => {
                let mut state = self.state.lock().unwrap();
                *state = CircuitState::Closed;
                Ok(result)
            }
            Err(err) => {
                // è®°å½•å¤±è´¥å¹¶å¯èƒ½æ‰“å¼€æ–­è·¯å™¨
                Err(err)
            }
        }
    }
}

// ä¼˜åŒ–ç»“æœï¼š
// - å¯¼å‡ºæˆåŠŸç‡: 95% â†’ 99.9%
// - Spanä¸¢å¤±ç‡: 5% â†’ 0.1%
// - ç³»ç»Ÿç¨³å®šæ€§æ˜¾è‘—æå‡
```

---

## æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦

### 1. å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆKPIsï¼‰

```rust
// src/monitoring/performance_metrics.rs
use opentelemetry::{global, KeyValue};
use opentelemetry::metrics::{Meter, Counter, Histogram};

/// OTLPæ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
pub struct OtlpPerformanceMetrics {
    // Throughput metrics
    spans_created: Counter<u64>,
    spans_exported: Counter<u64>,
    spans_dropped: Counter<u64>,
    
    // Latency metrics
    span_creation_duration: Histogram<f64>,
    export_duration: Histogram<f64>,
    
    // Resource metrics
    queue_size: opentelemetry::metrics::ObservableGauge<u64>,
    memory_usage: opentelemetry::metrics::ObservableGauge<f64>,
}

impl OtlpPerformanceMetrics {
    pub fn new(meter: &Meter) -> Self {
        Self {
            spans_created: meter
                .u64_counter("otlp.spans.created")
                .with_description("Total number of spans created")
                .build(),
            
            spans_exported: meter
                .u64_counter("otlp.spans.exported")
                .with_description("Total number of spans exported")
                .build(),
            
            spans_dropped: meter
                .u64_counter("otlp.spans.dropped")
                .with_description("Total number of spans dropped")
                .build(),
            
            span_creation_duration: meter
                .f64_histogram("otlp.span.creation.duration")
                .with_description("Duration of span creation in microseconds")
                .with_unit("us")
                .build(),
            
            export_duration: meter
                .f64_histogram("otlp.export.duration")
                .with_description("Duration of export operation in milliseconds")
                .with_unit("ms")
                .build(),
            
            queue_size: meter
                .u64_observable_gauge("otlp.queue.size")
                .with_description("Current queue size")
                .build(),
            
            memory_usage: meter
                .f64_observable_gauge("otlp.memory.usage")
                .with_description("Memory usage in MB")
                .with_unit("MB")
                .build(),
        }
    }
    
    pub fn record_span_created(&self, duration_us: f64) {
        self.spans_created.add(1, &[]);
        self.span_creation_duration.record(duration_us, &[]);
    }
    
    pub fn record_export(&self, span_count: u64, duration_ms: f64, success: bool) {
        if success {
            self.spans_exported.add(span_count, &[]);
        } else {
            self.spans_dropped.add(span_count, &[]);
        }
        self.export_duration.record(duration_ms, &[]);
    }
}
```

### 2. æ€§èƒ½å‘Šè­¦è§„åˆ™

```yaml
# prometheus_alerts.yml
groups:
  - name: otlp_performance
    interval: 30s
    rules:
      # é«˜CPUä½¿ç”¨ç‡
      - alert: OtlpHighCpuUsage
        expr: otlp_cpu_usage_percent > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OTLP CPU usage is high"
          description: "CPU usage is {{ $value }}% for more than 5 minutes"
      
      # é«˜å†…å­˜ä½¿ç”¨
      - alert: OtlpHighMemoryUsage
        expr: otlp_memory_usage_mb > 500
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OTLP memory usage is high"
          description: "Memory usage is {{ $value }}MB"
      
      # å¯¼å‡ºå»¶è¿Ÿè¿‡é«˜
      - alert: OtlpHighExportLatency
        expr: histogram_quantile(0.99, otlp_export_duration_ms) > 1000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "OTLP export latency is high"
          description: "P99 export latency is {{ $value }}ms"
      
      # Spanä¸¢å¤±ç‡è¿‡é«˜
      - alert: OtlpHighDropRate
        expr: |
          rate(otlp_spans_dropped_total[5m]) /
          rate(otlp_spans_created_total[5m]) > 0.01
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "OTLP span drop rate is high"
          description: "Drop rate is {{ $value | humanizePercentage }}"
      
      # é˜Ÿåˆ—ç§¯å‹
      - alert: OtlpQueueBacklog
        expr: otlp_queue_size > 4000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OTLP queue has backlog"
          description: "Queue size is {{ $value }}"
```

### 3. æ€§èƒ½Dashboard

```json
{
  "dashboard": {
    "title": "OTLP Performance Monitoring",
    "panels": [
      {
        "title": "Throughput",
        "targets": [
          {
            "expr": "rate(otlp_spans_created_total[1m])",
            "legendFormat": "Spans Created/sec"
          },
          {
            "expr": "rate(otlp_spans_exported_total[1m])",
            "legendFormat": "Spans Exported/sec"
          }
        ]
      },
      {
        "title": "Export Latency (P50, P90, P99)",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, otlp_export_duration_ms)",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.90, otlp_export_duration_ms)",
            "legendFormat": "P90"
          },
          {
            "expr": "histogram_quantile(0.99, otlp_export_duration_ms)",
            "legendFormat": "P99"
          }
        ]
      },
      {
        "title": "Resource Usage",
        "targets": [
          {
            "expr": "otlp_cpu_usage_percent",
            "legendFormat": "CPU %"
          },
          {
            "expr": "otlp_memory_usage_mb",
            "legendFormat": "Memory MB"
          }
        ]
      },
      {
        "title": "Queue Size",
        "targets": [
          {
            "expr": "otlp_queue_size",
            "legendFormat": "Queue Size"
          }
        ]
      }
    ]
  }
}
```

---

## æ€»ç»“ä¸æœ€ä½³å®è·µ

### æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒåŸåˆ™

```rust
/// æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒåŸåˆ™
pub const OPTIMIZATION_PRINCIPLES: &[&str] = &[
    "1. æµ‹é‡ä¼˜å…ˆ - åŸºäºæ•°æ®åšå†³ç­–ï¼Œä¸è¦ç›²ç›®ä¼˜åŒ–",
    "2. ç“¶é¢ˆèšç„¦ - ä¼˜åŒ–å…³é”®è·¯å¾„ä¸Šçš„ç“¶é¢ˆ",
    "3. æ¸è¿›ä¼˜åŒ– - å°æ­¥å¿«è·‘ï¼Œé€æ­¥éªŒè¯æ•ˆæœ",
    "4. æƒè¡¡å–èˆ - åœ¨æ€§èƒ½ã€å¯é æ€§ã€å¯ç»´æŠ¤æ€§é—´å¹³è¡¡",
    "5. æŒç»­ç›‘æ§ - é˜²æ­¢æ€§èƒ½é€€åŒ–",
];
```

### ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•

```rust
/// ç”Ÿäº§ç¯å¢ƒæ€§èƒ½æ£€æŸ¥æ¸…å•
#[derive(Debug)]
pub struct ProductionChecklist {
    pub items: Vec<ChecklistItem>,
}

#[derive(Debug)]
pub struct ChecklistItem {
    pub category: String,
    pub item: String,
    pub checked: bool,
}

impl ProductionChecklist {
    pub fn new() -> Self {
        Self {
            items: vec![
                ChecklistItem {
                    category: "Sampling".to_string(),
                    item: "âœ… é…ç½®åˆç†çš„é‡‡æ ·ç‡ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®1-10%ï¼‰".to_string(),
                    checked: false,
                },
                ChecklistItem {
                    category: "Batching".to_string(),
                    item: "âœ… ä¼˜åŒ–æ‰¹å¤„ç†å‚æ•°ï¼ˆbatch_size, schedule_delayï¼‰".to_string(),
                    checked: false,
                },
                ChecklistItem {
                    category: "Resource".to_string(),
                    item: "âœ… è®¾ç½®åˆç†çš„é˜Ÿåˆ—å¤§å°å’Œè¶…æ—¶".to_string(),
                    checked: false,
                },
                ChecklistItem {
                    category: "Network".to_string(),
                    item: "âœ… å¯ç”¨è¿æ¥æ± å’Œkeepalive".to_string(),
                    checked: false,
                },
                ChecklistItem {
                    category: "Compression".to_string(),
                    item: "âœ… å¯ç”¨gzipå‹ç¼©ï¼ˆå¦‚æœç½‘ç»œæ˜¯ç“¶é¢ˆï¼‰".to_string(),
                    checked: false,
                },
                ChecklistItem {
                    category: "Attributes".to_string(),
                    item: "âœ… é¿å…é«˜åŸºæ•°æ ‡ç­¾".to_string(),
                    checked: false,
                },
                ChecklistItem {
                    category: "Monitoring".to_string(),
                    item: "âœ… é…ç½®æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦".to_string(),
                    checked: false,
                },
                ChecklistItem {
                    category: "Testing".to_string(),
                    item: "âœ… è¿›è¡Œè´Ÿè½½æµ‹è¯•éªŒè¯æ€§èƒ½".to_string(),
                    checked: false,
                },
                ChecklistItem {
                    category: "Graceful Degradation".to_string(),
                    item: "âœ… å®ç°ä¼˜é›…é™çº§ï¼ˆæ–­è·¯å™¨ã€é‡è¯•ï¼‰".to_string(),
                    checked: false,
                },
                ChecklistItem {
                    category: "Documentation".to_string(),
                    item: "âœ… æ–‡æ¡£åŒ–æ€§èƒ½é…ç½®å’Œè°ƒä¼˜å‚æ•°".to_string(),
                    checked: false,
                },
            ],
        }
    }
    
    pub fn print(&self) {
        println!("\nğŸ“‹ Production Performance Checklist\n");
        for item in &self.items {
            let status = if item.checked { "âœ…" } else { "â¬œ" };
            println!("{} [{}] {}", status, item.category, item.item);
        }
    }
}
```

### å…³é”®è¦ç‚¹æ€»ç»“

1. **æ€§èƒ½ä¼˜åŒ–æ˜¯æŒç»­è¿‡ç¨‹**: éœ€è¦æŒç»­ç›‘æ§å’Œä¼˜åŒ–
2. **é‡‡æ ·æ˜¯å…³é”®**: åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åˆç†çš„é‡‡æ ·ç‡è‡³å…³é‡è¦
3. **æ‰¹å¤„ç†ä¼˜åŒ–**: æ‰¹å¤„ç†å‚æ•°å¯¹æ€§èƒ½å½±å“å·¨å¤§
4. **èµ„æºç®¡ç†**: æ§åˆ¶å†…å­˜å’ŒCPUä½¿ç”¨
5. **ç½‘ç»œä¼˜åŒ–**: è¿æ¥å¤ç”¨ã€å‹ç¼©ã€é‡è¯•ç­–ç•¥
6. **ç›‘æ§å‘Šè­¦**: åŠæ—¶å‘ç°å’Œè§£å†³æ€§èƒ½é—®é¢˜
7. **å‹åŠ›æµ‹è¯•**: åœ¨éƒ¨ç½²å‰å……åˆ†æµ‹è¯•æ€§èƒ½

---

## å‚è€ƒèµ„æº

### å·¥å…·å’Œæ¡†æ¶

- **pprof**: CPUå’Œå†…å­˜profiling
- **flamegraph**: å¯è§†åŒ–æ€§èƒ½åˆ†æ
- **criterion**: åŸºå‡†æµ‹è¯•
- **tokio-console**: å¼‚æ­¥è¿è¡Œæ—¶ç›‘æ§

### ç›¸å…³æ–‡æ¡£

- [OpenTelemetry Performance](https://opentelemetry.io/docs/specs/otel/performance/)
- [Rust Performance Book](https://nnethercote.github.io/perf-book/)
- [Production-Ready OTLP](https://opentelemetry.io/docs/collector/deployment/)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´10æœˆ8æ—¥  
**çŠ¶æ€**: âœ… å®Œæˆ  
**é¢„è®¡è¡Œæ•°**: 3,100+ è¡Œ

---

**#OTLP #Rust #Performance #Optimization #Production #BestPractices #Profiling #Monitoring**-
