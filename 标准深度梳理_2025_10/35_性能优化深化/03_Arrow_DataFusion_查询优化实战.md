# Arrow DataFusion æŸ¥è¯¢ä¼˜åŒ–å®æˆ˜ - OTLP æ•°æ®åˆ†æåŠ é€Ÿ

> **Rustç‰ˆæœ¬**: 1.90+  
> **DataFusion**: 43.0+  
> **Arrow**: 54.0+  
> **æœ€åæ›´æ–°**: 2025å¹´10æœˆ10æ—¥

---

## ğŸ“‹ ç›®å½•

- [1. DataFusion ç®€ä»‹](#1-datafusion-ç®€ä»‹)
- [2. OTLP æ•°æ®æŸ¥è¯¢åœºæ™¯](#2-otlp-æ•°æ®æŸ¥è¯¢åœºæ™¯)
- [3. æŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯](#3-æŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯)
- [4. å®æˆ˜æ¡ˆä¾‹](#4-å®æˆ˜æ¡ˆä¾‹)
- [5. æ€§èƒ½è°ƒä¼˜](#5-æ€§èƒ½è°ƒä¼˜)
- [6. ç”Ÿäº§éƒ¨ç½²](#6-ç”Ÿäº§éƒ¨ç½²)

---

## 1. DataFusion ç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯ DataFusion

```text
DataFusion = Arrow + SQL + æŸ¥è¯¢ä¼˜åŒ–å™¨

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          SQL/DataFrame API              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       é€»è¾‘è®¡åˆ’ & ä¼˜åŒ–å™¨                  â”‚
â”‚  - è°“è¯ä¸‹æ¨                              â”‚
â”‚  - æŠ•å½±ä¸‹æ¨                              â”‚
â”‚  - å¸¸é‡æŠ˜å                               â”‚
â”‚  - å…¬å…±å­è¡¨è¾¾å¼æ¶ˆé™¤                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       ç‰©ç†è®¡åˆ’ & æ‰§è¡Œ                    â”‚
â”‚  - å‘é‡åŒ–æ‰§è¡Œ                            â”‚
â”‚  - å¹¶è¡Œæ‰§è¡Œ                              â”‚
â”‚  - æµå¼å¤„ç†                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       Apache Arrow å†…å­˜æ ¼å¼              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒç‰¹æ€§

```rust
use datafusion::prelude::*;
use datafusion::arrow::array::*;

/// DataFusion 43.0 æ ¸å¿ƒç‰¹æ€§
pub struct DataFusionFeatures;

impl DataFusionFeatures {
    /// ç‰¹æ€§åˆ—è¡¨
    pub fn features() -> Vec<&'static str> {
        vec![
            "âœ… SQL æŸ¥è¯¢æ”¯æŒ (å®Œæ•´ SQL-92)",
            "âœ… DataFrame API",
            "âœ… æŸ¥è¯¢ä¼˜åŒ–å™¨ (è°“è¯ä¸‹æ¨ã€æŠ•å½±ä¸‹æ¨ç­‰)",
            "âœ… å‘é‡åŒ–æ‰§è¡Œå¼•æ“",
            "âœ… å¹¶è¡ŒæŸ¥è¯¢æ‰§è¡Œ",
            "âœ… æµå¼å¤„ç†",
            "âœ… è‡ªå®šä¹‰å‡½æ•° (UDF/UDAF)",
            "âœ… Parquet åŸç”Ÿæ”¯æŒ",
            "âœ… CSV, JSON, Avro æ”¯æŒ",
            "âœ… å†…å­˜è¡¨å’Œå¤–éƒ¨è¡¨",
            "âœ… åˆ†åŒºè¡¨æ”¯æŒ",
            "âœ… çª—å£å‡½æ•°",
            "âœ… å¤æ‚ç±»å‹ (Array, Struct, Map)",
        ]
    }
}
```

---

## 2. OTLP æ•°æ®æŸ¥è¯¢åœºæ™¯

### 2.1 å…¸å‹æŸ¥è¯¢åœºæ™¯

```rust
use datafusion::prelude::*;
use datafusion::arrow::datatypes::*;

/// OTLP Trace æ•°æ®æŸ¥è¯¢åœºæ™¯
pub struct OtlpQueryScenarios;

impl OtlpQueryScenarios {
    /// åœºæ™¯1: æŒ‰ TraceID æŸ¥è¯¢å®Œæ•´é“¾è·¯
    pub async fn query_full_trace(
        ctx: &SessionContext,
        trace_id: &str,
    ) -> Result<Vec<RecordBatch>, DataFusionError> {
        ctx.sql(&format!(
            "SELECT * FROM traces WHERE trace_id = '{}' ORDER BY start_time",
            trace_id
        ))
        .await?
        .collect()
        .await
    }
    
    /// åœºæ™¯2: æ…¢æŸ¥è¯¢åˆ†æ
    pub async fn analyze_slow_queries(
        ctx: &SessionContext,
        threshold_ms: i64,
    ) -> Result<Vec<RecordBatch>, DataFusionError> {
        ctx.sql(&format!(
            "
            SELECT 
                service_name,
                operation_name,
                COUNT(*) as slow_count,
                AVG(duration_ms) as avg_duration,
                MAX(duration_ms) as max_duration,
                PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_ms) as p95_duration
            FROM traces
            WHERE duration_ms > {}
            GROUP BY service_name, operation_name
            ORDER BY slow_count DESC
            LIMIT 100
            ",
            threshold_ms
        ))
        .await?
        .collect()
        .await
    }
    
    /// åœºæ™¯3: é”™è¯¯ç‡åˆ†æ
    pub async fn error_rate_analysis(
        ctx: &SessionContext,
    ) -> Result<Vec<RecordBatch>, DataFusionError> {
        ctx.sql(
            "
            SELECT 
                service_name,
                DATE_TRUNC('hour', start_time) as hour,
                COUNT(*) as total_requests,
                SUM(CASE WHEN status_code = 2 THEN 1 ELSE 0 END) as error_count,
                SUM(CASE WHEN status_code = 2 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as error_rate
            FROM traces
            WHERE start_time > NOW() - INTERVAL '24 hours'
            GROUP BY service_name, hour
            ORDER BY error_rate DESC
            "
        )
        .await?
        .collect()
        .await
    }
    
    /// åœºæ™¯4: æœåŠ¡ä¾èµ–å›¾
    pub async fn service_dependency_graph(
        ctx: &SessionContext,
    ) -> Result<Vec<RecordBatch>, DataFusionError> {
        ctx.sql(
            "
            WITH parent_child AS (
                SELECT 
                    parent.service_name as parent_service,
                    child.service_name as child_service,
                    COUNT(*) as call_count,
                    AVG(child.duration_ms) as avg_duration
                FROM traces parent
                JOIN traces child ON parent.trace_id = child.trace_id 
                    AND parent.span_id = child.parent_span_id
                WHERE parent.start_time > NOW() - INTERVAL '1 hour'
                GROUP BY parent_service, child_service
            )
            SELECT * FROM parent_child
            WHERE call_count > 10
            ORDER BY call_count DESC
            "
        )
        .await?
        .collect()
        .await
    }
    
    /// åœºæ™¯5: æ—¶é—´åºåˆ—èšåˆ
    pub async fn timeseries_aggregation(
        ctx: &SessionContext,
        interval: &str,
    ) -> Result<Vec<RecordBatch>, DataFusionError> {
        ctx.sql(&format!(
            "
            SELECT 
                DATE_TRUNC('{}', start_time) as time_bucket,
                service_name,
                COUNT(*) as request_count,
                AVG(duration_ms) as avg_duration,
                PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY duration_ms) as p50,
                PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_ms) as p95,
                PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY duration_ms) as p99
            FROM traces
            WHERE start_time > NOW() - INTERVAL '24 hours'
            GROUP BY time_bucket, service_name
            ORDER BY time_bucket, service_name
            ",
            interval
        ))
        .await?
        .collect()
        .await
    }
}
```

---

## 3. æŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯

### 3.1 è°“è¯ä¸‹æ¨ä¼˜åŒ–

```rust
use datafusion::logical_expr::*;
use datafusion::optimizer::*;

/// è°“è¯ä¸‹æ¨ç¤ºä¾‹
pub async fn predicate_pushdown_example() -> Result<(), DataFusionError> {
    let ctx = SessionContext::new();
    
    // æ³¨å†Œè¡¨
    ctx.sql(
        "CREATE EXTERNAL TABLE traces (
            trace_id VARCHAR,
            span_id VARCHAR,
            service_name VARCHAR,
            duration_ms BIGINT,
            start_time TIMESTAMP
        ) STORED AS PARQUET LOCATION './traces.parquet'"
    ).await?;
    
    // æŸ¥è¯¢ (è‡ªåŠ¨è°“è¯ä¸‹æ¨)
    let df = ctx.sql(
        "
        SELECT service_name, AVG(duration_ms)
        FROM traces
        WHERE service_name = 'api-gateway'
          AND start_time > NOW() - INTERVAL '1 hour'
        GROUP BY service_name
        "
    ).await?;
    
    // æŸ¥çœ‹ä¼˜åŒ–åçš„è®¡åˆ’
    let logical_plan = df.logical_plan().clone();
    println!("Logical Plan:\n{}", logical_plan.display_indent());
    
    // æŸ¥çœ‹ç‰©ç†è®¡åˆ’
    let physical_plan = df.create_physical_plan().await?;
    println!("\nPhysical Plan:\n{}", displayable(physical_plan.as_ref()).indent(true));
    
    Ok(())
}
```

### 3.2 æŠ•å½±ä¸‹æ¨ä¼˜åŒ–

```rust
/// æŠ•å½±ä¸‹æ¨ç¤ºä¾‹
pub async fn projection_pushdown_example() -> Result<(), DataFusionError> {
    let ctx = SessionContext::new();
    
    // åªè¯»å–éœ€è¦çš„åˆ— (æŠ•å½±ä¸‹æ¨åˆ° Parquet)
    let df = ctx.sql(
        "
        SELECT 
            service_name,
            duration_ms
        FROM traces
        WHERE start_time > NOW() - INTERVAL '1 hour'
        "
    ).await?;
    
    // DataFusion ä¼šä¼˜åŒ–ä¸ºåªè¯»å– service_name å’Œ duration_ms åˆ—
    // é¿å…è¯»å–æ•´è¡Œæ•°æ®
    
    let results = df.collect().await?;
    println!("Projection pushdown saved {} columns from reading", 
        results[0].num_columns());
    
    Ok(())
}
```

### 3.3 åˆ†åŒºå‰ªæ

```rust
/// åˆ†åŒºå‰ªæç¤ºä¾‹
pub async fn partition_pruning_example() -> Result<(), DataFusionError> {
    let ctx = SessionContext::new();
    
    // åˆ›å»ºåˆ†åŒºè¡¨ (æŒ‰æ—¥æœŸåˆ†åŒº)
    ctx.sql(
        "
        CREATE EXTERNAL TABLE traces (
            trace_id VARCHAR,
            service_name VARCHAR,
            duration_ms BIGINT,
            start_time TIMESTAMP
        ) 
        STORED AS PARQUET 
        LOCATION './traces/'
        PARTITIONED BY (date DATE)
        "
    ).await?;
    
    // æŸ¥è¯¢ç‰¹å®šåˆ†åŒº (è‡ªåŠ¨å‰ªæ)
    let df = ctx.sql(
        "
        SELECT service_name, COUNT(*)
        FROM traces
        WHERE date = '2025-10-10'
        GROUP BY service_name
        "
    ).await?;
    
    // DataFusion åªæ‰«æ 2025-10-10 åˆ†åŒº
    // è·³è¿‡å…¶ä»–åˆ†åŒºæ–‡ä»¶
    
    let results = df.collect().await?;
    println!("Partition pruning scanned only 1 partition");
    
    Ok(())
}
```

### 3.4 å‘é‡åŒ–æ‰§è¡Œ

```rust
use datafusion::physical_plan::*;

/// å‘é‡åŒ–æ‰§è¡Œç¤ºä¾‹
pub async fn vectorized_execution_example() -> Result<(), DataFusionError> {
    let ctx = SessionContext::new();
    
    // åˆ›å»ºæµ‹è¯•æ•°æ®
    let batch = create_test_batch()?;
    ctx.register_batch("traces", batch)?;
    
    // å‘é‡åŒ–èšåˆ
    let df = ctx.sql(
        "
        SELECT 
            service_name,
            COUNT(*) as count,
            AVG(duration_ms) as avg_duration,
            SUM(duration_ms) as total_duration,
            MIN(duration_ms) as min_duration,
            MAX(duration_ms) as max_duration
        FROM traces
        GROUP BY service_name
        "
    ).await?;
    
    // DataFusion ä½¿ç”¨å‘é‡åŒ–æ‰§è¡Œ:
    // - ä¸€æ¬¡å¤„ç†å¤šè¡Œ (batch)
    // - SIMD åŠ é€Ÿ
    // - å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
    
    let start = Instant::now();
    let results = df.collect().await?;
    let duration = start.elapsed();
    
    println!("Vectorized execution completed in {:?}", duration);
    
    Ok(())
}

fn create_test_batch() -> Result<RecordBatch, ArrowError> {
    let schema = Arc::new(Schema::new(vec![
        Field::new("service_name", DataType::Utf8, false),
        Field::new("duration_ms", DataType::Int64, false),
    ]));
    
    let services: Vec<&str> = (0..100000)
        .map(|i| if i % 3 == 0 { "api-gateway" } 
                 else if i % 3 == 1 { "user-service" } 
                 else { "order-service" })
        .collect();
    
    let durations: Vec<i64> = (0..100000)
        .map(|_| rand::random::<i64>() % 1000)
        .collect();
    
    RecordBatch::try_new(
        schema,
        vec![
            Arc::new(StringArray::from(services)),
            Arc::new(Int64Array::from(durations)),
        ],
    )
}
```

---

## 4. å®æˆ˜æ¡ˆä¾‹

### 4.1 å®Œæ•´çš„ OTLP æŸ¥è¯¢å¼•æ“

```rust
use datafusion::prelude::*;
use datafusion::arrow::datatypes::*;
use std::sync::Arc;
use tokio::sync::RwLock;

/// OTLP æŸ¥è¯¢å¼•æ“
pub struct OtlpQueryEngine {
    ctx: Arc<SessionContext>,
    config: QueryEngineConfig,
}

#[derive(Debug, Clone)]
pub struct QueryEngineConfig {
    /// å¹¶è¡Œåº¦
    pub parallelism: usize,
    
    /// æ‰¹å¤„ç†å¤§å°
    pub batch_size: usize,
    
    /// å¯ç”¨åˆ†åŒºå‰ªæ
    pub enable_partition_pruning: bool,
    
    /// å¯ç”¨è°“è¯ä¸‹æ¨
    pub enable_predicate_pushdown: bool,
}

impl OtlpQueryEngine {
    /// åˆ›å»ºæŸ¥è¯¢å¼•æ“
    pub async fn new(config: QueryEngineConfig) -> Result<Self, DataFusionError> {
        // åˆ›å»º SessionContext å¹¶é…ç½®
        let mut ctx_config = SessionConfig::new()
            .with_target_partitions(config.parallelism)
            .with_batch_size(config.batch_size);
        
        if config.enable_partition_pruning {
            ctx_config = ctx_config.set_bool("datafusion.execution.enable_partition_pruning", true);
        }
        
        let ctx = SessionContext::new_with_config(ctx_config);
        
        Ok(Self {
            ctx: Arc::new(ctx),
            config,
        })
    }
    
    /// æ³¨å†Œ Trace æ•°æ®æº
    pub async fn register_trace_source(
        &self,
        name: &str,
        path: &str,
    ) -> Result<(), DataFusionError> {
        self.ctx.sql(&format!(
            "
            CREATE EXTERNAL TABLE {} (
                trace_id VARCHAR,
                span_id VARCHAR,
                parent_span_id VARCHAR,
                service_name VARCHAR,
                operation_name VARCHAR,
                kind INT,
                start_time TIMESTAMP,
                end_time TIMESTAMP,
                duration_ms BIGINT,
                status_code INT,
                status_message VARCHAR,
                attributes VARCHAR,
                resource_attributes VARCHAR
            )
            STORED AS PARQUET
            LOCATION '{}'
            ",
            name, path
        )).await?;
        
        Ok(())
    }
    
    /// æŸ¥è¯¢ Trace
    pub async fn query_traces(
        &self,
        sql: &str,
    ) -> Result<Vec<RecordBatch>, DataFusionError> {
        let start = Instant::now();
        
        let df = self.ctx.sql(sql).await?;
        let results = df.collect().await?;
        
        let duration = start.elapsed();
        tracing::info!(
            "Query executed in {:?}, returned {} batches",
            duration,
            results.len()
        );
        
        Ok(results)
    }
    
    /// åˆ›å»ºç‰©åŒ–è§†å›¾ (é¢„èšåˆ)
    pub async fn create_materialized_view(
        &self,
        view_name: &str,
        query: &str,
    ) -> Result<(), DataFusionError> {
        // æ‰§è¡ŒæŸ¥è¯¢
        let df = self.ctx.sql(query).await?;
        let results = df.collect().await?;
        
        // æ³¨å†Œä¸ºå†…å­˜è¡¨
        self.ctx.register_batch(view_name, results[0].clone())?;
        
        tracing::info!("Materialized view '{}' created", view_name);
        
        Ok(())
    }
    
    /// æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°
    pub async fn register_custom_functions(&self) -> Result<(), DataFusionError> {
        use datafusion::logical_expr::{create_udf, Volatility};
        use datafusion::arrow::array::StringArray;
        
        // UDF: æå– TraceID çš„å‰8ä½
        let trace_id_prefix = create_udf(
            "trace_id_prefix",
            vec![DataType::Utf8],
            Arc::new(DataType::Utf8),
            Volatility::Immutable,
            Arc::new(move |args| {
                let trace_ids = args[0]
                    .as_any()
                    .downcast_ref::<StringArray>()
                    .unwrap();
                
                let result: StringArray = trace_ids
                    .iter()
                    .map(|trace_id| {
                        trace_id.map(|s| s.chars().take(8).collect::<String>())
                    })
                    .collect();
                
                Ok(Arc::new(result) as ArrayRef)
            }),
        );
        
        self.ctx.register_udf(trace_id_prefix);
        
        Ok(())
    }
}

/// ä½¿ç”¨ç¤ºä¾‹
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆ›å»ºæŸ¥è¯¢å¼•æ“
    let config = QueryEngineConfig {
        parallelism: num_cpus::get(),
        batch_size: 8192,
        enable_partition_pruning: true,
        enable_predicate_pushdown: true,
    };
    
    let engine = OtlpQueryEngine::new(config).await?;
    
    // æ³¨å†Œæ•°æ®æº
    engine.register_trace_source("traces", "./data/traces.parquet").await?;
    
    // æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°
    engine.register_custom_functions().await?;
    
    // æ‰§è¡ŒæŸ¥è¯¢
    let results = engine.query_traces(
        "
        SELECT 
            service_name,
            COUNT(*) as request_count,
            AVG(duration_ms) as avg_duration
        FROM traces
        WHERE start_time > NOW() - INTERVAL '1 hour'
        GROUP BY service_name
        ORDER BY request_count DESC
        "
    ).await?;
    
    // æ‰“å°ç»“æœ
    arrow::util::pretty::print_batches(&results)?;
    
    Ok(())
}
```

### 4.2 å®æ—¶èšåˆå¼•æ“

```rust
use tokio::sync::mpsc;
use tokio_stream::StreamExt;

/// å®æ—¶èšåˆå¼•æ“
pub struct RealtimeAggregationEngine {
    engine: Arc<OtlpQueryEngine>,
    aggregator: Arc<RwLock<WindowAggregator>>,
}

/// æ»‘åŠ¨çª—å£èšåˆå™¨
struct WindowAggregator {
    window_size: Duration,
    batches: VecDeque<TimestampedBatch>,
}

struct TimestampedBatch {
    timestamp: Instant,
    batch: RecordBatch,
}

impl RealtimeAggregationEngine {
    pub fn new(
        engine: Arc<OtlpQueryEngine>,
        window_size: Duration,
    ) -> Self {
        Self {
            engine,
            aggregator: Arc::new(RwLock::new(WindowAggregator {
                window_size,
                batches: VecDeque::new(),
            })),
        }
    }
    
    /// å¤„ç†å®æ—¶æ•°æ®æµ
    pub async fn process_stream(
        &self,
        mut stream: mpsc::Receiver<RecordBatch>,
    ) -> Result<(), DataFusionError> {
        while let Some(batch) = stream.recv().await {
            // æ·»åŠ åˆ°çª—å£
            self.add_batch(batch).await?;
            
            // æ‰§è¡Œèšåˆ
            let aggregated = self.aggregate_window().await?;
            
            // è¾“å‡ºç»“æœ
            self.emit_results(aggregated).await?;
        }
        
        Ok(())
    }
    
    async fn add_batch(&self, batch: RecordBatch) -> Result<(), DataFusionError> {
        let mut aggregator = self.aggregator.write().await;
        
        // æ·»åŠ æ–°æ‰¹æ¬¡
        aggregator.batches.push_back(TimestampedBatch {
            timestamp: Instant::now(),
            batch,
        });
        
        // ç§»é™¤è¿‡æœŸæ‰¹æ¬¡
        let cutoff = Instant::now() - aggregator.window_size;
        while let Some(oldest) = aggregator.batches.front() {
            if oldest.timestamp < cutoff {
                aggregator.batches.pop_front();
            } else {
                break;
            }
        }
        
        Ok(())
    }
    
    async fn aggregate_window(&self) -> Result<Vec<RecordBatch>, DataFusionError> {
        let aggregator = self.aggregator.read().await;
        
        if aggregator.batches.is_empty() {
            return Ok(vec![]);
        }
        
        // åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        let schema = aggregator.batches[0].batch.schema();
        let batches: Vec<RecordBatch> = aggregator.batches
            .iter()
            .map(|tb| tb.batch.clone())
            .collect();
        
        use arrow::compute::concat_batches;
        let combined = concat_batches(&schema, &batches)?;
        
        // æ³¨å†Œä¸´æ—¶è¡¨
        self.engine.ctx.register_batch("window_data", combined)?;
        
        // æ‰§è¡ŒèšåˆæŸ¥è¯¢
        let results = self.engine.query_traces(
            "
            SELECT 
                service_name,
                COUNT(*) as count,
                AVG(duration_ms) as avg_duration,
                PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY duration_ms) as p95
            FROM window_data
            GROUP BY service_name
            "
        ).await?;
        
        Ok(results)
    }
    
    async fn emit_results(&self, results: Vec<RecordBatch>) -> Result<(), DataFusionError> {
        for batch in results {
            arrow::util::pretty::print_batches(&[batch])?;
        }
        Ok(())
    }
}
```

---

## 5. æ€§èƒ½è°ƒä¼˜

### 5.1 æŸ¥è¯¢æ€§èƒ½åˆ†æ

```rust
use datafusion::physical_plan::display::DisplayableExecutionPlan;

/// æŸ¥è¯¢æ€§èƒ½åˆ†æå™¨
pub struct QueryProfiler {
    ctx: Arc<SessionContext>,
}

impl QueryProfiler {
    pub async fn analyze_query(
        &self,
        sql: &str,
    ) -> Result<QueryProfile, DataFusionError> {
        let start = Instant::now();
        
        // 1. è§£ææŸ¥è¯¢
        let df = self.ctx.sql(sql).await?;
        let parse_time = start.elapsed();
        
        // 2. æŸ¥çœ‹é€»è¾‘è®¡åˆ’
        let logical_plan = df.logical_plan().clone();
        println!("=== Logical Plan ===");
        println!("{}", logical_plan.display_indent());
        
        // 3. ä¼˜åŒ–é€»è¾‘è®¡åˆ’
        let optimized_plan = self.ctx.state().optimize(&logical_plan)?;
        println!("\n=== Optimized Logical Plan ===");
        println!("{}", optimized_plan.display_indent());
        
        // 4. åˆ›å»ºç‰©ç†è®¡åˆ’
        let physical_plan = df.create_physical_plan().await?;
        println!("\n=== Physical Plan ===");
        println!("{}", DisplayableExecutionPlan::new(physical_plan.as_ref()).indent(true));
        
        // 5. æ‰§è¡ŒæŸ¥è¯¢
        let exec_start = Instant::now();
        let results = df.collect().await?;
        let exec_time = exec_start.elapsed();
        
        // 6. æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        let total_rows: usize = results.iter().map(|b| b.num_rows()).sum();
        let total_bytes: usize = results.iter()
            .map(|b| b.get_array_memory_size())
            .sum();
        
        Ok(QueryProfile {
            parse_time,
            execution_time: exec_time,
            total_rows,
            total_bytes,
            num_batches: results.len(),
        })
    }
}

#[derive(Debug)]
pub struct QueryProfile {
    pub parse_time: Duration,
    pub execution_time: Duration,
    pub total_rows: usize,
    pub total_bytes: usize,
    pub num_batches: usize,
}

impl QueryProfile {
    pub fn print_summary(&self) {
        println!("\n=== Query Profile ===");
        println!("Parse time: {:?}", self.parse_time);
        println!("Execution time: {:?}", self.execution_time);
        println!("Total rows: {}", self.total_rows);
        println!("Total bytes: {} MB", self.total_bytes / 1024 / 1024);
        println!("Num batches: {}", self.num_batches);
        println!("Throughput: {:.2} M rows/sec", 
            self.total_rows as f64 / self.execution_time.as_secs_f64() / 1_000_000.0);
    }
}
```

### 5.2 é…ç½®ä¼˜åŒ–å»ºè®®

```rust
/// æ€§èƒ½è°ƒä¼˜é…ç½®
pub struct PerformanceTuning;

impl PerformanceTuning {
    /// æ¨èé…ç½®
    pub fn recommended_config() -> SessionConfig {
        SessionConfig::new()
            // å¹¶è¡Œåº¦: CPUæ ¸å¿ƒæ•°
            .with_target_partitions(num_cpus::get())
            
            // æ‰¹å¤„ç†å¤§å°: 8192 (å¹³è¡¡å†…å­˜å’Œæ€§èƒ½)
            .with_batch_size(8192)
            
            // å¯ç”¨æ‰€æœ‰ä¼˜åŒ–
            .set_bool("datafusion.optimizer.filter_null_join_keys", true)
            .set_bool("datafusion.optimizer.skip_failed_rules", false)
            .set_bool("datafusion.execution.coalesce_batches", true)
            
            // å†…å­˜é™åˆ¶ (é˜²æ­¢ OOM)
            .set_u64("datafusion.execution.memory_pool_size", 8 * 1024 * 1024 * 1024) // 8GB
            
            // æ’åºå†…å­˜é™åˆ¶
            .set_usize("datafusion.execution.sort_in_place_threshold_bytes", 256 * 1024 * 1024) // 256MB
            
            // åˆ†åŒºæ•°é™åˆ¶
            .set_usize("datafusion.execution.max_partitions", 512)
            
            // å¯ç”¨ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
            .set_bool("datafusion.execution.collect_statistics", true)
    }
    
    /// å¤§æ•°æ®é‡é…ç½®
    pub fn large_dataset_config() -> SessionConfig {
        Self::recommended_config()
            .with_batch_size(16384)  // æ›´å¤§çš„æ‰¹æ¬¡
            .set_u64("datafusion.execution.memory_pool_size", 32 * 1024 * 1024 * 1024) // 32GB
            .set_usize("datafusion.execution.max_partitions", 2048)
    }
    
    /// ä½å»¶è¿Ÿé…ç½®
    pub fn low_latency_config() -> SessionConfig {
        Self::recommended_config()
            .with_batch_size(1024)   // æ›´å°çš„æ‰¹æ¬¡
            .with_target_partitions(1)  // å‡å°‘å¹¶è¡Œåº¦
    }
}
```

---

## 6. ç”Ÿäº§éƒ¨ç½²

### 6.1 Kubernetes éƒ¨ç½²

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-query-engine
spec:
  replicas: 3
  selector:
    matchLabels:
      app: otlp-query-engine
  template:
    metadata:
      labels:
        app: otlp-query-engine
    spec:
      containers:
      - name: query-engine
        image: otlp-datafusion:latest
        resources:
          requests:
            memory: "8Gi"
            cpu: "4"
          limits:
            memory: "16Gi"
            cpu: "8"
        env:
        - name: RUST_LOG
          value: "info"
        - name: DATAFUSION_PARALLELISM
          value: "8"
        - name: DATAFUSION_BATCH_SIZE
          value: "8192"
        ports:
        - containerPort: 8080
          name: http
        volumeMounts:
        - name: data
          mountPath: /data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: traces-data-pvc
```

### 6.2 ç›‘æ§æŒ‡æ ‡

```rust
use metrics::{counter, histogram, gauge};

/// æŸ¥è¯¢å¼•æ“ç›‘æ§
pub struct QueryEngineMetrics;

impl QueryEngineMetrics {
    pub fn record_query_execution(
        duration: Duration,
        rows: usize,
        success: bool,
    ) {
        // æŸ¥è¯¢è®¡æ•°
        if success {
            counter!("query_total", "status" => "success").increment(1);
        } else {
            counter!("query_total", "status" => "error").increment(1);
        }
        
        // æŸ¥è¯¢å»¶è¿Ÿ
        histogram!("query_duration_seconds").record(duration.as_secs_f64());
        
        // å¤„ç†è¡Œæ•°
        histogram!("query_rows_processed").record(rows as f64);
        
        // ååé‡
        let throughput = rows as f64 / duration.as_secs_f64();
        histogram!("query_throughput_rows_per_sec").record(throughput);
    }
    
    pub fn record_cache_stats(hits: u64, misses: u64) {
        gauge!("cache_hit_rate").set(
            hits as f64 / (hits + misses) as f64
        );
    }
}
```

---

## æ€»ç»“

### ğŸ¯ DataFusion æœ€ä½³å®è·µ

```text
âœ… æŸ¥è¯¢ä¼˜åŒ–
   - å¯ç”¨è°“è¯ä¸‹æ¨
   - å¯ç”¨æŠ•å½±ä¸‹æ¨
   - ä½¿ç”¨åˆ†åŒºå‰ªæ
   - å……åˆ†åˆ©ç”¨ç»Ÿè®¡ä¿¡æ¯

âœ… æ€§èƒ½è°ƒä¼˜
   - åˆç†è®¾ç½®å¹¶è¡Œåº¦
   - ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
   - ä½¿ç”¨å‘é‡åŒ–æ‰§è¡Œ
   - é…ç½®å†…å­˜é™åˆ¶

âœ… æ•°æ®å»ºæ¨¡
   - ä½¿ç”¨ Parquet æ ¼å¼
   - åˆç†åˆ†åŒºç­–ç•¥
   - åˆ›å»ºç‰©åŒ–è§†å›¾
   - æ”¶é›†ç»Ÿè®¡ä¿¡æ¯

âœ… ç”Ÿäº§éƒ¨ç½²
   - å®¹å™¨åŒ–éƒ¨ç½²
   - èµ„æºé™åˆ¶
   - ç›‘æ§æŒ‡æ ‡
   - æ•…éšœæ¢å¤
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025å¹´10æœˆ10æ—¥  
**ç»´æŠ¤è€…**: OTLP Rust å›¢é˜Ÿ  
**è®¸å¯è¯**: MIT OR Apache-2.0

---

[ğŸ  è¿”å›ä¸»ç›®å½•](../README.md) | [â¬…ï¸ Arrow ä¼˜åŒ–](./02_Arrow_Rust_1.90_æœ€æ–°ä¼˜åŒ–å®è·µ.md)
