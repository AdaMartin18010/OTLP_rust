# ğŸ¤– AI/MLé©±åŠ¨å¯è§‚æµ‹æ€§ç”Ÿæ€è¿½è¸ªæŠ¥å‘Š

**æŠ¥å‘Šæ—¥æœŸ**: 2025-10-09  
**è¿½è¸ªå‘¨æœŸ**: æŒç»­æ›´æ–°  
**é‡è¦æ€§**: ğŸ”´ æœ€é«˜ä¼˜å…ˆçº§

---

## ğŸ“‹ ç›®å½•

- [ğŸ¤– AI/MLé©±åŠ¨å¯è§‚æµ‹æ€§ç”Ÿæ€è¿½è¸ªæŠ¥å‘Š](#-aimlé©±åŠ¨å¯è§‚æµ‹æ€§ç”Ÿæ€è¿½è¸ªæŠ¥å‘Š)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ“Š æ‰§è¡Œæ‘˜è¦](#-æ‰§è¡Œæ‘˜è¦)
  - [ğŸŒ AIé©±åŠ¨å¯è§‚æµ‹æ€§å…¨æ™¯å›¾](#-aié©±åŠ¨å¯è§‚æµ‹æ€§å…¨æ™¯å›¾)
    - [1. ä¸»æµå‚å•†AIèƒ½åŠ›å¯¹æ¯”](#1-ä¸»æµå‚å•†aièƒ½åŠ›å¯¹æ¯”)
    - [2. AIå¯è§‚æµ‹æ€§æŠ€æœ¯æ ˆ](#2-aiå¯è§‚æµ‹æ€§æŠ€æœ¯æ ˆ)
  - [ğŸ”¥ æ ¸å¿ƒæŠ€æœ¯æ·±åº¦åˆ†æ](#-æ ¸å¿ƒæŠ€æœ¯æ·±åº¦åˆ†æ)
    - [1. å¼‚å¸¸æ£€æµ‹ (Anomaly Detection)](#1-å¼‚å¸¸æ£€æµ‹-anomaly-detection)
      - [1.1 Datadog Watchdog](#11-datadog-watchdog)
      - [1.2 Dynatrace Davis AI](#12-dynatrace-davis-ai)
    - [2. å¤§è¯­è¨€æ¨¡å‹ (LLM) åº”ç”¨](#2-å¤§è¯­è¨€æ¨¡å‹-llm-åº”ç”¨)
      - [2.1 æœ¬é¡¹ç›®ç°çŠ¶](#21-æœ¬é¡¹ç›®ç°çŠ¶)
      - [2.2 å‰æ²¿è¶‹åŠ¿: å¤šæ¨¡æ€å¯è§‚æµ‹æ€§LLM](#22-å‰æ²¿è¶‹åŠ¿-å¤šæ¨¡æ€å¯è§‚æµ‹æ€§llm)
    - [3. é¢„æµ‹æ€§ç»´æŠ¤ (Predictive Maintenance)](#3-é¢„æµ‹æ€§ç»´æŠ¤-predictive-maintenance)
      - [3.1 æŠ€æœ¯åŸç†](#31-æŠ€æœ¯åŸç†)
  - [ğŸš€ æœ¬é¡¹ç›®æ”¹è¿›è¡ŒåŠ¨è®¡åˆ’](#-æœ¬é¡¹ç›®æ”¹è¿›è¡ŒåŠ¨è®¡åˆ’)
    - [çŸ­æœŸ (Q4 2025)](#çŸ­æœŸ-q4-2025)
      - [ä»»åŠ¡1: æ—¶åºå¼‚å¸¸æ£€æµ‹å®æˆ˜æŒ‡å— (ğŸ”´ P0)](#ä»»åŠ¡1-æ—¶åºå¼‚å¸¸æ£€æµ‹å®æˆ˜æŒ‡å—--p0)
      - [ä»»åŠ¡2: é¢„æµ‹æ€§ç»´æŠ¤å®Œæ•´æŒ‡å— (ğŸ”´ P0)](#ä»»åŠ¡2-é¢„æµ‹æ€§ç»´æŠ¤å®Œæ•´æŒ‡å—--p0)
      - [ä»»åŠ¡3: å¤šæ¨¡æ€LLMåˆ†æ (ğŸŸ¡ P1)](#ä»»åŠ¡3-å¤šæ¨¡æ€llmåˆ†æ--p1)
    - [ä¸­æœŸ (2026 H1)](#ä¸­æœŸ-2026-h1)
      - [ä»»åŠ¡4: AIå¯è§‚æµ‹æ€§å¹³å°æ¶æ„](#ä»»åŠ¡4-aiå¯è§‚æµ‹æ€§å¹³å°æ¶æ„)
      - [ä»»åŠ¡5: LLMå¾®è°ƒä¸RAGå®æˆ˜](#ä»»åŠ¡5-llmå¾®è°ƒä¸ragå®æˆ˜)
  - [ğŸ“š æ¨èå­¦ä¹ èµ„æº](#-æ¨èå­¦ä¹ èµ„æº)
    - [AI/MLè¯¾ç¨‹](#aimlè¯¾ç¨‹)
    - [å¯è§‚æµ‹æ€§ + AI](#å¯è§‚æµ‹æ€§--ai)
    - [æŠ€æœ¯è®ºæ–‡](#æŠ€æœ¯è®ºæ–‡)

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

AI/MLæ­£åœ¨ä»æ ¹æœ¬ä¸Šæ”¹å˜å¯è§‚æµ‹æ€§é¢†åŸŸã€‚ä»è¢«åŠ¨ç›‘æ§åˆ°ä¸»åŠ¨é¢„æµ‹,ä»äººå·¥åˆ†æåˆ°æ™ºèƒ½RCA,AIé©±åŠ¨çš„å¯è§‚æµ‹æ€§æ­£åœ¨æˆä¸º2025å¹´çš„æ ¸å¿ƒè¶‹åŠ¿ã€‚

**æ ¸å¿ƒå‘ç°**:

- âœ… Datadog Watchdogã€Grafana MLç­‰AIåŠŸèƒ½å·²ç”Ÿäº§å°±ç»ª
- ğŸ”¥ LLMç”¨äºæ—¥å¿—åˆ†æå’Œæ ¹å› åˆ†æå¿«é€Ÿæ™®åŠ
- ğŸ“ˆ æ—¶åºå¼‚å¸¸æ£€æµ‹ã€é¢„æµ‹æ€§ç»´æŠ¤æˆä¸ºæ ‡é…
- âš ï¸ æœ¬é¡¹ç›®AIèƒ½åŠ›éœ€è¦å¤§å¹…å¢å¼º (å½“å‰ä»…æœ‰LLMæ—¥å¿—åˆ†æåŸºç¡€)

---

## ğŸŒ AIé©±åŠ¨å¯è§‚æµ‹æ€§å…¨æ™¯å›¾

### 1. ä¸»æµå‚å•†AIèƒ½åŠ›å¯¹æ¯”

| å‚å•† | AIåŠŸèƒ½ | æˆç†Ÿåº¦ | æ ¸å¿ƒæŠ€æœ¯ | å•†ä¸šåŒ– |
|------|--------|--------|---------|--------|
| **Datadog** | Watchdog (å¼‚å¸¸æ£€æµ‹/RCA) | ç”Ÿäº§å°±ç»ª | ä¸“æœ‰AIå¼•æ“ | ä»˜è´¹åŠŸèƒ½ |
| **Dynatrace** | Davis AI (é¢„æµ‹/RCA) | ç”Ÿäº§å°±ç»ª | å› æœAI | åŒ…å«åœ¨å¥—é¤ |
| **New Relic** | AI Ops (å¼‚å¸¸æ£€æµ‹) | ç”Ÿäº§å°±ç»ª | æœºå™¨å­¦ä¹  | ä»˜è´¹åŠŸèƒ½ |
| **Splunk** | ITSI + MLTK | ç”Ÿäº§å°±ç»ª | ML Toolkit | ä»˜è´¹æ¨¡å— |
| **Elastic** | ML Jobs (å¼‚å¸¸æ£€æµ‹) | ç”Ÿäº§å°±ç»ª | X-Pack ML | ç™½é‡‘çº§+ |
| **Grafana Labs** | Machine Learning | Beta | Adaptive Metrics | è®¡åˆ’ä¸­ |
| **AWS** | DevOps Guru | ç”Ÿäº§å°±ç»ª | AWS MLæœåŠ¡ | æŒ‰ä½¿ç”¨ä»˜è´¹ |

### 2. AIå¯è§‚æµ‹æ€§æŠ€æœ¯æ ˆ

```mermaid
graph TB
    subgraph "æ•°æ®å±‚"
        TRACES[Traces]
        METRICS[Metrics]
        LOGS[Logs]
        PROFILES[Profiles]
    end
    
    subgraph "ç‰¹å¾å·¥ç¨‹"
        FEATURE[ç‰¹å¾æå–]
        TIMESERIES[æ—¶é—´åºåˆ—å¤„ç†]
        EMBEDDING[å‘é‡åµŒå…¥]
    end
    
    subgraph "AI/MLæ¨¡å‹"
        ANOMALY[å¼‚å¸¸æ£€æµ‹]
        FORECAST[é¢„æµ‹æ¨¡å‹]
        RCA[æ ¹å› åˆ†æ]
        LLM[å¤§è¯­è¨€æ¨¡å‹]
    end
    
    subgraph "æ™ºèƒ½åº”ç”¨"
        ALERT[æ™ºèƒ½å‘Šè­¦]
        AUTOHEAL[è‡ªæ„ˆä¿®å¤]
        CAPACITY[å®¹é‡è§„åˆ’]
        INCIDENT[äº‹ä»¶ç®¡ç†]
    end
    
    TRACES --> FEATURE
    METRICS --> TIMESERIES
    LOGS --> EMBEDDING
    PROFILES --> FEATURE
    
    FEATURE --> ANOMALY
    TIMESERIES --> FORECAST
    EMBEDDING --> LLM
    
    ANOMALY --> ALERT
    FORECAST --> CAPACITY
    RCA --> INCIDENT
    LLM --> AUTOHEAL
```

---

## ğŸ”¥ æ ¸å¿ƒæŠ€æœ¯æ·±åº¦åˆ†æ

### 1. å¼‚å¸¸æ£€æµ‹ (Anomaly Detection)

#### 1.1 Datadog Watchdog

**æŠ€æœ¯åŸç†**:

```yaml
ç®—æ³•:
  - åŠ¨æ€åŸºçº¿ (Adaptive Baselines)
  - å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹
  - å­£èŠ‚æ€§æ¨¡å¼è¯†åˆ« (Seasonal Patterns)
  - è¶‹åŠ¿åˆ†æ (Trend Analysis)

æ£€æµ‹èŒƒå›´:
  - Metricså¼‚å¸¸ (CPU/å†…å­˜/å»¶è¿Ÿ)
  - APMå¼‚å¸¸ (é”™è¯¯ç‡/ååé‡)
  - Logså¼‚å¸¸ (é”™è¯¯æ—¥å¿—çªå¢)
  - Traceså¼‚å¸¸ (æ…¢æŸ¥è¯¢/é”™è¯¯é“¾è·¯)

å®æ—¶æ€§:
  - æ£€æµ‹å»¶è¿Ÿ: < 1åˆ†é’Ÿ
  - è¯¯æŠ¥ç‡: < 2% (å®˜æ–¹æ•°æ®)
  - å‡†ç¡®ç‡: > 95%
```

**æ¶æ„è®¾è®¡**:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Datadog Watchdogå¼•æ“                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  1. æ•°æ®æ‘„å…¥ä¸é¢„å¤„ç†                 â”‚    â”‚
â”‚  â”‚     - å¤šæºæ•°æ®èšåˆ                   â”‚    â”‚
â”‚  â”‚     - å½’ä¸€åŒ–/æ¸…æ´—                    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                  â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  2. ç‰¹å¾å·¥ç¨‹                         â”‚    â”‚
â”‚  â”‚     - ç»Ÿè®¡ç‰¹å¾ (å‡å€¼/æ–¹å·®/åˆ†ä½æ•°)     â”‚    â”‚
â”‚  â”‚     - æ—¶é—´ç‰¹å¾ (å‘¨æœŸ/è¶‹åŠ¿)            â”‚    â”‚
â”‚  â”‚     - ä¸Šä¸‹æ–‡ç‰¹å¾ (æœåŠ¡å…³ç³»/ä¾èµ–)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                  â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  3. å¼‚å¸¸æ£€æµ‹æ¨¡å‹                     â”‚    â”‚
â”‚  â”‚     - å•æŒ‡æ ‡æ£€æµ‹: 3-Sigma, IQR       â”‚    â”‚
â”‚  â”‚     - å¤šæŒ‡æ ‡æ£€æµ‹: PCA, Isolation Forestâ”‚ â”‚
â”‚  â”‚     - æ—¶åºæ£€æµ‹: LSTM, Prophet        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                  â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  4. æ ¹å› åˆ†æ (RCA)                   â”‚    â”‚
â”‚  â”‚     - ä¾èµ–å›¾åˆ†æ                     â”‚    â”‚
â”‚  â”‚     - æ—¶åºç›¸å…³æ€§åˆ†æ                 â”‚    â”‚
â”‚  â”‚     - å†å²äº‹ä»¶åŒ¹é…                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                  â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  5. æ™ºèƒ½å‘Šè­¦                         â”‚    â”‚
â”‚  â”‚     - å‘Šè­¦èšåˆ                       â”‚    â”‚
â”‚  â”‚     - ä¼˜å…ˆçº§æ’åº                     â”‚    â”‚
â”‚  â”‚     - é™å™ª (Alert Fatigue Reduction) â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å®ç°ç¤ºä¾‹** (Python,åŸºäºæœ¬é¡¹ç›®å½“å‰LLMæ—¥å¿—åˆ†æ):

```python
import numpy as np
from sklearn.ensemble import IsolationForest
from prophet import Prophet
import pandas as pd

class AnomalyDetector:
    """æ—¶åºå¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.isolation_forest = IsolationForest(
            contamination=0.01,  # é¢„æœŸå¼‚å¸¸ç‡1%
            random_state=42
        )
        self.prophet_model = None
    
    def detect_multivariate_anomaly(
        self, 
        metrics: pd.DataFrame
    ) -> pd.DataFrame:
        """
        å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹ (Isolation Forest)
        
        Args:
            metrics: åŒ…å«å¤šä¸ªæŒ‡æ ‡çš„DataFrame (cpu, memory, latencyç­‰)
        
        Returns:
            å¼‚å¸¸æ£€æµ‹ç»“æœ (is_anomalyåˆ—)
        """
        # è®­ç»ƒIsolation Forest
        self.isolation_forest.fit(metrics)
        
        # é¢„æµ‹å¼‚å¸¸
        predictions = self.isolation_forest.predict(metrics)
        
        # -1è¡¨ç¤ºå¼‚å¸¸, 1è¡¨ç¤ºæ­£å¸¸
        metrics['is_anomaly'] = predictions == -1
        metrics['anomaly_score'] = self.isolation_forest.score_samples(metrics)
        
        return metrics
    
    def detect_timeseries_anomaly(
        self,
        timeseries: pd.DataFrame,
        forecast_periods: int = 24
    ) -> pd.DataFrame:
        """
        æ—¶åºå¼‚å¸¸æ£€æµ‹ (Prophet)
        
        Args:
            timeseries: æ—¶é—´åºåˆ—æ•°æ® (ds, yåˆ—)
            forecast_periods: é¢„æµ‹å‘¨æœŸæ•°
        
        Returns:
            å¼‚å¸¸æ£€æµ‹ç»“æœ
        """
        # è®­ç»ƒProphetæ¨¡å‹
        self.prophet_model = Prophet(
            interval_width=0.95,  # 95%ç½®ä¿¡åŒºé—´
            changepoint_prior_scale=0.5,  # è¶‹åŠ¿å˜åŒ–çµæ•åº¦
            seasonality_mode='multiplicative'  # å­£èŠ‚æ€§æ¨¡å¼
        )
        self.prophet_model.fit(timeseries)
        
        # é¢„æµ‹
        future = self.prophet_model.make_future_dataframe(
            periods=forecast_periods,
            freq='H'
        )
        forecast = self.prophet_model.predict(future)
        
        # æ£€æµ‹å¼‚å¸¸ (å®é™…å€¼è¶…å‡ºç½®ä¿¡åŒºé—´)
        merged = timeseries.merge(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']], on='ds')
        merged['is_anomaly'] = (
            (merged['y'] < merged['yhat_lower']) | 
            (merged['y'] > merged['yhat_upper'])
        )
        
        return merged

# ä½¿ç”¨ç¤ºä¾‹
detector = AnomalyDetector()

# 1. å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹
metrics_data = pd.DataFrame({
    'cpu_usage': np.random.normal(50, 10, 1000),
    'memory_usage': np.random.normal(70, 5, 1000),
    'latency_ms': np.random.normal(100, 20, 1000)
})
# æ³¨å…¥å‡ ä¸ªå¼‚å¸¸ç‚¹
metrics_data.loc[100:102, 'cpu_usage'] = 95
metrics_data.loc[100:102, 'latency_ms'] = 500

anomalies_multi = detector.detect_multivariate_anomaly(metrics_data)
print(f"æ£€æµ‹åˆ° {anomalies_multi['is_anomaly'].sum()} ä¸ªå¼‚å¸¸ç‚¹")

# 2. æ—¶åºå¼‚å¸¸æ£€æµ‹
timeseries_data = pd.DataFrame({
    'ds': pd.date_range('2024-01-01', periods=1000, freq='H'),
    'y': np.random.normal(100, 10, 1000)
})
# æ³¨å…¥è¶‹åŠ¿å˜åŒ–
timeseries_data.loc[500:, 'y'] += 50

anomalies_ts = detector.detect_timeseries_anomaly(timeseries_data)
print(f"æ£€æµ‹åˆ° {anomalies_ts['is_anomaly'].sum()} ä¸ªæ—¶åºå¼‚å¸¸")
```

**æœ¬é¡¹ç›®å·®è·**:

| ç»´åº¦ | Datadog Watchdog | æœ¬é¡¹ç›® | å·®è· |
|------|-----------------|--------|------|
| æ—¶åºå¼‚å¸¸æ£€æµ‹ | âœ… Prophet/LSTM | âŒ æ—  | å®Œå…¨ç¼ºå¤± |
| å¤šç»´åº¦æ£€æµ‹ | âœ… Isolation Forest | âŒ æ—  | å®Œå…¨ç¼ºå¤± |
| RCAå¼•æ“ | âœ… ä¾èµ–å›¾åˆ†æ | âš ï¸ LLMåŸºç¡€ | éœ€è¦å¢å¼º |
| å®æ—¶æ€§ | âœ… < 1åˆ†é’Ÿ | âŒ æ—  | ç¼ºå°‘å®æ—¶å¼•æ“ |

---

#### 1.2 Dynatrace Davis AI

**æ ¸å¿ƒæŠ€æœ¯**: å› æœAI (Causal AI)

**å·®å¼‚åŒ–ç‰¹ç‚¹**:

```yaml
å› æœæ¨ç†:
  - ä¸ä»…æ£€æµ‹ç›¸å…³æ€§,æ›´æ¨æ–­å› æœå…³ç³»
  - è‡ªåŠ¨æ„å»ºæœåŠ¡ä¾èµ–å›¾
  - æ ¹æ®å› æœé“¾å®šä½æ ¹å› 
  
è‡ªåŠ¨åŸºçº¿:
  - è‡ªåŠ¨å­¦ä¹ æ­£å¸¸è¡Œä¸ºæ¨¡å¼
  - æ— éœ€äººå·¥é…ç½®é˜ˆå€¼
  - åŠ¨æ€é€‚åº”ç¯å¢ƒå˜åŒ–
  
é¢„æµ‹æ€§ç»´æŠ¤:
  - æå‰3-7å¤©é¢„æµ‹æ•…éšœ
  - èµ„æºè€—å°½é¢„è­¦
  - å®¹é‡è§„åˆ’å»ºè®®
```

**æ ¹å› åˆ†æç¤ºä¾‹**:

```text
åœºæ™¯: ç”¨æˆ·æŠ¥å‘ŠæœåŠ¡å“åº”æ…¢

Davis AIåˆ†æè¿‡ç¨‹:
1. æ£€æµ‹åˆ° Web Service å“åº”æ—¶é—´å¼‚å¸¸ (ä»100msé£™å‡åˆ°500ms)
2. è¿½è¸ªä¾èµ–é“¾:
   Web Service â†’ API Gateway â†’ User Service â†’ Database
3. å‘ç° Database æŸ¥è¯¢æ—¶é—´å¢åŠ  (ä»10msåˆ°400ms)
4. åˆ†æ Database èµ„æº:
   - CPU: æ­£å¸¸ (50%)
   - å†…å­˜: æ­£å¸¸ (60%)
   - ç£ç›˜I/O: å¼‚å¸¸ (IOPSä»1000é£™å‡åˆ°10000)
5. å®šä½æ ¹å› :
   - ç´¢å¼•ç¼ºå¤±å¯¼è‡´å…¨è¡¨æ‰«æ
   - æ¨è: åœ¨user_idåˆ—æ·»åŠ ç´¢å¼•
6. å½±å“èŒƒå›´:
   - 3ä¸ªæœåŠ¡å—å½±å“
   - çº¦5000 req/så—æŸ
```

**æœ¬é¡¹ç›®æ”¹è¿›æ–¹å‘**:

- è¡¥å……å› æœAIç®—æ³•åŸç†
- å®ç°ç®€åŒ–ç‰ˆä¾èµ–å›¾åˆ†æ
- æä¾›æ ¹å› åˆ†æå®æˆ˜æ¡ˆä¾‹

---

### 2. å¤§è¯­è¨€æ¨¡å‹ (LLM) åº”ç”¨

#### 2.1 æœ¬é¡¹ç›®ç°çŠ¶

**å·²æœ‰å†…å®¹** (ğŸ¤–_AIé©±åŠ¨æ—¥å¿—åˆ†æå®Œæ•´æŒ‡å—):

- âœ… LLMæ—¥å¿—å¼‚å¸¸æ£€æµ‹
- âœ… LLMæ ¹å› åˆ†æ
- âœ… è‡ªç„¶è¯­è¨€æŸ¥è¯¢
- âœ… æˆæœ¬ä¼˜åŒ–ç­–ç•¥

**ä¼˜åŠ¿**:

- æ–‡æ¡£å®Œæ•´åº¦é«˜ (2400è¡Œ)
- ä»£ç ç¤ºä¾‹ä¸°å¯Œ
- æˆæœ¬ä¼˜åŒ–å®ç”¨

**ä¸è¶³**:

- ç¼ºå°‘å¤šæ¨¡æ€åˆ†æ (Logs + Metrics + Traces)
- ç¼ºå°‘LLMå¾®è°ƒæ¡ˆä¾‹ (Fine-tuning)
- ç¼ºå°‘RAG (Retrieval-Augmented Generation) æ·±åŒ–

---

#### 2.2 å‰æ²¿è¶‹åŠ¿: å¤šæ¨¡æ€å¯è§‚æµ‹æ€§LLM

**ä»£è¡¨é¡¹ç›®**: OpenAI GPT-4o, Google Gemini (å¤šæ¨¡æ€)

**åº”ç”¨åœºæ™¯**:

```yaml
1. è·¨ä¿¡å·å…³è”åˆ†æ:
   è¾“å…¥: 
     - Logs: "Database connection timeout"
     - Metrics: CPU 90%, Memory 95%
     - Traces: Slow Query Span (5s)
   è¾“å‡º:
     - æ ¹å› : å†…å­˜ä¸è¶³å¯¼è‡´æ•°æ®åº“è¿æ¥æ± è€—å°½
     - å»ºè®®: å¢åŠ å†…å­˜ OR ä¼˜åŒ–æŸ¥è¯¢ OR è°ƒæ•´è¿æ¥æ± 

2. å¯è§†åŒ–ç†è§£:
   è¾“å…¥: Grafanaæˆªå›¾ (CPU/Memoryæ›²çº¿)
   è¾“å‡º: 
     - åˆ†æ: "CPUåœ¨12:00çªç„¶é£™å‡,ç–‘ä¼¼å®šæ—¶ä»»åŠ¡è§¦å‘"
     - å»ºè®®: "æ£€æŸ¥Cron Jobé…ç½®"

3. ä»£ç çº§è¯Šæ–­:
   è¾“å…¥: 
     - Trace: Slow Span in function `getUserProfile()`
     - Code: Pythonå‡½æ•°ä»£ç 
   è¾“å‡º:
     - é—®é¢˜: "N+1æŸ¥è¯¢é—®é¢˜,å¾ªç¯ä¸­è°ƒç”¨æ•°æ®åº“"
     - å»ºè®®: "ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢æˆ–ç¼“å­˜"
```

**å®ç°ç¤ºä¾‹** (Python, åŸºäºGPT-4o API):

```python
    import openai
    from typing import Dict, List
    import json

    class MultimodalObservabilityLLM:
        """å¤šæ¨¡æ€å¯è§‚æµ‹æ€§LLMåˆ†æå™¨"""
        
        def __init__(self, api_key: str):
            self.client = openai.OpenAI(api_key=api_key)
        
        def analyze_multimodal(
            self,
            logs: List[str],
            metrics: Dict[str, float],
            trace_span: Dict,
            screenshot_url: str = None
        ) -> Dict:
            """
            å¤šæ¨¡æ€åˆ†æ (Logs + Metrics + Traces + Screenshot)
            
            Args:
                logs: æ—¥å¿—åˆ—è¡¨
                metrics: æŒ‡æ ‡å­—å…¸ {metric_name: value}
                trace_span: Trace Spanæ•°æ®
                screenshot_url: Grafanaæˆªå›¾URL (å¯é€‰)
            
            Returns:
                åˆ†æç»“æœ
            """
            # æ„å»ºå¤šæ¨¡æ€Prompt
            prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªå¯è§‚æµ‹æ€§ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹æ•°æ®å¹¶ç»™å‡ºæ ¹å› å’Œå»ºè®®ã€‚

    # æ—¥å¿— (Logs)
    ```

    {chr(10).join(logs)}

    ```

    # æŒ‡æ ‡ (Metrics)
    ```json
    {json.dumps(metrics, indent=2)}
    ```

    # è¿½è¸ª (Trace Span)

    ```json
    {json.dumps(trace_span, indent=2)}
    ```

    è¯·å›ç­”:

    1. æ ¹æœ¬åŸå› æ˜¯ä»€ä¹ˆ?
    2. ä¸ºä»€ä¹ˆä¼šå¯¼è‡´è¿™ä¸ªé—®é¢˜?
    3. æ¨èçš„ä¿®å¤æ–¹æ¡ˆæ˜¯ä»€ä¹ˆ? (è‡³å°‘3ä¸ª,æŒ‰ä¼˜å…ˆçº§æ’åº)
    4. å¦‚ä½•é¢„é˜²ç±»ä¼¼é—®é¢˜?
    """

            # æ„å»ºæ¶ˆæ¯ (åŒ…å«å›¾ç‰‡,å¦‚æœæœ‰)
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯è§‚æµ‹æ€§åˆ†æä¸“å®¶,æ“…é•¿æ ¹å› åˆ†æå’Œæ•…éšœè¯Šæ–­ã€‚"
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # å¦‚æœæœ‰æˆªå›¾,æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
            if screenshot_url:
                messages[1]["content"].append({
                    "type": "image_url",
                    "image_url": {"url": screenshot_url}
                })
            
            # è°ƒç”¨GPT-4o (æ”¯æŒå¤šæ¨¡æ€)
            response = self.client.chat.completions.create(
                model="gpt-4o",  # æˆ– "gpt-4-vision-preview"
                messages=messages,
                temperature=0.3,
                max_tokens=2000
            )
            
            # è§£æç»“æœ
            analysis = response.choices[0].message.content
            
            return {
                "root_cause": self._extract_section(analysis, "æ ¹æœ¬åŸå› "),
                "explanation": self._extract_section(analysis, "ä¸ºä»€ä¹ˆ"),
                "recommendations": self._extract_section(analysis, "æ¨è"),
                "prevention": self._extract_section(analysis, "é¢„é˜²"),
                "full_analysis": analysis
            }

        def _extract_section(self, text: str, section_name: str) -> str:
            """ä»LLMå“åº”ä¸­æå–ç‰¹å®šç« èŠ‚"""
            # ç®€åŒ–å®ç°,å®é™…å¯ç”¨æ›´å¤æ‚çš„è§£æé€»è¾‘
            lines = text.split('\n')
            section_lines = []
            in_section = False

            for line in lines:
                if section_name in line:
                    in_section = True
                    continue
                if in_section:
                    if line.startswith('#') or line.startswith('##'):
                        break
                    section_lines.append(line)
            
            return '\n'.join(section_lines).strip()

    # ä½¿ç”¨ç¤ºä¾‹

    analyzer = MultimodalObservabilityLLM(api_key="your-openai-api-key")

    # åœºæ™¯: æ•°æ®åº“æ…¢æŸ¥è¯¢

    result = analyzer.analyze_multimodal(
        logs=[
            "2025-10-09 10:15:32 ERROR Database query timeout after 30s",
            "2025-10-09 10:15:32 WARN Connection pool exhausted (100/100 connections)"
        ],
        metrics={
            "database.cpu_usage": 95.0,
            "database.memory_usage": 98.0,
            "database.connections": 100,
            "database.query_time_p99": 30000  # 30ç§’
        },
        trace_span={
            "span_id": "abc123",
            "operation_name": "SELECT *FROM users",
            "duration_ms": 30000,
            "attributes": {
                "db.system": "postgresql",
                "db.statement": "SELECT* FROM users WHERE created_at > '2024-01-01'"
            }
        },
        screenshot_url="<https://example.com/grafana-screenshot.png>"
    )

    print("æ ¹å› :", result["root_cause"])
    print("å»ºè®®:", result["recommendations"])

```

**æœ¬é¡¹ç›®æ”¹è¿›æ–¹å‘**:

1. è¡¥å……å¤šæ¨¡æ€LLMåˆ†æç« èŠ‚
2. æä¾›GPT-4o/Geminié›†æˆç¤ºä¾‹
3. ç¼–å†™å¯è§†åŒ–åˆ†ææ¡ˆä¾‹ (Grafanaæˆªå›¾ â†’ LLMè¯Šæ–­)
4. æ¢ç´¢LLMå¾®è°ƒ (Fine-tuning) - ä½¿ç”¨å…¬å¸å†å²æ•…éšœæ•°æ®

---

### 3. é¢„æµ‹æ€§ç»´æŠ¤ (Predictive Maintenance)

#### 3.1 æŠ€æœ¯åŸç†

**æ ¸å¿ƒç®—æ³•**:

```yaml
1. æ—¶åºé¢„æµ‹:
   - ARIMA (è‡ªå›å½’ç§»åŠ¨å¹³å‡)
   - Prophet (Facebookæ—¶åºé¢„æµ‹)
   - LSTM (é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ)
   - Transformer (Attentionæœºåˆ¶)

2. è¶‹åŠ¿å¤–æ¨:
   - çº¿æ€§å›å½’
   - æŒ‡æ•°å¹³æ»‘
   - å¤šé¡¹å¼æ‹Ÿåˆ

3. èµ„æºè€—å°½é¢„æµ‹:
   - ç£ç›˜ç©ºé—´é¢„æµ‹ (åŸºäºå†å²å¢é•¿ç‡)
   - å†…å­˜æ³„æ¼æ£€æµ‹ (å†…å­˜æŒç»­å¢é•¿)
   - CPUé¥±å’Œé¢„æµ‹ (è´Ÿè½½è¶‹åŠ¿)
```

**å®ç°ç¤ºä¾‹** (Python, Prophet):

```python
from prophet import Prophet
import pandas as pd
import numpy as np

class PredictiveMaintenance:
    """é¢„æµ‹æ€§ç»´æŠ¤å¼•æ“"""
    
    def predict_disk_full(
        self,
        disk_usage_history: pd.DataFrame,
        threshold: float = 0.9
    ) -> Dict:
        """
        é¢„æµ‹ç£ç›˜ä½•æ—¶è€—å°½
        
        Args:
            disk_usage_history: å†å²ç£ç›˜ä½¿ç”¨ç‡ (ds, yåˆ—)
            threshold: å‘Šè­¦é˜ˆå€¼ (é»˜è®¤90%)
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        # è®­ç»ƒProphetæ¨¡å‹
        model = Prophet(
            changepoint_prior_scale=0.05,  # é™ä½è¶‹åŠ¿å˜åŒ–çµæ•åº¦
            yearly_seasonality=False,
            weekly_seasonality=True,
            daily_seasonality=True
        )
        model.fit(disk_usage_history)
        
        # é¢„æµ‹æœªæ¥30å¤©
        future = model.make_future_dataframe(periods=30, freq='D')
        forecast = model.predict(future)
        
        # æŸ¥æ‰¾ä½•æ—¶è¶…è¿‡é˜ˆå€¼
        exceed_threshold = forecast[forecast['yhat'] >= threshold]
        
        if len(exceed_threshold) > 0:
            days_until_full = (exceed_threshold.iloc[0]['ds'] - pd.Timestamp.now()).days
            return {
                "status": "WARNING",
                "days_until_full": days_until_full,
                "predicted_date": exceed_threshold.iloc[0]['ds'].strftime('%Y-%m-%d'),
                "current_usage": disk_usage_history['y'].iloc[-1],
                "predicted_usage_at_threshold": exceed_threshold.iloc[0]['yhat'],
                "recommendation": f"è¯·åœ¨{days_until_full}å¤©å†…æ¸…ç†ç£ç›˜æˆ–æ‰©å®¹"
            }
        else:
            return {
                "status": "OK",
                "days_until_full": None,
                "message": "æœªæ¥30å¤©å†…ç£ç›˜ä¸ä¼šè€—å°½"
            }
    
    def detect_memory_leak(
        self,
        memory_usage_history: pd.DataFrame,
        window_size: int = 24
    ) -> Dict:
        """
        æ£€æµ‹å†…å­˜æ³„æ¼ (æŒç»­å¢é•¿çš„å†…å­˜ä½¿ç”¨)
        
        Args:
            memory_usage_history: å†å²å†…å­˜ä½¿ç”¨ç‡ (ds, yåˆ—)
            window_size: æ»‘åŠ¨çª—å£å¤§å° (å°æ—¶)
        
        Returns:
            æ£€æµ‹ç»“æœ
        """
        # è®¡ç®—æ»‘åŠ¨çª—å£å†…çš„çº¿æ€§å›å½’æ–œç‡
        memory_usage_history = memory_usage_history.sort_values('ds')
        memory_usage_history['slope'] = memory_usage_history['y'].rolling(
            window=window_size
        ).apply(
            lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == window_size else 0
        )
        
        # æœ€è¿‘çš„æ–œç‡
        recent_slope = memory_usage_history['slope'].iloc[-1]
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå†…å­˜æ³„æ¼ (æŒç»­æ­£æ–œç‡)
        if recent_slope > 0.001:  # æ¯å°æ—¶å¢é•¿ > 0.1%
            # é¢„æµ‹ä½•æ—¶å†…å­˜è€—å°½ (å‡è®¾æŒç»­å¢é•¿)
            current_usage = memory_usage_history['y'].iloc[-1]
            hours_until_full = (1.0 - current_usage) / recent_slope
            
            return {
                "status": "MEMORY_LEAK_DETECTED",
                "slope": recent_slope,
                "hours_until_full": hours_until_full,
                "recommendation": f"æ£€æµ‹åˆ°å†…å­˜æ³„æ¼!é¢„è®¡{hours_until_full:.1f}å°æ—¶åå†…å­˜è€—å°½,è¯·ç«‹å³æ’æŸ¥"
            }
        else:
            return {
                "status": "OK",
                "slope": recent_slope,
                "message": "æœªæ£€æµ‹åˆ°å†…å­˜æ³„æ¼"
            }

# ä½¿ç”¨ç¤ºä¾‹
pm = PredictiveMaintenance()

# 1. ç£ç›˜è€—å°½é¢„æµ‹
disk_data = pd.DataFrame({
    'ds': pd.date_range('2024-01-01', periods=90, freq='D'),
    'y': np.linspace(0.3, 0.85, 90) + np.random.normal(0, 0.02, 90)
})
disk_prediction = pm.predict_disk_full(disk_data)
print(disk_prediction)

# 2. å†…å­˜æ³„æ¼æ£€æµ‹
memory_data = pd.DataFrame({
    'ds': pd.date_range('2024-10-01', periods=72, freq='H'),
    'y': np.linspace(0.5, 0.75, 72) + np.random.normal(0, 0.01, 72)
})
leak_detection = pm.detect_memory_leak(memory_data)
print(leak_detection)
```

**æœ¬é¡¹ç›®å·®è·**:

| ç»´åº¦ | ä¸šç•Œæœ€ä½³ | æœ¬é¡¹ç›® | å·®è· |
|------|---------|--------|------|
| æ—¶åºé¢„æµ‹ | âœ… Prophet/LSTM | âŒ æ—  | å®Œå…¨ç¼ºå¤± |
| èµ„æºè€—å°½é¢„æµ‹ | âœ… å®Œæ•´ | âŒ æ—  | å®Œå…¨ç¼ºå¤± |
| å†…å­˜æ³„æ¼æ£€æµ‹ | âœ… å®Œæ•´ | âŒ æ—  | å®Œå…¨ç¼ºå¤± |
| å®¹é‡è§„åˆ’ | âœ… å®Œæ•´ | âŒ æ—  | å®Œå…¨ç¼ºå¤± |

---

## ğŸš€ æœ¬é¡¹ç›®æ”¹è¿›è¡ŒåŠ¨è®¡åˆ’

### çŸ­æœŸ (Q4 2025)

#### ä»»åŠ¡1: æ—¶åºå¼‚å¸¸æ£€æµ‹å®æˆ˜æŒ‡å— (ğŸ”´ P0)

**ç›®æ ‡**: è¡¥å……å®Œæ•´çš„æ—¶åºå¼‚å¸¸æ£€æµ‹èƒ½åŠ›

**äº¤ä»˜ç‰©**:

1. Prophetæ—¶åºé¢„æµ‹æ•™ç¨‹
2. LSTMå¼‚å¸¸æ£€æµ‹ç¤ºä¾‹
3. Isolation Forestå¤šç»´åº¦æ£€æµ‹
4. å®æˆ˜æ¡ˆä¾‹: CPU/å†…å­˜/å»¶è¿Ÿå¼‚å¸¸æ£€æµ‹

**æ–‡æ¡£è§„æ¨¡**: 2,000è¡Œ

**æ—¶é—´**: 3å‘¨

#### ä»»åŠ¡2: é¢„æµ‹æ€§ç»´æŠ¤å®Œæ•´æŒ‡å— (ğŸ”´ P0)

**ç›®æ ‡**: å»ºç«‹é¢„æµ‹æ€§ç»´æŠ¤èƒ½åŠ›

**äº¤ä»˜ç‰©**:

1. ç£ç›˜è€—å°½é¢„æµ‹
2. å†…å­˜æ³„æ¼æ£€æµ‹
3. å®¹é‡è§„åˆ’å»ºè®®
4. å®æˆ˜æ¡ˆä¾‹: 3ä¸ªçœŸå®åœºæ™¯

**æ–‡æ¡£è§„æ¨¡**: 1,500è¡Œ

**æ—¶é—´**: 2å‘¨

#### ä»»åŠ¡3: å¤šæ¨¡æ€LLMåˆ†æ (ğŸŸ¡ P1)

**ç›®æ ‡**: å¢å¼ºç°æœ‰LLMæ—¥å¿—åˆ†æ,æ”¯æŒå¤šæ¨¡æ€

**äº¤ä»˜ç‰©**:

1. GPT-4oå¤šæ¨¡æ€é›†æˆç¤ºä¾‹
2. Logs + Metrics + Tracesè”åˆåˆ†æ
3. Grafanaæˆªå›¾åˆ†ææ¡ˆä¾‹
4. ä»£ç çº§è¯Šæ–­ç¤ºä¾‹

**æ–‡æ¡£è§„æ¨¡**: 1,000è¡Œ

**æ—¶é—´**: 2å‘¨

### ä¸­æœŸ (2026 H1)

#### ä»»åŠ¡4: AIå¯è§‚æµ‹æ€§å¹³å°æ¶æ„

**ç›®æ ‡**: å»ºç«‹å®Œæ•´çš„AIé©±åŠ¨å¯è§‚æµ‹æ€§å¹³å°

**äº¤ä»˜ç‰©**:

1. æ¶æ„è®¾è®¡æ–‡æ¡£
2. æ ¸å¿ƒæ¨¡å—å®ç° (å¼€æº)
   - å¼‚å¸¸æ£€æµ‹å¼•æ“
   - æ ¹å› åˆ†æå¼•æ“
   - é¢„æµ‹æ€§ç»´æŠ¤å¼•æ“
3. éƒ¨ç½²æŒ‡å— (Kubernetes)
4. ç›‘æ§å‘Šè­¦é…ç½®

**æ–‡æ¡£è§„æ¨¡**: 5,000è¡Œ + å¼€æºä»£ç 

**æ—¶é—´**: 12å‘¨

#### ä»»åŠ¡5: LLMå¾®è°ƒä¸RAGå®æˆ˜

**ç›®æ ‡**: æ¢ç´¢LLMåœ¨å¯è§‚æµ‹æ€§é¢†åŸŸçš„æ·±åº¦åº”ç”¨

**äº¤ä»˜ç‰©**:

1. LLM Fine-tuningæ•™ç¨‹ (ä½¿ç”¨å…¬å¸å†å²æ•…éšœæ•°æ®)
2. RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) å®ç°
   - Vector Database (ChromaDB/Milvus)
   - æ•…éšœçŸ¥è¯†åº“æ„å»º
3. ç”Ÿäº§éƒ¨ç½²æŒ‡å—

**æ–‡æ¡£è§„æ¨¡**: 2,000è¡Œ + å¼€æºå·¥å…·

**æ—¶é—´**: 6å‘¨

---

## ğŸ“š æ¨èå­¦ä¹ èµ„æº

### AI/MLè¯¾ç¨‹

- [fast.ai Practical Deep Learning](https://course.fast.ai/)
- [Andrew Ng Machine Learning](https://www.coursera.org/learn/machine-learning)
- [Stanford CS224N (NLP)](http://web.stanford.edu/class/cs224n/)

### å¯è§‚æµ‹æ€§ + AI

- [Datadog Watchdog Blog](https://www.datadoghq.com/blog/tag/watchdog/)
- [Dynatrace Davis AI](https://www.dynatrace.com/platform/artificial-intelligence/)
- [AWS DevOps Guru](https://aws.amazon.com/devops-guru/)

### æŠ€æœ¯è®ºæ–‡

- "Robust Random Cut Forest Based Anomaly Detection On Streams" (AWS, 2016)
- "Unsupervised Anomaly Detection via Variational Auto-Encoder" (Microsoft, 2018)

---

**æœ€åæ›´æ–°**: 2025-10-09  
**ä¸‹æ¬¡æ›´æ–°**: 2025-11-09  
**è´Ÿè´£äºº**: OTLPé¡¹ç›®ç»„ - AI/MLè¿½è¸ªå°ç»„
