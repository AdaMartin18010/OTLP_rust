# ğŸ”® é¢„æµ‹æ€§ç»´æŠ¤å®Œæ•´æŒ‡å— - ç£ç›˜/å†…å­˜/å®¹é‡é¢„æµ‹

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-10-09  
**çŠ¶æ€**: ğŸŸ¡ è¿›è¡Œä¸­ (P0-2ä»»åŠ¡)  
**ç›®æ ‡**: æä¾›å®Œæ•´çš„é¢„æµ‹æ€§ç»´æŠ¤ç®—æ³•ä¸å®æˆ˜æŒ‡å—,æå‰å‘ç°ç³»ç»Ÿæ•…éšœ

---

## ğŸ“‹ ç›®å½•

- [ğŸ”® é¢„æµ‹æ€§ç»´æŠ¤å®Œæ•´æŒ‡å— - ç£ç›˜/å†…å­˜/å®¹é‡é¢„æµ‹](#-é¢„æµ‹æ€§ç»´æŠ¤å®Œæ•´æŒ‡å—---ç£ç›˜å†…å­˜å®¹é‡é¢„æµ‹)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ“Š æ‰§è¡Œæ‘˜è¦](#-æ‰§è¡Œæ‘˜è¦)
  - [ğŸ¯ ä»€ä¹ˆæ˜¯é¢„æµ‹æ€§ç»´æŠ¤?](#-ä»€ä¹ˆæ˜¯é¢„æµ‹æ€§ç»´æŠ¤)
    - [ä¼ ç»Ÿæ–¹å¼ vs é¢„æµ‹æ€§ç»´æŠ¤](#ä¼ ç»Ÿæ–¹å¼-vs-é¢„æµ‹æ€§ç»´æŠ¤)
    - [æ ¸å¿ƒä»·å€¼](#æ ¸å¿ƒä»·å€¼)
  - [ğŸ’¾ ç£ç›˜è€—å°½é¢„æµ‹](#-ç£ç›˜è€—å°½é¢„æµ‹)
    - [1.1 é—®é¢˜èƒŒæ™¯](#11-é—®é¢˜èƒŒæ™¯)
    - [1.2 ç®—æ³•è®¾è®¡](#12-ç®—æ³•è®¾è®¡)
    - [1.3 å®Œæ•´å®ç°](#13-å®Œæ•´å®ç°)
    - [1.4 å®æˆ˜æ¡ˆä¾‹: æ—¥å¿—æ–‡ä»¶å¢é•¿é¢„æµ‹](#14-å®æˆ˜æ¡ˆä¾‹-æ—¥å¿—æ–‡ä»¶å¢é•¿é¢„æµ‹)
    - [1.5 ä¼˜åŒ–ä¸è°ƒå‚](#15-ä¼˜åŒ–ä¸è°ƒå‚)
  - [ğŸ§  å†…å­˜æ³„æ¼æ£€æµ‹ä¸é¢„æµ‹](#-å†…å­˜æ³„æ¼æ£€æµ‹ä¸é¢„æµ‹)
    - [2.1 å†…å­˜æ³„æ¼ç‰¹å¾](#21-å†…å­˜æ³„æ¼ç‰¹å¾)
    - [2.2 æ£€æµ‹ç®—æ³•](#22-æ£€æµ‹ç®—æ³•)
    - [2.3 å®Œæ•´å®ç°](#23-å®Œæ•´å®ç°)
    - [2.4 å®æˆ˜æ¡ˆä¾‹: Javaåº”ç”¨å†…å­˜æ³„æ¼](#24-å®æˆ˜æ¡ˆä¾‹-javaåº”ç”¨å†…å­˜æ³„æ¼)
    - [2.5 é«˜çº§æŠ€å·§: ç»“åˆGCæ—¥å¿—åˆ†æ](#25-é«˜çº§æŠ€å·§-ç»“åˆgcæ—¥å¿—åˆ†æ)
  - [ğŸ“ˆ å®¹é‡è§„åˆ’é¢„æµ‹](#-å®¹é‡è§„åˆ’é¢„æµ‹)
    - [3.1 å®¹é‡è§„åˆ’æ ¸å¿ƒé—®é¢˜](#31-å®¹é‡è§„åˆ’æ ¸å¿ƒé—®é¢˜)
    - [3.2 é¢„æµ‹æ¨¡å‹](#32-é¢„æµ‹æ¨¡å‹)
    - [3.3 å®Œæ•´å®ç°](#33-å®Œæ•´å®ç°)
    - [3.4 å®æˆ˜æ¡ˆä¾‹: ç”µå•†å¤§ä¿ƒå®¹é‡è§„åˆ’](#34-å®æˆ˜æ¡ˆä¾‹-ç”µå•†å¤§ä¿ƒå®¹é‡è§„åˆ’)
    - [3.5 æˆæœ¬ä¼˜åŒ–](#35-æˆæœ¬ä¼˜åŒ–)
  - [ğŸš¨ å‘Šè­¦ä¸è‡ªåŠ¨åŒ–å“åº”](#-å‘Šè­¦ä¸è‡ªåŠ¨åŒ–å“åº”)
    - [4.1 åˆ†çº§å‘Šè­¦ç­–ç•¥](#41-åˆ†çº§å‘Šè­¦ç­–ç•¥)
    - [4.2 è‡ªåŠ¨åŒ–å“åº”](#42-è‡ªåŠ¨åŒ–å“åº”)
    - [4.3 å®Œæ•´é›†æˆæ¶æ„](#43-å®Œæ•´é›†æˆæ¶æ„)
  - [ğŸ“Š æ•ˆæœè¯„ä¼°](#-æ•ˆæœè¯„ä¼°)
    - [ä¸šåŠ¡æŒ‡æ ‡](#ä¸šåŠ¡æŒ‡æ ‡)
    - [æŠ€æœ¯æŒ‡æ ‡](#æŠ€æœ¯æŒ‡æ ‡)
  - [ğŸ†š ä¸å•†ä¸šæ–¹æ¡ˆå¯¹æ¯”](#-ä¸å•†ä¸šæ–¹æ¡ˆå¯¹æ¯”)
  - [ğŸ“š ç›¸å…³æ–‡æ¡£](#-ç›¸å…³æ–‡æ¡£)
  - [ğŸ’° TCOåˆ†æä¸ROIè®¡ç®—](#-tcoåˆ†æä¸roiè®¡ç®—)
    - [æŠ•å…¥æˆæœ¬](#æŠ•å…¥æˆæœ¬)
    - [å•†ä¸šæ–¹æ¡ˆå¯¹æ¯”](#å•†ä¸šæ–¹æ¡ˆå¯¹æ¯”)
    - [é¿å…çš„æ•…éšœæŸå¤±](#é¿å…çš„æ•…éšœæŸå¤±)
    - [ç»¼åˆROI](#ç»¼åˆroi)
  - [ğŸ¯ å®Œæˆæ€»ç»“ä¸åç»­å±•æœ›](#-å®Œæˆæ€»ç»“ä¸åç»­å±•æœ›)

---

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

é¢„æµ‹æ€§ç»´æŠ¤ (Predictive Maintenance) æ˜¯AIé©±åŠ¨å¯è§‚æµ‹æ€§çš„æ ¸å¿ƒèƒ½åŠ›ä¹‹ä¸€,é€šè¿‡åˆ†æå†å²æ•°æ®é¢„æµ‹æœªæ¥æ•…éšœã€‚

**æ ¸å¿ƒèƒ½åŠ›**:

- ğŸ”® **ç£ç›˜è€—å°½é¢„æµ‹**: æå‰7-30å¤©é¢„æµ‹ç£ç›˜æ»¡
- ğŸ§  **å†…å­˜æ³„æ¼æ£€æµ‹**: è‡ªåŠ¨è¯†åˆ«å†…å­˜æ³„æ¼è¶‹åŠ¿
- ğŸ“ˆ **å®¹é‡è§„åˆ’é¢„æµ‹**: è‡ªåŠ¨é¢„æµ‹æœªæ¥èµ„æºéœ€æ±‚

**é¢„æœŸæ”¶ç›Š**:

- ğŸ¯ æå‰å‘ç°ç‡: > 95%
- â° é¢„è­¦æå‰æœŸ: 7-30å¤©
- ğŸ’° é¿å…æ•…éšœæŸå¤±: æ¯æ¬¡ $10,000+
- ğŸ”§ å‡å°‘äººå·¥å·¡æ£€: 80%

---

## ğŸ¯ ä»€ä¹ˆæ˜¯é¢„æµ‹æ€§ç»´æŠ¤?

### ä¼ ç»Ÿæ–¹å¼ vs é¢„æµ‹æ€§ç»´æŠ¤

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ä¼ ç»Ÿè¢«åŠ¨å“åº”                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚  1. ç£ç›˜æ»¡ â†’ å‘Šè­¦ â†’ æ‰‹å·¥æ¸…ç† (å·²ç»å½±å“ä¸šåŠ¡)     â”‚
â”‚  2. å†…å­˜æ³„æ¼ â†’ OOM â†’ é‡å¯ (ä¸šåŠ¡ä¸­æ–­)            â”‚
â”‚  3. å®¹é‡ä¸è¶³ â†’ æ‹’ç»è¯·æ±‚ â†’ ç´§æ€¥æ‰©å®¹ (ç”¨æˆ·æµå¤±)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       âŒ è¢«åŠ¨ + æŸå¤±å¤§

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é¢„æµ‹æ€§ä¸»åŠ¨ç»´æŠ¤                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚  1. é¢„æµ‹7å¤©åç£ç›˜æ»¡ â†’ æå‰æ¸…ç† (é›¶å½±å“)         â”‚
â”‚  2. æ£€æµ‹å†…å­˜å¢é•¿è¶‹åŠ¿ â†’ æå‰ä¿®å¤ (æ— OOM)        â”‚
â”‚  3. é¢„æµ‹å¤§ä¿ƒæµé‡ â†’ æå‰æ‰©å®¹ (ä¸æ»‘ä½“éªŒ)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       âœ… ä¸»åŠ¨ + é›¶æŸå¤±
```

### æ ¸å¿ƒä»·å€¼

**1. é¿å…ç”Ÿäº§æ•…éšœ**:

```python
# æ¡ˆä¾‹: æŸç”µå•†å¹³å°
# - ç£ç›˜æ»¡å¯¼è‡´è®¢å•ä¸¢å¤±: $50,000æŸå¤±
# - é¢„æµ‹æ€§ç»´æŠ¤æˆæœ¬: $500
# ROI = 100å€
```

**2. æå‡ç”¨æˆ·ä½“éªŒ**:

```python
# ç”¨æˆ·æ„ŸçŸ¥:
# - ä¼ ç»Ÿ: å¶å°”å‡ºç°503é”™è¯¯,éœ€è¦é‡è¯•
# - é¢„æµ‹æ€§: é›¶æ„ŸçŸ¥,å§‹ç»ˆä¸æ»‘
# 
# NPSæå‡: 65 â†’ 82
```

**3. é™ä½è¿ç»´æˆæœ¬**:

```python
# äººå·¥æˆæœ¬:
# - ä¼ ç»Ÿ: æ¯å¤©äººå·¥å·¡æ£€2å°æ—¶,å¹´æˆæœ¬ $20,000
# - é¢„æµ‹æ€§: è‡ªåŠ¨é¢„æµ‹,å¹´æˆæœ¬ $2,000
# 
# èŠ‚çœ: 90%
```

---

## ğŸ’¾ ç£ç›˜è€—å°½é¢„æµ‹

### 1.1 é—®é¢˜èƒŒæ™¯

**å¸¸è§åœºæ™¯**:

- æ—¥å¿—æ–‡ä»¶æ— é™å¢é•¿
- æ•°æ®åº“æ•°æ®æŒç»­å†™å…¥
- ä¸´æ—¶æ–‡ä»¶æœªæ¸…ç†
- å¤‡ä»½æ–‡ä»¶å †ç§¯

**ä¼ ç»Ÿå‘Šè­¦çš„é—®é¢˜**:

```python
# âŒ ä¼ ç»Ÿå‘Šè­¦: ç£ç›˜ä½¿ç”¨ç‡ > 90%
if disk_usage > 0.9:
    alert("ç£ç›˜å³å°†æ»¡!")

# é—®é¢˜:
# 1. å‘Šè­¦æ—¶å·²ç»å¾ˆç´§æ€¥ (å¯èƒ½1-2å¤©å°±æ»¡)
# 2. é¢‘ç¹è¯¯æŠ¥ (é•¿æœŸä¿æŒ90%)
# 3. æ— æ³•æå‰è§„åˆ’
```

**é¢„æµ‹æ€§æ–¹æ¡ˆ**:

```python
# âœ… é¢„æµ‹æ€§å‘Šè­¦: é¢„æµ‹7å¤©åç£ç›˜ä½¿ç”¨ç‡
if predict_disk_usage(7_days) > 0.95:
    alert("é¢„è®¡7å¤©åç£ç›˜æ»¡,è¯·æå‰æ¸…ç†")

# ä¼˜åŠ¿:
# 1. æå‰7å¤©é¢„è­¦,æœ‰è¶³å¤Ÿæ—¶é—´å¤„ç†
# 2. å‡†ç¡®ç‡é«˜ (åŸºäºå†å²è¶‹åŠ¿)
# 3. æ”¯æŒä¸åŒç´§æ€¥ç­‰çº§
```

### 1.2 ç®—æ³•è®¾è®¡

**æ ¸å¿ƒæ€è·¯**: çº¿æ€§å›å½’ + è¶‹åŠ¿å¤–æ¨

```text
å†å²æ•°æ®:
  Day 1: 50% used
  Day 2: 52% used
  Day 3: 54% used
  Day 4: 56% used
  ...

æ‹Ÿåˆçº¿æ€§æ¨¡å‹:
  Usage(t) = a * t + b
  
  å…¶ä¸­:
  - t: æ—¶é—´ (å¤©)
  - a: å¢é•¿æ–œç‡ (æ¯å¤©å¢é•¿ç‡)
  - b: åˆå§‹å€¼

é¢„æµ‹æœªæ¥:
  Day 30: Usage(30) = a * 30 + b = 98% (é¢„è®¡æ»¡)
  
  è­¦å‘Š: è¿˜æœ‰ (100% - 56%) / a = 22å¤© ä¼šæ»¡
```

**è¿›é˜¶**: è€ƒè™‘å‘¨æœŸæ€§

```python
# ä¸šåŠ¡ç³»ç»Ÿé€šå¸¸æœ‰å‘¨æœŸæ€§
# å¦‚: å·¥ä½œæ—¥æ—¥å¿—å¤š,å‘¨æœ«æ—¥å¿—å°‘

# ä½¿ç”¨Prophetæˆ–ARIMAæ¨¡å‹å¤„ç†å‘¨æœŸæ€§
# 1. Prophet: è‡ªåŠ¨å¤„ç†æ—¥/å‘¨/å¹´å‘¨æœŸ
# 2. ARIMA: å¤„ç†å¤æ‚æ—¶åºæ¨¡å¼
```

### 1.3 å®Œæ•´å®ç°

```python
"""
ç£ç›˜è€—å°½é¢„æµ‹å®Œæ•´å®ç°
åŸºäºçº¿æ€§å›å½’ + è¶‹åŠ¿åˆ†æ
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from datetime import datetime, timedelta
from typing import Tuple, Optional
import warnings
warnings.filterwarnings('ignore')


class DiskUsagePredictor:
    """ç£ç›˜ä½¿ç”¨ç‡é¢„æµ‹å™¨"""
    
    def __init__(
        self,
        alert_thresholds: dict = None,
        min_data_points: int = 7,  # è‡³å°‘7å¤©æ•°æ®
        trend_threshold: float = 0.5  # å¢é•¿è¶‹åŠ¿é˜ˆå€¼ (% per day)
    ):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            alert_thresholds: å‘Šè­¦é˜ˆå€¼,å¦‚ {7: 0.90, 14: 0.85, 30: 0.80}
                             è¡¨ç¤º: 7å¤©å†…è¶…90%å‘Šè­¦, 14å¤©å†…è¶…85%å‘Šè­¦
            min_data_points: æœ€å°‘æ•°æ®ç‚¹æ•°
            trend_threshold: å¢é•¿è¶‹åŠ¿é˜ˆå€¼,ä½äºæ­¤å€¼è®¤ä¸ºç¨³å®š
        """
        self.alert_thresholds = alert_thresholds or {
            7: 0.90,   # 7å¤©å†…é¢„è®¡è¶…90%
            14: 0.85,  # 14å¤©å†…é¢„è®¡è¶…85%
            30: 0.80   # 30å¤©å†…é¢„è®¡è¶…80%
        }
        self.min_data_points = min_data_points
        self.trend_threshold = trend_threshold
        self.model: Optional[LinearRegression] = None
        self.r2_score: float = 0.0
    
    def fit(self, timestamps: np.ndarray, usage_pct: np.ndarray) -> 'DiskUsagePredictor':
        """
        è®­ç»ƒé¢„æµ‹æ¨¡å‹
        
        Args:
            timestamps: æ—¶é—´æˆ³æ•°ç»„ (Unixæ—¶é—´æˆ³æˆ–datetime)
            usage_pct: ç£ç›˜ä½¿ç”¨ç‡æ•°ç»„ (0-100)
        
        Returns:
            self
        """
        # éªŒè¯è¾“å…¥
        if len(timestamps) < self.min_data_points:
            raise ValueError(f"æ•°æ®ç‚¹ä¸è¶³,è‡³å°‘éœ€è¦{self.min_data_points}ä¸ªç‚¹")
        
        # è½¬æ¢æ—¶é—´æˆ³ä¸ºç›¸å¯¹å¤©æ•°
        if isinstance(timestamps[0], datetime):
            base_time = timestamps[0]
            days = np.array([(t - base_time).total_seconds() / 86400 for t in timestamps])
        else:
            base_time = timestamps[0]
            days = (timestamps - base_time) / 86400
        
        # çº¿æ€§å›å½’
        X = days.reshape(-1, 1)
        y = usage_pct
        
        self.model = LinearRegression()
        self.model.fit(X, y)
        
        # è®¡ç®—RÂ²è¯„åˆ†
        y_pred = self.model.predict(X)
        self.r2_score = r2_score(y, y_pred)
        
        # ä¿å­˜åŸºå‡†æ—¶é—´
        self.base_time = base_time
        
        return self
    
    def predict(self, days_ahead: int) -> Tuple[float, float]:
        """
        é¢„æµ‹æœªæ¥Nå¤©çš„ç£ç›˜ä½¿ç”¨ç‡
        
        Args:
            days_ahead: æœªæ¥å¤©æ•°
        
        Returns:
            (predicted_usage, confidence)
            - predicted_usage: é¢„æµ‹ä½¿ç”¨ç‡ (%)
            - confidence: ç½®ä¿¡åº¦ (0-1),åŸºäºRÂ²
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ,è¯·å…ˆè°ƒç”¨fit()")
        
        X_pred = np.array([[days_ahead]])
        usage_pred = self.model.predict(X_pred)[0]
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´
        usage_pred = np.clip(usage_pred, 0, 100)
        
        # ç½®ä¿¡åº¦åŸºäºRÂ²
        confidence = self.r2_score
        
        return usage_pred, confidence
    
    def analyze_trend(self) -> dict:
        """
        åˆ†æç£ç›˜ä½¿ç”¨è¶‹åŠ¿
        
        Returns:
            {
                'daily_growth': æ¯å¤©å¢é•¿ç‡ (% per day),
                'is_stable': æ˜¯å¦ç¨³å®š,
                'days_to_full': é¢„è®¡å¤šå°‘å¤©åæ»¡ (95%),
                'severity': ä¸¥é‡ç­‰çº§ ('low', 'medium', 'high', 'critical')
            }
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        # æ¯å¤©å¢é•¿ç‡ (æ–œç‡)
        daily_growth = self.model.coef_[0]
        
        # å½“å‰ä½¿ç”¨ç‡ (Day 0)
        current_usage = self.model.intercept_
        
        # æ˜¯å¦ç¨³å®š
        is_stable = abs(daily_growth) < self.trend_threshold
        
        # é¢„è®¡å¤šå°‘å¤©åæ»¡ (95%)
        if daily_growth > 0.01:  # æœ‰æ˜æ˜¾å¢é•¿
            days_to_full = (95 - current_usage) / daily_growth
            days_to_full = max(0, days_to_full)
        else:
            days_to_full = float('inf')  # æ— é™æœŸ
        
        # ä¸¥é‡ç­‰çº§
        if days_to_full < 3:
            severity = 'critical'
        elif days_to_full < 7:
            severity = 'high'
        elif days_to_full < 14:
            severity = 'medium'
        else:
            severity = 'low'
        
        return {
            'daily_growth': daily_growth,
            'is_stable': is_stable,
            'days_to_full': days_to_full,
            'severity': severity,
            'current_usage': current_usage,
            'r2_score': self.r2_score
        }
    
    def check_alerts(self) -> list:
        """
        æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
        
        Returns:
            å‘Šè­¦åˆ—è¡¨,æ¯é¡¹åŒ…å«:
            {
                'days': å¤©æ•°,
                'predicted_usage': é¢„æµ‹ä½¿ç”¨ç‡,
                'threshold': é˜ˆå€¼,
                'confidence': ç½®ä¿¡åº¦
            }
        """
        alerts = []
        
        for days, threshold in self.alert_thresholds.items():
            usage, confidence = self.predict(days)
            
            if usage > threshold * 100:  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                alerts.append({
                    'days': days,
                    'predicted_usage': usage,
                    'threshold': threshold * 100,
                    'confidence': confidence
                })
        
        return alerts


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========

def example_disk_prediction():
    """ç¤ºä¾‹: ç£ç›˜è€—å°½é¢„æµ‹"""
    
    print("ğŸ”§ ç”Ÿæˆæ¨¡æ‹Ÿç£ç›˜ä½¿ç”¨æ•°æ®...")
    
    # ç”Ÿæˆ30å¤©å†å²æ•°æ®
    dates = pd.date_range('2024-10-01', periods=30, freq='D')
    
    # æ¨¡æ‹Ÿçº¿æ€§å¢é•¿: 50% â†’ 85%
    base_usage = 50
    daily_growth = 1.2  # æ¯å¤©å¢é•¿1.2%
    noise = np.random.normal(0, 1, len(dates))
    
    usage = base_usage + daily_growth * np.arange(len(dates)) + noise
    usage = np.clip(usage, 0, 100)
    
    df = pd.DataFrame({
        'date': dates,
        'usage_pct': usage
    })
    
    print(f"ğŸ“Š å†å²æ•°æ® (å‰5å¤©):")
    print(df.head())
    print(f"\nğŸ“Š å†å²æ•°æ® (æœ€è¿‘5å¤©):")
    print(df.tail())
    
    # è®­ç»ƒé¢„æµ‹æ¨¡å‹
    print("\nğŸ“ è®­ç»ƒé¢„æµ‹æ¨¡å‹...")
    predictor = DiskUsagePredictor()
    predictor.fit(df['date'].values, df['usage_pct'].values)
    
    # åˆ†æè¶‹åŠ¿
    print("\nğŸ“ˆ è¶‹åŠ¿åˆ†æ:")
    trend = predictor.analyze_trend()
    print(f"  å½“å‰ä½¿ç”¨ç‡: {trend['current_usage']:.1f}%")
    print(f"  æ¯å¤©å¢é•¿: {trend['daily_growth']:.2f}% per day")
    print(f"  é¢„è®¡æ»¡çš„å¤©æ•°: {trend['days_to_full']:.0f} å¤©")
    print(f"  ä¸¥é‡ç­‰çº§: {trend['severity']}")
    print(f"  RÂ²è¯„åˆ†: {trend['r2_score']:.3f}")
    
    # é¢„æµ‹æœªæ¥
    print("\nğŸ”® æœªæ¥é¢„æµ‹:")
    for days in [7, 14, 30]:
        usage, confidence = predictor.predict(days)
        print(f"  {days}å¤©å: {usage:.1f}% (ç½®ä¿¡åº¦: {confidence:.2f})")
    
    # æ£€æŸ¥å‘Šè­¦
    print("\nğŸš¨ å‘Šè­¦æ£€æŸ¥:")
    alerts = predictor.check_alerts()
    if len(alerts) > 0:
        print(f"  å‘ç° {len(alerts)} ä¸ªå‘Šè­¦:")
        for alert in alerts:
            print(f"    - {alert['days']}å¤©å: {alert['predicted_usage']:.1f}% "
                  f"(é˜ˆå€¼: {alert['threshold']:.1f}%)")
    else:
        print("  âœ… æ— å‘Šè­¦")
    
    return predictor, df


# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    predictor, df = example_disk_prediction()
    print("\nâœ… ç£ç›˜è€—å°½é¢„æµ‹ç¤ºä¾‹å®Œæˆ!")
```

### 1.4 å®æˆ˜æ¡ˆä¾‹: æ—¥å¿—æ–‡ä»¶å¢é•¿é¢„æµ‹

**åœºæ™¯**: Kubernetesé›†ç¾¤ä¸­Podæ—¥å¿—æŒç»­å¢é•¿,éœ€è¦æå‰é¢„è­¦

**æ•°æ®é‡‡é›†**:

```python
# ä½¿ç”¨Prometheusé‡‡é›†ç£ç›˜ä½¿ç”¨ç‡
# PromQLæŸ¥è¯¢:
# 
# node_filesystem_avail_bytes{mountpoint="/var/log"} 
# / 
# node_filesystem_size_bytes{mountpoint="/var/log"}

import requests

def fetch_disk_usage_from_prometheus(
    prometheus_url: str,
    query: str,
    start_time: datetime,
    end_time: datetime
) -> pd.DataFrame:
    """ä»PrometheusæŸ¥è¯¢ç£ç›˜ä½¿ç”¨ç‡å†å²æ•°æ®"""
    
    params = {
        'query': query,
        'start': start_time.timestamp(),
        'end': end_time.timestamp(),
        'step': '1h'  # 1å°æ—¶é—´éš”
    }
    
    response = requests.get(f"{prometheus_url}/api/v1/query_range", params=params)
    data = response.json()['data']['result'][0]
    
    timestamps = []
    values = []
    for ts, val in data['values']:
        timestamps.append(datetime.fromtimestamp(ts))
        values.append((1 - float(val)) * 100)  # è½¬æ¢ä¸ºä½¿ç”¨ç‡ç™¾åˆ†æ¯”
    
    return pd.DataFrame({
        'timestamp': timestamps,
        'usage_pct': values
    })


# ä½¿ç”¨ç¤ºä¾‹
df = fetch_disk_usage_from_prometheus(
    prometheus_url="http://prometheus:9090",
    query='1 - (node_filesystem_avail_bytes{mountpoint="/var/log"} / node_filesystem_size_bytes{mountpoint="/var/log"})',
    start_time=datetime.now() - timedelta(days=30),
    end_time=datetime.now()
)

# è®­ç»ƒé¢„æµ‹æ¨¡å‹
predictor = DiskUsagePredictor()
predictor.fit(df['timestamp'].values, df['usage_pct'].values)

# åˆ†æä¸å‘Šè­¦
trend = predictor.analyze_trend()
if trend['days_to_full'] < 7:
    send_alert(
        title=f"ç£ç›˜å³å°†æ»¡: {trend['days_to_full']:.0f}å¤©",
        message=f"æ—¥å¿—ç›®å½•ä½¿ç”¨ç‡: {trend['current_usage']:.1f}%, "
                f"æ¯å¤©å¢é•¿: {trend['daily_growth']:.2f}%",
        severity=trend['severity']
    )
```

### 1.5 ä¼˜åŒ–ä¸è°ƒå‚

**1. å¤„ç†å¼‚å¸¸å€¼**:

```python
# æ•°æ®æ¸…æ´—: ç§»é™¤å¼‚å¸¸ç‚¹
from scipy import stats

def remove_outliers(data: np.ndarray, threshold: float = 3.0) -> np.ndarray:
    """ç§»é™¤è¶…è¿‡Nä¸ªæ ‡å‡†å·®çš„å¼‚å¸¸å€¼"""
    z_scores = np.abs(stats.zscore(data))
    return data[z_scores < threshold]

# ä½¿ç”¨
usage_cleaned = remove_outliers(df['usage_pct'].values)
```

**2. å‘¨æœŸæ€§è°ƒæ•´**:

```python
# å¦‚æœæ•°æ®æœ‰å‘¨æœŸæ€§ (å¦‚å‘¨æœ«æ—¥å¿—å°‘),ä½¿ç”¨Prophet
from prophet import Prophet

def disk_prediction_with_seasonality(df: pd.DataFrame) -> dict:
    """è€ƒè™‘å‘¨æœŸæ€§çš„ç£ç›˜é¢„æµ‹"""
    
    # å‡†å¤‡æ•°æ®
    prophet_df = df[['timestamp', 'usage_pct']].rename(
        columns={'timestamp': 'ds', 'usage_pct': 'y'}
    )
    
    # è®­ç»ƒProphetæ¨¡å‹
    model = Prophet(
        daily_seasonality=False,
        weekly_seasonality=True,  # å‘¨å‘¨æœŸ
        yearly_seasonality=False
    )
    model.fit(prophet_df)
    
    # é¢„æµ‹æœªæ¥30å¤©
    future = model.make_future_dataframe(periods=30, freq='D')
    forecast = model.predict(future)
    
    # æ£€æŸ¥æ˜¯å¦ä¼šæ»¡
    future_usage = forecast['yhat'].values[-1]
    
    return {
        'predicted_usage_30d': future_usage,
        'forecast': forecast
    }
```

---

## ğŸ§  å†…å­˜æ³„æ¼æ£€æµ‹ä¸é¢„æµ‹

### 2.1 å†…å­˜æ³„æ¼ç‰¹å¾

**å…¸å‹æ¨¡å¼**:

```text
æ­£å¸¸å†…å­˜ä½¿ç”¨ (æœ‰GCå›æ”¶):
Memory
100% â”‚     â•±â•²      â•±â•²      â•±â•²
     â”‚    â•±  â•²    â•±  â•²    â•±  â•²
 50% â”‚   â•±    â•²  â•±    â•²  â•±    â•²
     â”‚  â•±      â•²â•±      â•²â•±      â•²
  0% â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
     (é”¯é½¿çŠ¶,æœ‰æ˜æ˜¾å›è½)

å†…å­˜æ³„æ¼ (æŒç»­å¢é•¿):
Memory
100% â”‚                        â•±
     â”‚                      â•±
 50% â”‚                â•±â•±â•±â•±
     â”‚          â•±â•±â•±â•±
  0% â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
     (æŒç»­ä¸Šå‡,æ— å›è½)
```

**æ£€æµ‹æŒ‡æ ‡**:

- è¶‹åŠ¿: æŒç»­ä¸Šå‡
- GCååŸºçº¿: é€æ¸æŠ¬é«˜
- å¢é•¿ç‡: ç¨³å®š (çº¿æ€§æˆ–æŒ‡æ•°)

### 2.2 æ£€æµ‹ç®—æ³•

**Mann-Kendallè¶‹åŠ¿æ£€æµ‹**:

éå‚æ•°ç»Ÿè®¡æ£€éªŒ,æ£€æµ‹æ—¶åºæ•°æ®æ˜¯å¦æœ‰å•è°ƒè¶‹åŠ¿ã€‚

```python
from scipy import stats

def mann_kendall_test(data: np.ndarray) -> dict:
    """
    Mann-Kendallè¶‹åŠ¿æ£€æµ‹
    
    Returns:
        {
            'trend': 'increasing' | 'decreasing' | 'no trend',
            'p_value': æ˜¾è‘—æ€§,
            'tau': Kendallç›¸å…³ç³»æ•°
        }
    """
    n = len(data)
    s = 0
    
    for i in range(n-1):
        for j in range(i+1, n):
            s += np.sign(data[j] - data[i])
    
    # è®¡ç®—æ–¹å·®
    var_s = n * (n-1) * (2*n+5) / 18
    
    # æ ‡å‡†åŒ–
    if s > 0:
        z = (s - 1) / np.sqrt(var_s)
    elif s < 0:
        z = (s + 1) / np.sqrt(var_s)
    else:
        z = 0
    
    # På€¼
    p_value = 2 * (1 - stats.norm.cdf(abs(z)))
    
    # Kendall tau
    tau = s / (0.5 * n * (n-1))
    
    # åˆ¤æ–­è¶‹åŠ¿
    if p_value < 0.05:  # æ˜¾è‘—æ€§æ°´å¹³
        if tau > 0:
            trend = 'increasing'
        else:
            trend = 'decreasing'
    else:
        trend = 'no trend'
    
    return {
        'trend': trend,
        'p_value': p_value,
        'tau': tau
    }
```

### 2.3 å®Œæ•´å®ç°

```python
"""
å†…å­˜æ³„æ¼æ£€æµ‹å®Œæ•´å®ç°
ç»“åˆè¶‹åŠ¿åˆ†æ + åŸºçº¿æ¼‚ç§»æ£€æµ‹
"""

import numpy as np
import pandas as pd
from scipy import stats
from typing import Tuple, List
from dataclasses import dataclass


@dataclass
class MemoryLeakResult:
    """å†…å­˜æ³„æ¼æ£€æµ‹ç»“æœ"""
    is_leak: bool
    confidence: float
    growth_rate: float  # MB per hour
    baseline_drift: float  # åŸºçº¿æ¼‚ç§» (%)
    time_to_oom: float  # å°æ—¶
    severity: str  # 'low', 'medium', 'high', 'critical'


class MemoryLeakDetector:
    """å†…å­˜æ³„æ¼æ£€æµ‹å™¨"""
    
    def __init__(
        self,
        max_memory_mb: float = 4096,  # æœ€å¤§å†…å­˜é™åˆ¶ (MB)
        oom_threshold: float = 0.95,  # OOMé˜ˆå€¼ (95%)
        min_data_hours: int = 24  # è‡³å°‘24å°æ—¶æ•°æ®
    ):
        self.max_memory_mb = max_memory_mb
        self.oom_threshold = oom_threshold
        self.min_data_hours = min_data_hours
    
    def detect(
        self,
        timestamps: np.ndarray,
        memory_mb: np.ndarray
    ) -> MemoryLeakResult:
        """
        æ£€æµ‹å†…å­˜æ³„æ¼
        
        Args:
            timestamps: æ—¶é—´æˆ³æ•°ç»„ (datetime)
            memory_mb: å†…å­˜ä½¿ç”¨é‡æ•°ç»„ (MB)
        
        Returns:
            MemoryLeakResult
        """
        # 1. è¶‹åŠ¿æ£€æµ‹
        mk_result = self._mann_kendall_test(memory_mb)
        has_trend = (mk_result['trend'] == 'increasing' and 
                     mk_result['p_value'] < 0.05)
        
        # 2. åŸºçº¿æ¼‚ç§»æ£€æµ‹
        baseline_drift = self._detect_baseline_drift(memory_mb)
        
        # 3. å¢é•¿ç‡è®¡ç®—
        growth_rate = self._calculate_growth_rate(timestamps, memory_mb)
        
        # 4. OOMæ—¶é—´é¢„æµ‹
        current_memory = memory_mb[-1]
        if growth_rate > 0:
            memory_to_oom = self.max_memory_mb * self.oom_threshold - current_memory
            time_to_oom = memory_to_oom / growth_rate
        else:
            time_to_oom = float('inf')
        
        # 5. ç»¼åˆåˆ¤æ–­
        is_leak = (
            has_trend and 
            baseline_drift > 5.0 and  # åŸºçº¿æ¼‚ç§»è¶…è¿‡5%
            growth_rate > 0.5  # æ¯å°æ—¶å¢é•¿è¶…è¿‡0.5MB
        )
        
        # ç½®ä¿¡åº¦
        confidence = min(1.0, 
                        (1 - mk_result['p_value']) * 
                        min(1.0, baseline_drift / 10.0))
        
        # ä¸¥é‡ç­‰çº§
        if time_to_oom < 6:
            severity = 'critical'
        elif time_to_oom < 24:
            severity = 'high'
        elif time_to_oom < 72:
            severity = 'medium'
        else:
            severity = 'low'
        
        return MemoryLeakResult(
            is_leak=is_leak,
            confidence=confidence,
            growth_rate=growth_rate,
            baseline_drift=baseline_drift,
            time_to_oom=time_to_oom,
            severity=severity
        )
    
    def _mann_kendall_test(self, data: np.ndarray) -> dict:
        """Mann-Kendallè¶‹åŠ¿æ£€æµ‹"""
        n = len(data)
        s = 0
        
        for i in range(n-1):
            for j in range(i+1, n):
                s += np.sign(data[j] - data[i])
        
        var_s = n * (n-1) * (2*n+5) / 18
        
        if s > 0:
            z = (s - 1) / np.sqrt(var_s)
        elif s < 0:
            z = (s + 1) / np.sqrt(var_s)
        else:
            z = 0
        
        p_value = 2 * (1 - stats.norm.cdf(abs(z)))
        tau = s / (0.5 * n * (n-1))
        
        if p_value < 0.05:
            trend = 'increasing' if tau > 0 else 'decreasing'
        else:
            trend = 'no trend'
        
        return {
            'trend': trend,
            'p_value': p_value,
            'tau': tau
        }
    
    def _detect_baseline_drift(self, memory_mb: np.ndarray) -> float:
        """æ£€æµ‹åŸºçº¿æ¼‚ç§»"""
        # å°†æ•°æ®åˆ†ä¸ºå‰åŠå’ŒååŠ
        mid = len(memory_mb) // 2
        first_half = memory_mb[:mid]
        second_half = memory_mb[mid:]
        
        # è®¡ç®—å„è‡ªçš„åŸºçº¿ (25th percentile)
        baseline_1 = np.percentile(first_half, 25)
        baseline_2 = np.percentile(second_half, 25)
        
        # æ¼‚ç§»ç™¾åˆ†æ¯”
        drift_pct = ((baseline_2 - baseline_1) / baseline_1) * 100
        
        return drift_pct
    
    def _calculate_growth_rate(
        self,
        timestamps: np.ndarray,
        memory_mb: np.ndarray
    ) -> float:
        """è®¡ç®—å¢é•¿ç‡ (MB/hour)"""
        # è½¬æ¢æ—¶é—´ä¸ºå°æ—¶
        if isinstance(timestamps[0], datetime):
            hours = np.array([
                (t - timestamps[0]).total_seconds() / 3600 
                for t in timestamps
            ])
        else:
            hours = (timestamps - timestamps[0]) / 3600
        
        # çº¿æ€§å›å½’
        from sklearn.linear_model import LinearRegression
        model = LinearRegression()
        model.fit(hours.reshape(-1, 1), memory_mb)
        
        return model.coef_[0]  # MB/hour


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========

def example_memory_leak_detection():
    """ç¤ºä¾‹: å†…å­˜æ³„æ¼æ£€æµ‹"""
    
    print("ğŸ”§ ç”Ÿæˆæ¨¡æ‹Ÿå†…å­˜æ•°æ® (åŒ…å«æ³„æ¼)...")
    
    # ç”Ÿæˆ7å¤©çš„å°æ—¶çº§æ•°æ®
    hours = 7 * 24
    timestamps = pd.date_range('2024-10-01', periods=hours, freq='H')
    
    # æ¨¡æ‹Ÿå†…å­˜æ³„æ¼: åŸºçº¿ä»500MBé€æ¸å¢é•¿åˆ°3000MB
    base_memory = 500
    leak_rate = 15  # æ¯å°æ—¶æ³„æ¼15MB
    gc_cycle = 24  # æ¯24å°æ—¶ä¸€æ¬¡GC (å›æ”¶20%)
    
    memory = []
    current = base_memory
    for i in range(hours):
        # æ³„æ¼å¢é•¿
        current += leak_rate + np.random.normal(0, 5)
        
        # GCå›æ”¶ (ä½†åŸºçº¿æŒç»­æŠ¬é«˜)
        if i % gc_cycle == 0 and i > 0:
            current *= 0.8  # å›æ”¶20%
        
        memory.append(current)
    
    memory = np.array(memory)
    
    print(f"ğŸ“Š å†…å­˜æ•°æ®ç»Ÿè®¡:")
    print(f"  åˆå§‹: {memory[0]:.1f} MB")
    print(f"  æœ€ç»ˆ: {memory[-1]:.1f} MB")
    print(f"  å¢é•¿: {memory[-1] - memory[0]:.1f} MB")
    
    # æ£€æµ‹å†…å­˜æ³„æ¼
    print("\nğŸ” æ£€æµ‹å†…å­˜æ³„æ¼...")
    detector = MemoryLeakDetector(max_memory_mb=4096)
    result = detector.detect(timestamps.values, memory)
    
    print(f"\nğŸ“Š æ£€æµ‹ç»“æœ:")
    print(f"  æ˜¯å¦æ³„æ¼: {'âœ… æ˜¯' if result.is_leak else 'âŒ å¦'}")
    print(f"  ç½®ä¿¡åº¦: {result.confidence:.2f}")
    print(f"  å¢é•¿ç‡: {result.growth_rate:.2f} MB/hour")
    print(f"  åŸºçº¿æ¼‚ç§»: {result.baseline_drift:.1f}%")
    print(f"  é¢„è®¡OOM: {result.time_to_oom:.1f} å°æ—¶")
    print(f"  ä¸¥é‡ç­‰çº§: {result.severity}")
    
    return detector, result


# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    detector, result = example_memory_leak_detection()
    print("\nâœ… å†…å­˜æ³„æ¼æ£€æµ‹ç¤ºä¾‹å®Œæˆ!")
```

### 2.4 å®æˆ˜æ¡ˆä¾‹: Javaåº”ç”¨å†…å­˜æ³„æ¼

**åœºæ™¯**: Spring Bootåº”ç”¨å†…å­˜æŒç»­å¢é•¿

**å®Œæ•´ç›‘æ§æµç¨‹**:

```python
# 1. ä»Prometheusé‡‡é›†å†…å­˜æ•°æ®
query = 'container_memory_working_set_bytes{pod=~"my-app-.*"} / 1024 / 1024'

df = fetch_metrics_from_prometheus(
    query=query,
    start_time=datetime.now() - timedelta(days=7),
    end_time=datetime.now()
)

# 2. æ£€æµ‹å†…å­˜æ³„æ¼
detector = MemoryLeakDetector(max_memory_mb=2048)  # 2GBé™åˆ¶
result = detector.detect(df['timestamp'].values, df['memory_mb'].values)

# 3. å‘Šè­¦ä¸è‡ªåŠ¨åŒ–
if result.is_leak and result.severity in ['high', 'critical']:
    # å‘é€å‘Šè­¦
    send_slack_alert(
        channel="#ops-alerts",
        message=f"""
ğŸš¨ æ£€æµ‹åˆ°å†…å­˜æ³„æ¼!

åº”ç”¨: my-app
å¢é•¿ç‡: {result.growth_rate:.2f} MB/hour
é¢„è®¡OOM: {result.time_to_oom:.1f} å°æ—¶
ä¸¥é‡ç­‰çº§: {result.severity}

å»ºè®®æ“ä½œ:
1. æŸ¥çœ‹Heap Dump: kubectl exec my-app -- jmap -dump:file=/tmp/heap.hprof 1
2. åˆ†æå†…å­˜æ³„æ¼: MAT / VisualVM
3. å¦‚ç´§æ€¥,é‡å¯Pod: kubectl rollout restart deployment my-app
        """
    )
    
    # è‡ªåŠ¨åŒ–å“åº” (å¯é€‰)
    if result.time_to_oom < 2:  # 2å°æ—¶å†…ä¼šOOM
        # è‡ªåŠ¨é‡å¯
        os.system("kubectl rollout restart deployment my-app")
```

### 2.5 é«˜çº§æŠ€å·§: ç»“åˆGCæ—¥å¿—åˆ†æ

**åŸç†**: GCåå†…å­˜åŸºçº¿æŒç»­æŠ¬é«˜ = æ³„æ¼

```python
def analyze_gc_logs(gc_log_file: str) -> dict:
    """
    åˆ†æGCæ—¥å¿—,æ£€æµ‹æ³„æ¼
    
    GCæ—¥å¿—æ ¼å¼ (G1GC):
    2024-10-09T10:00:00.000+0000: [GC pause (G1 Evacuation Pause) 1024M->900M(2048M), 0.0123456 secs]
    """
    import re
    
    pattern = r'(\d+)M->(\d+)M\((\d+)M\)'
    
    gc_events = []
    with open(gc_log_file) as f:
        for line in f:
            match = re.search(pattern, line)
            if match:
                before_mb = int(match.group(1))
                after_mb = int(match.group(2))
                max_mb = int(match.group(3))
                gc_events.append({
                    'before': before_mb,
                    'after': after_mb,
                    'max': max_mb
                })
    
    # åˆ†æGCåå†…å­˜åŸºçº¿
    after_memory = np.array([e['after'] for e in gc_events])
    
    # è¶‹åŠ¿æ£€æµ‹
    from scipy import stats
    slope, intercept, r_value, p_value, std_err = stats.linregress(
        range(len(after_memory)), 
        after_memory
    )
    
    is_leak = (slope > 1 and p_value < 0.05)  # æ¯æ¬¡GCååŸºçº¿å¢é•¿>1MB
    
    return {
        'is_leak': is_leak,
        'baseline_growth_per_gc': slope,
        'p_value': p_value
    }
```

---

## ğŸ“ˆ å®¹é‡è§„åˆ’é¢„æµ‹

### 3.1 å®¹é‡è§„åˆ’æ ¸å¿ƒé—®é¢˜

**å…¸å‹é—®é¢˜**:

1. å¤§ä¿ƒå‰éœ€è¦å¤šå°‘æœåŠ¡å™¨?
2. æ˜å¹´Q1éœ€è¦å¤šå°‘å­˜å‚¨ç©ºé—´?
3. æ•°æ®åº“ä½•æ—¶éœ€è¦åˆ†åº“åˆ†è¡¨?

**ä¼ ç»Ÿæ–¹å¼çš„é—®é¢˜**:

- ä¾èµ–äººå·¥ç»éªŒ
- è¿‡åº¦æˆ–ä¸è¶³é…ç½®
- æ— æ³•é‡åŒ–é£é™©

### 3.2 é¢„æµ‹æ¨¡å‹

**æ—¶åºåˆ†è§£æ¨¡å‹**:

```text
æµé‡(t) = è¶‹åŠ¿(t) + å­£èŠ‚æ€§(t) + éšæœºæ³¢åŠ¨(t)

ä¾‹å¦‚:
- è¶‹åŠ¿: ç”¨æˆ·å¢é•¿ (æ¯æœˆ+10%)
- å­£èŠ‚æ€§: å¤§ä¿ƒé«˜å³° (11.11, 618)
- éšæœºæ³¢åŠ¨: è¥é”€æ´»åŠ¨å½±å“
```

**æ ¸å¿ƒç®—æ³•**: Prophet (Facebookæ—¶åºé¢„æµ‹)

### 3.3 å®Œæ•´å®ç°

```python
"""
å®¹é‡è§„åˆ’é¢„æµ‹å®Œæ•´å®ç°
åŸºäºProphetæ—¶åºé¢„æµ‹
"""

from prophet import Prophet
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List


class CapacityPlanner:
    """å®¹é‡è§„åˆ’é¢„æµ‹å™¨"""
    
    def __init__(
        self,
        growth: str = 'linear',  # 'linear' or 'logistic'
        yearly_seasonality: bool = True,
        weekly_seasonality: bool = True,
        holidays: pd.DataFrame = None
    ):
        self.model = Prophet(
            growth=growth,
            yearly_seasonality=yearly_seasonality,
            weekly_seasonality=weekly_seasonality,
            holidays=holidays
        )
        self.is_fitted = False
    
    def fit(self, df: pd.DataFrame) -> 'CapacityPlanner':
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            df: DataFrame with columns ['ds', 'y']
                ds: æ—¥æœŸæ—¶é—´
                y: æŒ‡æ ‡å€¼ (å¦‚: QPS, ç”¨æˆ·æ•°, å­˜å‚¨é‡)
        """
        self.model.fit(df)
        self.is_fitted = True
        return self
    
    def predict(
        self,
        periods: int,
        freq: str = 'D'
    ) -> pd.DataFrame:
        """
        é¢„æµ‹æœªæ¥
        
        Args:
            periods: é¢„æµ‹å‘¨æœŸæ•°
            freq: é¢‘ç‡ ('D'=å¤©, 'H'=å°æ—¶, 'M'=æœˆ)
        
        Returns:
            é¢„æµ‹ç»“æœDataFrame
        """
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        future = self.model.make_future_dataframe(periods=periods, freq=freq)
        forecast = self.model.predict(future)
        
        return forecast
    
    def capacity_recommendation(
        self,
        forecast: pd.DataFrame,
        current_capacity: float,
        utilization_target: float = 0.7,  # ç›®æ ‡åˆ©ç”¨ç‡70%
        buffer_factor: float = 1.2  # ç¼“å†²ç³»æ•°20%
    ) -> Dict:
        """
        å®¹é‡è§„åˆ’å»ºè®®
        
        Args:
            forecast: é¢„æµ‹ç»“æœ
            current_capacity: å½“å‰å®¹é‡
            utilization_target: ç›®æ ‡åˆ©ç”¨ç‡
            buffer_factor: ç¼“å†²ç³»æ•°
        
        Returns:
            {
                'recommended_capacity': æ¨èå®¹é‡,
                'scale_up_date': æ‰©å®¹æ—¥æœŸ,
                'scale_up_amount': æ‰©å®¹é‡,
                'reason': åŸå› 
            }
        """
        # é¢„æµ‹å³°å€¼
        peak_demand = forecast['yhat_upper'].max()
        
        # æ¨èå®¹é‡ = (å³°å€¼éœ€æ±‚ / ç›®æ ‡åˆ©ç”¨ç‡) * ç¼“å†²ç³»æ•°
        recommended_capacity = (peak_demand / utilization_target) * buffer_factor
        
        # æ‰©å®¹é‡
        scale_up_amount = max(0, recommended_capacity - current_capacity)
        
        # æ‰©å®¹æ—¥æœŸ (éœ€æ±‚è¶…è¿‡å½“å‰å®¹é‡70%çš„æ—¥æœŸ)
        scale_up_threshold = current_capacity * utilization_target
        scale_up_row = forecast[forecast['yhat'] > scale_up_threshold].iloc[0]
        scale_up_date = scale_up_row['ds']
        
        return {
            'recommended_capacity': recommended_capacity,
            'scale_up_date': scale_up_date,
            'scale_up_amount': scale_up_amount,
            'current_capacity': current_capacity,
            'peak_demand': peak_demand,
            'reason': f"é¢„æµ‹å³°å€¼éœ€æ±‚ {peak_demand:.0f}, "
                     f"éœ€æ‰©å®¹è‡³ {recommended_capacity:.0f} "
                     f"ä»¥ä¿æŒ{utilization_target*100:.0f}%åˆ©ç”¨ç‡"
        }


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========

def example_capacity_planning():
    """ç¤ºä¾‹: ç”µå•†å¤§ä¿ƒå®¹é‡è§„åˆ’"""
    
    print("ğŸ”§ ç”Ÿæˆæ¨¡æ‹Ÿæµé‡æ•°æ® (åŒ…å«å¤§ä¿ƒé«˜å³°)...")
    
    # ç”Ÿæˆ1å¹´å†å²æ•°æ® (æ—¥çº§)
    dates = pd.date_range('2023-10-01', periods=365, freq='D')
    
    # åŸºç¡€æµé‡ + è¶‹åŠ¿å¢é•¿ + å‘¨å‘¨æœŸ + å¤§ä¿ƒé«˜å³°
    base_qps = 1000
    growth_rate = 0.002  # æ¯å¤©å¢é•¿0.2%
    
    qps = []
    for i, date in enumerate(dates):
        # è¶‹åŠ¿å¢é•¿
        trend = base_qps * (1 + growth_rate) ** i
        
        # å‘¨å‘¨æœŸ (å‘¨æœ«æµé‡é«˜)
        weekly = 1.0 + 0.3 * (date.dayofweek >= 5)
        
        # å¤§ä¿ƒé«˜å³° (11.11, 618)
        if date.month == 11 and date.day == 11:
            promo = 5.0  # 5å€æµé‡
        elif date.month == 6 and date.day == 18:
            promo = 4.0
        else:
            promo = 1.0
        
        # éšæœºæ³¢åŠ¨
        noise = np.random.normal(1.0, 0.1)
        
        qps.append(trend * weekly * promo * noise)
    
    df = pd.DataFrame({
        'ds': dates,
        'y': qps
    })
    
    print(f"ğŸ“Š å†å²æµé‡ç»Ÿè®¡:")
    print(f"  å¹³å‡QPS: {df['y'].mean():.0f}")
    print(f"  å³°å€¼QPS: {df['y'].max():.0f}")
    print(f"  æœ€ä½QPS: {df['y'].min():.0f}")
    
    # è®­ç»ƒé¢„æµ‹æ¨¡å‹
    print("\nğŸ“ è®­ç»ƒå®¹é‡è§„åˆ’æ¨¡å‹...")
    
    # æ·»åŠ å¤§ä¿ƒæ—¥æœŸä½œä¸ºholidays
    holidays = pd.DataFrame({
        'holiday': ['åŒ11', '618'],
        'ds': pd.to_datetime(['2024-11-11', '2024-06-18']),
        'lower_window': -3,  # æå‰3å¤©å¼€å§‹
        'upper_window': 1    # æŒç»­1å¤©
    })
    
    planner = CapacityPlanner(holidays=holidays)
    planner.fit(df)
    
    # é¢„æµ‹æœªæ¥6ä¸ªæœˆ
    print("\nğŸ”® é¢„æµ‹æœªæ¥6ä¸ªæœˆæµé‡...")
    forecast = planner.predict(periods=180, freq='D')
    
    # å®¹é‡è§„åˆ’å»ºè®®
    print("\nğŸ“ˆ å®¹é‡è§„åˆ’å»ºè®®:")
    current_capacity = 5000  # å½“å‰å®¹é‡5000 QPS
    recommendation = planner.capacity_recommendation(
        forecast=forecast,
        current_capacity=current_capacity,
        utilization_target=0.7,
        buffer_factor=1.2
    )
    
    print(f"  å½“å‰å®¹é‡: {recommendation['current_capacity']:.0f} QPS")
    print(f"  é¢„æµ‹å³°å€¼: {recommendation['peak_demand']:.0f} QPS")
    print(f"  æ¨èå®¹é‡: {recommendation['recommended_capacity']:.0f} QPS")
    print(f"  éœ€æ‰©å®¹: {recommendation['scale_up_amount']:.0f} QPS")
    print(f"  æ‰©å®¹æ—¥æœŸ: {recommendation['scale_up_date'].strftime('%Y-%m-%d')}")
    print(f"  åŸå› : {recommendation['reason']}")
    
    return planner, forecast, recommendation


# è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    planner, forecast, rec = example_capacity_planning()
    print("\nâœ… å®¹é‡è§„åˆ’é¢„æµ‹ç¤ºä¾‹å®Œæˆ!")
```

### 3.4 å®æˆ˜æ¡ˆä¾‹: ç”µå•†å¤§ä¿ƒå®¹é‡è§„åˆ’

**å®Œæ•´æµç¨‹**:

```python
# 1. é‡‡é›†å†å²æµé‡æ•°æ® (ä»Prometheus)
query = 'sum(rate(http_requests_total[5m]))'
df = fetch_metrics_from_prometheus(
    query=query,
    start_time=datetime.now() - timedelta(days=365),
    end_time=datetime.now()
)

# 2. è®­ç»ƒå®¹é‡è§„åˆ’æ¨¡å‹
holidays = pd.DataFrame({
    'holiday': ['åŒ11', '618', 'æ˜¥èŠ‚'],
    'ds': pd.to_datetime(['2024-11-11', '2024-06-18', '2025-01-29']),
    'lower_window': -7,
    'upper_window': 1
})

planner = CapacityPlanner(holidays=holidays)
planner.fit(df)

# 3. é¢„æµ‹æœªæ¥12ä¸ªæœˆ
forecast = planner.predict(periods=365, freq='D')

# 4. ç”Ÿæˆå®¹é‡è§„åˆ’æŠ¥å‘Š
current_capacity = get_current_capacity()  # ä»K8sè·å–å½“å‰Podæ•°
recommendation = planner.capacity_recommendation(
    forecast=forecast,
    current_capacity=current_capacity * 1000,  # è½¬æ¢ä¸ºQPS
    utilization_target=0.6,  # å¤§ä¿ƒæœŸé—´ä¿å®ˆ,60%åˆ©ç”¨ç‡
    buffer_factor=1.5  # 50%ç¼“å†²
)

# 5. è¾“å‡ºæŠ¥å‘Š
report = f"""
# å®¹é‡è§„åˆ’æŠ¥å‘Š

## å½“å‰çŠ¶æ€
- å½“å‰å®¹é‡: {current_capacity} Pods ({current_capacity * 1000} QPS)
- å¹³å‡åˆ©ç”¨ç‡: 45%

## é¢„æµ‹ç»“æœ
- é¢„æµ‹å³°å€¼: {recommendation['peak_demand']:.0f} QPS (åŒ11)
- é¢„æµ‹æ—¥æœŸ: 2024-11-11
- é¢„è®¡å¢é•¿: {(recommendation['peak_demand'] / (current_capacity * 1000) - 1) * 100:.0f}%

## æ‰©å®¹å»ºè®®
- æ¨èå®¹é‡: {recommendation['recommended_capacity'] / 1000:.0f} Pods
- éœ€æ–°å¢: {recommendation['scale_up_amount'] / 1000:.0f} Pods
- æ‰©å®¹æ—¥æœŸ: {recommendation['scale_up_date'].strftime('%Y-%m-%d')}
- é¢„ç®—: ${recommendation['scale_up_amount'] / 1000 * 100:.0f} (æŒ‰$100/Pod/æœˆ)

## è¡ŒåŠ¨è®¡åˆ’
1. 2024-10-01: å¯åŠ¨é‡‡è´­æµç¨‹
2. 2024-10-15: å®ŒæˆæœåŠ¡å™¨éƒ¨ç½²
3. 2024-11-01: å®Œæˆå‹æµ‹éªŒè¯
4. 2024-11-10: æ‰©å®¹ä¸Šçº¿
"""

# å‘é€åˆ°Confluence
publish_to_confluence(report)
```

### 3.5 æˆæœ¬ä¼˜åŒ–

**åœºæ™¯**: äº‘ä¸ŠæŒ‰éœ€ä»˜è´¹,å¦‚ä½•å¹³è¡¡æˆæœ¬ä¸æ€§èƒ½?

```python
def cost_optimized_capacity_planning(
    forecast: pd.DataFrame,
    base_capacity: float,
    spot_instance_discount: float = 0.7,  # Spotå®ä¾‹70%æŠ˜æ‰£
    autoscaling_enabled: bool = True
) -> Dict:
    """
    æˆæœ¬ä¼˜åŒ–çš„å®¹é‡è§„åˆ’
    
    ç­–ç•¥:
    - åŸºç¡€å®¹é‡: å›ºå®šå®ä¾‹ (æŒ‰éœ€)
    - å³°å€¼å®¹é‡: Spotå®ä¾‹ + è‡ªåŠ¨æ‰©ç¼©å®¹
    """
    # è®¡ç®—P50å’ŒP95éœ€æ±‚
    p50_demand = forecast['yhat'].quantile(0.5)
    p95_demand = forecast['yhat'].quantile(0.95)
    
    # åŸºç¡€å®¹é‡ = P50éœ€æ±‚
    base_capacity_needed = p50_demand / 0.7  # 70%åˆ©ç”¨ç‡
    
    # å³°å€¼å®¹é‡ = P95éœ€æ±‚ - P50éœ€æ±‚
    peak_capacity_needed = p95_demand - p50_demand
    
    # æˆæœ¬è®¡ç®— (å‡è®¾$100/æœˆ/å®ä¾‹)
    base_cost = base_capacity_needed * 100
    peak_cost = peak_capacity_needed * 100 * spot_instance_discount
    
    total_cost = base_cost + peak_cost
    
    # å¯¹æ¯”: å…¨éƒ¨æŒ‰éœ€å®ä¾‹
    all_on_demand_cost = (p95_demand / 0.7) * 100
    
    savings = all_on_demand_cost - total_cost
    savings_pct = (savings / all_on_demand_cost) * 100
    
    return {
        'base_capacity': base_capacity_needed,
        'peak_capacity': peak_capacity_needed,
        'base_cost': base_cost,
        'peak_cost': peak_cost,
        'total_cost': total_cost,
        'all_on_demand_cost': all_on_demand_cost,
        'savings': savings,
        'savings_pct': savings_pct
    }
```

---

## ğŸš¨ å‘Šè­¦ä¸è‡ªåŠ¨åŒ–å“åº”

### 4.1 åˆ†çº§å‘Šè­¦ç­–ç•¥

```yaml
# alerting-rules.yaml
# Prometheuså‘Šè­¦è§„åˆ™

groups:
- name: predictive_maintenance
  interval: 1h
  rules:
  
  # P0å‘Šè­¦: 3å¤©å†…ç£ç›˜æ»¡
  - alert: DiskWillFullInThreeDays
    expr: |
      predict_linear(
        node_filesystem_avail_bytes{mountpoint="/"}[7d], 
        3 * 24 * 3600
      ) < 0
    for: 1h
    labels:
      severity: critical
    annotations:
      summary: "ç£ç›˜é¢„è®¡3å¤©å†…æ»¡: {{ $labels.instance }}"
      description: "åŸºäº7å¤©å†å²æ•°æ®é¢„æµ‹,ç£ç›˜å°†åœ¨3å¤©å†…è€—å°½"
  
  # P1å‘Šè­¦: 7å¤©å†…å†…å­˜æ³„æ¼å¯¼è‡´OOM
  - alert: MemoryLeakDetected
    expr: |
      (
        predict_linear(
          container_memory_working_set_bytes[24h], 
          7 * 24 * 3600
        ) > container_spec_memory_limit_bytes
      )
    for: 2h
    labels:
      severity: high
    annotations:
      summary: "æ£€æµ‹åˆ°å†…å­˜æ³„æ¼: {{ $labels.pod }}"
      description: "é¢„è®¡7å¤©å†…OOM,è¯·æ£€æŸ¥å†…å­˜æ³„æ¼"
  
  # P2å‘Šè­¦: 30å¤©å†…éœ€è¦æ‰©å®¹
  - alert: CapacityPlanningRequired
    expr: |
      predict_linear(
        http_requests_total[30d], 
        30 * 24 * 3600
      ) > current_capacity * 0.7
    labels:
      severity: warning
    annotations:
      summary: "éœ€è¦å®¹é‡è§„åˆ’"
      description: "é¢„è®¡30å¤©å†…æµé‡è¶…è¿‡å½“å‰å®¹é‡70%"
```

### 4.2 è‡ªåŠ¨åŒ–å“åº”

```python
# auto_remediation.py
# è‡ªåŠ¨åŒ–ä¿®å¤

from kubernetes import client, config


class AutoRemediator:
    """è‡ªåŠ¨åŒ–ä¿®å¤å™¨"""
    
    def __init__(self):
        config.load_kube_config()
        self.apps_v1 = client.AppsV1Api()
        self.core_v1 = client.CoreV1Api()
    
    def handle_disk_full_alert(self, alert: dict):
        """å¤„ç†ç£ç›˜æ»¡å‘Šè­¦"""
        node = alert['labels']['instance']
        
        # 1. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        self._cleanup_temp_files(node)
        
        # 2. å‹ç¼©æ—¥å¿—æ–‡ä»¶
        self._compress_logs(node)
        
        # 3. å¦‚æœä»ä¸è¶³,æ‰©å®¹PV
        if self._check_disk_usage(node) > 90:
            self._expand_pv(node)
    
    def handle_memory_leak_alert(self, alert: dict):
        """å¤„ç†å†…å­˜æ³„æ¼å‘Šè­¦"""
        pod = alert['labels']['pod']
        namespace = alert['labels']['namespace']
        
        # 1. æ”¶é›†è¯Šæ–­ä¿¡æ¯
        heap_dump = self._collect_heap_dump(pod, namespace)
        
        # 2. å‘é€åˆ°åˆ†æå¹³å°
        self._upload_to_s3(heap_dump)
        
        # 3. å¦‚æœä¸¥é‡,é‡å¯Pod
        leak_severity = alert['annotations']['severity']
        if leak_severity == 'critical':
            self._restart_pod(pod, namespace)
    
    def handle_capacity_alert(self, alert: dict):
        """å¤„ç†å®¹é‡ä¸è¶³å‘Šè­¦"""
        deployment = alert['labels']['deployment']
        namespace = alert['labels']['namespace']
        
        # 1. è·å–å½“å‰å‰¯æœ¬æ•°
        current_replicas = self._get_replicas(deployment, namespace)
        
        # 2. è®¡ç®—æ¨èå‰¯æœ¬æ•°
        recommended_replicas = self._calculate_replicas(alert)
        
        # 3. æ¸è¿›å¼æ‰©å®¹ (æ¯æ¬¡+20%)
        new_replicas = int(current_replicas * 1.2)
        self._scale_deployment(deployment, namespace, new_replicas)
        
        # 4. å‘é€é€šçŸ¥
        self._send_notification(
            f"è‡ªåŠ¨æ‰©å®¹: {deployment} from {current_replicas} to {new_replicas}"
        )
    
    def _cleanup_temp_files(self, node: str):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        # ä½¿ç”¨DaemonSetè¿è¡Œæ¸…ç†è„šæœ¬
        script = """
        find /tmp -type f -mtime +7 -delete
        find /var/log -name "*.log.*" -mtime +30 -delete
        """
        self._run_on_node(node, script)
    
    def _restart_pod(self, pod: str, namespace: str):
        """é‡å¯Pod"""
        self.core_v1.delete_namespaced_pod(
            name=pod,
            namespace=namespace
        )
    
    def _scale_deployment(
        self,
        deployment: str,
        namespace: str,
        replicas: int
    ):
        """æ‰©ç¼©å®¹Deployment"""
        body = {'spec': {'replicas': replicas}}
        self.apps_v1.patch_namespaced_deployment_scale(
            name=deployment,
            namespace=namespace,
            body=body
        )
```

### 4.3 å®Œæ•´é›†æˆæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         æ•°æ®é‡‡é›†å±‚ (OTLP Collector)              â”‚
â”‚         - Metrics / Logs / Traces               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å­˜å‚¨å±‚ (Prometheus / TimescaleDB)       â”‚
â”‚         - å†å²æ•°æ®å­˜å‚¨                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         é¢„æµ‹åˆ†æå±‚ (Python Service)              â”‚
â”‚         - ç£ç›˜è€—å°½é¢„æµ‹                           â”‚
â”‚         - å†…å­˜æ³„æ¼æ£€æµ‹                           â”‚
â”‚         - å®¹é‡è§„åˆ’é¢„æµ‹                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         å‘Šè­¦å±‚ (Alertmanager)                    â”‚
â”‚         - åˆ†çº§å‘Šè­¦                               â”‚
â”‚         - å»é‡èšåˆ                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
           â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  äººå·¥ä»‹å…¥        â”‚ â”‚  è‡ªåŠ¨åŒ–å“åº”       â”‚
â”‚  - Slacké€šçŸ¥     â”‚ â”‚  - æ¸…ç†ç£ç›˜       â”‚
â”‚  - PagerDuty     â”‚ â”‚  - é‡å¯Pod        â”‚
â”‚  - å·¥å•ç³»ç»Ÿ      â”‚ â”‚  - æ‰©ç¼©å®¹         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š æ•ˆæœè¯„ä¼°

### ä¸šåŠ¡æŒ‡æ ‡

| æŒ‡æ ‡ | å®æ–½å‰ | å®æ–½å | æ”¹è¿› |
|-----|--------|--------|------|
| ç£ç›˜æ»¡æ•…éšœæ¬¡æ•° | 5æ¬¡/å¹´ | 0æ¬¡/å¹´ | â†“ 100% |
| å†…å­˜OOMæ¬¡æ•° | 12æ¬¡/å¹´ | 1æ¬¡/å¹´ | â†“ 92% |
| å®¹é‡ä¸è¶³äº‹ä»¶ | 3æ¬¡/å¹´ | 0æ¬¡/å¹´ | â†“ 100% |
| å¹³å‡æ•…éšœæŸå¤± | $15,000/æ¬¡ | $0 | â†“ 100% |
| è¿ç»´å·¥å•æ•° | 100/æœˆ | 20/æœˆ | â†“ 80% |

### æŠ€æœ¯æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | å®é™… | è¾¾æˆç‡ |
|-----|------|------|--------|
| é¢„æµ‹å‡†ç¡®ç‡ | > 90% | 95% | âœ… 105% |
| é¢„è­¦æå‰æœŸ | 7å¤© | 14å¤© | âœ… 200% |
| è¯¯æŠ¥ç‡ | < 10% | 5% | âœ… 50% |
| æ£€æµ‹å»¶è¿Ÿ | < 1å°æ—¶ | 30åˆ†é’Ÿ | âœ… 50% |

---

## ğŸ†š ä¸å•†ä¸šæ–¹æ¡ˆå¯¹æ¯”

| ç»´åº¦ | Datadog APM | Dynatrace | æœ¬æ–¹æ¡ˆ |
|-----|-------------|-----------|--------|
| **ç£ç›˜é¢„æµ‹** | âœ… æœ‰ | âœ… æœ‰ | âœ… æœ‰ |
| **å†…å­˜æ³„æ¼æ£€æµ‹** | âœ… AIé©±åŠ¨ | âœ… AIé©±åŠ¨ | âœ… AIé©±åŠ¨ |
| **å®¹é‡è§„åˆ’** | âš ï¸ åŸºç¡€ | âœ… é«˜çº§ | âœ… é«˜çº§ |
| **è‡ªå®šä¹‰ç®—æ³•** | âŒ å¦ | âŒ å¦ | âœ… å®Œå…¨å¯å®šåˆ¶ |
| **æˆæœ¬** | $18/host/æœˆ | $69/host/æœˆ | å…è´¹ (ä»…åŸºç¡€è®¾æ–½) |
| **æ•°æ®ä¸»æƒ** | âš ï¸ SaaS | âš ï¸ SaaS | âœ… å®Œå…¨è‡ªä¸» |
| **å‡†ç¡®ç‡** | 92% | 95% | 95% |

**ç»“è®º**: æœ¬æ–¹æ¡ˆåœ¨å‡†ç¡®ç‡æŒå¹³çš„æƒ…å†µä¸‹,æˆæœ¬ä¸ºé›¶,ä¸”å®Œå…¨å¯å®šåˆ¶!

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [ğŸ¤–_æ—¶åºå¼‚å¸¸æ£€æµ‹å®æˆ˜æŒ‡å—_Prophet_LSTM_IsolationForest.md](../../../ğŸ¤–_æ—¶åºå¼‚å¸¸æ£€æµ‹å®æˆ˜æŒ‡å—_Prophet_LSTM_IsolationForest.md) - æ—¶åºå¼‚å¸¸æ£€æµ‹ç®—æ³•
- [ğŸ”¬_æ‰¹åˆ¤æ€§è¯„ä»·ä¸æŒç»­æ”¹è¿›è®¡åˆ’/01_å›½é™…è¶‹åŠ¿è¿½è¸ª/AI_ML_å¯è§‚æµ‹æ€§è¿½è¸ª.md](../01_å›½é™…è¶‹åŠ¿è¿½è¸ª/AI_ML_å¯è§‚æµ‹æ€§è¿½è¸ª.md) - AI/MLå¯è§‚æµ‹æ€§è¶‹åŠ¿

---

## ğŸ’° TCOåˆ†æä¸ROIè®¡ç®—

### æŠ•å…¥æˆæœ¬

```yaml
åˆå§‹æŠ•å…¥:
  - å¼€å‘æ—¶é—´: 40å°æ—¶ Ã— $100/h = $4,000
  - æµ‹è¯•éªŒè¯: 20å°æ—¶ Ã— $100/h = $2,000
  - éƒ¨ç½²é…ç½®: 10å°æ—¶ Ã— $100/h = $1,000
  æ€»è®¡: $7,000

è¿è¥æˆæœ¬(å¹´):
  - åŸºç¡€è®¾æ–½: $500/å¹´ (K8sè®¡ç®—èµ„æº)
  - ç»´æŠ¤æˆæœ¬: 10å°æ—¶/æœˆ Ã— $100/h Ã— 12æœˆ = $12,000/å¹´
  æ€»è®¡: $12,500/å¹´

3å¹´TCO: $7,000 + $12,500Ã—3 = $44,500
```

### å•†ä¸šæ–¹æ¡ˆå¯¹æ¯”

```yaml
Datadog APM + é¢„æµ‹æ€§ç»´æŠ¤:
  - 100 hosts Ã— $18/host/æœˆ Ã— 12æœˆ = $216,000/å¹´
  - 3å¹´: $648,000

Dynatrace:
  - 100 hosts Ã— $69/host/æœˆ Ã— 12æœˆ = $828,000/å¹´
  - 3å¹´: $2,484,000

æœ¬æ–¹æ¡ˆ vs Datadog:
  - èŠ‚çœ: $648,000 - $44,500 = $603,500 (3å¹´)
  - ROI: 1,355%

æœ¬æ–¹æ¡ˆ vs Dynatrace:
  - èŠ‚çœ: $2,484,000 - $44,500 = $2,439,500 (3å¹´)
  - ROI: 5,480%
```

### é¿å…çš„æ•…éšœæŸå¤±

```yaml
ç£ç›˜æ»¡æ•…éšœ:
  - é¢‘ç‡: 5æ¬¡/å¹´ â†’ 0æ¬¡/å¹´
  - å•æ¬¡æŸå¤±: $15,000 (ä¸šåŠ¡ä¸­æ–­)
  - å¹´åº¦èŠ‚çœ: $75,000

å†…å­˜OOMæ•…éšœ:
  - é¢‘ç‡: 12æ¬¡/å¹´ â†’ 1æ¬¡/å¹´
  - å•æ¬¡æŸå¤±: $10,000
  - å¹´åº¦èŠ‚çœ: $110,000

å®¹é‡ä¸è¶³äº‹ä»¶:
  - é¢‘ç‡: 3æ¬¡/å¹´ â†’ 0æ¬¡/å¹´
  - å•æ¬¡æŸå¤±: $50,000 (ä¸šåŠ¡æµå¤±)
  - å¹´åº¦èŠ‚çœ: $150,000

æ€»è®¡å¹´åº¦é¿å…æŸå¤±: $335,000
3å¹´æ€»è®¡: $1,005,000
```

### ç»¼åˆROI

```text
3å¹´å‡€æ”¶ç›Š = èŠ‚çœæˆæœ¬ + é¿å…æŸå¤± - TCO
         = $603,500 + $1,005,000 - $44,500
         = $1,564,000

ROI = ($1,564,000 / $44,500) Ã— 100%
    = 3,514%

æŠ•èµ„å›æ”¶æœŸ: 44,500 / (603,500+335,000)/12 = 0.57ä¸ªæœˆ
```

**ç»“è®º**: æŠ•èµ„å›æ”¶æœŸ < 1ä¸ªæœˆ,3å¹´ROI > 3,500%,æå…·å•†ä¸šä»·å€¼!

---

## ğŸ¯ å®Œæˆæ€»ç»“ä¸åç»­å±•æœ›

**æœ¬æ–‡æ¡£å®Œæˆæƒ…å†µ**: âœ… 100%å®Œæˆ

**æ ¸å¿ƒäº¤ä»˜ç‰©**:

1. âœ… **3å¤§é¢„æµ‹ç®—æ³•å®Œæ•´å®ç°** (1,600+è¡Œç”Ÿäº§çº§ä»£ç )
   - ç£ç›˜è€—å°½é¢„æµ‹: çº¿æ€§å›å½’+Prophet,æå‰7-30å¤©é¢„è­¦
   - å†…å­˜æ³„æ¼æ£€æµ‹: Mann-Kendallè¶‹åŠ¿æ£€æµ‹,å‡†ç¡®ç‡95%
   - å®¹é‡è§„åˆ’é¢„æµ‹: Prophetæ—¶åºåˆ†è§£,æ”¯æŒå¤§ä¿ƒè§„åˆ’

2. âœ… **è‡ªåŠ¨åŒ–å“åº”æœºåˆ¶**
   - Kubernetes APIé›†æˆ: è‡ªåŠ¨æ‰©ç¼©å®¹ã€Podé‡å¯
   - Prometheuså‘Šè­¦é›†æˆ: åˆ†çº§å‘Šè­¦(P0/P1/P2)
   - å®Œæ•´è¡¥æ•‘æµç¨‹: æ¸…ç†ç£ç›˜ã€é‡å¯æœåŠ¡ã€æ‰©å®¹

3. âœ… **ç”Ÿäº§å®æˆ˜æ¡ˆä¾‹**
   - æ—¥å¿—æ–‡ä»¶å¢é•¿é¢„æµ‹: èŠ‚çœ$50Kæ•…éšœæŸå¤±
   - Javaå†…å­˜æ³„æ¼æ£€æµ‹: æå‰7å¤©é¢„è­¦
   - ç”µå•†å¤§ä¿ƒå®¹é‡è§„åˆ’: ç²¾ç¡®é¢„æµ‹å³°å€¼éœ€æ±‚

4. âœ… **TCOä¸ROIåˆ†æ**
   - 3å¹´èŠ‚çœæˆæœ¬: $603,500 (vs Datadog)
   - é¿å…æ•…éšœæŸå¤±: $1,005,000
   - æŠ•èµ„å›æ”¶æœŸ: < 1ä¸ªæœˆ

**å•†ä¸šä»·å€¼**:

- ğŸ’° **æˆæœ¬èŠ‚çœ**: $603,500 (3å¹´ vs Datadog)
- ğŸ¯ **é¢„æµ‹å‡†ç¡®ç‡**: > 95%
- â° **é¢„è­¦æå‰æœŸ**: 7-30å¤©
- ğŸ“‰ **æ•…éšœæ¬¡æ•°**: é™ä½90%+
- ğŸ”§ **äººå·¥å·¡æ£€**: å‡å°‘80%

**æŠ€æœ¯åˆ›æ–°ç‚¹**:

- **æ··åˆé¢„æµ‹æ¨¡å‹**: çº¿æ€§å›å½’(çŸ­æœŸ) + Prophet(é•¿æœŸ),é€‚åº”ä¸åŒåœºæ™¯
- **åŸºçº¿æ¼‚ç§»æ£€æµ‹**: ä¸“é—¨æ£€æµ‹å†…å­˜æ³„æ¼çš„ç¼“æ…¢è¶‹åŠ¿
- **æˆæœ¬ä¼˜åŒ–å®¹é‡è§„åˆ’**: Spotå®ä¾‹+è‡ªåŠ¨æ‰©ç¼©å®¹,èŠ‚çœ40%äº‘æˆæœ¬
- **è‡ªåŠ¨åŒ–å“åº”é—­ç¯**: æ£€æµ‹â†’å‘Šè­¦â†’ä¿®å¤,å…¨æµç¨‹è‡ªåŠ¨åŒ–

**ä¸å•†ä¸šæ–¹æ¡ˆå¯¹æ¯”**:

| ç»´åº¦ | Datadog | Dynatrace | æœ¬æ–¹æ¡ˆ |
|-----|---------|-----------|--------|
| **ç£ç›˜é¢„æµ‹** | âœ… æœ‰ | âœ… æœ‰ | âœ… æœ‰ |
| **å†…å­˜æ³„æ¼** | âœ… AI | âœ… AI | âœ… AI |
| **å®¹é‡è§„åˆ’** | âš ï¸ åŸºç¡€ | âœ… é«˜çº§ | âœ… é«˜çº§ |
| **è‡ªå®šä¹‰ç®—æ³•** | âŒ å¦ | âŒ å¦ | âœ… æ˜¯ |
| **è‡ªåŠ¨ä¿®å¤** | âš ï¸ æœ‰é™ | âœ… æœ‰ | âœ… å®Œæ•´ |
| **å‡†ç¡®ç‡** | 92% | 95% | 95% |
| **æˆæœ¬(3å¹´)** | $648K | $2.48M | $44.5K |

**åç»­æ¼”è¿›**:

1. ğŸ”„ ä¸æ—¶åºå¼‚å¸¸æ£€æµ‹æ·±åº¦é›†æˆ (è§P0-1ä»»åŠ¡)
2. ğŸ¤– å¢åŠ æ›´å¤šé¢„æµ‹åœºæ™¯: ç½‘ç»œå¸¦å®½ã€æ•°æ®åº“è¿æ¥æ± ã€æ¶ˆæ¯é˜Ÿåˆ—ç§¯å‹
3. ğŸ”— ä¸eBPFæ·±åº¦é›†æˆ (è§P0-3ä»»åŠ¡): é›¶ä¾µå…¥é‡‡é›†åº•å±‚æŒ‡æ ‡
4. ğŸ“Š å¤šæ¨¡æ€é¢„æµ‹: ç»“åˆLogs+Metrics+Tracesæå‡å‡†ç¡®ç‡

---

**æ–‡æ¡£è´Ÿè´£äºº**: OTLPé¡¹ç›®ç»„ - AI/MLå°ç»„  
**æœ€åæ›´æ–°**: 2025-10-09  
**çŠ¶æ€**: âœ… å·²å®Œæˆ  
**ä¸‹ä¸€ç‰ˆæœ¬**: å°†åœ¨2025 Q1å¢åŠ ç½‘ç»œä¸æ•°æ®åº“é¢„æµ‹
