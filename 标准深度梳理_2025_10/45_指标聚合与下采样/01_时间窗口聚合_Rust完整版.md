# æ—¶é—´çª—å£èšåˆ - Rust å®Œæ•´å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **Rust ç‰ˆæœ¬**: 1.90+  
> **æœ€åæ›´æ–°**: 2025-10-10

---

## ğŸ“‹ ç›®å½•

- [æ—¶é—´çª—å£èšåˆ - Rust å®Œæ•´å®ç°](#æ—¶é—´çª—å£èšåˆ---rust-å®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
    - [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
    - [åº”ç”¨åœºæ™¯](#åº”ç”¨åœºæ™¯)
  - [â° æ—¶é—´çª—å£èšåˆ](#-æ—¶é—´çª—å£èšåˆ)
    - [1. å›ºå®šçª—å£èšåˆ](#1-å›ºå®šçª—å£èšåˆ)
    - [2. æ»‘åŠ¨çª—å£èšåˆ](#2-æ»‘åŠ¨çª—å£èšåˆ)
    - [3. ä¼šè¯çª—å£èšåˆ](#3-ä¼šè¯çª—å£èšåˆ)
  - [ğŸ“Š èšåˆå‡½æ•°](#-èšåˆå‡½æ•°)
    - [1. åŸºç¡€èšåˆå‡½æ•°](#1-åŸºç¡€èšåˆå‡½æ•°)
    - [2. åˆ†ä½æ•°èšåˆ](#2-åˆ†ä½æ•°èšåˆ)
    - [3. Histogram èšåˆ](#3-histogram-èšåˆ)
  - [ğŸ”„ æµå¼èšåˆ](#-æµå¼èšåˆ)
    - [1. æµå¼èšåˆå¤„ç†å™¨](#1-æµå¼èšåˆå¤„ç†å™¨)
    - [2. å¢é‡èšåˆ](#2-å¢é‡èšåˆ)
  - [ğŸ’¾ èšåˆç»“æœå­˜å‚¨](#-èšåˆç»“æœå­˜å‚¨)
    - [1. èšåˆç»“æœå†™å…¥](#1-èšåˆç»“æœå†™å…¥)
    - [2. ç‰©åŒ–è§†å›¾](#2-ç‰©åŒ–è§†å›¾)
  - [ğŸ“ˆ å¤šçº§èšåˆ](#-å¤šçº§èšåˆ)
    - [1. åˆ†å±‚èšåˆå™¨](#1-åˆ†å±‚èšåˆå™¨)
  - [ğŸ“Š å®Œæ•´ç¤ºä¾‹ï¼šMetrics èšåˆç³»ç»Ÿ](#-å®Œæ•´ç¤ºä¾‹metrics-èšåˆç³»ç»Ÿ)
  - [ğŸ¯ æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)
    - [çª—å£é€‰æ‹©](#çª—å£é€‰æ‹©)
    - [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
    - [å­˜å‚¨ä¼˜åŒ–](#å­˜å‚¨ä¼˜åŒ–)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## ğŸ¯ æ¦‚è¿°

æ—¶é—´çª—å£èšåˆæ˜¯ OTLP Metrics ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œé€šè¿‡å¯¹æŒ‡æ ‡æ•°æ®è¿›è¡Œæ—¶é—´ç»´åº¦çš„èšåˆï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘æ•°æ®é‡ã€æå‡æŸ¥è¯¢æ€§èƒ½ï¼Œå¹¶æä¾›ä¸åŒç²’åº¦çš„æ•°æ®è§†å›¾ã€‚

### æ ¸å¿ƒåŠŸèƒ½

- âœ… **å›ºå®šçª—å£èšåˆ** - æŒ‰å›ºå®šæ—¶é—´é—´éš”èšåˆ
- âœ… **æ»‘åŠ¨çª—å£èšåˆ** - è¿ç»­çš„æ—¶é—´çª—å£èšåˆ
- âœ… **ä¼šè¯çª—å£èšåˆ** - åŸºäºæ´»åŠ¨é—´éš”çš„èšåˆ
- âœ… **å¤šç§èšåˆå‡½æ•°** - Sum, Avg, Min, Max, Count, P95, P99
- âœ… **æµå¼èšåˆ** - å®æ—¶å¢é‡èšåˆ
- âœ… **å¤šçº§èšåˆ** - 1åˆ†é’Ÿ â†’ 1å°æ—¶ â†’ 1å¤©

### åº”ç”¨åœºæ™¯

- ğŸ”§ **å®æ—¶ç›‘æ§** - 1åˆ†é’Ÿçº§åˆ«çš„å®æ—¶èšåˆ
- ğŸ”§ **å†å²åˆ†æ** - å°æ—¶/å¤©çº§åˆ«çš„å†å²æ•°æ®
- ğŸ”§ **å­˜å‚¨ä¼˜åŒ–** - å‡å°‘åŸå§‹æ•°æ®å­˜å‚¨é‡
- ğŸ”§ **æŸ¥è¯¢åŠ é€Ÿ** - é¢„èšåˆæå‡æŸ¥è¯¢æ€§èƒ½

---

## â° æ—¶é—´çª—å£èšåˆ

### 1. å›ºå®šçª—å£èšåˆ

```rust
use std::sync::Arc;
use std::collections::HashMap;
use chrono::{DateTime, Utc, Duration};
use tokio::sync::RwLock;

/// å›ºå®šçª—å£èšåˆå™¨
pub struct FixedWindowAggregator {
    window_size: Duration,
    aggregated_data: Arc<RwLock<HashMap<(String, i64), AggregatedMetric>>>,
}

impl FixedWindowAggregator {
    pub fn new(window_size: Duration) -> Self {
        Self {
            window_size,
            aggregated_data: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// èšåˆå•ä¸ªæ•°æ®ç‚¹
    pub async fn aggregate(
        &self,
        metric_name: String,
        timestamp: DateTime<Utc>,
        value: f64,
    ) {
        // è®¡ç®—çª—å£é”®
        let window_key = self.get_window_key(timestamp);
        
        let mut data = self.aggregated_data.write().await;
        
        let aggregated = data
            .entry((metric_name.clone(), window_key))
            .or_insert_with(|| AggregatedMetric {
                metric_name: metric_name.clone(),
                window_start: self.window_key_to_timestamp(window_key),
                window_end: self.window_key_to_timestamp(window_key) + self.window_size,
                count: 0,
                sum: 0.0,
                min: f64::MAX,
                max: f64::MIN,
                values: Vec::new(),
            });
        
        // æ›´æ–°èšåˆå€¼
        aggregated.count += 1;
        aggregated.sum += value;
        aggregated.min = aggregated.min.min(value);
        aggregated.max = aggregated.max.max(value);
        aggregated.values.push(value);
    }
    
    /// è·å–çª—å£é”®
    fn get_window_key(&self, timestamp: DateTime<Utc>) -> i64 {
        timestamp.timestamp() / self.window_size.num_seconds()
    }
    
    /// çª—å£é”®è½¬æ—¶é—´æˆ³
    fn window_key_to_timestamp(&self, window_key: i64) -> DateTime<Utc> {
        DateTime::from_timestamp(window_key * self.window_size.num_seconds(), 0).unwrap()
    }
    
    /// è·å–èšåˆç»“æœ
    pub async fn get_aggregated_results(&self) -> Vec<AggregatedMetric> {
        let data = self.aggregated_data.read().await;
        
        data.values()
            .map(|metric| {
                let mut result = metric.clone();
                
                // è®¡ç®—å¹³å‡å€¼
                result.avg = if result.count > 0 {
                    Some(result.sum / result.count as f64)
                } else {
                    None
                };
                
                // è®¡ç®—åˆ†ä½æ•°
                result.calculate_percentiles();
                
                result
            })
            .collect()
    }
    
    /// æ¸…é™¤è¿‡æœŸçª—å£
    pub async fn evict_old_windows(&self, retention: Duration) {
        let cutoff_key = self.get_window_key(Utc::now() - retention);
        
        let mut data = self.aggregated_data.write().await;
        
        data.retain(|(_, window_key), _| *window_key >= cutoff_key);
    }
}

#[derive(Debug, Clone)]
pub struct AggregatedMetric {
    pub metric_name: String,
    pub window_start: DateTime<Utc>,
    pub window_end: DateTime<Utc>,
    pub count: usize,
    pub sum: f64,
    pub min: f64,
    pub max: f64,
    pub avg: Option<f64>,
    pub values: Vec<f64>,
    pub p50: Option<f64>,
    pub p95: Option<f64>,
    pub p99: Option<f64>,
}

impl AggregatedMetric {
    /// è®¡ç®—åˆ†ä½æ•°
    fn calculate_percentiles(&mut self) {
        if self.values.is_empty() {
            return;
        }
        
        self.values.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        let len = self.values.len();
        
        self.p50 = Some(self.values[len / 2]);
        self.p95 = Some(self.values[len * 95 / 100]);
        self.p99 = Some(self.values[len * 99 / 100]);
    }
}
```

---

### 2. æ»‘åŠ¨çª—å£èšåˆ

```rust
/// æ»‘åŠ¨çª—å£èšåˆå™¨
pub struct SlidingWindowAggregator {
    window_size: Duration,
    slide_interval: Duration,
    data_points: Arc<RwLock<Vec<DataPoint>>>,
}

impl SlidingWindowAggregator {
    pub fn new(window_size: Duration, slide_interval: Duration) -> Self {
        Self {
            window_size,
            slide_interval,
            data_points: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    /// æ·»åŠ æ•°æ®ç‚¹
    pub async fn add_data_point(
        &self,
        metric_name: String,
        timestamp: DateTime<Utc>,
        value: f64,
    ) {
        let mut points = self.data_points.write().await;
        
        points.push(DataPoint {
            metric_name,
            timestamp,
            value,
        });
        
        // æ¸…ç†è¿‡æœŸæ•°æ®ç‚¹
        let cutoff = Utc::now() - self.window_size;
        points.retain(|p| p.timestamp >= cutoff);
    }
    
    /// è®¡ç®—æ»‘åŠ¨çª—å£èšåˆ
    pub async fn compute_sliding_windows(&self) -> Vec<SlidingWindowResult> {
        let points = self.data_points.read().await;
        
        if points.is_empty() {
            return Vec::new();
        }
        
        let mut results = Vec::new();
        
        let now = Utc::now();
        let mut window_start = now - self.window_size;
        
        while window_start <= now {
            let window_end = window_start + self.window_size;
            
            // æ”¶é›†çª—å£å†…çš„æ•°æ®ç‚¹
            let window_points: Vec<&DataPoint> = points
                .iter()
                .filter(|p| p.timestamp >= window_start && p.timestamp < window_end)
                .collect();
            
            if !window_points.is_empty() {
                // æŒ‰æŒ‡æ ‡åç§°åˆ†ç»„
                let mut metric_groups: HashMap<String, Vec<f64>> = HashMap::new();
                
                for point in window_points {
                    metric_groups
                        .entry(point.metric_name.clone())
                        .or_insert_with(Vec::new)
                        .push(point.value);
                }
                
                // å¯¹æ¯ä¸ªæŒ‡æ ‡è®¡ç®—èšåˆ
                for (metric_name, values) in metric_groups {
                    let count = values.len();
                    let sum: f64 = values.iter().sum();
                    let avg = sum / count as f64;
                    
                    results.push(SlidingWindowResult {
                        metric_name,
                        window_start,
                        window_end,
                        count,
                        sum,
                        avg,
                    });
                }
            }
            
            window_start = window_start + self.slide_interval;
        }
        
        results
    }
}

#[derive(Debug, Clone)]
pub struct DataPoint {
    pub metric_name: String,
    pub timestamp: DateTime<Utc>,
    pub value: f64,
}

#[derive(Debug, Clone)]
pub struct SlidingWindowResult {
    pub metric_name: String,
    pub window_start: DateTime<Utc>,
    pub window_end: DateTime<Utc>,
    pub count: usize,
    pub sum: f64,
    pub avg: f64,
}
```

---

### 3. ä¼šè¯çª—å£èšåˆ

```rust
/// ä¼šè¯çª—å£èšåˆå™¨
pub struct SessionWindowAggregator {
    gap_duration: Duration,
    sessions: Arc<RwLock<HashMap<String, Session>>>,
}

impl SessionWindowAggregator {
    pub fn new(gap_duration: Duration) -> Self {
        Self {
            gap_duration,
            sessions: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// å¤„ç†æ•°æ®ç‚¹
    pub async fn process_data_point(
        &self,
        metric_name: String,
        timestamp: DateTime<Utc>,
        value: f64,
    ) {
        let mut sessions = self.sessions.write().await;
        
        let session = sessions
            .entry(metric_name.clone())
            .or_insert_with(|| Session {
                metric_name: metric_name.clone(),
                start_time: timestamp,
                last_activity: timestamp,
                values: Vec::new(),
            });
        
        // æ£€æŸ¥æ˜¯å¦å±äºå½“å‰ä¼šè¯
        if timestamp - session.last_activity > self.gap_duration {
            // å¼€å§‹æ–°ä¼šè¯
            *session = Session {
                metric_name: metric_name.clone(),
                start_time: timestamp,
                last_activity: timestamp,
                values: vec![value],
            };
        } else {
            // ç»§ç»­å½“å‰ä¼šè¯
            session.last_activity = timestamp;
            session.values.push(value);
        }
    }
    
    /// è·å–å·²å®Œæˆçš„ä¼šè¯
    pub async fn get_completed_sessions(&self) -> Vec<SessionResult> {
        let mut sessions = self.sessions.write().await;
        
        let now = Utc::now();
        let mut completed = Vec::new();
        
        sessions.retain(|_, session| {
            if now - session.last_activity > self.gap_duration {
                // ä¼šè¯å·²ç»“æŸ
                completed.push(SessionResult::from_session(session));
                false
            } else {
                true
            }
        });
        
        completed
    }
}

#[derive(Debug, Clone)]
pub struct Session {
    pub metric_name: String,
    pub start_time: DateTime<Utc>,
    pub last_activity: DateTime<Utc>,
    pub values: Vec<f64>,
}

#[derive(Debug, Clone)]
pub struct SessionResult {
    pub metric_name: String,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub duration: Duration,
    pub count: usize,
    pub sum: f64,
    pub avg: f64,
}

impl SessionResult {
    fn from_session(session: &Session) -> Self {
        let count = session.values.len();
        let sum: f64 = session.values.iter().sum();
        let avg = if count > 0 { sum / count as f64 } else { 0.0 };
        
        Self {
            metric_name: session.metric_name.clone(),
            start_time: session.start_time,
            end_time: session.last_activity,
            duration: session.last_activity - session.start_time,
            count,
            sum,
            avg,
        }
    }
}
```

---

## ğŸ“Š èšåˆå‡½æ•°

### 1. åŸºç¡€èšåˆå‡½æ•°

```rust
/// èšåˆå‡½æ•°é›†åˆ
pub struct AggregationFunctions;

impl AggregationFunctions {
    /// Sum èšåˆ
    pub fn sum(values: &[f64]) -> f64 {
        values.iter().sum()
    }
    
    /// Average èšåˆ
    pub fn avg(values: &[f64]) -> Option<f64> {
        if values.is_empty() {
            return None;
        }
        
        Some(Self::sum(values) / values.len() as f64)
    }
    
    /// Min èšåˆ
    pub fn min(values: &[f64]) -> Option<f64> {
        values.iter().min_by(|a, b| a.partial_cmp(b).unwrap()).copied()
    }
    
    /// Max èšåˆ
    pub fn max(values: &[f64]) -> Option<f64> {
        values.iter().max_by(|a, b| a.partial_cmp(b).unwrap()).copied()
    }
    
    /// Count èšåˆ
    pub fn count(values: &[f64]) -> usize {
        values.len()
    }
    
    /// æ ‡å‡†å·®
    pub fn std_dev(values: &[f64]) -> Option<f64> {
        let avg = Self::avg(values)?;
        
        let variance = values
            .iter()
            .map(|v| {
                let diff = v - avg;
                diff * diff
            })
            .sum::<f64>()
            / values.len() as f64;
        
        Some(variance.sqrt())
    }
}
```

---

### 2. åˆ†ä½æ•°èšåˆ

```rust
/// åˆ†ä½æ•°èšåˆå™¨
pub struct PercentileAggregator;

impl PercentileAggregator {
    /// è®¡ç®—åˆ†ä½æ•°
    pub fn calculate_percentile(values: &[f64], percentile: f64) -> Option<f64> {
        if values.is_empty() {
            return None;
        }
        
        let mut sorted = values.to_vec();
        sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        let index = (sorted.len() as f64 * percentile / 100.0).ceil() as usize - 1;
        let index = index.min(sorted.len() - 1);
        
        Some(sorted[index])
    }
    
    /// è®¡ç®—å¤šä¸ªåˆ†ä½æ•°
    pub fn calculate_percentiles(values: &[f64], percentiles: &[f64]) -> HashMap<String, f64> {
        let mut results = HashMap::new();
        
        for &percentile in percentiles {
            if let Some(value) = Self::calculate_percentile(values, percentile) {
                results.insert(format!("p{}", percentile as i32), value);
            }
        }
        
        results
    }
    
    /// T-Digest è¿‘ä¼¼åˆ†ä½æ•°ï¼ˆé€‚åˆå¤§æ•°æ®é›†ï¼‰
    pub fn approximate_percentile(values: &[f64], percentile: f64) -> Option<f64> {
        // ç®€åŒ–å®ç°ï¼šä½¿ç”¨é‡‡æ ·
        if values.is_empty() {
            return None;
        }
        
        let sample_size = 1000.min(values.len());
        let step = values.len() / sample_size;
        
        let samples: Vec<f64> = values
            .iter()
            .step_by(step)
            .copied()
            .collect();
        
        Self::calculate_percentile(&samples, percentile)
    }
}
```

---

### 3. Histogram èšåˆ

```rust
/// Histogram èšåˆå™¨
pub struct HistogramAggregator {
    buckets: Vec<f64>,
}

impl HistogramAggregator {
    pub fn new(buckets: Vec<f64>) -> Self {
        let mut sorted_buckets = buckets;
        sorted_buckets.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        Self { buckets: sorted_buckets }
    }
    
    /// èšåˆ Histogram
    pub fn aggregate(&self, values: &[f64]) -> HistogramResult {
        let mut bucket_counts = vec![0usize; self.buckets.len()];
        
        for &value in values {
            for (i, &boundary) in self.buckets.iter().enumerate() {
                if value <= boundary {
                    bucket_counts[i] += 1;
                    break;
                }
            }
        }
        
        HistogramResult {
            buckets: self.buckets.clone(),
            counts: bucket_counts,
            total_count: values.len(),
            sum: values.iter().sum(),
        }
    }
    
    /// åˆå¹¶å¤šä¸ª Histogram
    pub fn merge(&self, histograms: Vec<HistogramResult>) -> HistogramResult {
        if histograms.is_empty() {
            return HistogramResult {
                buckets: self.buckets.clone(),
                counts: vec![0; self.buckets.len()],
                total_count: 0,
                sum: 0.0,
            };
        }
        
        let mut merged_counts = vec![0usize; self.buckets.len()];
        let mut total_count = 0;
        let mut total_sum = 0.0;
        
        for hist in histograms {
            for (i, count) in hist.counts.iter().enumerate() {
                merged_counts[i] += count;
            }
            
            total_count += hist.total_count;
            total_sum += hist.sum;
        }
        
        HistogramResult {
            buckets: self.buckets.clone(),
            counts: merged_counts,
            total_count,
            sum: total_sum,
        }
    }
}

#[derive(Debug, Clone)]
pub struct HistogramResult {
    pub buckets: Vec<f64>,
    pub counts: Vec<usize>,
    pub total_count: usize,
    pub sum: f64,
}
```

---

## ğŸ”„ æµå¼èšåˆ

### 1. æµå¼èšåˆå¤„ç†å™¨

```rust
use futures::stream::{Stream, StreamExt};
use std::pin::Pin;

/// æµå¼èšåˆå¤„ç†å™¨
pub struct StreamingAggregationProcessor {
    aggregator: Arc<FixedWindowAggregator>,
}

impl StreamingAggregationProcessor {
    pub fn new(aggregator: Arc<FixedWindowAggregator>) -> Self {
        Self { aggregator }
    }
    
    /// å¤„ç†æ•°æ®æµ
    pub async fn process_stream(
        &self,
        mut stream: Pin<Box<dyn Stream<Item = DataPoint> + Send>>,
    ) {
        while let Some(point) = stream.next().await {
            self.aggregator
                .aggregate(
                    point.metric_name,
                    point.timestamp,
                    point.value,
                )
                .await;
        }
    }
    
    /// å®šæœŸè¾“å‡ºèšåˆç»“æœ
    pub async fn emit_aggregated_results(
        &self,
        interval: std::time::Duration,
    ) {
        let mut interval_timer = tokio::time::interval(interval);
        
        loop {
            interval_timer.tick().await;
            
            let results = self.aggregator.get_aggregated_results().await;
            
            tracing::info!("Emitting {} aggregated metrics", results.len());
            
            // è¿™é‡Œå¯ä»¥å°†ç»“æœå†™å…¥å­˜å‚¨æˆ–å‘é€åˆ°ä¸‹æ¸¸
            self.emit_results(results).await;
        }
    }
    
    async fn emit_results(&self, results: Vec<AggregatedMetric>) {
        // å®ç°ç»“æœè¾“å‡ºé€»è¾‘
        for result in results {
            tracing::debug!(
                "Metric: {}, Window: {} - {}, Count: {}, Avg: {:?}",
                result.metric_name,
                result.window_start,
                result.window_end,
                result.count,
                result.avg
            );
        }
    }
}
```

---

### 2. å¢é‡èšåˆ

```rust
/// å¢é‡èšåˆå™¨ï¼ˆç©ºé—´é«˜æ•ˆï¼‰
pub struct IncrementalAggregator {
    state: Arc<RwLock<HashMap<String, IncrementalState>>>,
}

impl IncrementalAggregator {
    pub fn new() -> Self {
        Self {
            state: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// å¢é‡æ›´æ–°
    pub async fn update(&self, metric_name: String, value: f64) {
        let mut state_map = self.state.write().await;
        
        let state = state_map
            .entry(metric_name)
            .or_insert_with(IncrementalState::new);
        
        state.update(value);
    }
    
    /// è·å–èšåˆç»“æœ
    pub async fn get_result(&self, metric_name: &str) -> Option<IncrementalResult> {
        let state_map = self.state.read().await;
        
        state_map
            .get(metric_name)
            .map(|state| state.to_result(metric_name))
    }
    
    /// é‡ç½®çŠ¶æ€
    pub async fn reset(&self, metric_name: &str) {
        let mut state_map = self.state.write().await;
        state_map.remove(metric_name);
    }
}

#[derive(Debug, Clone)]
pub struct IncrementalState {
    count: usize,
    sum: f64,
    sum_of_squares: f64,
    min: f64,
    max: f64,
}

impl IncrementalState {
    fn new() -> Self {
        Self {
            count: 0,
            sum: 0.0,
            sum_of_squares: 0.0,
            min: f64::MAX,
            max: f64::MIN,
        }
    }
    
    fn update(&mut self, value: f64) {
        self.count += 1;
        self.sum += value;
        self.sum_of_squares += value * value;
        self.min = self.min.min(value);
        self.max = self.max.max(value);
    }
    
    fn to_result(&self, metric_name: &str) -> IncrementalResult {
        let avg = if self.count > 0 {
            self.sum / self.count as f64
        } else {
            0.0
        };
        
        let variance = if self.count > 0 {
            (self.sum_of_squares / self.count as f64) - (avg * avg)
        } else {
            0.0
        };
        
        IncrementalResult {
            metric_name: metric_name.to_string(),
            count: self.count,
            sum: self.sum,
            avg,
            min: self.min,
            max: self.max,
            std_dev: variance.sqrt(),
        }
    }
}

#[derive(Debug, Clone)]
pub struct IncrementalResult {
    pub metric_name: String,
    pub count: usize,
    pub sum: f64,
    pub avg: f64,
    pub min: f64,
    pub max: f64,
    pub std_dev: f64,
}
```

---

## ğŸ’¾ èšåˆç»“æœå­˜å‚¨

### 1. èšåˆç»“æœå†™å…¥

```rust
/// èšåˆç»“æœå­˜å‚¨å™¨
pub struct AggregationStorage {
    timescaledb: Arc<TimescaleDbClient>,
}

impl AggregationStorage {
    pub fn new(timescaledb: Arc<TimescaleDbClient>) -> Self {
        Self { timescaledb }
    }
    
    /// å†™å…¥èšåˆç»“æœ
    pub async fn write_aggregated_metrics(
        &self,
        metrics: Vec<AggregatedMetric>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        for metric in metrics {
            self.timescaledb
                .execute(
                    "INSERT INTO aggregated_metrics (time, metric_name, count, sum, avg, min, max, p50, p95, p99)
                     VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                     ON CONFLICT (time, metric_name) DO UPDATE SET
                         count = EXCLUDED.count,
                         sum = EXCLUDED.sum,
                         avg = EXCLUDED.avg,
                         min = EXCLUDED.min,
                         max = EXCLUDED.max,
                         p50 = EXCLUDED.p50,
                         p95 = EXCLUDED.p95,
                         p99 = EXCLUDED.p99",
                    &[
                        &metric.window_start,
                        &metric.metric_name,
                        &(metric.count as i64),
                        &metric.sum,
                        &metric.avg.unwrap_or(0.0),
                        &metric.min,
                        &metric.max,
                        &metric.p50.unwrap_or(0.0),
                        &metric.p95.unwrap_or(0.0),
                        &metric.p99.unwrap_or(0.0),
                    ],
                )
                .await?;
        }
        
        Ok(())
    }
}
```

---

### 2. ç‰©åŒ–è§†å›¾

```rust
/// ç‰©åŒ–è§†å›¾ç®¡ç†å™¨
pub struct MaterializedViewManager {
    timescaledb: Arc<TimescaleDbClient>,
}

impl MaterializedViewManager {
    pub fn new(timescaledb: Arc<TimescaleDbClient>) -> Self {
        Self { timescaledb }
    }
    
    /// åˆ›å»ºè¿ç»­èšåˆè§†å›¾
    pub async fn create_continuous_aggregate(
        &self,
        view_name: &str,
        source_table: &str,
        window_size: &str,
    ) -> Result<(), Box<dyn std::error::Error>> {
        self.timescaledb
            .execute(
                &format!(
                    "CREATE MATERIALIZED VIEW {} WITH (timescaledb.continuous) AS
                     SELECT time_bucket('{}', time) AS bucket,
                            metric_name,
                            COUNT(*) as count,
                            SUM(value) as sum,
                            AVG(value) as avg,
                            MIN(value) as min,
                            MAX(value) as max
                     FROM {}
                     GROUP BY bucket, metric_name",
                    view_name, window_size, source_table
                ),
                &[],
            )
            .await?;
        
        // è®¾ç½®åˆ·æ–°ç­–ç•¥
        self.timescaledb
            .execute(
                &format!(
                    "SELECT add_continuous_aggregate_policy('{}',
                        start_offset => INTERVAL '1 month',
                        end_offset => INTERVAL '1 minute',
                        schedule_interval => INTERVAL '1 minute')",
                    view_name
                ),
                &[],
            )
            .await?;
        
        Ok(())
    }
}

// å ä½å®ç°
pub struct TimescaleDbClient {}
impl TimescaleDbClient {
    pub async fn execute(&self, _query: &str, _params: &[&dyn std::any::Any]) -> Result<(), Box<dyn std::error::Error>> {
        Ok(())
    }
}
```

---

## ğŸ“ˆ å¤šçº§èšåˆ

### 1. åˆ†å±‚èšåˆå™¨

```rust
/// åˆ†å±‚èšåˆå™¨ï¼ˆ1åˆ†é’Ÿ â†’ 1å°æ—¶ â†’ 1å¤©ï¼‰
pub struct HierarchicalAggregator {
    minute_aggregator: Arc<FixedWindowAggregator>,
    hour_aggregator: Arc<FixedWindowAggregator>,
    day_aggregator: Arc<FixedWindowAggregator>,
}

impl HierarchicalAggregator {
    pub fn new() -> Self {
        Self {
            minute_aggregator: Arc::new(FixedWindowAggregator::new(Duration::minutes(1))),
            hour_aggregator: Arc::new(FixedWindowAggregator::new(Duration::hours(1))),
            day_aggregator: Arc::new(FixedWindowAggregator::new(Duration::days(1))),
        }
    }
    
    /// å¤„ç†åŸå§‹æ•°æ®ç‚¹
    pub async fn process(&self, metric_name: String, timestamp: DateTime<Utc>, value: f64) {
        // å†™å…¥1åˆ†é’Ÿèšåˆ
        self.minute_aggregator
            .aggregate(metric_name.clone(), timestamp, value)
            .await;
    }
    
    /// ä»åˆ†é’ŸèšåˆRollupåˆ°å°æ—¶èšåˆ
    pub async fn rollup_to_hour(&self) -> Result<(), Box<dyn std::error::Error>> {
        let minute_results = self.minute_aggregator.get_aggregated_results().await;
        
        for result in minute_results {
            // å°†åˆ†é’Ÿçº§æ•°æ®çš„sumé‡æ–°èšåˆåˆ°å°æ—¶çº§
            self.hour_aggregator
                .aggregate(
                    result.metric_name,
                    result.window_start,
                    result.sum,
                )
                .await;
        }
        
        Ok(())
    }
    
    /// ä»å°æ—¶èšåˆRollupåˆ°å¤©èšåˆ
    pub async fn rollup_to_day(&self) -> Result<(), Box<dyn std::error::Error>> {
        let hour_results = self.hour_aggregator.get_aggregated_results().await;
        
        for result in hour_results {
            self.day_aggregator
                .aggregate(
                    result.metric_name,
                    result.window_start,
                    result.sum,
                )
                .await;
        }
        
        Ok(())
    }
    
    /// å¯åŠ¨å®šæœŸRollupä»»åŠ¡
    pub async fn start_rollup_tasks(&self) {
        // æ¯å°æ—¶Rollupä¸€æ¬¡
        let hour_aggregator = self.hour_aggregator.clone();
        let minute_aggregator = self.minute_aggregator.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(std::time::Duration::from_secs(3600));
            
            loop {
                interval.tick().await;
                
                let minute_results = minute_aggregator.get_aggregated_results().await;
                
                for result in minute_results {
                    hour_aggregator
                        .aggregate(result.metric_name, result.window_start, result.sum)
                        .await;
                }
            }
        });
        
        // æ¯å¤©Rollupä¸€æ¬¡
        let day_aggregator = self.day_aggregator.clone();
        let hour_aggregator = self.hour_aggregator.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(std::time::Duration::from_secs(86400));
            
            loop {
                interval.tick().await;
                
                let hour_results = hour_aggregator.get_aggregated_results().await;
                
                for result in hour_results {
                    day_aggregator
                        .aggregate(result.metric_name, result.window_start, result.sum)
                        .await;
                }
            }
        });
    }
}
```

---

## ğŸ“Š å®Œæ•´ç¤ºä¾‹ï¼šMetrics èšåˆç³»ç»Ÿ

```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt::init();
    
    println!("ğŸ¯ Metrics èšåˆç³»ç»Ÿå¯åŠ¨ä¸­...\n");
    
    // åˆ›å»ºåˆ†å±‚èšåˆå™¨
    let hierarchical_aggregator = Arc::new(HierarchicalAggregator::new());
    
    // å¯åŠ¨Rollupä»»åŠ¡
    hierarchical_aggregator.start_rollup_tasks().await;
    
    println!("âœ… Rollup ä»»åŠ¡å·²å¯åŠ¨\n");
    
    // æ¨¡æ‹Ÿæ•°æ®ç‚¹
    let metric_name = "http_requests_total".to_string();
    
    for i in 0..1000 {
        let timestamp = Utc::now();
        let value = 100.0 + (i as f64 % 50.0);
        
        hierarchical_aggregator
            .process(metric_name.clone(), timestamp, value)
            .await;
        
        tokio::time::sleep(std::time::Duration::from_millis(10)).await;
    }
    
    println!("ğŸ“Š å¤„ç†äº† 1000 ä¸ªæ•°æ®ç‚¹\n");
    
    // è·å–èšåˆç»“æœ
    let minute_results = hierarchical_aggregator
        .minute_aggregator
        .get_aggregated_results()
        .await;
    
    println!("ğŸ“ˆ 1åˆ†é’Ÿçº§èšåˆç»“æœ:");
    
    for result in minute_results.iter().take(5) {
        println!(
            "  {} | çª—å£: {} - {} | è®¡æ•°: {} | å¹³å‡: {:.2}",
            result.metric_name,
            result.window_start.format("%H:%M:%S"),
            result.window_end.format("%H:%M:%S"),
            result.count,
            result.avg.unwrap_or(0.0)
        );
    }
    
    println!("\nâœ… èšåˆç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼");
    
    Ok(())
}
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### çª—å£é€‰æ‹©

1. **å®æ—¶ç›‘æ§** - 1åˆ†é’Ÿå›ºå®šçª—å£
2. **çŸ­æœŸåˆ†æ** - 5åˆ†é’Ÿæˆ–15åˆ†é’Ÿçª—å£
3. **é•¿æœŸå­˜å‚¨** - 1å°æ—¶æˆ–1å¤©çª—å£

### æ€§èƒ½ä¼˜åŒ–

1. **å¢é‡èšåˆ** - é¿å…å…¨é‡é‡æ–°è®¡ç®—
2. **å¹¶è¡Œå¤„ç†** - å¤šä¸ªçª—å£å¹¶è¡Œèšåˆ
3. **å»¶è¿Ÿè®¡ç®—** - æ‡’è®¡ç®—åˆ†ä½æ•°ç­‰æ˜‚è´µæ“ä½œ

### å­˜å‚¨ä¼˜åŒ–

1. **å¤šçº§Rollup** - 1åˆ†é’Ÿ â†’ 1å°æ—¶ â†’ 1å¤©
2. **TTLç­–ç•¥** - åŸå§‹æ•°æ®7å¤©ï¼Œèšåˆæ•°æ®90å¤©
3. **å‹ç¼©** - ä½¿ç”¨æ—¶åºæ•°æ®åº“çš„åŸç”Ÿå‹ç¼©

---

## ğŸ“š å‚è€ƒèµ„æº

- [Apache Flink Windowing](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/windows/)
- [TimescaleDB Continuous Aggregates](https://docs.timescale.com/timescaledb/latest/how-to-guides/continuous-aggregates/)
- [Prometheus Downsampling](https://prometheus.io/docs/prometheus/latest/querying/basics/)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-10  
**ä½œè€…**: OTLP Rust é¡¹ç›®ç»„
