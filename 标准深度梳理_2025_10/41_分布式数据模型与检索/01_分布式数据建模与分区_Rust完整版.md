# åˆ†å¸ƒå¼æ•°æ®å»ºæ¨¡ä¸åˆ†åŒºç­–ç•¥ - Rust å®Œæ•´å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **Rust ç‰ˆæœ¬**: 1.90+  
> **æœ€åæ›´æ–°**: 2025-10-10

---

## ğŸ“‹ ç›®å½•

- [åˆ†å¸ƒå¼æ•°æ®å»ºæ¨¡ä¸åˆ†åŒºç­–ç•¥ - Rust å®Œæ•´å®ç°](#åˆ†å¸ƒå¼æ•°æ®å»ºæ¨¡ä¸åˆ†åŒºç­–ç•¥---rust-å®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
    - [æ ¸å¿ƒç›®æ ‡](#æ ¸å¿ƒç›®æ ‡)
    - [å…³é”®æŒ‘æˆ˜](#å…³é”®æŒ‘æˆ˜)
  - [ğŸ—ï¸ åˆ†å¸ƒå¼æ•°æ®æ¨¡å‹è®¾è®¡](#ï¸-åˆ†å¸ƒå¼æ•°æ®æ¨¡å‹è®¾è®¡)
    - [1. Trace æ•°æ®æ¨¡å‹](#1-trace-æ•°æ®æ¨¡å‹)
    - [2. Metric æ•°æ®æ¨¡å‹](#2-metric-æ•°æ®æ¨¡å‹)
    - [3. Log æ•°æ®æ¨¡å‹](#3-log-æ•°æ®æ¨¡å‹)
  - [ğŸ“Š æ•°æ®åˆ†åŒºç­–ç•¥](#-æ•°æ®åˆ†åŒºç­–ç•¥)
    - [1. æ—¶é—´èŒƒå›´åˆ†åŒº (Time-Range Partitioning)](#1-æ—¶é—´èŒƒå›´åˆ†åŒº-time-range-partitioning)
    - [2. å“ˆå¸Œåˆ†åŒº (Hash Partitioning)](#2-å“ˆå¸Œåˆ†åŒº-hash-partitioning)
    - [3. èŒƒå›´åˆ†åŒº (Range Partitioning)](#3-èŒƒå›´åˆ†åŒº-range-partitioning)
    - [4. å¤åˆåˆ†åŒºç­–ç•¥](#4-å¤åˆåˆ†åŒºç­–ç•¥)
  - [ğŸ” æ•°æ®å®šä½ä¸è·¯ç”±](#-æ•°æ®å®šä½ä¸è·¯ç”±)
    - [1. ä¸€è‡´æ€§å“ˆå¸Œè·¯ç”±](#1-ä¸€è‡´æ€§å“ˆå¸Œè·¯ç”±)
    - [2. å…ƒæ•°æ®æœåŠ¡](#2-å…ƒæ•°æ®æœåŠ¡)
    - [3. åˆ†ç‰‡ç®¡ç†å™¨](#3-åˆ†ç‰‡ç®¡ç†å™¨)
  - [âš¡ åˆ†åŒºé‡å¹³è¡¡](#-åˆ†åŒºé‡å¹³è¡¡)
    - [1. è‡ªåŠ¨é‡å¹³è¡¡ç®¡ç†å™¨](#1-è‡ªåŠ¨é‡å¹³è¡¡ç®¡ç†å™¨)
  - [ğŸ’¾ æ•°æ®æœ¬åœ°åŒ–ä¼˜åŒ–](#-æ•°æ®æœ¬åœ°åŒ–ä¼˜åŒ–)
    - [1. æ•°æ®å±€éƒ¨æ€§æ„ŸçŸ¥è·¯ç”±](#1-æ•°æ®å±€éƒ¨æ€§æ„ŸçŸ¥è·¯ç”±)
  - [ğŸ“ˆ å®Œæ•´ç¤ºä¾‹ï¼šåˆ†å¸ƒå¼ OTLP å­˜å‚¨ç³»ç»Ÿ](#-å®Œæ•´ç¤ºä¾‹åˆ†å¸ƒå¼-otlp-å­˜å‚¨ç³»ç»Ÿ)
  - [ğŸ¯ æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)
    - [åˆ†åŒºå¤§å°](#åˆ†åŒºå¤§å°)
    - [åˆ†åŒºé”®é€‰æ‹©](#åˆ†åŒºé”®é€‰æ‹©)
    - [å‰¯æœ¬ç­–ç•¥](#å‰¯æœ¬ç­–ç•¥)
  - [ğŸ“Š æ€§èƒ½æŒ‡æ ‡](#-æ€§èƒ½æŒ‡æ ‡)
  - [ğŸ”§ æ•…éšœå¤„ç†](#-æ•…éšœå¤„ç†)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## ğŸ¯ æ¦‚è¿°

åˆ†å¸ƒå¼ OTLP æ•°æ®æ¨¡å‹éœ€è¦åœ¨**å¯æ‰©å±•æ€§**ã€**æŸ¥è¯¢æ€§èƒ½**å’Œ**æ•°æ®ä¸€è‡´æ€§**ä¹‹é—´å–å¾—å¹³è¡¡ã€‚
æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨ Rust å®ç°é«˜æ€§èƒ½çš„åˆ†å¸ƒå¼æ•°æ®å»ºæ¨¡ä¸åˆ†åŒºç­–ç•¥ã€‚

### æ ¸å¿ƒç›®æ ‡

- âœ… **æ°´å¹³æ‰©å±•**: æ”¯æŒ PB çº§æ•°æ®å­˜å‚¨
- âœ… **æŸ¥è¯¢ä¼˜åŒ–**: é€šè¿‡æ™ºèƒ½åˆ†åŒºæå‡æŸ¥è¯¢æ€§èƒ½
- âœ… **è´Ÿè½½å‡è¡¡**: æ•°æ®å‡åŒ€åˆ†å¸ƒåˆ°å„èŠ‚ç‚¹
- âœ… **é«˜å¯ç”¨æ€§**: å¤šå‰¯æœ¬ä¿è¯æ•°æ®å¯é æ€§
- âœ… **å¼¹æ€§ä¼¸ç¼©**: åŠ¨æ€å¢å‡èŠ‚ç‚¹

### å…³é”®æŒ‘æˆ˜

- ğŸ”´ **çƒ­ç‚¹é—®é¢˜**: é¿å…æ•°æ®å€¾æ–œ
- ğŸ”´ **è·¨åˆ†åŒºæŸ¥è¯¢**: æœ€å°åŒ–åˆ†å¸ƒå¼æŸ¥è¯¢å¼€é”€
- ğŸ”´ **æ•°æ®è¿ç§»**: å¹³æ»‘çš„é‡å¹³è¡¡è¿‡ç¨‹
- ğŸ”´ **æ—¶é—´åºåˆ—ç‰¹æ€§**: å¤„ç†æ—¶åºæ•°æ®çš„ç‰¹æ®Šæ€§

---

## ğŸ—ï¸ åˆ†å¸ƒå¼æ•°æ®æ¨¡å‹è®¾è®¡

### 1. Trace æ•°æ®æ¨¡å‹

**æ ¸å¿ƒå­—æ®µä¸åˆ†åŒºé”®**:

```rust
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use chrono::{DateTime, Utc};

/// Trace æ•°æ®æ¨¡å‹
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DistributedTrace {
    /// å…¨å±€å”¯ä¸€çš„ Trace ID (128-bit)
    pub trace_id: TraceId,
    
    /// Spans åˆ—è¡¨
    pub spans: Vec<DistributedSpan>,
    
    /// Resource å±æ€§
    pub resource: ResourceAttributes,
    
    /// åˆ›å»ºæ—¶é—´ï¼ˆç”¨äºæ—¶é—´åˆ†åŒºï¼‰
    pub timestamp: DateTime<Utc>,
    
    /// åˆ†åŒºé”®ï¼ˆç”¨äºæ•°æ®å®šä½ï¼‰
    pub partition_key: PartitionKey,
}

#[derive(Debug, Clone, Serialize, Deserialize, Hash, PartialEq, Eq)]
pub struct TraceId([u8; 16]);

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DistributedSpan {
    pub span_id: SpanId,
    pub trace_id: TraceId,
    pub parent_span_id: Option<SpanId>,
    pub name: String,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
    pub attributes: HashMap<String, AttributeValue>,
    pub events: Vec<SpanEvent>,
    pub links: Vec<SpanLink>,
    pub status: SpanStatus,
}

#[derive(Debug, Clone, Serialize, Deserialize, Hash, PartialEq, Eq)]
pub struct SpanId([u8; 8]);

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResourceAttributes {
    pub service_name: String,
    pub service_namespace: Option<String>,
    pub service_instance_id: Option<String>,
    pub deployment_environment: Option<String>,
    pub attributes: HashMap<String, String>,
}

/// åˆ†åŒºé”®è®¾è®¡
#[derive(Debug, Clone, Serialize, Deserialize, Hash, PartialEq, Eq)]
pub struct PartitionKey {
    /// æœåŠ¡åï¼ˆç”¨äºæœåŠ¡çº§åˆ«åˆ†åŒºï¼‰
    pub service_name: String,
    
    /// æ—¶é—´æ¡¶ï¼ˆå°æ—¶çº§ï¼‰
    pub time_bucket: String, // format: "2025-10-10-14"
    
    /// TraceID çš„å“ˆå¸Œï¼ˆç”¨äºè´Ÿè½½å‡è¡¡ï¼‰
    pub trace_hash: u64,
}

impl PartitionKey {
    pub fn from_trace(trace: &DistributedTrace) -> Self {
        let time_bucket = trace.timestamp.format("%Y-%m-%d-%H").to_string();
        let trace_hash = Self::hash_trace_id(&trace.trace_id);
        
        Self {
            service_name: trace.resource.service_name.clone(),
            time_bucket,
            trace_hash,
        }
    }
    
    fn hash_trace_id(trace_id: &TraceId) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        trace_id.hash(&mut hasher);
        hasher.finish()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AttributeValue {
    String(String),
    Int(i64),
    Double(f64),
    Bool(bool),
    Array(Vec<AttributeValue>),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SpanEvent {
    pub name: String,
    pub timestamp: DateTime<Utc>,
    pub attributes: HashMap<String, AttributeValue>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SpanLink {
    pub trace_id: TraceId,
    pub span_id: SpanId,
    pub attributes: HashMap<String, AttributeValue>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SpanStatus {
    Unset,
    Ok,
    Error { message: String },
}
```

---

### 2. Metric æ•°æ®æ¨¡å‹

```rust
/// Metric æ•°æ®æ¨¡å‹
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DistributedMetric {
    /// Metric åç§°
    pub name: String,
    
    /// Metric ç±»å‹
    pub metric_type: MetricType,
    
    /// æ•°æ®ç‚¹
    pub data_points: Vec<DataPoint>,
    
    /// Resource å±æ€§
    pub resource: ResourceAttributes,
    
    /// æ—¶é—´æˆ³
    pub timestamp: DateTime<Utc>,
    
    /// åˆ†åŒºé”®
    pub partition_key: MetricPartitionKey,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum MetricType {
    Counter,
    Gauge,
    Histogram,
    Summary,
    ExponentialHistogram,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DataPoint {
    pub timestamp: DateTime<Utc>,
    pub value: MetricValue,
    pub attributes: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum MetricValue {
    Int(i64),
    Double(f64),
    Histogram {
        count: u64,
        sum: f64,
        buckets: Vec<HistogramBucket>,
    },
    Summary {
        count: u64,
        sum: f64,
        quantiles: Vec<Quantile>,
    },
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HistogramBucket {
    pub upper_bound: f64,
    pub count: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Quantile {
    pub quantile: f64,
    pub value: f64,
}

/// Metric åˆ†åŒºé”®ï¼ˆåŸºäºåç§°å’Œæ—¶é—´ï¼‰
#[derive(Debug, Clone, Serialize, Deserialize, Hash, PartialEq, Eq)]
pub struct MetricPartitionKey {
    /// Metric åç§°
    pub name: String,
    
    /// æœåŠ¡å
    pub service_name: String,
    
    /// æ—¶é—´æ¡¶ï¼ˆå°æ—¶çº§ï¼‰
    pub time_bucket: String,
    
    /// æ ‡ç­¾å“ˆå¸Œï¼ˆç”¨äºé«˜åŸºæ•°æ ‡ç­¾å¤„ç†ï¼‰
    pub label_hash: u64,
}

impl MetricPartitionKey {
    pub fn from_metric(metric: &DistributedMetric) -> Self {
        let time_bucket = metric.timestamp.format("%Y-%m-%d-%H").to_string();
        
        // è®¡ç®—æ ‡ç­¾å“ˆå¸Œä»¥å¤„ç†é«˜åŸºæ•°åœºæ™¯
        let label_hash = if let Some(first_dp) = metric.data_points.first() {
            Self::hash_labels(&first_dp.attributes)
        } else {
            0
        };
        
        Self {
            name: metric.name.clone(),
            service_name: metric.resource.service_name.clone(),
            time_bucket,
            label_hash,
        }
    }
    
    fn hash_labels(labels: &HashMap<String, String>) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        
        // æŒ‰ key æ’åºä»¥ä¿è¯ä¸€è‡´æ€§
        let mut keys: Vec<_> = labels.keys().collect();
        keys.sort();
        
        for key in keys {
            key.hash(&mut hasher);
            labels[key].hash(&mut hasher);
        }
        
        hasher.finish()
    }
}
```

---

### 3. Log æ•°æ®æ¨¡å‹

```rust
/// Log æ•°æ®æ¨¡å‹
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DistributedLog {
    /// æ—¥å¿—å†…å®¹
    pub body: LogBody,
    
    /// ä¸¥é‡ç¨‹åº¦
    pub severity: SeverityLevel,
    
    /// æ—¶é—´æˆ³
    pub timestamp: DateTime<Utc>,
    
    /// Trace ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
    pub trace_context: Option<TraceContext>,
    
    /// Resource å±æ€§
    pub resource: ResourceAttributes,
    
    /// æ—¥å¿—å±æ€§
    pub attributes: HashMap<String, AttributeValue>,
    
    /// åˆ†åŒºé”®
    pub partition_key: LogPartitionKey,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum LogBody {
    String(String),
    Structured(serde_json::Value),
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, PartialOrd, Ord)]
pub enum SeverityLevel {
    Trace = 1,
    Debug = 5,
    Info = 9,
    Warn = 13,
    Error = 17,
    Fatal = 21,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TraceContext {
    pub trace_id: TraceId,
    pub span_id: SpanId,
}

/// Log åˆ†åŒºé”®ï¼ˆåŸºäºæ—¶é—´å’Œä¸¥é‡ç¨‹åº¦ï¼‰
#[derive(Debug, Clone, Serialize, Deserialize, Hash, PartialEq, Eq)]
pub struct LogPartitionKey {
    /// æœåŠ¡å
    pub service_name: String,
    
    /// æ—¶é—´æ¡¶ï¼ˆå°æ—¶çº§ï¼‰
    pub time_bucket: String,
    
    /// ä¸¥é‡ç¨‹åº¦ï¼ˆç”¨äºå¿«é€Ÿè¿‡æ»¤ï¼‰
    pub severity_level: u8,
    
    /// å†…å®¹å“ˆå¸Œï¼ˆç”¨äºå»é‡ï¼‰
    pub content_hash: u64,
}

impl LogPartitionKey {
    pub fn from_log(log: &DistributedLog) -> Self {
        let time_bucket = log.timestamp.format("%Y-%m-%d-%H").to_string();
        let severity_level = log.severity.clone() as u8;
        let content_hash = Self::hash_content(&log.body);
        
        Self {
            service_name: log.resource.service_name.clone(),
            time_bucket,
            severity_level,
            content_hash,
        }
    }
    
    fn hash_content(body: &LogBody) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        match body {
            LogBody::String(s) => s.hash(&mut hasher),
            LogBody::Structured(v) => v.to_string().hash(&mut hasher),
        }
        hasher.finish()
    }
}
```

---

## ğŸ“Š æ•°æ®åˆ†åŒºç­–ç•¥

### 1. æ—¶é—´èŒƒå›´åˆ†åŒº (Time-Range Partitioning)

**é€‚ç”¨åœºæ™¯**: æ—¶åºæ•°æ®ï¼Œæ”¯æŒæ—¶é—´èŒƒå›´æŸ¥è¯¢

```rust
use std::ops::Range;

/// æ—¶é—´èŒƒå›´åˆ†åŒºå™¨
pub struct TimeRangePartitioner {
    /// åˆ†åŒºç²’åº¦ï¼ˆå°æ—¶ï¼‰
    granularity_hours: u32,
    
    /// åˆ†åŒºä¿ç•™æ—¶é—´ï¼ˆå¤©ï¼‰
    retention_days: u32,
}

impl TimeRangePartitioner {
    pub fn new(granularity_hours: u32, retention_days: u32) -> Self {
        Self {
            granularity_hours,
            retention_days,
        }
    }
    
    /// è®¡ç®—æ•°æ®ç‚¹æ‰€å±çš„åˆ†åŒº
    pub fn get_partition(&self, timestamp: DateTime<Utc>) -> TimePartition {
        let hour_bucket = timestamp.timestamp() / (self.granularity_hours as i64 * 3600);
        
        TimePartition {
            bucket_id: hour_bucket,
            start_time: DateTime::from_timestamp(
                hour_bucket * self.granularity_hours as i64 * 3600,
                0
            ).unwrap(),
            end_time: DateTime::from_timestamp(
                (hour_bucket + 1) * self.granularity_hours as i64 * 3600,
                0
            ).unwrap(),
        }
    }
    
    /// è·å–æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰åˆ†åŒº
    pub fn get_partitions_in_range(
        &self,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    ) -> Vec<TimePartition> {
        let mut partitions = Vec::new();
        let mut current = start;
        
        while current < end {
            partitions.push(self.get_partition(current));
            current += chrono::Duration::hours(self.granularity_hours as i64);
        }
        
        partitions
    }
    
    /// è·å–éœ€è¦æ¸…ç†çš„è¿‡æœŸåˆ†åŒº
    pub fn get_expired_partitions(&self, now: DateTime<Utc>) -> Vec<TimePartition> {
        let retention_timestamp = now - chrono::Duration::days(self.retention_days as i64);
        let mut partitions = Vec::new();
        
        let start = DateTime::from_timestamp(0, 0).unwrap();
        let mut current = start;
        
        while current < retention_timestamp {
            partitions.push(self.get_partition(current));
            current += chrono::Duration::hours(self.granularity_hours as i64);
        }
        
        partitions
    }
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct TimePartition {
    pub bucket_id: i64,
    pub start_time: DateTime<Utc>,
    pub end_time: DateTime<Utc>,
}

impl TimePartition {
    pub fn to_string(&self) -> String {
        format!("time_partition_{}", self.bucket_id)
    }
}
```

---

### 2. å“ˆå¸Œåˆ†åŒº (Hash Partitioning)

**é€‚ç”¨åœºæ™¯**: è´Ÿè½½å‡è¡¡ï¼Œé¿å…çƒ­ç‚¹

```rust
use std::sync::Arc;

/// å“ˆå¸Œåˆ†åŒºå™¨
pub struct HashPartitioner {
    /// åˆ†åŒºæ•°é‡
    num_partitions: usize,
    
    /// è™šæ‹ŸèŠ‚ç‚¹æ•°é‡ï¼ˆç”¨äºä¸€è‡´æ€§å“ˆå¸Œï¼‰
    virtual_nodes: usize,
}

impl HashPartitioner {
    pub fn new(num_partitions: usize, virtual_nodes: usize) -> Self {
        Self {
            num_partitions,
            virtual_nodes,
        }
    }
    
    /// åŸºäº TraceID è®¡ç®—åˆ†åŒº
    pub fn get_partition_for_trace(&self, trace_id: &TraceId) -> usize {
        self.hash(trace_id) % self.num_partitions
    }
    
    /// åŸºäº MetricName + Labels è®¡ç®—åˆ†åŒº
    pub fn get_partition_for_metric(&self, key: &MetricPartitionKey) -> usize {
        self.hash(key) % self.num_partitions
    }
    
    /// åŸºäºæ—¥å¿—å†…å®¹è®¡ç®—åˆ†åŒº
    pub fn get_partition_for_log(&self, key: &LogPartitionKey) -> usize {
        self.hash(key) % self.num_partitions
    }
    
    fn hash<T: std::hash::Hash>(&self, value: &T) -> usize {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::Hasher;
        
        let mut hasher = DefaultHasher::new();
        value.hash(&mut hasher);
        hasher.finish() as usize
    }
}
```

---

### 3. èŒƒå›´åˆ†åŒº (Range Partitioning)

**é€‚ç”¨åœºæ™¯**: é¡ºåºè®¿é—®ï¼ŒèŒƒå›´æŸ¥è¯¢

```rust
use std::cmp::Ordering;

/// èŒƒå›´åˆ†åŒºå™¨
pub struct RangePartitioner<K: Ord> {
    /// åˆ†åŒºè¾¹ç•Œ
    boundaries: Vec<K>,
}

impl<K: Ord + Clone> RangePartitioner<K> {
    pub fn new(boundaries: Vec<K>) -> Self {
        Self { boundaries }
    }
    
    /// æŸ¥æ‰¾æ•°æ®ç‚¹æ‰€å±çš„åˆ†åŒº
    pub fn get_partition(&self, key: &K) -> usize {
        match self.boundaries.binary_search(key) {
            Ok(index) => index,
            Err(index) => index.saturating_sub(1),
        }
    }
    
    /// è·å–èŒƒå›´å†…çš„æ‰€æœ‰åˆ†åŒº
    pub fn get_partitions_in_range(&self, start: &K, end: &K) -> Vec<usize> {
        let start_partition = self.get_partition(start);
        let end_partition = self.get_partition(end);
        
        (start_partition..=end_partition).collect()
    }
}

/// æœåŠ¡åèŒƒå›´åˆ†åŒºç¤ºä¾‹
pub type ServiceNameRangePartitioner = RangePartitioner<String>;

impl ServiceNameRangePartitioner {
    /// åŸºäºå­—æ¯é¡ºåºåˆ›å»ºåˆ†åŒº
    pub fn new_alphabetic(num_partitions: usize) -> Self {
        let mut boundaries = Vec::new();
        let step = 26 / num_partitions;
        
        for i in 0..num_partitions {
            let letter = (b'a' + (i * step) as u8) as char;
            boundaries.push(letter.to_string());
        }
        
        Self::new(boundaries)
    }
}
```

---

### 4. å¤åˆåˆ†åŒºç­–ç•¥

**ç»“åˆæ—¶é—´å’Œå“ˆå¸Œåˆ†åŒº**:

```rust
use std::sync::Arc;

/// å¤åˆåˆ†åŒºå™¨ï¼ˆæ—¶é—´ + å“ˆå¸Œï¼‰
pub struct CompositePartitioner {
    time_partitioner: Arc<TimeRangePartitioner>,
    hash_partitioner: Arc<HashPartitioner>,
}

impl CompositePartitioner {
    pub fn new(
        time_partitioner: Arc<TimeRangePartitioner>,
        hash_partitioner: Arc<HashPartitioner>,
    ) -> Self {
        Self {
            time_partitioner,
            hash_partitioner,
        }
    }
    
    /// ä¸º Trace è®¡ç®—å¤åˆåˆ†åŒº
    pub fn get_partition_for_trace(&self, trace: &DistributedTrace) -> CompositePartition {
        let time_partition = self.time_partitioner.get_partition(trace.timestamp);
        let hash_partition = self.hash_partitioner.get_partition_for_trace(&trace.trace_id);
        
        CompositePartition {
            time_partition,
            hash_partition,
        }
    }
    
    /// ä¸º Metric è®¡ç®—å¤åˆåˆ†åŒº
    pub fn get_partition_for_metric(&self, metric: &DistributedMetric) -> CompositePartition {
        let time_partition = self.time_partitioner.get_partition(metric.timestamp);
        let hash_partition = self.hash_partitioner.get_partition_for_metric(&metric.partition_key);
        
        CompositePartition {
            time_partition,
            hash_partition,
        }
    }
    
    /// è·å–æ—¶é—´èŒƒå›´å’Œå“ˆå¸Œæ¡¶å†…çš„æ‰€æœ‰åˆ†åŒº
    pub fn get_partitions_in_range(
        &self,
        start_time: DateTime<Utc>,
        end_time: DateTime<Utc>,
        hash_buckets: Option<Vec<usize>>,
    ) -> Vec<CompositePartition> {
        let time_partitions = self.time_partitioner.get_partitions_in_range(start_time, end_time);
        
        let hash_buckets = hash_buckets.unwrap_or_else(|| {
            (0..self.hash_partitioner.num_partitions).collect()
        });
        
        let mut composite_partitions = Vec::new();
        
        for time_partition in time_partitions {
            for &hash_partition in &hash_buckets {
                composite_partitions.push(CompositePartition {
                    time_partition: time_partition.clone(),
                    hash_partition,
                });
            }
        }
        
        composite_partitions
    }
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct CompositePartition {
    pub time_partition: TimePartition,
    pub hash_partition: usize,
}

impl CompositePartition {
    pub fn to_string(&self) -> String {
        format!(
            "partition_{}_{:04}",
            self.time_partition.bucket_id,
            self.hash_partition
        )
    }
}
```

---

## ğŸ” æ•°æ®å®šä½ä¸è·¯ç”±

### 1. ä¸€è‡´æ€§å“ˆå¸Œè·¯ç”±

```rust
use std::collections::BTreeMap;
use std::sync::RwLock;

/// ä¸€è‡´æ€§å“ˆå¸Œç¯
pub struct ConsistentHashRing {
    /// è™šæ‹ŸèŠ‚ç‚¹æ˜ å°„ï¼šå“ˆå¸Œå€¼ -> ç‰©ç†èŠ‚ç‚¹
    ring: Arc<RwLock<BTreeMap<u64, String>>>,
    
    /// æ¯ä¸ªç‰©ç†èŠ‚ç‚¹çš„è™šæ‹ŸèŠ‚ç‚¹æ•°
    virtual_nodes_per_node: usize,
}

impl ConsistentHashRing {
    pub fn new(virtual_nodes_per_node: usize) -> Self {
        Self {
            ring: Arc::new(RwLock::new(BTreeMap::new())),
            virtual_nodes_per_node,
        }
    }
    
    /// æ·»åŠ èŠ‚ç‚¹åˆ°å“ˆå¸Œç¯
    pub fn add_node(&self, node_id: String) {
        let mut ring = self.ring.write().unwrap();
        
        for i in 0..self.virtual_nodes_per_node {
            let virtual_node_key = format!("{}:{}", node_id, i);
            let hash = self.hash_string(&virtual_node_key);
            ring.insert(hash, node_id.clone());
        }
    }
    
    /// ä»å“ˆå¸Œç¯ç§»é™¤èŠ‚ç‚¹
    pub fn remove_node(&self, node_id: &str) {
        let mut ring = self.ring.write().unwrap();
        
        for i in 0..self.virtual_nodes_per_node {
            let virtual_node_key = format!("{}:{}", node_id, i);
            let hash = self.hash_string(&virtual_node_key);
            ring.remove(&hash);
        }
    }
    
    /// æŸ¥æ‰¾ key å¯¹åº”çš„èŠ‚ç‚¹
    pub fn get_node(&self, key: &str) -> Option<String> {
        let ring = self.ring.read().unwrap();
        
        if ring.is_empty() {
            return None;
        }
        
        let hash = self.hash_string(key);
        
        // æ‰¾åˆ°ç¬¬ä¸€ä¸ª >= hash çš„èŠ‚ç‚¹
        for (_, node_id) in ring.range(hash..) {
            return Some(node_id.clone());
        }
        
        // å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆç¯å½¢ï¼‰
        ring.values().next().cloned()
    }
    
    /// è·å– N ä¸ªå‰¯æœ¬èŠ‚ç‚¹
    pub fn get_nodes(&self, key: &str, replica_count: usize) -> Vec<String> {
        let ring = self.ring.read().unwrap();
        
        if ring.is_empty() {
            return Vec::new();
        }
        
        let hash = self.hash_string(key);
        let mut nodes = Vec::new();
        let mut seen = std::collections::HashSet::new();
        
        // ä» hash ä½ç½®å¼€å§‹éå†
        for (_, node_id) in ring.range(hash..) {
            if seen.insert(node_id.clone()) {
                nodes.push(node_id.clone());
                if nodes.len() >= replica_count {
                    return nodes;
                }
            }
        }
        
        // ç¯å½¢ï¼Œä»å¤´ç»§ç»­
        for (_, node_id) in ring.iter() {
            if seen.insert(node_id.clone()) {
                nodes.push(node_id.clone());
                if nodes.len() >= replica_count {
                    return nodes;
                }
            }
        }
        
        nodes
    }
    
    fn hash_string(&self, s: &str) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        s.hash(&mut hasher);
        hasher.finish()
    }
}
```

---

### 2. å…ƒæ•°æ®æœåŠ¡

```rust
use tokio::sync::RwLock as TokioRwLock;

/// åˆ†åŒºå…ƒæ•°æ®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PartitionMetadata {
    pub partition_id: String,
    pub nodes: Vec<String>,
    pub leader: String,
    pub state: PartitionState,
    pub size_bytes: u64,
    pub record_count: u64,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
pub enum PartitionState {
    Active,
    Rebalancing,
    Migrating,
    ReadOnly,
}

/// å…ƒæ•°æ®æœåŠ¡
pub struct MetadataService {
    /// åˆ†åŒºå…ƒæ•°æ®ç¼“å­˜
    partitions: Arc<TokioRwLock<HashMap<String, PartitionMetadata>>>,
    
    /// etcd å®¢æˆ·ç«¯ï¼ˆæŒä¹…åŒ–ï¼‰
    etcd_client: Option<etcd_client::Client>,
}

impl MetadataService {
    pub async fn new(etcd_endpoints: Vec<String>) -> Result<Self, Box<dyn std::error::Error>> {
        let etcd_client = if !etcd_endpoints.is_empty() {
            Some(etcd_client::Client::connect(etcd_endpoints, None).await?)
        } else {
            None
        };
        
        Ok(Self {
            partitions: Arc::new(TokioRwLock::new(HashMap::new())),
            etcd_client,
        })
    }
    
    /// æ³¨å†Œæ–°åˆ†åŒº
    pub async fn register_partition(
        &self,
        partition_id: String,
        nodes: Vec<String>,
        leader: String,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let metadata = PartitionMetadata {
            partition_id: partition_id.clone(),
            nodes,
            leader,
            state: PartitionState::Active,
            size_bytes: 0,
            record_count: 0,
            created_at: Utc::now(),
            updated_at: Utc::now(),
        };
        
        // æ›´æ–°æœ¬åœ°ç¼“å­˜
        {
            let mut partitions = self.partitions.write().await;
            partitions.insert(partition_id.clone(), metadata.clone());
        }
        
        // æŒä¹…åŒ–åˆ° etcd
        if let Some(ref mut client) = self.etcd_client.as_ref() {
            let key = format!("/otlp/partitions/{}", partition_id);
            let value = serde_json::to_string(&metadata)?;
            let mut client = client.clone();
            client.put(key, value, None).await?;
        }
        
        Ok(())
    }
    
    /// è·å–åˆ†åŒºå…ƒæ•°æ®
    pub async fn get_partition(&self, partition_id: &str) -> Option<PartitionMetadata> {
        let partitions = self.partitions.read().await;
        partitions.get(partition_id).cloned()
    }
    
    /// è·å–æ‰€æœ‰æ´»è·ƒåˆ†åŒº
    pub async fn get_active_partitions(&self) -> Vec<PartitionMetadata> {
        let partitions = self.partitions.read().await;
        partitions
            .values()
            .filter(|p| p.state == PartitionState::Active)
            .cloned()
            .collect()
    }
    
    /// æ›´æ–°åˆ†åŒºçŠ¶æ€
    pub async fn update_partition_state(
        &self,
        partition_id: &str,
        new_state: PartitionState,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let mut partitions = self.partitions.write().await;
        
        if let Some(metadata) = partitions.get_mut(partition_id) {
            metadata.state = new_state;
            metadata.updated_at = Utc::now();
            
            // æŒä¹…åŒ–
            if let Some(ref mut client) = self.etcd_client.as_ref() {
                let key = format!("/otlp/partitions/{}", partition_id);
                let value = serde_json::to_string(metadata)?;
                let mut client = client.clone();
                client.put(key, value, None).await?;
            }
        }
        
        Ok(())
    }
    
    /// ä» etcd åŠ è½½æ‰€æœ‰åˆ†åŒºå…ƒæ•°æ®
    pub async fn load_from_etcd(&self) -> Result<(), Box<dyn std::error::Error>> {
        if let Some(ref mut client) = self.etcd_client.as_ref() {
            let mut client = client.clone();
            let resp = client.get("/otlp/partitions/", Some(etcd_client::GetOptions::new().with_prefix())).await?;
            
            let mut partitions = self.partitions.write().await;
            
            for kv in resp.kvs() {
                if let Ok(metadata) = serde_json::from_slice::<PartitionMetadata>(kv.value()) {
                    partitions.insert(metadata.partition_id.clone(), metadata);
                }
            }
        }
        
        Ok(())
    }
}
```

---

### 3. åˆ†ç‰‡ç®¡ç†å™¨

```rust
/// åˆ†ç‰‡ç®¡ç†å™¨ï¼ˆæ•´åˆåˆ†åŒºå’Œè·¯ç”±ï¼‰
pub struct ShardManager {
    composite_partitioner: Arc<CompositePartitioner>,
    consistent_hash_ring: Arc<ConsistentHashRing>,
    metadata_service: Arc<MetadataService>,
}

impl ShardManager {
    pub fn new(
        composite_partitioner: Arc<CompositePartitioner>,
        consistent_hash_ring: Arc<ConsistentHashRing>,
        metadata_service: Arc<MetadataService>,
    ) -> Self {
        Self {
            composite_partitioner,
            consistent_hash_ring,
            metadata_service,
        }
    }
    
    /// ä¸º Trace ç¡®å®šç›®æ ‡èŠ‚ç‚¹
    pub async fn get_nodes_for_trace(
        &self,
        trace: &DistributedTrace,
        replica_count: usize,
    ) -> Result<Vec<String>, String> {
        // 1. è®¡ç®—å¤åˆåˆ†åŒº
        let partition = self.composite_partitioner.get_partition_for_trace(trace);
        let partition_key = partition.to_string();
        
        // 2. ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸ŒæŸ¥æ‰¾èŠ‚ç‚¹
        let nodes = self.consistent_hash_ring.get_nodes(&partition_key, replica_count);
        
        if nodes.is_empty() {
            return Err("No available nodes".to_string());
        }
        
        Ok(nodes)
    }
    
    /// ä¸ºæ—¶é—´èŒƒå›´æŸ¥è¯¢ç¡®å®šéœ€è¦è®¿é—®çš„æ‰€æœ‰èŠ‚ç‚¹
    pub async fn get_nodes_for_time_range_query(
        &self,
        start_time: DateTime<Utc>,
        end_time: DateTime<Utc>,
        service_name: Option<String>,
    ) -> Result<Vec<(CompositePartition, Vec<String>)>, String> {
        // 1. è·å–æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰åˆ†åŒº
        let partitions = self.composite_partitioner.get_partitions_in_range(
            start_time,
            end_time,
            None, // æŸ¥è¯¢æ‰€æœ‰å“ˆå¸Œæ¡¶
        );
        
        // 2. ä¸ºæ¯ä¸ªåˆ†åŒºæŸ¥æ‰¾èŠ‚ç‚¹
        let mut result = Vec::new();
        
        for partition in partitions {
            let partition_key = partition.to_string();
            let nodes = self.consistent_hash_ring.get_nodes(&partition_key, 1);
            
            if !nodes.is_empty() {
                result.push((partition, nodes));
            }
        }
        
        Ok(result)
    }
}
```

---

## âš¡ åˆ†åŒºé‡å¹³è¡¡

### 1. è‡ªåŠ¨é‡å¹³è¡¡ç®¡ç†å™¨

```rust
/// é‡å¹³è¡¡ç®¡ç†å™¨
pub struct RebalanceManager {
    shard_manager: Arc<ShardManager>,
    metadata_service: Arc<MetadataService>,
    config: RebalanceConfig,
}

#[derive(Debug, Clone)]
pub struct RebalanceConfig {
    /// è´Ÿè½½å·®å¼‚é˜ˆå€¼ï¼ˆè§¦å‘é‡å¹³è¡¡ï¼‰
    pub imbalance_threshold: f64,
    
    /// æ£€æŸ¥é—´éš”
    pub check_interval: Duration,
    
    /// æ¯æ¬¡è¿ç§»çš„æœ€å¤§åˆ†åŒºæ•°
    pub max_concurrent_migrations: usize,
}

impl Default for RebalanceConfig {
    fn default() -> Self {
        Self {
            imbalance_threshold: 0.2, // 20%
            check_interval: Duration::from_secs(300), // 5åˆ†é’Ÿ
            max_concurrent_migrations: 3,
        }
    }
}

impl RebalanceManager {
    pub fn new(
        shard_manager: Arc<ShardManager>,
        metadata_service: Arc<MetadataService>,
        config: RebalanceConfig,
    ) -> Self {
        Self {
            shard_manager,
            metadata_service,
            config,
        }
    }
    
    /// å¯åŠ¨è‡ªåŠ¨é‡å¹³è¡¡
    pub async fn start_auto_rebalance(&self) {
        let mut interval = tokio::time::interval(self.config.check_interval);
        
        loop {
            interval.tick().await;
            
            if let Err(e) = self.check_and_rebalance().await {
                tracing::error!("Rebalance failed: {}", e);
            }
        }
    }
    
    /// æ£€æŸ¥å¹¶æ‰§è¡Œé‡å¹³è¡¡
    async fn check_and_rebalance(&self) -> Result<(), Box<dyn std::error::Error>> {
        // 1. è·å–æ‰€æœ‰åˆ†åŒºçš„è´Ÿè½½ä¿¡æ¯
        let partitions = self.metadata_service.get_active_partitions().await;
        
        if partitions.is_empty() {
            return Ok(());
        }
        
        // 2. æŒ‰èŠ‚ç‚¹èšåˆè´Ÿè½½
        let mut node_loads: HashMap<String, u64> = HashMap::new();
        
        for partition in &partitions {
            *node_loads.entry(partition.leader.clone()).or_insert(0) += partition.size_bytes;
        }
        
        // 3. è®¡ç®—å¹³å‡è´Ÿè½½å’Œåå·®
        let total_load: u64 = node_loads.values().sum();
        let avg_load = total_load as f64 / node_loads.len() as f64;
        
        // 4. æŸ¥æ‰¾è¿‡è½½å’Œç©ºé—²èŠ‚ç‚¹
        let mut overloaded: Vec<_> = node_loads
            .iter()
            .filter(|(_, &load)| load as f64 > avg_load * (1.0 + self.config.imbalance_threshold))
            .collect();
        
        let mut underloaded: Vec<_> = node_loads
            .iter()
            .filter(|(_, &load)| load as f64 < avg_load * (1.0 - self.config.imbalance_threshold))
            .collect();
        
        if overloaded.is_empty() || underloaded.is_empty() {
            tracing::info!("Cluster is balanced");
            return Ok(());
        }
        
        // 5. ç”Ÿæˆè¿ç§»è®¡åˆ’
        overloaded.sort_by_key(|(_, load)| std::cmp::Reverse(*load));
        underloaded.sort_by_key(|(_, load)| *load);
        
        let migrations = self.plan_migrations(&overloaded, &underloaded, &partitions);
        
        // 6. æ‰§è¡Œè¿ç§»
        self.execute_migrations(migrations).await?;
        
        Ok(())
    }
    
    fn plan_migrations(
        &self,
        overloaded: &[(&String, &u64)],
        underloaded: &[(&String, &u64)],
        partitions: &[PartitionMetadata],
    ) -> Vec<Migration> {
        let mut migrations = Vec::new();
        let mut overloaded_iter = overloaded.iter();
        let mut underloaded_iter = underloaded.iter();
        
        'outer: for _ in 0..self.config.max_concurrent_migrations {
            if let (Some((from_node, _)), Some((to_node, _))) =
                (overloaded_iter.next(), underloaded_iter.next())
            {
                // æ‰¾åˆ°è¯¥èŠ‚ç‚¹ä¸Šçš„ä¸€ä¸ªåˆ†åŒº
                if let Some(partition) = partitions.iter().find(|p| &p.leader == *from_node) {
                    migrations.push(Migration {
                        partition_id: partition.partition_id.clone(),
                        from_node: (*from_node).clone(),
                        to_node: (*to_node).clone(),
                    });
                }
            } else {
                break 'outer;
            }
        }
        
        migrations
    }
    
    async fn execute_migrations(
        &self,
        migrations: Vec<Migration>,
    ) -> Result<(), Box<dyn std::error::Error>> {
        for migration in migrations {
            tracing::info!(
                "Migrating partition {} from {} to {}",
                migration.partition_id,
                migration.from_node,
                migration.to_node
            );
            
            // 1. æ ‡è®°åˆ†åŒºä¸º Migrating çŠ¶æ€
            self.metadata_service
                .update_partition_state(&migration.partition_id, PartitionState::Migrating)
                .await?;
            
            // 2. æ‰§è¡Œæ•°æ®è¿ç§»ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            // TODO: å®é™…å®ç°éœ€è¦æµå¼ä¼ è¾“æ•°æ®
            
            // 3. æ›´æ–°å…ƒæ•°æ®
            if let Some(mut metadata) = self.metadata_service.get_partition(&migration.partition_id).await {
                metadata.leader = migration.to_node.clone();
                metadata.state = PartitionState::Active;
                metadata.updated_at = Utc::now();
                
                // é‡æ–°æ³¨å†Œ
                self.metadata_service
                    .register_partition(
                        metadata.partition_id.clone(),
                        metadata.nodes.clone(),
                        metadata.leader.clone(),
                    )
                    .await?;
            }
            
            tracing::info!("Migration completed for partition {}", migration.partition_id);
        }
        
        Ok(())
    }
}

#[derive(Debug, Clone)]
struct Migration {
    partition_id: String,
    from_node: String,
    to_node: String,
}
```

---

## ğŸ’¾ æ•°æ®æœ¬åœ°åŒ–ä¼˜åŒ–

### 1. æ•°æ®å±€éƒ¨æ€§æ„ŸçŸ¥è·¯ç”±

```rust
/// æ•°æ®å±€éƒ¨æ€§æ„ŸçŸ¥çš„è·¯ç”±å™¨
pub struct LocalityAwareRouter {
    shard_manager: Arc<ShardManager>,
    
    /// å½“å‰èŠ‚ç‚¹ ID
    local_node_id: String,
    
    /// èŠ‚ç‚¹ä½ç½®ä¿¡æ¯ï¼ˆæ•°æ®ä¸­å¿ƒã€å¯ç”¨åŒºï¼‰
    node_locations: Arc<RwLock<HashMap<String, NodeLocation>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NodeLocation {
    pub data_center: String,
    pub availability_zone: String,
    pub region: String,
}

impl LocalityAwareRouter {
    pub fn new(
        shard_manager: Arc<ShardManager>,
        local_node_id: String,
    ) -> Self {
        Self {
            shard_manager,
            local_node_id,
            node_locations: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// æ³¨å†ŒèŠ‚ç‚¹ä½ç½®
    pub fn register_node_location(&self, node_id: String, location: NodeLocation) {
        let mut locations = self.node_locations.write().unwrap();
        locations.insert(node_id, location);
    }
    
    /// é€‰æ‹©æœ€è¿‘çš„èŠ‚ç‚¹
    pub async fn select_nearest_node(
        &self,
        candidate_nodes: Vec<String>,
    ) -> Option<String> {
        let locations = self.node_locations.read().unwrap();
        
        let local_location = locations.get(&self.local_node_id)?;
        
        // ä¼˜å…ˆé€‰æ‹©åŒä¸€æ•°æ®ä¸­å¿ƒçš„èŠ‚ç‚¹
        for node in &candidate_nodes {
            if let Some(location) = locations.get(node) {
                if location.data_center == local_location.data_center {
                    return Some(node.clone());
                }
            }
        }
        
        // å…¶æ¬¡é€‰æ‹©åŒä¸€å¯ç”¨åŒºçš„èŠ‚ç‚¹
        for node in &candidate_nodes {
            if let Some(location) = locations.get(node) {
                if location.availability_zone == local_location.availability_zone {
                    return Some(node.clone());
                }
            }
        }
        
        // æœ€åé€‰æ‹©åŒä¸€åŒºåŸŸçš„èŠ‚ç‚¹
        for node in &candidate_nodes {
            if let Some(location) = locations.get(node) {
                if location.region == local_location.region {
                    return Some(node.clone());
                }
            }
        }
        
        // éšæœºé€‰æ‹©
        candidate_nodes.into_iter().next()
    }
    
    /// ä¸ºæŸ¥è¯¢é€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹é›†åˆ
    pub async fn select_optimal_nodes_for_query(
        &self,
        required_partitions: Vec<(CompositePartition, Vec<String>)>,
    ) -> HashMap<CompositePartition, String> {
        let mut result = HashMap::new();
        
        for (partition, candidate_nodes) in required_partitions {
            if let Some(node) = self.select_nearest_node(candidate_nodes).await {
                result.insert(partition, node);
            }
        }
        
        result
    }
}
```

---

## ğŸ“ˆ å®Œæ•´ç¤ºä¾‹ï¼šåˆ†å¸ƒå¼ OTLP å­˜å‚¨ç³»ç»Ÿ

```rust
use tokio;

/// åˆ†å¸ƒå¼ OTLP å­˜å‚¨ç³»ç»Ÿ
pub struct DistributedOtlpStorage {
    shard_manager: Arc<ShardManager>,
    locality_router: Arc<LocalityAwareRouter>,
    rebalance_manager: Arc<RebalanceManager>,
}

impl DistributedOtlpStorage {
    pub async fn new(
        nodes: Vec<String>,
        local_node_id: String,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        // 1. åˆ›å»ºæ—¶é—´åˆ†åŒºå™¨ï¼ˆæŒ‰å°æ—¶ï¼‰
        let time_partitioner = Arc::new(TimeRangePartitioner::new(
            1,  // 1å°æ—¶ä¸€ä¸ªåˆ†åŒº
            30, // ä¿ç•™30å¤©
        ));
        
        // 2. åˆ›å»ºå“ˆå¸Œåˆ†åŒºå™¨ï¼ˆ256ä¸ªå“ˆå¸Œæ¡¶ï¼‰
        let hash_partitioner = Arc::new(HashPartitioner::new(256, 100));
        
        // 3. åˆ›å»ºå¤åˆåˆ†åŒºå™¨
        let composite_partitioner = Arc::new(CompositePartitioner::new(
            time_partitioner,
            hash_partitioner,
        ));
        
        // 4. åˆ›å»ºä¸€è‡´æ€§å“ˆå¸Œç¯
        let consistent_hash_ring = Arc::new(ConsistentHashRing::new(150));
        for node in &nodes {
            consistent_hash_ring.add_node(node.clone());
        }
        
        // 5. åˆ›å»ºå…ƒæ•°æ®æœåŠ¡
        let metadata_service = Arc::new(
            MetadataService::new(vec!["127.0.0.1:2379".to_string()]).await?
        );
        metadata_service.load_from_etcd().await?;
        
        // 6. åˆ›å»ºåˆ†ç‰‡ç®¡ç†å™¨
        let shard_manager = Arc::new(ShardManager::new(
            composite_partitioner,
            consistent_hash_ring,
            metadata_service.clone(),
        ));
        
        // 7. åˆ›å»ºå±€éƒ¨æ€§æ„ŸçŸ¥è·¯ç”±å™¨
        let locality_router = Arc::new(LocalityAwareRouter::new(
            shard_manager.clone(),
            local_node_id,
        ));
        
        // 8. åˆ›å»ºé‡å¹³è¡¡ç®¡ç†å™¨
        let rebalance_manager = Arc::new(RebalanceManager::new(
            shard_manager.clone(),
            metadata_service,
            RebalanceConfig::default(),
        ));
        
        Ok(Self {
            shard_manager,
            locality_router,
            rebalance_manager,
        })
    }
    
    /// å¯åŠ¨åå°ä»»åŠ¡
    pub async fn start(&self) {
        let rebalance_manager = self.rebalance_manager.clone();
        
        tokio::spawn(async move {
            rebalance_manager.start_auto_rebalance().await;
        });
    }
    
    /// å†™å…¥ Trace
    pub async fn write_trace(&self, trace: DistributedTrace) -> Result<(), String> {
        // 1. ç¡®å®šç›®æ ‡èŠ‚ç‚¹
        let nodes = self.shard_manager.get_nodes_for_trace(&trace, 3).await?;
        
        tracing::info!("Writing trace {} to nodes: {:?}", 
            format!("{:?}", trace.trace_id), nodes);
        
        // 2. é€‰æ‹©æœ€è¿‘çš„èŠ‚ç‚¹å†™å…¥
        if let Some(primary_node) = self.locality_router.select_nearest_node(nodes.clone()).await {
            // TODO: å®é™…å‘é€æ•°æ®åˆ°èŠ‚ç‚¹
            tracing::info!("Primary write to node: {}", primary_node);
        }
        
        // 3. å¼‚æ­¥å¤åˆ¶åˆ°å‰¯æœ¬èŠ‚ç‚¹
        for node in nodes.iter().skip(1) {
            let node = node.clone();
            tokio::spawn(async move {
                // TODO: å¼‚æ­¥å¤åˆ¶
                tracing::info!("Async replication to node: {}", node);
            });
        }
        
        Ok(())
    }
    
    /// æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„ Traces
    pub async fn query_traces(
        &self,
        start_time: DateTime<Utc>,
        end_time: DateTime<Utc>,
        service_name: Option<String>,
    ) -> Result<Vec<DistributedTrace>, String> {
        // 1. ç¡®å®šéœ€è¦æŸ¥è¯¢çš„åˆ†åŒºå’ŒèŠ‚ç‚¹
        let partitions = self.shard_manager
            .get_nodes_for_time_range_query(start_time, end_time, service_name)
            .await?;
        
        // 2. ä¸ºæ¯ä¸ªåˆ†åŒºé€‰æ‹©æœ€ä¼˜èŠ‚ç‚¹
        let optimal_nodes = self.locality_router
            .select_optimal_nodes_for_query(partitions)
            .await;
        
        // 3. å¹¶å‘æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹
        let mut handles = Vec::new();
        
        for (partition, node) in optimal_nodes {
            let handle = tokio::spawn(async move {
                // TODO: å®é™…æŸ¥è¯¢èŠ‚ç‚¹
                tracing::info!("Querying partition {} from node {}", 
                    partition.to_string(), node);
                
                Vec::<DistributedTrace>::new()
            });
            
            handles.push(handle);
        }
        
        // 4. æ”¶é›†ç»“æœ
        let mut results = Vec::new();
        
        for handle in handles {
            if let Ok(traces) = handle.await {
                results.extend(traces);
            }
        }
        
        Ok(results)
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt::init();
    
    // åˆå§‹åŒ–é›†ç¾¤èŠ‚ç‚¹
    let nodes = vec![
        "node-1".to_string(),
        "node-2".to_string(),
        "node-3".to_string(),
        "node-4".to_string(),
    ];
    
    // åˆ›å»ºå­˜å‚¨ç³»ç»Ÿ
    let storage = DistributedOtlpStorage::new(nodes, "node-1".to_string()).await?;
    
    // å¯åŠ¨åå°ä»»åŠ¡
    storage.start().await;
    
    // å†™å…¥ç¤ºä¾‹ Trace
    let trace = DistributedTrace {
        trace_id: TraceId([1; 16]),
        spans: vec![],
        resource: ResourceAttributes {
            service_name: "example-service".to_string(),
            service_namespace: Some("production".to_string()),
            service_instance_id: Some("instance-1".to_string()),
            deployment_environment: Some("prod".to_string()),
            attributes: HashMap::new(),
        },
        timestamp: Utc::now(),
        partition_key: PartitionKey {
            service_name: "example-service".to_string(),
            time_bucket: Utc::now().format("%Y-%m-%d-%H").to_string(),
            trace_hash: 12345,
        },
    };
    
    storage.write_trace(trace).await?;
    
    // æŸ¥è¯¢ç¤ºä¾‹
    let start = Utc::now() - chrono::Duration::hours(1);
    let end = Utc::now();
    
    let traces = storage.query_traces(start, end, Some("example-service".to_string())).await?;
    
    println!("Found {} traces", traces.len());
    
    Ok(())
}
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### åˆ†åŒºå¤§å°

- **Trace åˆ†åŒº**: æ¯ä¸ªåˆ†åŒº 1-5 GB
- **Metric åˆ†åŒº**: æ¯ä¸ªåˆ†åŒº 100-500 MB
- **Log åˆ†åŒº**: æ¯ä¸ªåˆ†åŒº 500 MB - 2 GB

### åˆ†åŒºé”®é€‰æ‹©

1. **é«˜åŸºæ•°å­—æ®µ**: TraceID, SpanID
2. **æ—¶é—´ç»´åº¦**: å¿…é¡»åŒ…å«æ—¶é—´å­—æ®µ
3. **æŸ¥è¯¢æ¨¡å¼**: åŸºäºå¸¸è§æŸ¥è¯¢é€‰æ‹©

### å‰¯æœ¬ç­–ç•¥

- **ç”Ÿäº§ç¯å¢ƒ**: 3 å‰¯æœ¬
- **å¼€å‘ç¯å¢ƒ**: 1 å‰¯æœ¬
- **å…³é”®æ•°æ®**: 5 å‰¯æœ¬

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

- **å†™å…¥ååé‡**: > 100K traces/s
- **æŸ¥è¯¢å»¶è¿Ÿ**: < 100ms (P99)
- **åˆ†åŒºé‡å¹³è¡¡**: < 10% æ€§èƒ½å½±å“
- **æ•°æ®å€¾æ–œç‡**: < 10%

---

## ğŸ”§ æ•…éšœå¤„ç†

- **èŠ‚ç‚¹æ•…éšœ**: è‡ªåŠ¨æ•…éšœè½¬ç§»åˆ°å‰¯æœ¬
- **ç½‘ç»œåˆ†åŒº**: æœ€ç»ˆä¸€è‡´æ€§ä¿è¯
- **æ•°æ®æŸå**: CRC æ ¡éªŒå’Œä¿®å¤
- **åˆ†åŒºè¿‡å¤§**: è‡ªåŠ¨åˆ†è£‚

---

## ğŸ“š å‚è€ƒèµ„æº

- [Consistent Hashing](https://en.wikipedia.org/wiki/Consistent_hashing)
- [Time-Series Data Partitioning](https://docs.timescale.com/)
- [Distributed Systems Patterns](https://martinfowler.com/articles/patterns-of-distributed-systems/)
- [Apache Cassandra Data Model](https://cassandra.apache.org/doc/latest/data_modeling/)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-10  
**ä½œè€…**: OTLP Rust é¡¹ç›®ç»„
