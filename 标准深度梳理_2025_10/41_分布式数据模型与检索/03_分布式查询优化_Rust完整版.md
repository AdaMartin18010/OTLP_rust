# åˆ†å¸ƒå¼æŸ¥è¯¢ä¼˜åŒ– - Rust å®Œæ•´å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **Rust ç‰ˆæœ¬**: 1.90+  
> **æœ€åæ›´æ–°**: 2025-10-10

---

## ğŸ“‹ ç›®å½•

- [åˆ†å¸ƒå¼æŸ¥è¯¢ä¼˜åŒ– - Rust å®Œæ•´å®ç°](#åˆ†å¸ƒå¼æŸ¥è¯¢ä¼˜åŒ–---rust-å®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
    - [ä¼˜åŒ–ç›®æ ‡](#ä¼˜åŒ–ç›®æ ‡)
    - [æ ¸å¿ƒæŠ€æœ¯](#æ ¸å¿ƒæŠ€æœ¯)
  - [ğŸ” æŸ¥è¯¢è§„åˆ’å™¨](#-æŸ¥è¯¢è§„åˆ’å™¨)
    - [1. æŸ¥è¯¢è§£æä¸ä¼˜åŒ–](#1-æŸ¥è¯¢è§£æä¸ä¼˜åŒ–)
    - [2. æŸ¥è¯¢è®¡åˆ’ç”Ÿæˆå™¨](#2-æŸ¥è¯¢è®¡åˆ’ç”Ÿæˆå™¨)
  - [âš¡ æŸ¥è¯¢æ‰§è¡Œå¼•æ“](#-æŸ¥è¯¢æ‰§è¡Œå¼•æ“)
    - [1. å¹¶è¡ŒæŸ¥è¯¢æ‰§è¡Œå™¨](#1-å¹¶è¡ŒæŸ¥è¯¢æ‰§è¡Œå™¨)
    - [2. æµå¼æŸ¥è¯¢å¤„ç†](#2-æµå¼æŸ¥è¯¢å¤„ç†)
  - [ğŸ’¾ æŸ¥è¯¢ç¼“å­˜](#-æŸ¥è¯¢ç¼“å­˜)
    - [1. å¤šçº§ç¼“å­˜ç³»ç»Ÿ](#1-å¤šçº§ç¼“å­˜ç³»ç»Ÿ)
  - [ğŸ“Š æŸ¥è¯¢ä¸‹æ¨ä¼˜åŒ–](#-æŸ¥è¯¢ä¸‹æ¨ä¼˜åŒ–)
    - [1. è°“è¯ä¸‹æ¨](#1-è°“è¯ä¸‹æ¨)
    - [2. æŠ•å½±ä¸‹æ¨](#2-æŠ•å½±ä¸‹æ¨)
  - [ğŸ”§ è‡ªé€‚åº”æŸ¥è¯¢ä¼˜åŒ–](#-è‡ªé€‚åº”æŸ¥è¯¢ä¼˜åŒ–)
    - [1. ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å™¨](#1-ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å™¨)
    - [2. ä»£ä»·è¯„ä¼°å™¨](#2-ä»£ä»·è¯„ä¼°å™¨)
  - [ğŸŒŠ æŸ¥è¯¢ç»“æœåˆå¹¶](#-æŸ¥è¯¢ç»“æœåˆå¹¶)
    - [1. æµå¼å½’å¹¶æ’åº](#1-æµå¼å½’å¹¶æ’åº)
  - [ğŸ“ˆ å®Œæ•´ç¤ºä¾‹ï¼šåˆ†å¸ƒå¼æŸ¥è¯¢ç³»ç»Ÿ](#-å®Œæ•´ç¤ºä¾‹åˆ†å¸ƒå¼æŸ¥è¯¢ç³»ç»Ÿ)
  - [ğŸ¯ æŸ¥è¯¢ä¼˜åŒ–æ¨¡å¼](#-æŸ¥è¯¢ä¼˜åŒ–æ¨¡å¼)
    - [åˆ†åŒºå‰ªæ](#åˆ†åŒºå‰ªæ)
    - [ç´¢å¼•é€‰æ‹©](#ç´¢å¼•é€‰æ‹©)
    - [å¹¶è¡Œåº¦è°ƒæ•´](#å¹¶è¡Œåº¦è°ƒæ•´)
  - [ğŸ“Š æ€§èƒ½æŒ‡æ ‡](#-æ€§èƒ½æŒ‡æ ‡)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## ğŸ¯ æ¦‚è¿°

åˆ†å¸ƒå¼æŸ¥è¯¢ä¼˜åŒ–æ˜¯æå‡ OTLP ç³»ç»Ÿæ€§èƒ½çš„å…³é”®ã€‚æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä½¿ç”¨ Rust å®ç°é«˜æ•ˆçš„æŸ¥è¯¢è§„åˆ’ã€æ‰§è¡Œå’Œä¼˜åŒ–æœºåˆ¶ã€‚

### ä¼˜åŒ–ç›®æ ‡

- âœ… **æœ€å°åŒ–æ•°æ®ä¼ è¾“**: æŸ¥è¯¢ä¸‹æ¨ï¼Œå‡å°‘ç½‘ç»œå¼€é”€
- âœ… **æœ€å¤§åŒ–å¹¶è¡Œåº¦**: å……åˆ†åˆ©ç”¨é›†ç¾¤èµ„æº
- âœ… **æ™ºèƒ½ç¼“å­˜**: é¿å…é‡å¤è®¡ç®—
- âœ… **è‡ªé€‚åº”ä¼˜åŒ–**: åŸºäºç»Ÿè®¡ä¿¡æ¯åŠ¨æ€è°ƒæ•´

### æ ¸å¿ƒæŠ€æœ¯

- ğŸ” **æŸ¥è¯¢è§„åˆ’**: CBO (Cost-Based Optimization)
- âš¡ **å¹¶è¡Œæ‰§è¡Œ**: æµæ°´çº¿å¹¶è¡Œ + æ•°æ®å¹¶è¡Œ
- ğŸ’¾ **ç»“æœç¼“å­˜**: å¤šçº§ç¼“å­˜ç­–ç•¥
- ğŸ“Š **ç»Ÿè®¡é©±åŠ¨**: åŸºäºæ•°æ®åˆ†å¸ƒçš„ä¼˜åŒ–

---

## ğŸ” æŸ¥è¯¢è§„åˆ’å™¨

### 1. æŸ¥è¯¢è§£æä¸ä¼˜åŒ–

```rust
use serde::{Serialize, Deserialize};
use std::collections::HashMap;
use chrono::{DateTime, Utc};

/// æŸ¥è¯¢è¡¨è¾¾å¼
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum QueryExpr {
    /// å±æ€§è¿‡æ»¤
    AttributeFilter {
        key: String,
        operator: ComparisonOperator,
        value: String,
    },
    
    /// æ—¶é—´èŒƒå›´
    TimeRange {
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    },
    
    /// æœåŠ¡åè¿‡æ»¤
    ServiceFilter {
        service_name: String,
    },
    
    /// é€»è¾‘ä¸
    And(Vec<QueryExpr>),
    
    /// é€»è¾‘æˆ–
    Or(Vec<QueryExpr>),
    
    /// é€»è¾‘é
    Not(Box<QueryExpr>),
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub enum ComparisonOperator {
    Equal,
    NotEqual,
    GreaterThan,
    LessThan,
    GreaterThanOrEqual,
    LessThanOrEqual,
    Contains,
    StartsWith,
    EndsWith,
}

/// æŸ¥è¯¢ä¼˜åŒ–å™¨
pub struct QueryOptimizer {
    /// ç»Ÿè®¡ä¿¡æ¯
    statistics: std::sync::Arc<tokio::sync::RwLock<QueryStatistics>>,
}

#[derive(Debug, Clone)]
pub struct QueryStatistics {
    /// å±æ€§åŸºæ•°ç»Ÿè®¡
    attribute_cardinality: HashMap<String, usize>,
    
    /// æœåŠ¡ååˆ†å¸ƒ
    service_distribution: HashMap<String, usize>,
    
    /// æ—¶é—´åˆ†åŒºç»Ÿè®¡
    time_partition_stats: HashMap<String, PartitionStats>,
}

#[derive(Debug, Clone)]
pub struct PartitionStats {
    pub record_count: usize,
    pub size_bytes: u64,
    pub min_timestamp: DateTime<Utc>,
    pub max_timestamp: DateTime<Utc>,
}

impl QueryOptimizer {
    pub fn new(statistics: std::sync::Arc<tokio::sync::RwLock<QueryStatistics>>) -> Self {
        Self { statistics }
    }
    
    /// ä¼˜åŒ–æŸ¥è¯¢è¡¨è¾¾å¼
    pub async fn optimize(&self, expr: QueryExpr) -> QueryExpr {
        // 1. å¸¸é‡æŠ˜å 
        let expr = self.constant_folding(expr);
        
        // 2. è°“è¯ä¸‹æ¨
        let expr = self.push_down_predicates(expr);
        
        // 3. é‡æ’åºï¼ˆé«˜é€‰æ‹©æ€§çš„æ¡ä»¶ä¼˜å…ˆï¼‰
        let expr = self.reorder_predicates(expr).await;
        
        expr
    }
    
    /// å¸¸é‡æŠ˜å 
    fn constant_folding(&self, expr: QueryExpr) -> QueryExpr {
        match expr {
            QueryExpr::And(exprs) => {
                let optimized: Vec<_> = exprs
                    .into_iter()
                    .map(|e| self.constant_folding(e))
                    .collect();
                
                // ç§»é™¤æ’çœŸæ¡ä»¶
                let filtered: Vec<_> = optimized
                    .into_iter()
                    .filter(|e| !self.is_always_true(e))
                    .collect();
                
                if filtered.is_empty() {
                    // å…¨éƒ¨æ˜¯æ’çœŸï¼Œè¿”å›ä»»æ„ä¸€ä¸ªæ’çœŸæ¡ä»¶
                    return QueryExpr::And(vec![]);
                }
                
                if filtered.len() == 1 {
                    return filtered.into_iter().next().unwrap();
                }
                
                QueryExpr::And(filtered)
            }
            
            QueryExpr::Or(exprs) => {
                let optimized: Vec<_> = exprs
                    .into_iter()
                    .map(|e| self.constant_folding(e))
                    .collect();
                
                QueryExpr::Or(optimized)
            }
            
            QueryExpr::Not(expr) => {
                QueryExpr::Not(Box::new(self.constant_folding(*expr)))
            }
            
            _ => expr,
        }
    }
    
    fn is_always_true(&self, _expr: &QueryExpr) -> bool {
        // ç®€åŒ–å®ç°
        false
    }
    
    /// è°“è¯ä¸‹æ¨
    fn push_down_predicates(&self, expr: QueryExpr) -> QueryExpr {
        // ç®€åŒ–å®ç°ï¼šå°† AND æ¡ä»¶æ‹†åˆ†ä»¥ä¾¿ä¸‹æ¨
        match expr {
            QueryExpr::And(exprs) => {
                let mut time_filters = Vec::new();
                let mut service_filters = Vec::new();
                let mut attr_filters = Vec::new();
                let mut others = Vec::new();
                
                for e in exprs {
                    match e {
                        QueryExpr::TimeRange { .. } => time_filters.push(e),
                        QueryExpr::ServiceFilter { .. } => service_filters.push(e),
                        QueryExpr::AttributeFilter { .. } => attr_filters.push(e),
                        _ => others.push(e),
                    }
                }
                
                // é‡æ–°ç»„åˆï¼šæ—¶é—´è¿‡æ»¤ -> æœåŠ¡è¿‡æ»¤ -> å±æ€§è¿‡æ»¤ -> å…¶ä»–
                let mut result = Vec::new();
                result.extend(time_filters);
                result.extend(service_filters);
                result.extend(attr_filters);
                result.extend(others);
                
                QueryExpr::And(result)
            }
            
            _ => expr,
        }
    }
    
    /// åŸºäºé€‰æ‹©æ€§é‡æ’åºè°“è¯
    async fn reorder_predicates(&self, expr: QueryExpr) -> QueryExpr {
        match expr {
            QueryExpr::And(mut exprs) => {
                let statistics = self.statistics.read().await;
                
                // è®¡ç®—æ¯ä¸ªè°“è¯çš„é€‰æ‹©æ€§
                exprs.sort_by(|a, b| {
                    let selectivity_a = self.estimate_selectivity(a, &statistics);
                    let selectivity_b = self.estimate_selectivity(b, &statistics);
                    
                    // é€‰æ‹©æ€§ä½çš„ï¼ˆè¿‡æ»¤æ•ˆæœå¥½çš„ï¼‰æ’å‰é¢
                    selectivity_a.partial_cmp(&selectivity_b).unwrap()
                });
                
                QueryExpr::And(exprs)
            }
            
            _ => expr,
        }
    }
    
    /// ä¼°è®¡é€‰æ‹©æ€§ï¼ˆè¿”å›å€¼è¶Šå°ï¼Œè¿‡æ»¤æ•ˆæœè¶Šå¥½ï¼‰
    fn estimate_selectivity(&self, expr: &QueryExpr, stats: &QueryStatistics) -> f64 {
        match expr {
            QueryExpr::AttributeFilter { key, operator, value } => {
                // åŸºäºå±æ€§åŸºæ•°ä¼°è®¡
                let cardinality = stats.attribute_cardinality.get(key).copied().unwrap_or(1000);
                
                match operator {
                    ComparisonOperator::Equal => 1.0 / cardinality as f64,
                    ComparisonOperator::NotEqual => 1.0 - (1.0 / cardinality as f64),
                    _ => 0.5, // å…¶ä»–æ“ä½œé»˜è®¤ 50% é€‰æ‹©æ€§
                }
            }
            
            QueryExpr::ServiceFilter { service_name } => {
                let total: usize = stats.service_distribution.values().sum();
                let count = stats.service_distribution.get(service_name).copied().unwrap_or(0);
                
                if total > 0 {
                    count as f64 / total as f64
                } else {
                    0.5
                }
            }
            
            QueryExpr::TimeRange { .. } => {
                // æ—¶é—´èŒƒå›´é€šå¸¸é€‰æ‹©æ€§è¾ƒå¥½
                0.1
            }
            
            _ => 0.5,
        }
    }
}
```

---

### 2. æŸ¥è¯¢è®¡åˆ’ç”Ÿæˆå™¨

```rust
use std::sync::Arc;

/// æŸ¥è¯¢è®¡åˆ’
#[derive(Debug, Clone)]
pub struct QueryPlan {
    /// æŸ¥è¯¢é˜¶æ®µ
    pub stages: Vec<QueryStage>,
    
    /// é¢„ä¼°ä»£ä»·
    pub estimated_cost: f64,
    
    /// å¹¶è¡Œåº¦
    pub parallelism: usize,
}

#[derive(Debug, Clone)]
pub struct QueryStage {
    /// é˜¶æ®µç±»å‹
    pub stage_type: StageType,
    
    /// è¾“å…¥é˜¶æ®µ
    pub inputs: Vec<usize>,
    
    /// æ‰§è¡ŒèŠ‚ç‚¹
    pub execution_nodes: Vec<String>,
    
    /// é¢„ä¼°è¾“å‡ºè¡Œæ•°
    pub estimated_rows: usize,
}

#[derive(Debug, Clone)]
pub enum StageType {
    /// æ‰«æåˆ†åŒº
    PartitionScan {
        partitions: Vec<String>,
        filter: Option<QueryExpr>,
    },
    
    /// ç´¢å¼•æŸ¥æ‰¾
    IndexLookup {
        index_type: IndexType,
        key: Vec<u8>,
    },
    
    /// è¿‡æ»¤
    Filter {
        predicate: QueryExpr,
    },
    
    /// èšåˆ
    Aggregate {
        group_by: Vec<String>,
        aggregations: Vec<AggregationFunc>,
    },
    
    /// æ’åº
    Sort {
        order_by: Vec<(String, SortOrder)>,
    },
    
    /// é™åˆ¶
    Limit {
        limit: usize,
        offset: usize,
    },
    
    /// åˆå¹¶
    Merge {
        merge_type: MergeType,
    },
}

#[derive(Debug, Clone)]
pub enum IndexType {
    BloomFilter,
    LsmTree,
    InvertedIndex,
    FullText,
}

#[derive(Debug, Clone)]
pub enum AggregationFunc {
    Count,
    Sum(String),
    Avg(String),
    Min(String),
    Max(String),
}

#[derive(Debug, Clone, Copy)]
pub enum SortOrder {
    Ascending,
    Descending,
}

#[derive(Debug, Clone, Copy)]
pub enum MergeType {
    Union,
    Intersect,
    SortedMerge,
}

/// æŸ¥è¯¢è®¡åˆ’ç”Ÿæˆå™¨
pub struct QueryPlanGenerator {
    optimizer: Arc<QueryOptimizer>,
    shard_manager: Arc<crate::ShardManager>,
}

impl QueryPlanGenerator {
    pub fn new(
        optimizer: Arc<QueryOptimizer>,
        shard_manager: Arc<crate::ShardManager>,
    ) -> Self {
        Self {
            optimizer,
            shard_manager,
        }
    }
    
    /// ç”ŸæˆæŸ¥è¯¢è®¡åˆ’
    pub async fn generate_plan(&self, query: &TraceQuery) -> Result<QueryPlan, String> {
        // 1. ä¼˜åŒ–æŸ¥è¯¢è¡¨è¾¾å¼
        let optimized_expr = self.optimizer.optimize(query.filter.clone()).await;
        
        // 2. ç¡®å®šéœ€è¦æ‰«æçš„åˆ†åŒº
        let partitions = self.determine_partitions(&query.time_range, &optimized_expr).await?;
        
        // 3. ç¡®å®šæ‰§è¡ŒèŠ‚ç‚¹
        let nodes = self.determine_execution_nodes(&partitions).await?;
        
        // 4. ç”ŸæˆæŸ¥è¯¢é˜¶æ®µ
        let mut stages = Vec::new();
        
        // Stage 0: åˆ†åŒºæ‰«æï¼ˆå¹¶è¡Œï¼‰
        stages.push(QueryStage {
            stage_type: StageType::PartitionScan {
                partitions: partitions.clone(),
                filter: Some(optimized_expr.clone()),
            },
            inputs: vec![],
            execution_nodes: nodes.clone(),
            estimated_rows: 100_000, // ç®€åŒ–ä¼°è®¡
        });
        
        // Stage 1: è¿‡æ»¤ï¼ˆå¦‚æœæœ‰é¢å¤–çš„è¿‡æ»¤æ¡ä»¶ï¼‰
        if self.needs_additional_filter(&optimized_expr) {
            stages.push(QueryStage {
                stage_type: StageType::Filter {
                    predicate: optimized_expr,
                },
                inputs: vec![0],
                execution_nodes: nodes.clone(),
                estimated_rows: 50_000,
            });
        }
        
        // Stage 2: æ’åºï¼ˆå¦‚æœéœ€è¦ï¼‰
        if let Some(order_by) = &query.order_by {
            stages.push(QueryStage {
                stage_type: StageType::Sort {
                    order_by: order_by.clone(),
                },
                inputs: vec![stages.len() - 1],
                execution_nodes: nodes.clone(),
                estimated_rows: 50_000,
            });
        }
        
        // Stage 3: åˆå¹¶ç»“æœ
        stages.push(QueryStage {
            stage_type: StageType::Merge {
                merge_type: if query.order_by.is_some() {
                    MergeType::SortedMerge
                } else {
                    MergeType::Union
                },
            },
            inputs: vec![stages.len() - 1],
            execution_nodes: vec!["coordinator".to_string()],
            estimated_rows: 50_000,
        });
        
        // Stage 4: é™åˆ¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if let Some(limit) = query.limit {
            stages.push(QueryStage {
                stage_type: StageType::Limit {
                    limit,
                    offset: query.offset.unwrap_or(0),
                },
                inputs: vec![stages.len() - 1],
                execution_nodes: vec!["coordinator".to_string()],
                estimated_rows: limit.min(50_000),
            });
        }
        
        Ok(QueryPlan {
            stages,
            estimated_cost: 1000.0, // ç®€åŒ–ä¼°è®¡
            parallelism: nodes.len(),
        })
    }
    
    async fn determine_partitions(
        &self,
        time_range: &(DateTime<Utc>, DateTime<Utc>),
        _expr: &QueryExpr,
    ) -> Result<Vec<String>, String> {
        // åŸºäºæ—¶é—´èŒƒå›´ç¡®å®šéœ€è¦æ‰«æçš„åˆ†åŒº
        let (start, end) = time_range;
        
        let partitions = self.shard_manager
            .get_nodes_for_time_range_query(*start, *end, None)
            .await?;
        
        Ok(partitions.into_iter().map(|(p, _)| p.to_string()).collect())
    }
    
    async fn determine_execution_nodes(&self, partitions: &[String]) -> Result<Vec<String>, String> {
        // ç®€åŒ–å®ç°ï¼šæ¯ä¸ªåˆ†åŒºå¯¹åº”ä¸€ä¸ªèŠ‚ç‚¹
        Ok(partitions.iter().map(|p| format!("node-for-{}", p)).collect())
    }
    
    fn needs_additional_filter(&self, _expr: &QueryExpr) -> bool {
        // ç®€åŒ–å®ç°
        false
    }
}

/// Trace æŸ¥è¯¢è¯·æ±‚
#[derive(Debug, Clone)]
pub struct TraceQuery {
    pub filter: QueryExpr,
    pub time_range: (DateTime<Utc>, DateTime<Utc>),
    pub order_by: Option<Vec<(String, SortOrder)>>,
    pub limit: Option<usize>,
    pub offset: Option<usize>,
}
```

---

## âš¡ æŸ¥è¯¢æ‰§è¡Œå¼•æ“

### 1. å¹¶è¡ŒæŸ¥è¯¢æ‰§è¡Œå™¨

```rust
use tokio::sync::mpsc;
use futures::stream::{Stream, StreamExt};

/// å¹¶è¡ŒæŸ¥è¯¢æ‰§è¡Œå™¨
pub struct ParallelQueryExecutor {
    plan_generator: Arc<QueryPlanGenerator>,
    worker_pool: Arc<WorkerPool>,
}

impl ParallelQueryExecutor {
    pub fn new(
        plan_generator: Arc<QueryPlanGenerator>,
        worker_pool: Arc<WorkerPool>,
    ) -> Self {
        Self {
            plan_generator,
            worker_pool,
        }
    }
    
    /// æ‰§è¡ŒæŸ¥è¯¢
    pub async fn execute(&self, query: TraceQuery) -> Result<QueryResultStream, String> {
        // 1. ç”ŸæˆæŸ¥è¯¢è®¡åˆ’
        let plan = self.plan_generator.generate_plan(&query).await?;
        
        tracing::info!("Executing query plan with {} stages", plan.stages.len());
        
        // 2. æ‰§è¡Œè®¡åˆ’
        let results = self.execute_plan(plan).await?;
        
        Ok(results)
    }
    
    /// æ‰§è¡ŒæŸ¥è¯¢è®¡åˆ’
    async fn execute_plan(&self, plan: QueryPlan) -> Result<QueryResultStream, String> {
        let mut stage_results: Vec<Vec<TraceRecord>> = vec![Vec::new(); plan.stages.len()];
        
        // æŒ‰é˜¶æ®µé¡ºåºæ‰§è¡Œ
        for (stage_idx, stage) in plan.stages.iter().enumerate() {
            tracing::debug!("Executing stage {}: {:?}", stage_idx, stage.stage_type);
            
            // è·å–è¾“å…¥æ•°æ®
            let inputs: Vec<Vec<TraceRecord>> = stage.inputs
                .iter()
                .map(|&input_idx| stage_results[input_idx].clone())
                .collect();
            
            // æ‰§è¡Œå½“å‰é˜¶æ®µ
            let results = self.execute_stage(stage, inputs).await?;
            
            stage_results[stage_idx] = results;
        }
        
        // è¿”å›æœ€åé˜¶æ®µçš„ç»“æœ
        let final_results = stage_results.pop().unwrap_or_default();
        
        Ok(QueryResultStream::from_vec(final_results))
    }
    
    /// æ‰§è¡Œå•ä¸ªé˜¶æ®µ
    async fn execute_stage(
        &self,
        stage: &QueryStage,
        inputs: Vec<Vec<TraceRecord>>,
    ) -> Result<Vec<TraceRecord>, String> {
        match &stage.stage_type {
            StageType::PartitionScan { partitions, filter } => {
                self.execute_partition_scan(partitions, filter.as_ref(), &stage.execution_nodes).await
            }
            
            StageType::Filter { predicate } => {
                self.execute_filter(inputs, predicate).await
            }
            
            StageType::Sort { order_by } => {
                self.execute_sort(inputs, order_by).await
            }
            
            StageType::Limit { limit, offset } => {
                self.execute_limit(inputs, *limit, *offset).await
            }
            
            StageType::Merge { merge_type } => {
                self.execute_merge(inputs, *merge_type).await
            }
            
            _ => Err("Unsupported stage type".to_string()),
        }
    }
    
    /// æ‰§è¡Œåˆ†åŒºæ‰«æï¼ˆå¹¶è¡Œï¼‰
    async fn execute_partition_scan(
        &self,
        partitions: &[String],
        filter: Option<&QueryExpr>,
        nodes: &[String],
    ) -> Result<Vec<TraceRecord>, String> {
        let mut handles = Vec::new();
        
        for (partition, node) in partitions.iter().zip(nodes.iter()) {
            let partition = partition.clone();
            let node = node.clone();
            let filter = filter.cloned();
            let worker_pool = self.worker_pool.clone();
            
            let handle = tokio::spawn(async move {
                worker_pool.scan_partition(&partition, &node, filter.as_ref()).await
            });
            
            handles.push(handle);
        }
        
        // æ”¶é›†ç»“æœ
        let mut all_results = Vec::new();
        
        for handle in handles {
            match handle.await {
                Ok(Ok(results)) => all_results.extend(results),
                Ok(Err(e)) => tracing::error!("Partition scan failed: {}", e),
                Err(e) => tracing::error!("Task join failed: {}", e),
            }
        }
        
        Ok(all_results)
    }
    
    /// æ‰§è¡Œè¿‡æ»¤
    async fn execute_filter(
        &self,
        inputs: Vec<Vec<TraceRecord>>,
        predicate: &QueryExpr,
    ) -> Result<Vec<TraceRecord>, String> {
        let mut results = Vec::new();
        
        for input in inputs {
            for record in input {
                if self.evaluate_predicate(&record, predicate) {
                    results.push(record);
                }
            }
        }
        
        Ok(results)
    }
    
    /// æ‰§è¡Œæ’åº
    async fn execute_sort(
        &self,
        mut inputs: Vec<Vec<TraceRecord>>,
        order_by: &[(String, SortOrder)],
    ) -> Result<Vec<TraceRecord>, String> {
        let mut all_records: Vec<_> = inputs.into_iter().flatten().collect();
        
        all_records.sort_by(|a, b| {
            for (field, order) in order_by {
                let cmp = self.compare_field(a, b, field);
                
                let cmp = match order {
                    SortOrder::Ascending => cmp,
                    SortOrder::Descending => cmp.reverse(),
                };
                
                if cmp != std::cmp::Ordering::Equal {
                    return cmp;
                }
            }
            
            std::cmp::Ordering::Equal
        });
        
        Ok(all_records)
    }
    
    /// æ‰§è¡Œé™åˆ¶
    async fn execute_limit(
        &self,
        inputs: Vec<Vec<TraceRecord>>,
        limit: usize,
        offset: usize,
    ) -> Result<Vec<TraceRecord>, String> {
        let all_records: Vec<_> = inputs.into_iter().flatten().collect();
        
        Ok(all_records
            .into_iter()
            .skip(offset)
            .take(limit)
            .collect())
    }
    
    /// æ‰§è¡Œåˆå¹¶
    async fn execute_merge(
        &self,
        inputs: Vec<Vec<TraceRecord>>,
        merge_type: MergeType,
    ) -> Result<Vec<TraceRecord>, String> {
        match merge_type {
            MergeType::Union => {
                Ok(inputs.into_iter().flatten().collect())
            }
            
            MergeType::SortedMerge => {
                // å‡è®¾è¾“å…¥å·²æ’åºï¼Œæ‰§è¡Œå½’å¹¶
                self.merge_sorted(inputs).await
            }
            
            _ => Err("Unsupported merge type".to_string()),
        }
    }
    
    /// å½’å¹¶å·²æ’åºçš„ç»“æœ
    async fn merge_sorted(&self, inputs: Vec<Vec<TraceRecord>>) -> Result<Vec<TraceRecord>, String> {
        // ç®€åŒ–å®ç°ï¼šç›´æ¥åˆå¹¶åæ’åº
        let mut all_records: Vec<_> = inputs.into_iter().flatten().collect();
        all_records.sort_by_key(|r| r.timestamp);
        Ok(all_records)
    }
    
    fn evaluate_predicate(&self, _record: &TraceRecord, _predicate: &QueryExpr) -> bool {
        // ç®€åŒ–å®ç°
        true
    }
    
    fn compare_field(&self, a: &TraceRecord, b: &TraceRecord, field: &str) -> std::cmp::Ordering {
        match field {
            "timestamp" => a.timestamp.cmp(&b.timestamp),
            "service_name" => a.service_name.cmp(&b.service_name),
            _ => std::cmp::Ordering::Equal,
        }
    }
}

/// Worker æ± 
pub struct WorkerPool {
    // ç®€åŒ–å®ç°
}

impl WorkerPool {
    pub async fn scan_partition(
        &self,
        _partition: &str,
        _node: &str,
        _filter: Option<&QueryExpr>,
    ) -> Result<Vec<TraceRecord>, String> {
        // TODO: å®é™…å®ç°éœ€è¦è¿œç¨‹è°ƒç”¨æˆ–æœ¬åœ°æ‰«æ
        Ok(Vec::new())
    }
}

/// Trace è®°å½•
#[derive(Debug, Clone)]
pub struct TraceRecord {
    pub trace_id: [u8; 16],
    pub service_name: String,
    pub timestamp: DateTime<Utc>,
    pub duration_ms: u64,
    pub attributes: HashMap<String, String>,
}

/// æŸ¥è¯¢ç»“æœæµ
pub struct QueryResultStream {
    inner: Box<dyn Stream<Item = TraceRecord> + Unpin + Send>,
}

impl QueryResultStream {
    pub fn from_vec(records: Vec<TraceRecord>) -> Self {
        Self {
            inner: Box::new(futures::stream::iter(records)),
        }
    }
    
    pub async fn collect(mut self) -> Vec<TraceRecord> {
        let mut results = Vec::new();
        
        while let Some(record) = self.inner.next().await {
            results.push(record);
        }
        
        results
    }
}
```

---

### 2. æµå¼æŸ¥è¯¢å¤„ç†

```rust
/// æµå¼æŸ¥è¯¢å¤„ç†å™¨
pub struct StreamingQueryProcessor {
    executor: Arc<ParallelQueryExecutor>,
}

impl StreamingQueryProcessor {
    pub fn new(executor: Arc<ParallelQueryExecutor>) -> Self {
        Self { executor }
    }
    
    /// åˆ›å»ºæµå¼æŸ¥è¯¢
    pub async fn execute_streaming(
        &self,
        query: TraceQuery,
    ) -> Result<impl Stream<Item = Result<TraceRecord, String>>, String> {
        let result_stream = self.executor.execute(query).await?;
        
        // è½¬æ¢ä¸ºå¼‚æ­¥æµ
        Ok(futures::stream::unfold(result_stream, |mut stream| async move {
            match stream.inner.next().await {
                Some(record) => Some((Ok(record), stream)),
                None => None,
            }
        }))
    }
    
    /// åˆ†æ‰¹æµå¼å¤„ç†
    pub async fn execute_batched(
        &self,
        query: TraceQuery,
        batch_size: usize,
    ) -> Result<impl Stream<Item = Result<Vec<TraceRecord>, String>>, String> {
        let result_stream = self.executor.execute(query).await?;
        
        Ok(futures::stream::unfold(
            (result_stream, Vec::with_capacity(batch_size)),
            move |(mut stream, mut batch)| async move {
                batch.clear();
                
                while batch.len() < batch_size {
                    match stream.inner.next().await {
                        Some(record) => batch.push(record),
                        None => {
                            if batch.is_empty() {
                                return None;
                            } else {
                                break;
                            }
                        }
                    }
                }
                
                let result_batch = batch.clone();
                Some((Ok(result_batch), (stream, batch)))
            },
        ))
    }
}
```

---

## ğŸ’¾ æŸ¥è¯¢ç¼“å­˜

### 1. å¤šçº§ç¼“å­˜ç³»ç»Ÿ

```rust
use lru::LruCache;
use std::num::NonZeroUsize;

/// æŸ¥è¯¢ç¼“å­˜
pub struct QueryCache {
    /// L1 ç¼“å­˜ï¼ˆå†…å­˜ï¼Œçƒ­ç‚¹æŸ¥è¯¢ï¼‰
    l1_cache: Arc<tokio::sync::Mutex<LruCache<String, CachedQueryResult>>>,
    
    /// L2 ç¼“å­˜ï¼ˆRedisï¼Œè·¨èŠ‚ç‚¹å…±äº«ï¼‰
    l2_cache: Option<redis::Client>,
    
    /// ç¼“å­˜ TTL
    ttl: std::time::Duration,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CachedQueryResult {
    pub results: Vec<TraceRecord>,
    pub cached_at: DateTime<Utc>,
    pub query_time_ms: u64,
}

impl QueryCache {
    pub fn new(l1_capacity: usize, l2_redis_url: Option<String>, ttl: std::time::Duration) -> Self {
        let l2_cache = l2_redis_url.and_then(|url| redis::Client::open(url).ok());
        
        Self {
            l1_cache: Arc::new(tokio::sync::Mutex::new(
                LruCache::new(NonZeroUsize::new(l1_capacity).unwrap())
            )),
            l2_cache,
            ttl,
        }
    }
    
    /// è·å–ç¼“å­˜çš„æŸ¥è¯¢ç»“æœ
    pub async fn get(&self, query_key: &str) -> Option<CachedQueryResult> {
        // 1. å°è¯• L1 ç¼“å­˜
        {
            let mut l1 = self.l1_cache.lock().await;
            if let Some(result) = l1.get(query_key) {
                if !self.is_expired(&result.cached_at) {
                    tracing::debug!("L1 cache hit: {}", query_key);
                    return Some(result.clone());
                } else {
                    l1.pop(query_key);
                }
            }
        }
        
        // 2. å°è¯• L2 ç¼“å­˜
        if let Some(ref redis_client) = self.l2_cache {
            if let Ok(mut conn) = redis_client.get_async_connection().await {
                use redis::AsyncCommands;
                
                if let Ok(Some(data)) = conn.get::<_, Option<Vec<u8>>>(query_key).await {
                    if let Ok(result) = bincode::deserialize::<CachedQueryResult>(&data) {
                        if !self.is_expired(&result.cached_at) {
                            tracing::debug!("L2 cache hit: {}", query_key);
                            
                            // å›å¡« L1 ç¼“å­˜
                            let mut l1 = self.l1_cache.lock().await;
                            l1.put(query_key.to_string(), result.clone());
                            
                            return Some(result);
                        }
                    }
                }
            }
        }
        
        None
    }
    
    /// å­˜å‚¨æŸ¥è¯¢ç»“æœåˆ°ç¼“å­˜
    pub async fn put(&self, query_key: String, result: CachedQueryResult) {
        // 1. å­˜å‚¨åˆ° L1 ç¼“å­˜
        {
            let mut l1 = self.l1_cache.lock().await;
            l1.put(query_key.clone(), result.clone());
        }
        
        // 2. å­˜å‚¨åˆ° L2 ç¼“å­˜
        if let Some(ref redis_client) = self.l2_cache {
            if let Ok(mut conn) = redis_client.get_async_connection().await {
                if let Ok(data) = bincode::serialize(&result) {
                    use redis::AsyncCommands;
                    let _: Result<(), _> = conn.set_ex(query_key, data, self.ttl.as_secs() as u64).await;
                }
            }
        }
    }
    
    /// æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
    fn is_expired(&self, cached_at: &DateTime<Utc>) -> bool {
        let now = Utc::now();
        let elapsed = now.signed_duration_since(*cached_at);
        
        elapsed.num_seconds() as u64 > self.ttl.as_secs()
    }
    
    /// ä½¿ç¼“å­˜å¤±æ•ˆ
    pub async fn invalidate(&self, query_key: &str) {
        // 1. ä» L1 ç¼“å­˜ç§»é™¤
        {
            let mut l1 = self.l1_cache.lock().await;
            l1.pop(query_key);
        }
        
        // 2. ä» L2 ç¼“å­˜ç§»é™¤
        if let Some(ref redis_client) = self.l2_cache {
            if let Ok(mut conn) = redis_client.get_async_connection().await {
                use redis::AsyncCommands;
                let _: Result<(), _> = conn.del(query_key).await;
            }
        }
    }
    
    /// ç”ŸæˆæŸ¥è¯¢é”®
    pub fn generate_query_key(query: &TraceQuery) -> String {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        
        format!("{:?}", query).hash(&mut hasher);
        
        format!("query:{:x}", hasher.finish())
    }
}
```

---

## ğŸ“Š æŸ¥è¯¢ä¸‹æ¨ä¼˜åŒ–

### 1. è°“è¯ä¸‹æ¨

```rust
/// è°“è¯ä¸‹æ¨ä¼˜åŒ–å™¨
pub struct PredicatePushdown {
    // é…ç½®
}

impl PredicatePushdown {
    /// å°†è°“è¯ä¸‹æ¨åˆ°æ‰«æé˜¶æ®µ
    pub fn push_down(&self, plan: &mut QueryPlan) {
        for i in 0..plan.stages.len() {
            if let StageType::Filter { predicate } = &plan.stages[i].stage_type {
                // æŸ¥æ‰¾è¾“å…¥é˜¶æ®µ
                if let Some(&input_idx) = plan.stages[i].inputs.first() {
                    if let StageType::PartitionScan { partitions, filter } = 
                        &mut plan.stages[input_idx].stage_type 
                    {
                        // å°†è¿‡æ»¤æ¡ä»¶åˆå¹¶åˆ°æ‰«æé˜¶æ®µ
                        let combined = if let Some(existing_filter) = filter.take() {
                            QueryExpr::And(vec![existing_filter, predicate.clone()])
                        } else {
                            predicate.clone()
                        };
                        
                        *filter = Some(combined);
                        
                        // æ ‡è®°è¿‡æ»¤é˜¶æ®µä¸ºå·²ä¼˜åŒ–
                        tracing::info!("Pushed down predicate to scan stage");
                    }
                }
            }
        }
    }
}
```

---

### 2. æŠ•å½±ä¸‹æ¨

```rust
/// æŠ•å½±ä¸‹æ¨ä¼˜åŒ–å™¨
pub struct ProjectionPushdown {
    // é…ç½®
}

#[derive(Debug, Clone)]
pub struct Projection {
    pub fields: Vec<String>,
}

impl ProjectionPushdown {
    /// å°†æŠ•å½±ä¸‹æ¨åˆ°æ‰«æé˜¶æ®µ
    pub fn push_down(&self, _plan: &mut QueryPlan, _projection: &Projection) {
        // ç®€åŒ–å®ç°ï¼šæ ‡è®°éœ€è¦è¯»å–çš„å­—æ®µ
        tracing::info!("Projection pushdown applied");
    }
}
```

---

## ğŸ”§ è‡ªé€‚åº”æŸ¥è¯¢ä¼˜åŒ–

### 1. ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å™¨

```rust
/// ç»Ÿè®¡ä¿¡æ¯æ”¶é›†å™¨
pub struct StatisticsCollector {
    statistics: Arc<tokio::sync::RwLock<QueryStatistics>>,
}

impl StatisticsCollector {
    pub fn new(statistics: Arc<tokio::sync::RwLock<QueryStatistics>>) -> Self {
        Self { statistics }
    }
    
    /// æ›´æ–°å±æ€§åŸºæ•°
    pub async fn update_attribute_cardinality(&self, key: String, cardinality: usize) {
        let mut stats = self.statistics.write().await;
        stats.attribute_cardinality.insert(key, cardinality);
    }
    
    /// æ›´æ–°æœåŠ¡åˆ†å¸ƒ
    pub async fn update_service_distribution(&self, distribution: HashMap<String, usize>) {
        let mut stats = self.statistics.write().await;
        stats.service_distribution = distribution;
    }
    
    /// æ›´æ–°åˆ†åŒºç»Ÿè®¡
    pub async fn update_partition_stats(&self, partition_id: String, stats_data: PartitionStats) {
        let mut stats = self.statistics.write().await;
        stats.time_partition_stats.insert(partition_id, stats_data);
    }
    
    /// å®šæœŸæ”¶é›†ç»Ÿè®¡ä¿¡æ¯
    pub async fn collect_periodically(&self, interval: std::time::Duration) {
        let mut interval = tokio::time::interval(interval);
        
        loop {
            interval.tick().await;
            
            if let Err(e) = self.collect_once().await {
                tracing::error!("Statistics collection failed: {}", e);
            }
        }
    }
    
    async fn collect_once(&self) -> Result<(), Box<dyn std::error::Error>> {
        // TODO: å®é™…å®ç°éœ€è¦æ‰«ææ•°æ®å¹¶ç»Ÿè®¡
        tracing::info!("Collecting statistics...");
        
        Ok(())
    }
}
```

---

### 2. ä»£ä»·è¯„ä¼°å™¨

```rust
/// ä»£ä»·è¯„ä¼°å™¨
pub struct CostEstimator {
    statistics: Arc<tokio::sync::RwLock<QueryStatistics>>,
}

impl CostEstimator {
    pub fn new(statistics: Arc<tokio::sync::RwLock<QueryStatistics>>) -> Self {
        Self { statistics }
    }
    
    /// ä¼°è®¡æŸ¥è¯¢è®¡åˆ’çš„ä»£ä»·
    pub async fn estimate_cost(&self, plan: &QueryPlan) -> f64 {
        let mut total_cost = 0.0;
        
        for stage in &plan.stages {
            total_cost += self.estimate_stage_cost(stage).await;
        }
        
        total_cost
    }
    
    /// ä¼°è®¡å•ä¸ªé˜¶æ®µçš„ä»£ä»·
    async fn estimate_stage_cost(&self, stage: &QueryStage) -> f64 {
        match &stage.stage_type {
            StageType::PartitionScan { partitions, .. } => {
                // æ‰«æä»£ä»· = åˆ†åŒºæ•°é‡ * å•åˆ†åŒºæ‰«æä»£ä»·
                partitions.len() as f64 * 100.0
            }
            
            StageType::Filter { .. } => {
                // è¿‡æ»¤ä»£ä»· = è¾“å…¥è¡Œæ•° * å•è¡Œè¿‡æ»¤ä»£ä»·
                stage.estimated_rows as f64 * 0.01
            }
            
            StageType::Sort { .. } => {
                // æ’åºä»£ä»· = n * log(n)
                let n = stage.estimated_rows as f64;
                n * n.log2() * 0.1
            }
            
            StageType::Merge { .. } => {
                // åˆå¹¶ä»£ä»· = è¾“å…¥è¡Œæ•°
                stage.estimated_rows as f64 * 0.05
            }
            
            _ => 10.0,
        }
    }
    
    /// æ¯”è¾ƒä¸¤ä¸ªæŸ¥è¯¢è®¡åˆ’
    pub async fn compare_plans(&self, plan1: &QueryPlan, plan2: &QueryPlan) -> std::cmp::Ordering {
        let cost1 = self.estimate_cost(plan1).await;
        let cost2 = self.estimate_cost(plan2).await;
        
        cost1.partial_cmp(&cost2).unwrap_or(std::cmp::Ordering::Equal)
    }
}
```

---

## ğŸŒŠ æŸ¥è¯¢ç»“æœåˆå¹¶

### 1. æµå¼å½’å¹¶æ’åº

```rust
use futures::stream::StreamExt;

/// æµå¼å½’å¹¶æ’åºå™¨
pub struct StreamingMergeSorter {
    // é…ç½®
}

impl StreamingMergeSorter {
    /// å½’å¹¶å¤šä¸ªå·²æ’åºçš„æµ
    pub async fn merge_sorted_streams(
        &self,
        streams: Vec<impl Stream<Item = TraceRecord> + Unpin>,
        order_by: Vec<(String, SortOrder)>,
    ) -> impl Stream<Item = TraceRecord> {
        // ä½¿ç”¨ä¼˜å…ˆé˜Ÿåˆ—å®ç° K-way å½’å¹¶
        use std::collections::BinaryHeap;
        use std::cmp::Reverse;
        
        // ç®€åŒ–å®ç°ï¼šæ”¶é›†æ‰€æœ‰å…ƒç´ åæ’åº
        let mut all_records = Vec::new();
        
        for mut stream in streams {
            while let Some(record) = stream.next().await {
                all_records.push(record);
            }
        }
        
        all_records.sort_by(|a, b| {
            for (field, order) in &order_by {
                let cmp = self.compare_field(a, b, field);
                
                let cmp = match order {
                    SortOrder::Ascending => cmp,
                    SortOrder::Descending => cmp.reverse(),
                };
                
                if cmp != std::cmp::Ordering::Equal {
                    return cmp;
                }
            }
            
            std::cmp::Ordering::Equal
        });
        
        futures::stream::iter(all_records)
    }
    
    fn compare_field(&self, a: &TraceRecord, b: &TraceRecord, field: &str) -> std::cmp::Ordering {
        match field {
            "timestamp" => a.timestamp.cmp(&b.timestamp),
            "service_name" => a.service_name.cmp(&b.service_name),
            _ => std::cmp::Ordering::Equal,
        }
    }
}
```

---

## ğŸ“ˆ å®Œæ•´ç¤ºä¾‹ï¼šåˆ†å¸ƒå¼æŸ¥è¯¢ç³»ç»Ÿ

```rust
/// åˆ†å¸ƒå¼æŸ¥è¯¢ç³»ç»Ÿ
pub struct DistributedQuerySystem {
    optimizer: Arc<QueryOptimizer>,
    plan_generator: Arc<QueryPlanGenerator>,
    executor: Arc<ParallelQueryExecutor>,
    cache: Arc<QueryCache>,
    statistics_collector: Arc<StatisticsCollector>,
}

impl DistributedQuerySystem {
    pub async fn new(
        shard_manager: Arc<crate::ShardManager>,
        cache_config: (usize, Option<String>, std::time::Duration),
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let statistics = Arc::new(tokio::sync::RwLock::new(QueryStatistics {
            attribute_cardinality: HashMap::new(),
            service_distribution: HashMap::new(),
            time_partition_stats: HashMap::new(),
        }));
        
        let optimizer = Arc::new(QueryOptimizer::new(statistics.clone()));
        let plan_generator = Arc::new(QueryPlanGenerator::new(optimizer.clone(), shard_manager));
        
        let worker_pool = Arc::new(WorkerPool {});
        let executor = Arc::new(ParallelQueryExecutor::new(plan_generator.clone(), worker_pool));
        
        let cache = Arc::new(QueryCache::new(cache_config.0, cache_config.1, cache_config.2));
        
        let statistics_collector = Arc::new(StatisticsCollector::new(statistics));
        
        Ok(Self {
            optimizer,
            plan_generator,
            executor,
            cache,
            statistics_collector,
        })
    }
    
    /// å¯åŠ¨åå°ä»»åŠ¡
    pub async fn start(&self) {
        let collector = self.statistics_collector.clone();
        
        tokio::spawn(async move {
            collector.collect_periodically(std::time::Duration::from_secs(300)).await;
        });
    }
    
    /// æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå¸¦ç¼“å­˜ï¼‰
    pub async fn execute_query(&self, query: TraceQuery) -> Result<Vec<TraceRecord>, String> {
        let query_key = QueryCache::generate_query_key(&query);
        
        // 1. å°è¯•ç¼“å­˜
        if let Some(cached) = self.cache.get(&query_key).await {
            tracing::info!("Query result from cache");
            return Ok(cached.results);
        }
        
        // 2. æ‰§è¡ŒæŸ¥è¯¢
        let start = std::time::Instant::now();
        
        let result_stream = self.executor.execute(query.clone()).await?;
        let results = result_stream.collect().await;
        
        let query_time_ms = start.elapsed().as_millis() as u64;
        
        // 3. ç¼“å­˜ç»“æœ
        let cached_result = CachedQueryResult {
            results: results.clone(),
            cached_at: Utc::now(),
            query_time_ms,
        };
        
        self.cache.put(query_key, cached_result).await;
        
        tracing::info!("Query executed in {} ms", query_time_ms);
        
        Ok(results)
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt::init();
    
    // åˆå§‹åŒ–æŸ¥è¯¢ç³»ç»Ÿ
    // let shard_manager = ...; // ä»å‰é¢çš„ç¤ºä¾‹è·å–
    // let query_system = DistributedQuerySystem::new(
    //     shard_manager,
    //     (1000, Some("redis://127.0.0.1:6379".to_string()), std::time::Duration::from_secs(300)),
    // ).await?;
    
    // query_system.start().await;
    
    // æ‰§è¡ŒæŸ¥è¯¢ç¤ºä¾‹
    // let query = TraceQuery {
    //     filter: QueryExpr::ServiceFilter {
    //         service_name: "example-service".to_string(),
    //     },
    //     time_range: (Utc::now() - chrono::Duration::hours(1), Utc::now()),
    //     order_by: Some(vec![("timestamp".to_string(), SortOrder::Descending)]),
    //     limit: Some(100),
    //     offset: None,
    // };
    
    // let results = query_system.execute_query(query).await?;
    // println!("Found {} traces", results.len());
    
    Ok(())
}
```

---

## ğŸ¯ æŸ¥è¯¢ä¼˜åŒ–æ¨¡å¼

### åˆ†åŒºå‰ªæ

- åŸºäºæ—¶é—´èŒƒå›´è¿‡æ»¤åˆ†åŒº
- åŸºäºæœåŠ¡åè¿‡æ»¤åˆ†åŒº
- åŸºäº Bloom Filter è¿‡æ»¤åˆ†åŒº

### ç´¢å¼•é€‰æ‹©

- TraceID æŸ¥è¯¢ â†’ Bloom Filter + LSM
- å±æ€§æŸ¥è¯¢ â†’ å€’æ’ç´¢å¼•
- å…¨æ–‡æœç´¢ â†’ å…¨æ–‡ç´¢å¼•

### å¹¶è¡Œåº¦è°ƒæ•´

- å°æŸ¥è¯¢ï¼šä½å¹¶è¡Œåº¦
- å¤§æŸ¥è¯¢ï¼šé«˜å¹¶è¡Œåº¦
- åŠ¨æ€è°ƒæ•´åŸºäºè´Ÿè½½

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

- **æŸ¥è¯¢å»¶è¿Ÿ**: < 100ms (P99)
- **ç¼“å­˜å‘½ä¸­ç‡**: > 70%
- **å¹¶è¡Œæ•ˆç‡**: > 80%
- **ç½‘ç»œä¼ è¾“**: æœ€å°åŒ–

---

## ğŸ“š å‚è€ƒèµ„æº

- [Query Optimization in Distributed Systems](https://15721.courses.cs.cmu.edu/)
- [Apache Calcite](https://calcite.apache.org/)
- [Presto Query Optimizer](https://prestodb.io/)
- [Apache Spark Catalyst](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-10  
**ä½œè€…**: OTLP Rust é¡¹ç›®ç»„
