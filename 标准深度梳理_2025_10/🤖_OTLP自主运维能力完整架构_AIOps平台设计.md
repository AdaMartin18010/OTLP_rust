# ğŸ¤– OTLP è‡ªä¸»è¿ç»´èƒ½åŠ›å®Œæ•´æ¶æ„ - AIOps å¹³å°è®¾è®¡

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025å¹´10æœˆ9æ—¥  
> **æ–‡æ¡£ç±»å‹**: P0 ä¼˜å…ˆçº§ - æ¶æ„è®¾è®¡  
> **é¢„ä¼°ç¯‡å¹…**: 6,000+ è¡Œ  
> **æŠ€æœ¯æ ˆ**: OTLP + Flink + TimescaleDB + Neo4j + PyTorch + GPT-4 + Temporal.io  
> **ç›®æ ‡**: å°† OTLP ä»"æ•°æ®é‡‡é›†+ä¼ è¾“"æ¼”è¿›ä¸º"æ•°æ®é‡‡é›†+ä¼ è¾“+æ™ºèƒ½åˆ†æ+è‡ªä¸»è¿ç»´"

---

## ğŸ“‹ ç›®å½•

- [ğŸ¤– OTLP è‡ªä¸»è¿ç»´èƒ½åŠ›å®Œæ•´æ¶æ„ - AIOps å¹³å°è®¾è®¡](#-otlp-è‡ªä¸»è¿ç»´èƒ½åŠ›å®Œæ•´æ¶æ„---aiops-å¹³å°è®¾è®¡)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ç¬¬ä¸€éƒ¨åˆ†: æ¶æ„æ¦‚è¿°ä¸æ„¿æ™¯](#ç¬¬ä¸€éƒ¨åˆ†-æ¶æ„æ¦‚è¿°ä¸æ„¿æ™¯)
    - [1.1 ä¸ºä»€ä¹ˆéœ€è¦ AIOps?](#11-ä¸ºä»€ä¹ˆéœ€è¦-aiops)
      - [ä¼ ç»Ÿè¿ç»´çš„å›°å¢ƒ](#ä¼ ç»Ÿè¿ç»´çš„å›°å¢ƒ)
      - [AIOps çš„ä»·å€¼ä¸»å¼ ](#aiops-çš„ä»·å€¼ä¸»å¼ )
    - [1.2 OTLP + AIOps èåˆæ¶æ„](#12-otlp--aiops-èåˆæ¶æ„)
      - [æ€»ä½“æ¶æ„å›¾](#æ€»ä½“æ¶æ„å›¾)
    - [1.3 æ ¸å¿ƒèƒ½åŠ›çŸ©é˜µ](#13-æ ¸å¿ƒèƒ½åŠ›çŸ©é˜µ)
      - [è‡ªä¸»è¿ç»´èƒ½åŠ›ç­‰çº§ (0-5 çº§)](#è‡ªä¸»è¿ç»´èƒ½åŠ›ç­‰çº§-0-5-çº§)
    - [1.4 æŠ€æœ¯æ ˆè¯¦è§£](#14-æŠ€æœ¯æ ˆè¯¦è§£)
      - [æ•°æ®å¤„ç†å±‚](#æ•°æ®å¤„ç†å±‚)
      - [å­˜å‚¨å±‚](#å­˜å‚¨å±‚)
      - [AI/ML å±‚](#aiml-å±‚)
      - [å·¥ä½œæµç¼–æ’](#å·¥ä½œæµç¼–æ’)
    - [1.5 æ ¸å¿ƒä¼˜åŠ¿](#15-æ ¸å¿ƒä¼˜åŠ¿)
      - [ä¸ä¼ ç»Ÿ APM çš„å¯¹æ¯”](#ä¸ä¼ ç»Ÿ-apm-çš„å¯¹æ¯”)
  - [ç¬¬äºŒéƒ¨åˆ†: æ•°æ®å±‚è®¾è®¡](#ç¬¬äºŒéƒ¨åˆ†-æ•°æ®å±‚è®¾è®¡)
    - [2.1 ç»Ÿä¸€æ•°æ®æ¨¡å‹](#21-ç»Ÿä¸€æ•°æ®æ¨¡å‹)
      - [OTLP åŸç”Ÿæ•°æ®ç»“æ„](#otlp-åŸç”Ÿæ•°æ®ç»“æ„)
      - [AI ç‰¹å¾æ‰©å±•æ¨¡å‹](#ai-ç‰¹å¾æ‰©å±•æ¨¡å‹)
      - [æœåŠ¡ä¾èµ–å›¾æ¨¡å‹ (Neo4j)](#æœåŠ¡ä¾èµ–å›¾æ¨¡å‹-neo4j)
    - [2.2 å®æ—¶æ•°æ®æµæ°´çº¿ (Apache Flink)](#22-å®æ—¶æ•°æ®æµæ°´çº¿-apache-flink)
      - [Flink Job æ¶æ„](#flink-job-æ¶æ„)
      - [Python å®ç° (PyFlink)](#python-å®ç°-pyflink)
    - [2.3 æ•°æ®è´¨é‡ä¿è¯](#23-æ•°æ®è´¨é‡ä¿è¯)
      - [ç¼ºå¤±å€¼å¤„ç†](#ç¼ºå¤±å€¼å¤„ç†)
  - [ç¬¬ä¸‰éƒ¨åˆ†: AI/ML æ¨¡å‹è®¾è®¡](#ç¬¬ä¸‰éƒ¨åˆ†-aiml-æ¨¡å‹è®¾è®¡)
    - [3.1 å¼‚å¸¸æ£€æµ‹æ¨¡å‹](#31-å¼‚å¸¸æ£€æµ‹æ¨¡å‹)
      - [3.1.1 æ— ç›‘ç£å­¦ä¹  (å†·å¯åŠ¨é˜¶æ®µ)](#311-æ— ç›‘ç£å­¦ä¹ -å†·å¯åŠ¨é˜¶æ®µ)
      - [3.1.2 ç›‘ç£å­¦ä¹  (æœ‰æ ‡æ³¨æ•°æ®å)](#312-ç›‘ç£å­¦ä¹ -æœ‰æ ‡æ³¨æ•°æ®å)
    - [3.2 æ ¹å› åˆ†ææ¨¡å‹](#32-æ ¹å› åˆ†ææ¨¡å‹)
      - [3.2.1 å› æœæ¨æ–­ (Causal Inference)](#321-å› æœæ¨æ–­-causal-inference)
      - [3.2.2 å›¾ç¥ç»ç½‘ç»œ (GNN) æ ¹å› åˆ†æ](#322-å›¾ç¥ç»ç½‘ç»œ-gnn-æ ¹å› åˆ†æ)
  - [ç¬¬å››éƒ¨åˆ†: å†³ç­–ä¸æ‰§è¡Œå±‚](#ç¬¬å››éƒ¨åˆ†-å†³ç­–ä¸æ‰§è¡Œå±‚)
    - [4.1 å†³ç­–å¼•æ“æ¶æ„](#41-å†³ç­–å¼•æ“æ¶æ„)
      - [4.1.1 è§„åˆ™å¼•æ“ + AI å†³ç­–èåˆ](#411-è§„åˆ™å¼•æ“--ai-å†³ç­–èåˆ)
      - [4.1.2 å®¡æ‰¹æµç¨‹](#412-å®¡æ‰¹æµç¨‹)
    - [4.2 è¡ŒåŠ¨æ‰§è¡Œå™¨](#42-è¡ŒåŠ¨æ‰§è¡Œå™¨)
  - [ç¬¬äº”éƒ¨åˆ†: æ¨¡å‹è®­ç»ƒä¸ MLOps](#ç¬¬äº”éƒ¨åˆ†-æ¨¡å‹è®­ç»ƒä¸-mlops)
    - [5.1 ç¦»çº¿æ¨¡å‹è®­ç»ƒæµç¨‹](#51-ç¦»çº¿æ¨¡å‹è®­ç»ƒæµç¨‹)
    - [5.2 æ¨¡å‹éƒ¨ç½²ä¸åœ¨çº¿æœåŠ¡](#52-æ¨¡å‹éƒ¨ç½²ä¸åœ¨çº¿æœåŠ¡)
    - [5.3 æ¨¡å‹ç›‘æ§ä¸æŒç»­æ”¹è¿›](#53-æ¨¡å‹ç›‘æ§ä¸æŒç»­æ”¹è¿›)
  - [ç¬¬äº”éƒ¨åˆ†: MLOps ä¸æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†](#ç¬¬äº”éƒ¨åˆ†-mlops-ä¸æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†)
    - [5.1 æ¨¡å‹è®­ç»ƒç®¡é“](#51-æ¨¡å‹è®­ç»ƒç®¡é“)
      - [5.1.1 å®Œæ•´è®­ç»ƒæµç¨‹](#511-å®Œæ•´è®­ç»ƒæµç¨‹)
      - [5.2.2 æ¨¡å‹æœåŠ¡åŒ– (TorchServe/TensorFlow Serving)](#522-æ¨¡å‹æœåŠ¡åŒ–-torchservetensorflow-serving)
    - [5.3 æ¨¡å‹ç›‘æ§ä¸å‘Šè­¦](#53-æ¨¡å‹ç›‘æ§ä¸å‘Šè­¦)
      - [5.3.1 æ•°æ®è´¨é‡ç›‘æ§](#531-æ•°æ®è´¨é‡ç›‘æ§)
      - [5.3.2 æ¨¡å‹æ€§èƒ½ç›‘æ§](#532-æ¨¡å‹æ€§èƒ½ç›‘æ§)
  - [ç¬¬å…­éƒ¨åˆ†: å®Œæ•´æ¡ˆä¾‹ç ”ç©¶](#ç¬¬å…­éƒ¨åˆ†-å®Œæ•´æ¡ˆä¾‹ç ”ç©¶)
    - [6.1 æ¡ˆä¾‹ 1: ç”µå•†ç³»ç»Ÿæ™ºèƒ½è¿ç»´](#61-æ¡ˆä¾‹-1-ç”µå•†ç³»ç»Ÿæ™ºèƒ½è¿ç»´)
      - [6.1.1 ç³»ç»ŸèƒŒæ™¯](#611-ç³»ç»ŸèƒŒæ™¯)
      - [6.1.2 è§£å†³æ–¹æ¡ˆæ¶æ„](#612-è§£å†³æ–¹æ¡ˆæ¶æ„)
      - [6.1.3 å®æ–½æ•ˆæœ](#613-å®æ–½æ•ˆæœ)
      - [6.1.4 æŠ•èµ„å›æŠ¥](#614-æŠ•èµ„å›æŠ¥)
    - [6.2 æ¡ˆä¾‹ 2: é‡‘èç³»ç»Ÿ (ç•¥)](#62-æ¡ˆä¾‹-2-é‡‘èç³»ç»Ÿ-ç•¥)
  - [ç¬¬ä¸ƒéƒ¨åˆ†: éƒ¨ç½²ä¸è¿ç»´](#ç¬¬ä¸ƒéƒ¨åˆ†-éƒ¨ç½²ä¸è¿ç»´)
    - [7.1 Kubernetes éƒ¨ç½²](#71-kubernetes-éƒ¨ç½²)
      - [7.1.1 å®Œæ•´éƒ¨ç½²æ¸…å•](#711-å®Œæ•´éƒ¨ç½²æ¸…å•)
    - [7.2 ç›‘æ§ä¸å¯è§‚æµ‹æ€§ (è‡ªå·±åƒè‡ªå·±çš„ç‹—ç²®)](#72-ç›‘æ§ä¸å¯è§‚æµ‹æ€§-è‡ªå·±åƒè‡ªå·±çš„ç‹—ç²®)
    - [7.3 æˆæœ¬ä¼˜åŒ–](#73-æˆæœ¬ä¼˜åŒ–)
  - [ç¬¬å…«éƒ¨åˆ†: è·¯çº¿å›¾ä¸æœªæ¥å±•æœ›](#ç¬¬å…«éƒ¨åˆ†-è·¯çº¿å›¾ä¸æœªæ¥å±•æœ›)
    - [8.1 çŸ­æœŸè·¯çº¿å›¾ (2026 Q1-Q2)](#81-çŸ­æœŸè·¯çº¿å›¾-2026-q1-q2)
    - [8.2 ä¸­æœŸè·¯çº¿å›¾ (2026 Q3-2027)](#82-ä¸­æœŸè·¯çº¿å›¾-2026-q3-2027)
    - [8.3 é•¿æœŸæ„¿æ™¯ (2027-2029)](#83-é•¿æœŸæ„¿æ™¯-2027-2029)
  - [ğŸ“š ç›¸å…³æ–‡æ¡£](#-ç›¸å…³æ–‡æ¡£)
    - [æ ¸å¿ƒé›†æˆ â­â­â­](#æ ¸å¿ƒé›†æˆ-)
    - [æ€§èƒ½ä¸åˆ†æ â­â­â­](#æ€§èƒ½ä¸åˆ†æ-)
    - [è‡ªåŠ¨åŒ–å·¥ä½œæµ â­â­](#è‡ªåŠ¨åŒ–å·¥ä½œæµ-)
    - [æ¶æ„å¯è§†åŒ– â­â­â­](#æ¶æ„å¯è§†åŒ–-)
    - [å·¥å…·é“¾æ”¯æŒ â­â­](#å·¥å…·é“¾æ”¯æŒ-)
    - [æ·±å…¥å­¦ä¹  â­](#æ·±å…¥å­¦ä¹ -)

---

## ç¬¬ä¸€éƒ¨åˆ†: æ¶æ„æ¦‚è¿°ä¸æ„¿æ™¯

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ AIOps?

#### ä¼ ç»Ÿè¿ç»´çš„å›°å¢ƒ

```text
ğŸ“Š è¡Œä¸šæ•°æ® (2024-2025):

1. å‘Šè­¦ç–²åŠ³
   - å¹³å‡æ¯å¤©äº§ç”Ÿ 10,000+ æ¡å‘Šè­¦
   - å…¶ä¸­ 50-70% æ˜¯è¯¯æŠ¥
   - è¿ç»´äººå‘˜æ¯å¤©èŠ±è´¹ 4-6 å°æ—¶å¤„ç†å‘Šè­¦

2. æ•…éšœå®šä½æ…¢
   - MTTD (å¹³å‡æ£€æµ‹æ—¶é—´): 8-15 åˆ†é’Ÿ
   - MTTR (å¹³å‡ä¿®å¤æ—¶é—´): 30-60 åˆ†é’Ÿ
   - æ ¹å› åˆ†æä¾èµ–äººå·¥ç»éªŒ

3. è¢«åŠ¨å“åº”
   - 90% çš„æ•…éšœç”±ç”¨æˆ·å‘ç°
   - ç¼ºå°‘é¢„æµ‹æ€§ç»´æŠ¤
   - æ— æ³•æå‰é¢„è­¦

4. äººåŠ›æˆæœ¬é«˜
   - 24x7 å€¼ç­
   - äººå·¥æˆæœ¬å è¿ç»´æ€»æˆæœ¬ 60-70%
   - äººæ‰æµå¤±ç‡é«˜ (20-30%)
```

#### AIOps çš„ä»·å€¼ä¸»å¼ 

```text
ğŸ’¡ AIOps å¸¦æ¥çš„æ”¹è¿›:

1. æ™ºèƒ½å‘Šè­¦
   - å‘Šè­¦é™å™ª 80-90%
   - è‡ªåŠ¨åˆ†ç»„ä¸å…³è”
   - æ™ºèƒ½ä¼˜å…ˆçº§æ’åº
   â†’ å‡å°‘å‘Šè­¦ç–²åŠ³ 85%

2. å¿«é€Ÿå®šä½
   - MTTD: 8åˆ†é’Ÿ â†’ 1åˆ†é’Ÿ (87.5% â†“)
   - è‡ªåŠ¨æ ¹å› åˆ†æ
   - å¯è§†åŒ–æ•…éšœé“¾è·¯
   â†’ æ•…éšœå®šä½æ•ˆç‡æå‡ 8 å€

3. ä¸»åŠ¨é¢„è­¦
   - æå‰ 30-60 åˆ†é’Ÿé¢„æµ‹æ•…éšœ
   - é¢„é˜²æ€§ç»´æŠ¤
   - å®¹é‡è§„åˆ’è‡ªåŠ¨åŒ–
   â†’ æ•…éšœé¢„é˜²ç‡ 70%

4. è‡ªåŠ¨ä¿®å¤
   - 80% å¸¸è§æ•…éšœè‡ªåŠ¨ä¿®å¤
   - MTTR: 30åˆ†é’Ÿ â†’ 5åˆ†é’Ÿ (83% â†“)
   - å‡å°‘äººå·¥å¹²é¢„ 75%
   â†’ è¿ç»´æˆæœ¬é™ä½ 60%
```

### 1.2 OTLP + AIOps èåˆæ¶æ„

#### æ€»ä½“æ¶æ„å›¾

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         åº”ç”¨å±‚ (Applications)                    â”‚
â”‚  å¾®æœåŠ¡ | æ•°æ®åº“ | æ¶ˆæ¯é˜Ÿåˆ— | ç¼“å­˜ | å‰ç«¯ | ç§»åŠ¨ç«¯ | IoT è®¾å¤‡      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“ (è‡ªåŠ¨æ’æ¡© / SDK / eBPF)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OTLP æ•°æ®é‡‡é›†å±‚                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Traces      â”‚ Metrics     â”‚ Logs                         â”‚   â”‚
â”‚  â”‚ (åˆ†å¸ƒå¼è¿½è¸ª) â”‚ (æ—¶åºæŒ‡æ ‡)   â”‚ (ç»“æ„åŒ–æ—¥å¿—)                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“ (OTLP gRPC/HTTP)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OpenTelemetry Collector                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Receiver â†’ Processor â†’ Exporter                          â”‚   â”‚
â”‚  â”‚  - æ‰¹å¤„ç† (Batch)                                         â”‚   â”‚
â”‚  â”‚  - é‡‡æ · (Tail Sampling)                                   â”‚   â”‚
â”‚  â”‚  - å±æ€§å¢å¼º (Attributes)                                  â”‚   â”‚
â”‚  â”‚  - è¿‡æ»¤ (Filter)                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AIOps å¹³å° (æ ¸å¿ƒ)                             â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬ 1 å±‚: æ•°æ®å¤„ç†å±‚ (Real-time Stream Processing)         â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚ â”‚ Apache Flink é›†ç¾¤                                  â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ ç‰¹å¾å·¥ç¨‹ (Feature Engineering)                   â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ â”‚  â””â”€ æ—¶é—´çª—å£èšåˆ (1m, 5m, 15m)                    â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ å®æ—¶å…³è” (Traces â†” Metrics â†” Logs)               â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ ä¾èµ–å›¾æ„å»º (Service Dependency Graph)            â”‚   â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€ å¼‚å¸¸æ£€æµ‹ (åœ¨çº¿)                                  â”‚   â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â†“                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬ 2 å±‚: å­˜å‚¨å±‚ (Multi-Model Storage)                     â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚ â”‚ TimescaleDBâ”‚ ClickHouse â”‚ Neo4j      â”‚ Redis        â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ (æ—¶åº)      â”‚ (åˆ—å¼)      â”‚ (å›¾æ•°æ®åº“)  â”‚ (ç¼“å­˜)      â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ - Features â”‚ - Traces   â”‚ - ä¾èµ–å›¾   â”‚ - çƒ­æ•°æ®      â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ - Metrics  â”‚ - Logs     â”‚ - çŸ¥è¯†å›¾è°± â”‚ - ä¼šè¯çŠ¶æ€    â”‚  â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬ 3 å±‚: AI/ML å±‚ (Intelligence Engine)                  â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚ â”‚ å¼‚å¸¸æ£€æµ‹å¼•æ“ (Anomaly Detection)                     â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ Isolation Forest (æ— ç›‘ç£, å†·å¯åŠ¨)                 â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ LSTM (æ—¶åºå¼‚å¸¸, æœ‰ç›‘ç£)                           â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€ Ensemble (é›†æˆæ¨¡å‹)                               â”‚  â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚ â”‚ æ ¹å› åˆ†æå¼•æ“ (RCA Engine)                            â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ å› æœæ¨æ–­ (DoWhy / CausalML)                       â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ å›¾ç¥ç»ç½‘ç»œ (GNN for Service Graph)                â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€ LLM æ¨ç† (GPT-4 / Claude)                        â”‚  â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚ â”‚ é¢„æµ‹å¼•æ“ (Forecasting)                               â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ Prophet (æ—¶åºé¢„æµ‹)                                â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ LSTM (æ·±åº¦å­¦ä¹ )                                   â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€ XGBoost (å®¹é‡è§„åˆ’)                                â”‚  â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚ â”‚ NLP å¼•æ“ (Natural Language Processing)              â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ æ—¥å¿—è§£æ (Log Parsing)                            â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ å¼‚å¸¸è¯†åˆ« (LLM-based)                              â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€ è‡ªç„¶è¯­è¨€æŸ¥è¯¢ (Text-to-SQL/PromQL)                 â”‚  â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬ 4 å±‚: å†³ç­–å±‚ (Decision Making)                         â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚ â”‚ æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ (Smart Alerting)                        â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ é™å™ª (Noise Reduction)                           â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ åˆ†ç»„ (Alert Grouping)                            â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ ä¼˜å…ˆçº§ (Priority Scoring)                        â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€ ä¾èµ–æŠ‘åˆ¶ (Dependency Suppression)                â”‚  â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚ â”‚ çŸ¥è¯†å›¾è°± (Knowledge Graph)                           â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ æ•…éšœæ¨¡å¼åº“                                        â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ ä¿®å¤æ–¹æ¡ˆåº“                                        â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€ æ¨èå¼•æ“                                          â”‚  â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â†“                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ç¬¬ 5 å±‚: æ‰§è¡Œå±‚ (Execution & Orchestration)               â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚ â”‚ å·¥ä½œæµå¼•æ“ (Temporal.io)                             â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ æ•…éšœè¯Šæ–­å·¥ä½œæµ                                    â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ è‡ªåŠ¨ä¿®å¤å·¥ä½œæµ                                    â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€ äººå·¥å®¡æ‰¹ (å…³é”®æ“ä½œ)                               â”‚  â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚ â”‚ è‡ªåŠ¨ä¿®å¤æ‰§è¡Œå™¨ (Remediation Executors)               â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ Kubernetes Operator (æ‰©ç¼©å®¹, é‡å¯)                â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ Terraform (åŸºç¡€è®¾æ–½)                              â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â”œâ”€ Ansible (é…ç½®ç®¡ç†)                                â”‚  â”‚   â”‚
â”‚  â”‚ â”‚ â””â”€ Custom Scripts                                   â”‚  â”‚   â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¯è§†åŒ–ä¸äº¤äº’å±‚ (UI/API)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Web Dashboardâ”‚ ChatOps (Slack)â”‚ CLI        â”‚ REST API    â”‚   â”‚
â”‚  â”‚ - å®æ—¶å¤§å±    â”‚ - å‘Šè­¦é€šçŸ¥     â”‚ - aiops-cli â”‚ - é›†æˆæ¥å£  â”‚   â”‚
â”‚  â”‚ - ä¾èµ–å›¾å¯è§†åŒ–â”‚ - è‡ªç„¶è¯­è¨€äº¤äº’  â”‚            â”‚             â”‚   â”‚
â”‚  â”‚ - RCA æŠ¥å‘Š    â”‚               â”‚            â”‚             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ ¸å¿ƒèƒ½åŠ›çŸ©é˜µ

#### è‡ªä¸»è¿ç»´èƒ½åŠ›ç­‰çº§ (0-5 çº§)

| èƒ½åŠ›ç»´åº¦ | L0(æ‰‹åŠ¨) | L1(ç›‘æ§) | L2(åˆ†æ) | L3(é¢„æµ‹) | L4(è‡ªæ„ˆ) | L5(è‡ªä¸») | ç›®æ ‡ |
|---------|------------|------------|------------|------------|------------|------------|------|
| **å¼‚å¸¸æ£€æµ‹** | äººå·¥æŸ¥çœ‹æ—¥å¿— | é˜ˆå€¼å‘Šè­¦ | ç»Ÿè®¡åˆ†æ(å‡å€¼/P95) | AI å¼‚å¸¸æ£€æµ‹(Isolation Forest) | å®æ—¶æ£€æµ‹+è‡ªåŠ¨æ¢å¤ | è‡ªæˆ‘ä¼˜åŒ–æ¨¡å‹ | **L4** |
| **æ ¹å› åˆ†æ** | äººå·¥æ’æŸ¥(4-8å°æ—¶) | æ—¥å¿—å…³é”®å­—æ£€ç´¢ | ä¾èµ–å›¾åˆ†æ | AI RCA(å› æœæ¨æ–­+GNN) | è‡ªåŠ¨å®šä½+ä¿®å¤ | é¢„é˜²æ€§ç»´æŠ¤ | **L4** |
| **å‘Šè­¦ç®¡ç†** | å…¨é‡å‘Šè­¦(10,000+/å¤©) | åˆ†çº§å‘Šè­¦(P0/P1/P2) | å‘Šè­¦èšåˆ(æ—¶é—´çª—å£) | æ™ºèƒ½é™å™ª(80% å‡å°‘) | è‡ªåŠ¨å¤„ç†(å…³è”æŠ‘åˆ¶) | è‡ªä¸»å­¦ä¹ (åŠ¨æ€é˜ˆå€¼) | **L4** |
| **æ•…éšœä¿®å¤** | äººå·¥ä¿®å¤(30-60åˆ†é’Ÿ) | Runbook æ‰‹å†Œ | åŠè‡ªåŠ¨è„šæœ¬ | æ¨èä¿®å¤æ–¹æ¡ˆ | è‡ªåŠ¨ä¿®å¤(80% åœºæ™¯) | è‡ªä¸»å†³ç­– | **L4** |
| **å®¹é‡è§„åˆ’** | ç»éªŒä¼°ç®— | èµ„æºç›‘æ§ | è¶‹åŠ¿åˆ†æ | AI é¢„æµ‹(Prophet) | è‡ªåŠ¨æ‰©ç¼©å®¹ | æˆæœ¬ä¼˜åŒ– | **L3** |
| **å˜æ›´ç®¡ç†** | æ‰‹åŠ¨å®¡æ‰¹ | å˜æ›´æ—¥å¿— | å½±å“åˆ†æ | é£é™©é¢„æµ‹ | è‡ªåŠ¨å›æ»š | æ™ºèƒ½å‘å¸ƒ | **L3** |

**å½“å‰è¡Œä¸šå¹³å‡æ°´å¹³**: L1-L2  
**æœ¬é¡¹ç›®ç›®æ ‡**: L3-L4 (2026-2027)  
**è¡Œä¸šé¢†å…ˆ**: L4-L5 (Google SRE, Netflix)

### 1.4 æŠ€æœ¯æ ˆè¯¦è§£

#### æ•°æ®å¤„ç†å±‚

```yaml
Apache Flink:
  ç‰ˆæœ¬: 1.18+
  ç”¨é€”: å®æ—¶æµå¤„ç†
  ç‰¹æ€§:
    - æ¯«ç§’çº§å»¶è¿Ÿ
    - Exactly-once è¯­ä¹‰
    - çŠ¶æ€ç®¡ç† (RocksDB)
    - å¤æ‚äº‹ä»¶å¤„ç† (CEP)
  éƒ¨ç½²: Kubernetes (Flink Operator)
  èµ„æº: 10 TaskManager Ã— (8 CPU, 16GB RAM)
```

#### å­˜å‚¨å±‚

```yaml
TimescaleDB (æ—¶åºæ•°æ®):
  ç‰ˆæœ¬: 2.13+
  ç”¨é€”: AI ç‰¹å¾å­˜å‚¨, Metrics
  ç‰¹æ€§:
    - è‡ªåŠ¨åˆ†åŒº (Hypertable)
    - è¿ç»­èšåˆ (Continuous Aggregates)
    - å‹ç¼© (90% èŠ‚çœ)
  èµ„æº: 16 CPU, 64GB RAM, 2TB SSD

ClickHouse (åˆ†æ):
  ç‰ˆæœ¬: 23.8+
  ç”¨é€”: Traces, Logs å­˜å‚¨ä¸æŸ¥è¯¢
  ç‰¹æ€§:
    - åˆ—å¼å­˜å‚¨
    - é«˜å¹¶å‘æŸ¥è¯¢
    - å®æ—¶èšåˆ
  èµ„æº: 24 CPU, 96GB RAM, 4TB SSD

Neo4j (å›¾æ•°æ®åº“):
  ç‰ˆæœ¬: 5.15+
  ç”¨é€”: æœåŠ¡ä¾èµ–å›¾, çŸ¥è¯†å›¾è°±
  ç‰¹æ€§:
    - åŸç”Ÿå›¾å­˜å‚¨
    - Cypher æŸ¥è¯¢è¯­è¨€
    - å›¾ç®—æ³•åº“
  èµ„æº: 8 CPU, 32GB RAM, 500GB SSD

Redis (ç¼“å­˜):
  ç‰ˆæœ¬: 7.2+
  ç”¨é€”: çƒ­æ•°æ®ç¼“å­˜, ä¼šè¯çŠ¶æ€
  èµ„æº: 4 CPU, 16GB RAM
```

#### AI/ML å±‚

```yaml
Python ç”Ÿæ€:
  - PyTorch 2.1+ (æ·±åº¦å­¦ä¹ )
  - Scikit-learn 1.3+ (ä¼ ç»Ÿ ML)
  - Prophet 1.1+ (æ—¶åºé¢„æµ‹)
  - DoWhy 0.11+ (å› æœæ¨æ–­)
  - PyTorch Geometric 2.4+ (GNN)

æ¨¡å‹æœåŠ¡:
  - TorchServe (PyTorch æ¨¡å‹éƒ¨ç½²)
  - ONNX Runtime (è·¨å¹³å°æ¨ç†)
  - Triton Inference Server (é«˜æ€§èƒ½)

LLM æœåŠ¡:
  - OpenAI GPT-4 (äº‘ç«¯)
  - Anthropic Claude 3 (äº‘ç«¯)
  - Meta Llama 3 70B (æœ¬åœ°, vLLM åŠ é€Ÿ)
  - æ··åˆæ–¹æ¡ˆ (æˆæœ¬ä¼˜åŒ–)
```

#### å·¥ä½œæµç¼–æ’

```yaml
Temporal.io:
  ç‰ˆæœ¬: 1.22+
  ç”¨é€”: å¤æ‚å·¥ä½œæµç¼–æ’
  ç‰¹æ€§:
    - æŒä¹…åŒ–å·¥ä½œæµ
    - äººå·¥å®¡æ‰¹
    - é‡è¯•ä¸è¡¥å¿
    - é•¿æ—¶é—´è¿è¡Œ (å¤©çº§)
  SDK: Python, Go
```

### 1.5 æ ¸å¿ƒä¼˜åŠ¿

#### ä¸ä¼ ç»Ÿ APM çš„å¯¹æ¯”

| ç»´åº¦ | ä¼ ç»Ÿ APM(Datadog, Dynatrace) | æœ¬é¡¹ç›®(OTLP + AIOps) | ä¼˜åŠ¿ |
|------|--------------------------------|-------------------------|------|
| **æ•°æ®æ ¼å¼** | ç§æœ‰æ ¼å¼ | OTLP (å¼€æ”¾æ ‡å‡†) | âœ… å‚å•†ä¸­ç«‹ |
| **å¼‚å¸¸æ£€æµ‹** | é™æ€é˜ˆå€¼ | AI åŠ¨æ€æ£€æµ‹ | âœ… å‡†ç¡®ç‡é«˜ 80% |
| **æ ¹å› åˆ†æ** | äººå·¥åˆ†æ | è‡ªåŠ¨ RCA (å› æœæ¨æ–­+GNN) | âœ… é€Ÿåº¦å¿« 8 å€ |
| **å‘Šè­¦é™å™ª** | åŸºç¡€èšåˆ | æ™ºèƒ½é™å™ª (ä¾èµ–æŠ‘åˆ¶) | âœ… å‡å°‘è¯¯æŠ¥ 90% |
| **è‡ªåŠ¨ä¿®å¤** | âŒ ä¸æ”¯æŒ | âœ… å·¥ä½œæµå¼•æ“ | âœ… MTTR é™ä½ 83% |
| **æˆæœ¬** | $100-500/ä¸»æœº/æœˆ | $20-50/ä¸»æœº/æœˆ | âœ… èŠ‚çœ 80% |
| **æ•°æ®ä¸»æƒ** | äº‘ç«¯ SaaS | æœ¬åœ°éƒ¨ç½² | âœ… æ•°æ®å®‰å…¨ |
| **å¯æ‰©å±•æ€§** | æœ‰é™ | å¼€æºç”Ÿæ€ | âœ… æ— é™æ‰©å±• |

---

## ç¬¬äºŒéƒ¨åˆ†: æ•°æ®å±‚è®¾è®¡

### 2.1 ç»Ÿä¸€æ•°æ®æ¨¡å‹

#### OTLP åŸç”Ÿæ•°æ®ç»“æ„

```protobuf
// OpenTelemetry Protocol æ•°æ®æ¨¡å‹

message Span {
  bytes trace_id = 1;
  bytes span_id = 2;
  string name = 3;
  SpanKind kind = 4;
  fixed64 start_time_unix_nano = 5;
  fixed64 end_time_unix_nano = 6;
  repeated KeyValue attributes = 7;
  repeated Event events = 8;
  repeated Link links = 9;
  Status status = 10;
}

message Metric {
  string name = 1;
  string description = 2;
  string unit = 3;
  oneof data {
    Gauge gauge = 4;
    Sum sum = 5;
    Histogram histogram = 6;
    ExponentialHistogram exponential_histogram = 7;
    Summary summary = 8;
  }
  repeated Exemplar exemplars = 9;  // å…³é”®: Metric â†’ Trace å…³è”
}

message LogRecord {
  fixed64 time_unix_nano = 1;
  fixed64 observed_time_unix_nano = 2;
  SeverityNumber severity_number = 3;
  string severity_text = 4;
  AnyValue body = 5;
  repeated KeyValue attributes = 6;
  bytes trace_id = 7;  // å…³é”®: Log â†’ Trace å…³è”
  bytes span_id = 8;
}
```

#### AI ç‰¹å¾æ‰©å±•æ¨¡å‹

```sql
-- TimescaleDB æ‰©å±•è¡¨: AI ç‰¹å¾å·¥ç¨‹

CREATE TABLE otlp_ai_features (
  -- åŸºç¡€ç»´åº¦
  feature_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  time TIMESTAMPTZ NOT NULL,
  service_name VARCHAR(255) NOT NULL,
  operation VARCHAR(255),
  http_status_code INT,
  
  -- å®æ—¶ç‰¹å¾ (æ—¶é—´çª—å£èšåˆ)
  request_rate_1m DOUBLE PRECISION,      -- è¯·æ±‚é€Ÿç‡ (1åˆ†é’Ÿ)
  request_rate_5m DOUBLE PRECISION,      -- è¯·æ±‚é€Ÿç‡ (5åˆ†é’Ÿ)
  request_rate_15m DOUBLE PRECISION,     -- è¯·æ±‚é€Ÿç‡ (15åˆ†é’Ÿ)
  request_rate_1h DOUBLE PRECISION,      -- è¯·æ±‚é€Ÿç‡ (1å°æ—¶)
  
  error_rate_1m DOUBLE PRECISION,        -- é”™è¯¯ç‡ (1åˆ†é’Ÿ)
  error_rate_5m DOUBLE PRECISION,        -- é”™è¯¯ç‡ (5åˆ†é’Ÿ)
  
  p50_latency_1m DOUBLE PRECISION,       -- P50 å»¶è¿Ÿ (æ¯«ç§’)
  p95_latency_1m DOUBLE PRECISION,       -- P95 å»¶è¿Ÿ
  p99_latency_1m DOUBLE PRECISION,       -- P99 å»¶è¿Ÿ
  p50_latency_5m DOUBLE PRECISION,
  p95_latency_5m DOUBLE PRECISION,
  p99_latency_5m DOUBLE PRECISION,
  
  -- æ—¶é—´ç‰¹å¾ (ç”¨äºå­£èŠ‚æ€§æ£€æµ‹)
  hour_of_day SMALLINT,                  -- 0-23
  day_of_week SMALLINT,                  -- 1-7
  day_of_month SMALLINT,                 -- 1-31
  is_weekend BOOLEAN,
  is_business_hour BOOLEAN,              -- 9:00-18:00
  is_peak_hour BOOLEAN,                  -- 10:00-12:00, 14:00-16:00
  
  -- ä¾èµ–ç‰¹å¾
  upstream_services TEXT[],              -- ä¸Šæ¸¸æœåŠ¡åˆ—è¡¨
  downstream_services TEXT[],            -- ä¸‹æ¸¸æœåŠ¡åˆ—è¡¨
  dependency_count INT,                  -- ä¾èµ–æœåŠ¡æ•°é‡
  
  -- èµ„æºç‰¹å¾ (æ¥è‡ª Metrics)
  cpu_usage DOUBLE PRECISION,            -- CPU ä½¿ç”¨ç‡ (%)
  memory_usage DOUBLE PRECISION,         -- å†…å­˜ä½¿ç”¨ç‡ (%)
  disk_io_read DOUBLE PRECISION,         -- ç£ç›˜ I/O è¯» (MB/s)
  disk_io_write DOUBLE PRECISION,        -- ç£ç›˜ I/O å†™ (MB/s)
  network_in DOUBLE PRECISION,           -- ç½‘ç»œå…¥æµé‡ (MB/s)
  network_out DOUBLE PRECISION,          -- ç½‘ç»œå‡ºæµé‡ (MB/s)
  
  -- ä¸šåŠ¡ç‰¹å¾ (æ¥è‡ª Span Attributes)
  user_id VARCHAR(255),
  tenant_id VARCHAR(255),
  region VARCHAR(64),
  deployment_version VARCHAR(64),
  
  -- æ ‡ç­¾ (ç”¨äºç›‘ç£å­¦ä¹ )
  is_anomaly BOOLEAN DEFAULT FALSE,      -- æ˜¯å¦å¼‚å¸¸
  anomaly_type VARCHAR(64),              -- å¼‚å¸¸ç±»å‹
  anomaly_score DOUBLE PRECISION,        -- å¼‚å¸¸åˆ†æ•° (0-1)
  root_cause VARCHAR(255),               -- æ ¹æœ¬åŸå› 
  incident_id VARCHAR(255),              -- å…³è”äº‹ä»¶ ID
  
  -- å…ƒæ•°æ®
  created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
  
  -- ç´¢å¼•çº¦æŸ
  CONSTRAINT valid_rates CHECK (
    request_rate_1m >= 0 AND
    error_rate_1m >= 0 AND error_rate_1m <= 1
  )
);

-- åˆ›å»º TimescaleDB è¶…è¡¨ (è‡ªåŠ¨åˆ†åŒº)
SELECT create_hypertable(
  'otlp_ai_features',
  'time',
  chunk_time_interval => INTERVAL '1 hour',
  if_not_exists => TRUE
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_features_service_time 
  ON otlp_ai_features (service_name, time DESC);

CREATE INDEX idx_features_anomaly 
  ON otlp_ai_features (time DESC) 
  WHERE is_anomaly = TRUE;

-- åˆ›å»ºè¿ç»­èšåˆ (Continuous Aggregate)
CREATE MATERIALIZED VIEW otlp_features_1h
WITH (timescaledb.continuous) AS
SELECT
  time_bucket('1 hour', time) AS bucket,
  service_name,
  AVG(request_rate_5m) AS avg_request_rate,
  AVG(error_rate_5m) AS avg_error_rate,
  AVG(p99_latency_5m) AS avg_p99_latency,
  AVG(cpu_usage) AS avg_cpu,
  AVG(memory_usage) AS avg_memory,
  COUNT(*) FILTER (WHERE is_anomaly = TRUE) AS anomaly_count
FROM otlp_ai_features
GROUP BY bucket, service_name;

-- è‡ªåŠ¨å‹ç¼©ç­–ç•¥ (7å¤©åå‹ç¼©)
SELECT add_compression_policy('otlp_ai_features', INTERVAL '7 days');

-- æ•°æ®ä¿ç•™ç­–ç•¥ (90å¤©ååˆ é™¤)
SELECT add_retention_policy('otlp_ai_features', INTERVAL '90 days');
```

#### æœåŠ¡ä¾èµ–å›¾æ¨¡å‹ (Neo4j)

```cypher
// Neo4j å›¾æ•°æ®åº“æ¨¡å‹

// èŠ‚ç‚¹: æœåŠ¡
CREATE CONSTRAINT service_name_unique IF NOT EXISTS
  FOR (s:Service) REQUIRE s.name IS UNIQUE;

MERGE (s:Service {
  name: "payment-service",
  type: "microservice",
  language: "Go",
  team: "payment-team",
  criticality: "high",  // high, medium, low
  sla_target: 99.95,
  created_at: datetime()
})
RETURN s;

// èŠ‚ç‚¹: æ“ä½œ
CREATE (o:Operation {
  name: "POST /api/v1/payments",
  service: "payment-service",
  method: "POST",
  avg_latency_ms: 150,
  p99_latency_ms: 500,
  error_rate: 0.01
});

// èŠ‚ç‚¹: æ•°æ®åº“
CREATE (d:Database {
  name: "payment-db",
  type: "PostgreSQL",
  version: "15.4",
  host: "payment-db.prod.internal"
});

// èŠ‚ç‚¹: å¤–éƒ¨æœåŠ¡
CREATE (e:ExternalService {
  name: "stripe-api",
  type: "payment-gateway",
  vendor: "Stripe"
});

// å…³ç³»: è°ƒç”¨ä¾èµ–
MATCH (a:Service {name: "order-service"})
MATCH (b:Service {name: "payment-service"})
CREATE (a)-[:CALLS {
  protocol: "gRPC",
  avg_latency_ms: 150,
  p99_latency_ms: 500,
  calls_per_minute: 1000,
  error_rate: 0.01,
  last_seen: datetime()
}]->(b);

// å…³ç³»: æ•°æ®åº“è®¿é—®
MATCH (s:Service {name: "payment-service"})
MATCH (d:Database {name: "payment-db"})
CREATE (s)-[:ACCESSES {
  query_type: "READ_WRITE",
  avg_query_time_ms: 10,
  queries_per_minute: 5000
}]->(d);

// æŸ¥è¯¢: æ‰¾åˆ°æŸæœåŠ¡çš„æ‰€æœ‰ä¸Šæ¸¸
MATCH (upstream:Service)-[:CALLS]->(target:Service {name: "payment-service"})
RETURN upstream.name, upstream.criticality;

// æŸ¥è¯¢: æ‰¾åˆ°æŸæ•…éšœçš„å½±å“èŒƒå›´ (ä¸‹æ¸¸æœåŠ¡)
MATCH path = (root:Service {name: "payment-db"})<-[:ACCESSES*]-(affected:Service)
RETURN affected.name, length(path) AS distance
ORDER BY distance;

// å›¾ç®—æ³•: PageRank (è¯†åˆ«å…³é”®æœåŠ¡)
CALL gds.pageRank.stream('service-dependency-graph')
YIELD nodeId, score
RETURN gds.util.asNode(nodeId).name AS service, score
ORDER BY score DESC
LIMIT 10;
```

### 2.2 å®æ—¶æ•°æ®æµæ°´çº¿ (Apache Flink)

#### Flink Job æ¶æ„

```java
// Flink å®æ—¶ç‰¹å¾å·¥ç¨‹ Job

import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;

public class OTLPFeatureEngineeringJob {
    
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        
        // 1. ä» Kafka è¯»å– OTLP æ•°æ®
        DataStream<Span> spans = env
            .addSource(new FlinkKafkaConsumer<>(
                "otlp-spans",
                new SpanDeserializationSchema(),
                kafkaProps
            ))
            .name("OTLP Spans Source");
        
        DataStream<Metric> metrics = env
            .addSource(new FlinkKafkaConsumer<>(
                "otlp-metrics",
                new MetricDeserializationSchema(),
                kafkaProps
            ))
            .name("OTLP Metrics Source");
        
        // 2. æå–ç‰¹å¾
        DataStream<ServiceFeature> spanFeatures = spans
            .keyBy(span -> span.getServiceName())
            .window(TumblingEventTimeWindows.of(Time.minutes(1)))
            .aggregate(new SpanFeatureAggregator())
            .name("Span Feature Aggregation");
        
        DataStream<ResourceMetrics> resourceFeatures = metrics
            .keyBy(metric -> metric.getServiceName())
            .window(TumblingEventTimeWindows.of(Time.minutes(1)))
            .aggregate(new ResourceMetricAggregator())
            .name("Resource Feature Aggregation");
        
        // 3. ç‰¹å¾å…³è” (Join)
        DataStream<AIFeature> combinedFeatures = spanFeatures
            .connect(resourceFeatures)
            .keyBy(f -> f.getServiceName(), f -> f.getServiceName())
            .process(new FeatureCombiner())
            .name("Feature Combiner");
        
        // 4. å®æ—¶å¼‚å¸¸æ£€æµ‹ (åœ¨çº¿æ¨ç†)
        DataStream<Anomaly> anomalies = combinedFeatures
            .process(new AnomalyDetectionFunction(modelPath))
            .name("Real-time Anomaly Detection");
        
        // 5. è¾“å‡ºåˆ°å¤šä¸ª Sink
        combinedFeatures
            .addSink(new JdbcSink<>(timescaledbConfig))
            .name("Sink to TimescaleDB");
        
        anomalies
            .filter(a -> a.getScore() > 0.8)
            .addSink(new AlertingSink())
            .name("Sink to Alerting System");
        
        // 6. æ„å»ºæœåŠ¡ä¾èµ–å›¾
        spans
            .filter(span -> span.getKind() == SpanKind.CLIENT)
            .process(new DependencyGraphBuilder(neo4jConfig))
            .name("Dependency Graph Builder");
        
        env.execute("OTLP Feature Engineering Job");
    }
    
    // ç‰¹å¾èšåˆå™¨
    public static class SpanFeatureAggregator 
        implements AggregateFunction<Span, FeatureAccumulator, ServiceFeature> {
        
        @Override
        public FeatureAccumulator createAccumulator() {
            return new FeatureAccumulator();
        }
        
        @Override
        public FeatureAccumulator add(Span span, FeatureAccumulator acc) {
            acc.count++;
            acc.totalDuration += span.getDurationNanos();
            acc.durations.add(span.getDurationNanos());
            
            if (span.getStatus().getCode() == StatusCode.ERROR) {
                acc.errorCount++;
            }
            
            return acc;
        }
        
        @Override
        public ServiceFeature getResult(FeatureAccumulator acc) {
            ServiceFeature feature = new ServiceFeature();
            feature.setRequestRate((double) acc.count / 60.0); // per second
            feature.setErrorRate((double) acc.errorCount / acc.count);
            feature.setP50Latency(acc.durations.percentile(50) / 1_000_000.0); // ms
            feature.setP95Latency(acc.durations.percentile(95) / 1_000_000.0);
            feature.setP99Latency(acc.durations.percentile(99) / 1_000_000.0);
            return feature;
        }
        
        @Override
        public FeatureAccumulator merge(FeatureAccumulator a, FeatureAccumulator b) {
            a.count += b.count;
            a.errorCount += b.errorCount;
            a.durations.merge(b.durations);
            return a;
        }
    }
}
```

#### Python å®ç° (PyFlink)

```python
# PyFlink å®æ—¶ç‰¹å¾å·¥ç¨‹

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import ProcessFunction
from pyflink.common.time import Time
from pyflink.datastream.window import TumblingEventTimeWindows

class OTLPFeatureExtractor(ProcessFunction):
    """å®æ—¶æå– OTLP ç‰¹å¾"""
    
    def __init__(self):
        self.state = None  # ValueState for windowed aggregation
    
    def process_element(self, value, ctx):
        """å¤„ç†æ¯æ¡ OTLP æ•°æ®"""
        span = value
        service_name = span['resource']['service.name']
        duration_ms = (span['endTimeUnixNano'] - span['startTimeUnixNano']) / 1_000_000
        
        # æå–ç‰¹å¾
        features = {
            'time': ctx.timestamp(),
            'service_name': service_name,
            'operation': span['name'],
            'duration_ms': duration_ms,
            'status': span['status']['code'],
            'http_method': span['attributes'].get('http.method'),
            'http_status': span['attributes'].get('http.status_code'),
            # ... æ›´å¤šç‰¹å¾
        }
        
        yield features

def main():
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_parallelism(4)
    
    # 1. Source: Kafka
    spans = env.add_source(
        FlinkKafkaConsumer(
            topics=['otlp-spans'],
            deserialization_schema=SimpleStringSchema(),
            properties={'bootstrap.servers': 'kafka:9092'}
        )
    )
    
    # 2. ç‰¹å¾æå–
    features = spans.process(OTLPFeatureExtractor())
    
    # 3. æ—¶é—´çª—å£èšåˆ
    windowed_features = (
        features
        .key_by(lambda x: x['service_name'])
        .window(TumblingEventTimeWindows.of(Time.minutes(1)))
        .reduce(lambda a, b: {
            'request_count': a.get('request_count', 0) + 1,
            'total_duration': a.get('total_duration', 0) + b['duration_ms'],
            # ...
        })
    )
    
    # 4. Sink: TimescaleDB
    windowed_features.add_sink(JdbcSink(...))
    
    env.execute("OTLP Feature Engineering")

if __name__ == '__main__':
    main()
```

### 2.3 æ•°æ®è´¨é‡ä¿è¯

#### ç¼ºå¤±å€¼å¤„ç†

```python
# æ•°æ®æ¸…æ´—ä¸ç¼ºå¤±å€¼å¤„ç†

import pandas as pd
import numpy as np

class DataQualityProcessor:
    """æ•°æ®è´¨é‡å¤„ç†å™¨"""
    
    def __init__(self):
        self.imputers = {}
    
    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†ç¼ºå¤±å€¼"""
        
        # 1. æ•°å€¼å‹ç‰¹å¾: ä½¿ç”¨å‰å‘å¡«å…… + åå‘å¡«å……
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(method='bfill')
        
        # 2. ç±»åˆ«å‹ç‰¹å¾: ä½¿ç”¨ 'unknown'
        categorical_cols = df.select_dtypes(include=['object']).columns
        df[categorical_cols] = df[categorical_cols].fillna('unknown')
        
        # 3. å…³é”®ç‰¹å¾ç¼ºå¤±: åˆ é™¤æ•´è¡Œ
        critical_cols = ['service_name', 'time', 'request_rate_1m']
        df = df.dropna(subset=critical_cols)
        
        return df
    
    def detect_outliers(self, df: pd.DataFrame, column: str) -> pd.Series:
        """ä½¿ç”¨ IQR æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼"""
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        return (df[column] < lower_bound) | (df[column] > upper_bound)
    
    def remove_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç§»é™¤å¼‚å¸¸å€¼ (ç”¨äºè®­ç»ƒæ•°æ®)"""
        
        # ä»…ç§»é™¤æ˜æ˜¾ä¸åˆç†çš„å¼‚å¸¸å€¼
        df = df[df['p99_latency_1m'] < 60000]  # 60 ç§’
        df = df[df['error_rate_1m'] <= 1.0]    # æœ€å¤§ 100%
        df = df[df['cpu_usage'] <= 100]         # æœ€å¤§ 100%
        
        return df
    
    def balance_dataset(self, df: pd.DataFrame, target_col: str = 'is_anomaly'):
        """å¹³è¡¡æ•°æ®é›† (å¤„ç†ç±»åˆ«ä¸å¹³è¡¡)"""
        from imblearn.over_sampling import SMOTE
        
        X = df.drop(columns=[target_col])
        y = df[target_col]
        
        # SMOTE è¿‡é‡‡æ ·
        smote = SMOTE(sampling_strategy=0.5, random_state=42)  # å¼‚å¸¸:æ­£å¸¸ = 1:2
        X_resampled, y_resampled = smote.fit_resample(X, y)
        
        return pd.concat([X_resampled, y_resampled], axis=1)
```

---

## ç¬¬ä¸‰éƒ¨åˆ†: AI/ML æ¨¡å‹è®¾è®¡

### 3.1 å¼‚å¸¸æ£€æµ‹æ¨¡å‹

#### 3.1.1 æ— ç›‘ç£å­¦ä¹  (å†·å¯åŠ¨é˜¶æ®µ)

**Isolation Forest (éš”ç¦»æ£®æ—)**:

```python
# æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹: Isolation Forest

from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import joblib

class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨ - æ— ç›‘ç£å­¦ä¹ """
    
    def __init__(self, contamination=0.01):
        """
        Args:
            contamination: é¢„æœŸå¼‚å¸¸æ¯”ä¾‹ (é»˜è®¤ 1%)
        """
        self.contamination = contamination
        self.scaler = StandardScaler()
        self.model = IsolationForest(
            contamination=contamination,
            n_estimators=100,
            max_samples='auto',
            random_state=42,
            n_jobs=-1
        )
    
    def extract_features(self, df):
        """æå–ç”¨äºå¼‚å¸¸æ£€æµ‹çš„ç‰¹å¾"""
        feature_cols = [
            'request_rate_1m',
            'request_rate_5m',
            'error_rate_1m',
            'error_rate_5m',
            'p50_latency_1m',
            'p95_latency_1m',
            'p99_latency_1m',
            'cpu_usage',
            'memory_usage',
            'dependency_count',
        ]
        return df[feature_cols]
    
    def train(self, df):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        X = self.extract_features(df)
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # è®­ç»ƒ
        self.model.fit(X_scaled)
        
        print(f"âœ… Isolation Forest trained on {len(X)} samples")
    
    def predict(self, df):
        """é¢„æµ‹å¼‚å¸¸ (åœ¨çº¿æ¨ç†)"""
        X = self.extract_features(df)
        X_scaled = self.scaler.transform(X)
        
        # é¢„æµ‹: -1 è¡¨ç¤ºå¼‚å¸¸, 1 è¡¨ç¤ºæ­£å¸¸
        predictions = self.model.predict(X_scaled)
        
        # å¼‚å¸¸åˆ†æ•°: è¶Šå°è¶Šå¼‚å¸¸
        scores = self.model.score_samples(X_scaled)
        
        # è½¬æ¢ä¸ºæ¦‚ç‡ (0-1)
        anomaly_probs = 1 - (scores - scores.min()) / (scores.max() - scores.min())
        
        return predictions, anomaly_probs
    
    def save(self, path):
        """ä¿å­˜æ¨¡å‹"""
        joblib.dump({
            'scaler': self.scaler,
            'model': self.model,
            'contamination': self.contamination
        }, path)
    
    @classmethod
    def load(cls, path):
        """åŠ è½½æ¨¡å‹"""
        data = joblib.load(path)
        detector = cls(contamination=data['contamination'])
        detector.scaler = data['scaler']
        detector.model = data['model']
        return detector

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    import pandas as pd
    
    # 1. åŠ è½½å†å²æ­£å¸¸æ•°æ® (7å¤©)
    df = pd.read_sql("""
        SELECT * FROM otlp_ai_features
        WHERE time >= NOW() - INTERVAL '7 days'
          AND is_anomaly = FALSE
    """, conn)
    
    # 2. è®­ç»ƒ
    detector = AnomalyDetector(contamination=0.01)
    detector.train(df)
    
    # 3. ä¿å­˜
    detector.save('models/anomaly_detector_v1.pkl')
    
    # 4. å®æ—¶é¢„æµ‹
    new_data = pd.read_sql("""
        SELECT * FROM otlp_ai_features
        WHERE time >= NOW() - INTERVAL '5 minutes'
    """, conn)
    
    predictions, scores = detector.predict(new_data)
    
    # 5. å‘Šè­¦
    anomalies = new_data[predictions == -1]
    print(f"âš ï¸ Detected {len(anomalies)} anomalies:")
    print(anomalies[['service_name', 'p99_latency_1m', 'error_rate_1m']])
```

**æ•ˆæœè¯„ä¼°**:

```python
# æ¨¡å‹è¯„ä¼°

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt

def evaluate_anomaly_detector(detector, test_df):
    """è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
    
    # é¢„æµ‹
    predictions, scores = detector.predict(test_df)
    y_pred = (predictions == -1).astype(int)
    y_true = test_df['is_anomaly'].values
    
    # æŒ‡æ ‡
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    
    print(f"""
    ğŸ“Š Evaluation Results:
    -------------------
    Precision: {precision:.3f}  (é¢„æµ‹ä¸ºå¼‚å¸¸ä¸­,çœŸæ­£å¼‚å¸¸çš„æ¯”ä¾‹)
    Recall:    {recall:.3f}  (æ‰€æœ‰å¼‚å¸¸ä¸­,è¢«æ£€æµ‹å‡ºçš„æ¯”ä¾‹)
    F1 Score:  {f1:.3f}
    
    ğŸ¯ è¡Œä¸šåŸºå‡†:
    - Precision > 0.80 (å‡å°‘è¯¯æŠ¥)
    - Recall > 0.85 (ä¸æ¼æŠ¥å…³é”®æ•…éšœ)
    - F1 > 0.82
    """)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    print(f"\næ··æ·†çŸ©é˜µ:")
    print(f"              é¢„æµ‹æ­£å¸¸  é¢„æµ‹å¼‚å¸¸")
    print(f"å®é™…æ­£å¸¸      {cm[0,0]:6d}    {cm[0,1]:6d}  (è¯¯æŠ¥)")
    print(f"å®é™…å¼‚å¸¸      {cm[1,0]:6d}    {cm[1,1]:6d}  (æ¼æŠ¥)")
    
    return precision, recall, f1
```

#### 3.1.2 ç›‘ç£å­¦ä¹  (æœ‰æ ‡æ³¨æ•°æ®å)

**LSTM æ—¶åºå¼‚å¸¸æ£€æµ‹**:

```python
# æ·±åº¦å­¦ä¹ å¼‚å¸¸æ£€æµ‹: LSTM

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

class TimeSeriesDataset(Dataset):
    """æ—¶åºæ•°æ®é›†"""
    
    def __init__(self, df, sequence_length=60, features=None):
        """
        Args:
            df: DataFrame with time-sorted features
            sequence_length: æ—¶é—´çª—å£é•¿åº¦ (60 = 1å°æ—¶, å‡è®¾1åˆ†é’Ÿé‡‡æ ·)
            features: ç‰¹å¾åˆ—è¡¨
        """
        self.sequence_length = sequence_length
        self.features = features or [
            'request_rate_1m', 'error_rate_1m', 'p99_latency_1m',
            'cpu_usage', 'memory_usage'
        ]
        
        # æ ‡å‡†åŒ–
        from sklearn.preprocessing import StandardScaler
        self.scaler = StandardScaler()
        
        X = df[self.features].values
        X_scaled = self.scaler.fit_transform(X)
        
        y = df['is_anomaly'].values
        
        # åˆ›å»ºåºåˆ—
        self.X, self.y = self._create_sequences(X_scaled, y)
    
    def _create_sequences(self, X, y):
        """åˆ›å»ºæ—¶åºåºåˆ—"""
        sequences_X, sequences_y = [], []
        
        for i in range(len(X) - self.sequence_length):
            sequences_X.append(X[i:i+self.sequence_length])
            sequences_y.append(y[i+self.sequence_length])  # é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶åˆ»
        
        return torch.FloatTensor(sequences_X), torch.FloatTensor(sequences_y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


class LSTMAnomalyDetector(nn.Module):
    """LSTM å¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
    
    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.2):
        super().__init__()
        
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        # x: (batch, sequence_length, input_dim)
        lstm_out, (h_n, c_n) = self.lstm(x)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]  # (batch, hidden_dim)
        
        # å…¨è¿æ¥å±‚
        out = self.dropout(last_output)
        out = self.fc(out)
        out = self.sigmoid(out)  # è¾“å‡ºå¼‚å¸¸æ¦‚ç‡ (0-1)
        
        return out.squeeze()


def train_lstm_detector(train_df, val_df, epochs=50):
    """è®­ç»ƒ LSTM å¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
    
    # 1. å‡†å¤‡æ•°æ®
    train_dataset = TimeSeriesDataset(train_df, sequence_length=60)
    val_dataset = TimeSeriesDataset(val_df, sequence_length=60)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # 2. æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = LSTMAnomalyDetector(
        input_dim=len(train_dataset.features),
        hidden_dim=64,
        num_layers=2,
        dropout=0.2
    ).to(device)
    
    # 3. æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨
    criterion = nn.BCELoss()  # äºŒåˆ†ç±»äº¤å‰ç†µ
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # 4. è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # éªŒè¯
        model.eval()
        val_loss = 0.0
        
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
        
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        print(f"Epoch {epoch+1}/{epochs} - "
              f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'model_state_dict': model.state_dict(),
                'scaler': train_dataset.scaler,
                'features': train_dataset.features
            }, 'models/lstm_anomaly_detector_best.pth')
    
    return model


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åŠ è½½å·²æ ‡æ³¨æ•°æ®
    df = pd.read_sql("""
        SELECT * FROM otlp_ai_features
        WHERE time >= NOW() - INTERVAL '30 days'
        ORDER BY time
    """, conn)
    
    # åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†
    split_idx = int(len(df) * 0.8)
    train_df = df[:split_idx]
    val_df = df[split_idx:]
    
    # è®­ç»ƒ
    model = train_lstm_detector(train_df, val_df, epochs=50)
```

**åœ¨çº¿æ¨ç†**:

```python
# LSTM åœ¨çº¿æ¨ç†

class LSTMInferenceEngine:
    """LSTM æ¨ç†å¼•æ“ (ç”¨äºå®æ—¶æ£€æµ‹)"""
    
    def __init__(self, model_path, device='cpu'):
        """
        åˆå§‹åŒ– LSTM æ¨ç†å¼•æ“
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ ('cpu' æˆ– 'cuda')
        
        Raises:
            FileNotFoundError: å¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨
            KeyError: å¦‚æœæ¨¡å‹æ–‡ä»¶ç¼ºå°‘å¿…è¦å­—æ®µ
            RuntimeError: å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥
        """
        import os
        import logging
        
        self.logger = logging.getLogger(__name__)
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}")
        
        # éªŒè¯è®¾å¤‡å¯ç”¨æ€§
        if device == 'cuda' and not torch.cuda.is_available():
            self.logger.warning("CUDA requested but not available, falling back to CPU")
            device = 'cpu'
        
        try:
            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(model_path, map_location=device)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_keys = ['scaler', 'features', 'model_state_dict']
            missing_keys = [k for k in required_keys if k not in checkpoint]
            if missing_keys:
                raise KeyError(f"Checkpoint missing required keys: {missing_keys}")
            
            self.scaler = checkpoint['scaler']
            self.features = checkpoint['features']
            
            # åˆ›å»ºå’ŒåŠ è½½æ¨¡å‹
            self.model = LSTMAnomalyDetector(
                input_dim=len(self.features),
                hidden_dim=checkpoint.get('hidden_dim', 64),
                num_layers=checkpoint.get('num_layers', 2)
            ).to(device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()
            
            self.device = device
            self.sequence_buffer = []  # æ»‘åŠ¨çª—å£ç¼“å†²åŒº
            self.sequence_length = checkpoint.get('sequence_length', 60)
            
            self.logger.info(
                f"Model loaded successfully: {model_path}, "
                f"device={device}, features={len(self.features)}"
            )
        
        except Exception as e:
            self.logger.error(f"Failed to load model from {model_path}: {e}")
            raise RuntimeError(f"Model initialization failed: {e}") from e
    
    def predict(self, new_data_point):
        """
        å®æ—¶é¢„æµ‹å•ä¸ªæ•°æ®ç‚¹
        
        Args:
            new_data_point: æ•°æ®ç‚¹å­—å…¸,åŒ…å«æ‰€æœ‰ç‰¹å¾
        
        Returns:
            å¼‚å¸¸æ¦‚ç‡ (0.0-1.0)
        
        Raises:
            KeyError: å¦‚æœç¼ºå°‘å¿…è¦ç‰¹å¾
            ValueError: å¦‚æœç‰¹å¾å€¼æ— æ•ˆ
        """
        try:
            # 1. éªŒè¯å¹¶æå–ç‰¹å¾
            missing_features = [f for f in self.features if f not in new_data_point]
            if missing_features:
                raise KeyError(f"Missing features: {missing_features}")
            
            features = [new_data_point[f] for f in self.features]
            
            # éªŒè¯ç‰¹å¾å€¼
            if not all(isinstance(f, (int, float)) and not np.isnan(f) for f in features):
                raise ValueError("Invalid feature values (must be numeric and not NaN)")
            
            features_scaled = self.scaler.transform([features])
            
            # 2. æ›´æ–°æ»‘åŠ¨çª—å£
            self.sequence_buffer.append(features_scaled[0])
            if len(self.sequence_buffer) > self.sequence_length:
                self.sequence_buffer.pop(0)
            
            # 3. å¦‚æœçª—å£æœªæ»¡,è¿”å›æ­£å¸¸
            if len(self.sequence_buffer) < self.sequence_length:
                return 0.0  # æ­£å¸¸
            
            # 4. æ¨ç†
            sequence = torch.FloatTensor([self.sequence_buffer]).to(self.device)
            
            with torch.no_grad():
                anomaly_prob = self.model(sequence).item()
            
            # é™åˆ¶è¾“å‡ºèŒƒå›´
            anomaly_prob = max(0.0, min(1.0, anomaly_prob))
            
            return anomaly_prob
        
        except Exception as e:
            self.logger.error(f"Prediction failed: {e}")
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return 0.0
    
    def predict_batch(self, df):
        """æ‰¹é‡é¢„æµ‹"""
        dataset = TimeSeriesDataset(df, sequence_length=60, features=self.features)
        dataset.scaler = self.scaler  # ä½¿ç”¨è®­ç»ƒæ—¶çš„ scaler
        
        loader = DataLoader(dataset, batch_size=64, shuffle=False)
        
        predictions = []
        with torch.no_grad():
            for X_batch, _ in loader:
                X_batch = X_batch.to(self.device)
                probs = self.model(X_batch).cpu().numpy()
                predictions.extend(probs)
        
        return np.array(predictions)


# Flink ProcessFunction é›†æˆ
class LSTMAnomalyDetectionFunction(ProcessFunction):
    """Flink ProcessFunction for LSTM å¼‚å¸¸æ£€æµ‹"""
    
    def open(self, runtime_context):
        self.engine = LSTMInferenceEngine(
            model_path='models/lstm_anomaly_detector_best.pth',
            device='cpu'
        )
    
    def process_element(self, value, ctx):
        """å¤„ç†æ¯æ¡ç‰¹å¾æ•°æ®"""
        anomaly_prob = self.engine.predict(value)
        
        if anomaly_prob > 0.8:  # é˜ˆå€¼
            yield {
                'time': value['time'],
                'service_name': value['service_name'],
                'anomaly_score': anomaly_prob,
                'features': value
            }
```

### 3.2 æ ¹å› åˆ†ææ¨¡å‹

#### 3.2.1 å› æœæ¨æ–­ (Causal Inference)

**ä½¿ç”¨ DoWhy åº“è¿›è¡Œæ ¹å› åˆ†æ**:

```python
# å› æœæ¨æ–­æ ¹å› åˆ†æ

import dowhy
from dowhy import CausalModel
import networkx as nx

class CausalRCAEngine:
    """åŸºäºå› æœæ¨æ–­çš„æ ¹å› åˆ†æå¼•æ“"""
    
    def __init__(self):
        self.causal_graph = self._build_causal_graph()
    
    def _build_causal_graph(self):
        """æ„å»ºå› æœå›¾ (é¢†åŸŸçŸ¥è¯†)"""
        
        # å®šä¹‰å› æœå…³ç³» (DOT æ ¼å¼)
        causal_graph_dot = """
        digraph {
            // åŸºç¡€èµ„æº â†’ æœåŠ¡æ€§èƒ½
            database_cpu -> database_query_time;
            database_memory -> database_query_time;
            database_connections -> database_query_time;
            
            // æ•°æ®åº“ â†’ æœåŠ¡
            database_query_time -> service_b_latency;
            
            // æœåŠ¡ä¾èµ–é“¾
            service_b_latency -> service_a_latency;
            service_c_latency -> service_a_latency;
            
            // ç½‘ç»œ
            network_latency -> service_b_latency;
            network_latency -> service_c_latency;
            
            // ç¼“å­˜
            cache_hit_rate -> service_b_latency;
            cache_cpu -> cache_hit_rate;
            
            // æ¶ˆæ¯é˜Ÿåˆ—
            mq_lag -> service_c_latency;
            mq_cpu -> mq_lag;
            
            // è´Ÿè½½
            request_rate -> service_a_cpu;
            service_a_cpu -> service_a_latency;
        }
        """
        
        return causal_graph_dot
    
    def identify_root_cause(self, df, anomaly_service, anomaly_metric):
        """è¯†åˆ«æ ¹å› """
        
        # 1. å®šä¹‰ outcome (è§‚å¯Ÿåˆ°çš„å¼‚å¸¸)
        outcome = f"{anomaly_service}_{anomaly_metric}"  # ä¾‹å¦‚: "service_a_latency"
        
        # 2. å€™é€‰æ ¹å›  (æ‰€æœ‰ä¸Šæ¸¸å› ç´ )
        candidate_causes = self._get_upstream_causes(outcome)
        
        # 3. å¯¹æ¯ä¸ªå€™é€‰æ ¹å› ,ä¼°è®¡å› æœæ•ˆåº”
        results = []
        
        for treatment in candidate_causes:
            try:
                # åˆ›å»ºå› æœæ¨¡å‹
                model = CausalModel(
                    data=df,
                    treatment=treatment,
                    outcome=outcome,
                    graph=self.causal_graph
                )
                
                # è¯†åˆ«å› æœæ•ˆåº”
                identified_estimand = model.identify_effect(
                    proceed_when_unidentifiable=True
                )
                
                # ä¼°è®¡å› æœæ•ˆåº” (ä½¿ç”¨çº¿æ€§å›å½’)
                estimate = model.estimate_effect(
                    identified_estimand,
                    method_name="backdoor.linear_regression"
                )
                
                # åäº‹å®éªŒè¯
                refute_result = model.refute_estimate(
                    identified_estimand,
                    estimate,
                    method_name="random_common_cause"
                )
                
                results.append({
                    'cause': treatment,
                    'effect_size': estimate.value,
                    'confidence': 1 - refute_result.new_effect,
                    'p_value': estimate.get_confidence_intervals()[2]  # p-value
                })
                
            except Exception as e:
                print(f"âš ï¸ Cannot estimate effect for {treatment}: {e}")
                continue
        
        # 4. æ ¹æ®æ•ˆåº”å¤§å°æ’åº
        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values('effect_size', ascending=False)
        
        # 5. ç­›é€‰æ˜¾è‘—çš„æ ¹å›  (p < 0.05, effect_size > threshold)
        root_causes = results_df[
            (results_df['p_value'] < 0.05) & 
            (results_df['effect_size'].abs() > 0.1)
        ]
        
        return root_causes
    
    def _get_upstream_causes(self, node):
        """è·å–æŸèŠ‚ç‚¹çš„æ‰€æœ‰ä¸Šæ¸¸èŠ‚ç‚¹"""
        # è§£æ DOT å›¾
        G = nx.DiGraph(nx.nx_pydot.from_pydot(
            pydot.graph_from_dot_data(self.causal_graph)[0]
        ))
        
        # æ‰¾åˆ°æ‰€æœ‰ç¥–å…ˆèŠ‚ç‚¹
        ancestors = nx.ancestors(G, node)
        
        return list(ancestors)
    
    def explain_root_cause(self, root_cause, anomaly_service):
        """è§£é‡Šæ ¹å›  (äººç±»å¯è¯»)"""
        
        explanations = {
            'database_cpu': f"ğŸ’¾ æ•°æ®åº“ CPU è¿‡é«˜å¯¼è‡´ {anomaly_service} å“åº”å˜æ…¢",
            'database_query_time': f"ğŸŒ æ•°æ®åº“æŸ¥è¯¢æ—¶é—´è¿‡é•¿å½±å“ {anomaly_service}",
            'cache_hit_rate': f"âš¡ ç¼“å­˜å‘½ä¸­ç‡ä¸‹é™å¯¼è‡´ {anomaly_service} æ€§èƒ½ä¸‹é™",
            'network_latency': f"ğŸŒ ç½‘ç»œå»¶è¿Ÿå¢åŠ å½±å“ {anomaly_service}",
            'mq_lag': f"ğŸ“® æ¶ˆæ¯é˜Ÿåˆ—å †ç§¯å¯¼è‡´ {anomaly_service} å¤„ç†å»¶è¿Ÿ",
        }
        
        return explanations.get(root_cause, f"âš ï¸ {root_cause} å¼‚å¸¸")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # 1. æ£€æµ‹åˆ°å¼‚å¸¸
    anomaly = {
        'service': 'service_a',
        'metric': 'latency',
        'value': 1500,  # ms
        'threshold': 500
    }
    
    # 2. æ”¶é›†ç›¸å…³æŒ‡æ ‡æ•°æ® (è¿‡å» 1 å°æ—¶)
    df = pd.read_sql("""
        SELECT
            time,
            service_a_latency,
            service_a_cpu,
            service_b_latency,
            service_c_latency,
            database_cpu,
            database_query_time,
            database_connections,
            cache_hit_rate,
            cache_cpu,
            mq_lag,
            mq_cpu,
            network_latency,
            request_rate
        FROM otlp_ai_features
        WHERE time >= NOW() - INTERVAL '1 hour'
        ORDER BY time
    """, conn)
    
    # 3. æ ¹å› åˆ†æ
    rca_engine = CausalRCAEngine()
    root_causes = rca_engine.identify_root_cause(
        df,
        anomaly_service='service_a',
        anomaly_metric='latency'
    )
    
    # 4. è¾“å‡ºç»“æœ
    print("ğŸ” æ ¹å› åˆ†æç»“æœ:\n")
    for idx, row in root_causes.head(3).iterrows():
        explanation = rca_engine.explain_root_cause(row['cause'], 'service_a')
        print(f"{idx+1}. {explanation}")
        print(f"   - å› æœæ•ˆåº”: {row['effect_size']:.3f}")
        print(f"   - ç½®ä¿¡åº¦: {row['confidence']:.2%}")
        print(f"   - På€¼: {row['p_value']:.4f}\n")
```

**è¾“å‡ºç¤ºä¾‹**:

```text
ğŸ” æ ¹å› åˆ†æç»“æœ:

1. ğŸ’¾ æ•°æ®åº“ CPU è¿‡é«˜å¯¼è‡´ service_a å“åº”å˜æ…¢
   - å› æœæ•ˆåº”: 0.856
   - ç½®ä¿¡åº¦: 92%
   - På€¼: 0.0023

2. âš¡ ç¼“å­˜å‘½ä¸­ç‡ä¸‹é™å¯¼è‡´ service_a æ€§èƒ½ä¸‹é™
   - å› æœæ•ˆåº”: 0.423
   - ç½®ä¿¡åº¦: 85%
   - På€¼: 0.0156

3. ğŸŒ ç½‘ç»œå»¶è¿Ÿå¢åŠ å½±å“ service_a
   - å› æœæ•ˆåº”: 0.218
   - ç½®ä¿¡åº¦: 78%
   - På€¼: 0.0431
```

#### 3.2.2 å›¾ç¥ç»ç½‘ç»œ (GNN) æ ¹å› åˆ†æ

**ä½¿ç”¨ PyTorch Geometric æ„å»º GNN æ¨¡å‹**:

```python
# GNN æ ¹å› åˆ†æ

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, DataLoader

class ServiceGraphRCAModel(torch.nn.Module):
    """åŸºäº GNN çš„æ ¹å› åˆ†ææ¨¡å‹"""
    
    def __init__(self, node_feature_dim, hidden_dim=64, num_layers=3):
        super().__init__()
        
        # å›¾å·ç§¯å±‚
        self.convs = torch.nn.ModuleList()
        self.convs.append(GCNConv(node_feature_dim, hidden_dim))
        
        for _ in range(num_layers - 1):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))
        
        # è¾“å‡ºå±‚ (é¢„æµ‹æ¯ä¸ªèŠ‚ç‚¹æ˜¯æ ¹å› çš„æ¦‚ç‡)
        self.fc = torch.nn.Linear(hidden_dim, 1)
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        
        # å›¾å·ç§¯ + ReLU + Dropout
        for conv in self.convs[:-1]:
            x = conv(x, edge_index)
            x = F.relu(x)
            x = F.dropout(x, p=0.2, training=self.training)
        
        # æœ€åä¸€å±‚
        x = self.convs[-1](x, edge_index)
        
        # è¾“å‡ºå±‚
        x = self.fc(x)
        
        return torch.sigmoid(x).squeeze()  # (num_nodes,) - æ¯ä¸ªèŠ‚ç‚¹çš„æ ¹å› æ¦‚ç‡


def prepare_service_graph_data(service_graph, features):
    """å‡†å¤‡ PyTorch Geometric æ•°æ®"""
    
    # 1. èŠ‚ç‚¹ç‰¹å¾ (æ¯ä¸ªæœåŠ¡çš„å½“å‰çŠ¶æ€)
    node_features = []
    node_names = []
    
    for node in service_graph.nodes():
        node_names.append(node)
        feature_vector = [
            features[node]['cpu_usage'],
            features[node]['memory_usage'],
            features[node]['request_rate'],
            features[node]['error_rate'],
            features[node]['p99_latency'],
            # ...
        ]
        node_features.append(feature_vector)
    
    x = torch.tensor(node_features, dtype=torch.float)
    
    # 2. è¾¹ (æœåŠ¡ä¾èµ–å…³ç³»)
    edge_index = []
    for source, target in service_graph.edges():
        source_idx = node_names.index(source)
        target_idx = node_names.index(target)
        edge_index.append([source_idx, target_idx])
    
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    
    # 3. æ ‡ç­¾ (å“ªä¸ªæœåŠ¡æ˜¯æ ¹å› )
    y = torch.tensor([features[node]['is_root_cause'] for node in node_names], 
                     dtype=torch.float)
    
    return Data(x=x, edge_index=edge_index, y=y), node_names


def train_gnn_rca_model(training_graphs, epochs=100):
    """è®­ç»ƒ GNN æ ¹å› åˆ†ææ¨¡å‹"""
    
    # æ•°æ®åŠ è½½å™¨
    loader = DataLoader(training_graphs, batch_size=16, shuffle=True)
    
    # æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = ServiceGraphRCAModel(
        node_feature_dim=training_graphs[0].x.shape[1],
        hidden_dim=64,
        num_layers=3
    ).to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = torch.nn.BCELoss()
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        
        for batch in loader:
            batch = batch.to(device)
            
            optimizer.zero_grad()
            out = model(batch)
            loss = criterion(out, batch.y)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(loader):.4f}")
    
    return model


# ä½¿ç”¨ç¤ºä¾‹: åœ¨çº¿æ ¹å› åˆ†æ
def online_gnn_rca(model, current_service_graph, current_features):
    """åœ¨çº¿ GNN æ ¹å› åˆ†æ"""
    
    # å‡†å¤‡æ•°æ®
    data, node_names = prepare_service_graph_data(
        current_service_graph,
        current_features
    )
    
    # æ¨ç†
    model.eval()
    with torch.no_grad():
        root_cause_probs = model(data).numpy()
    
    # æ’åº
    results = list(zip(node_names, root_cause_probs))
    results.sort(key=lambda x: x[1], reverse=True)
    
    print("ğŸ” GNN æ ¹å› åˆ†æç»“æœ:")
    for service, prob in results[:5]:
        print(f"  {service}: {prob:.2%}")
    
    return results


# ç¤ºä¾‹
if __name__ == '__main__':
    # ä» Neo4j åŠ è½½æœåŠ¡ä¾èµ–å›¾
    import neo4j
    
    driver = neo4j.GraphDatabase.driver("bolt://localhost:7687")
    
    with driver.session() as session:
        result = session.run("""
            MATCH (s:Service)-[r:CALLS]->(t:Service)
            RETURN s.name AS source, t.name AS target
        """)
        
        service_graph = nx.DiGraph()
        for record in result:
            service_graph.add_edge(record['source'], record['target'])
    
    # è·å–å®æ—¶ç‰¹å¾
    current_features = {}  # ä» TimescaleDB æŸ¥è¯¢
    
    # GNN æ¨ç†
    # online_gnn_rca(model, service_graph, current_features)
```

---

## ç¬¬å››éƒ¨åˆ†: å†³ç­–ä¸æ‰§è¡Œå±‚

### 4.1 å†³ç­–å¼•æ“æ¶æ„

#### 4.1.1 è§„åˆ™å¼•æ“ + AI å†³ç­–èåˆ

```python
# decision_engine.py - æ™ºèƒ½å†³ç­–å¼•æ“

from typing import Dict, List, Optional
from enum import Enum
import json

class ActionType(Enum):
    """è¡ŒåŠ¨ç±»å‹"""
    ALERT = "alert"              # å‘Šè­¦
    AUTO_SCALE = "auto_scale"    # è‡ªåŠ¨æ‰©ç¼©å®¹
    RESTART = "restart"          # é‡å¯
    ROLLBACK = "rollback"        # å›æ»š
    CIRCUIT_BREAK = "circuit_break"  # ç†”æ–­
    RATE_LIMIT = "rate_limit"    # é™æµ
    MANUAL_INTERVENTION = "manual_intervention"  # äººå·¥ä»‹å…¥


class DecisionEngine:
    """æ™ºèƒ½å†³ç­–å¼•æ“"""
    
    def __init__(self, rule_config_path: str):
        """
        Args:
            rule_config_path: è§„åˆ™é…ç½®æ–‡ä»¶è·¯å¾„
        """
        with open(rule_config_path, 'r') as f:
            self.rules = json.load(f)['rules']
        
        self.action_history = []  # è¡ŒåŠ¨å†å²
    
    def make_decision(
        self,
        anomaly: Dict,
        root_cause: Dict,
        context: Dict
    ) -> Dict:
        """
        ç»¼åˆå†³ç­–
        
        Args:
            anomaly: å¼‚å¸¸ä¿¡æ¯
            root_cause: æ ¹å› åˆ†æç»“æœ
            context: ä¸Šä¸‹æ–‡ (å†å²è¡ŒåŠ¨ã€ç³»ç»ŸçŠ¶æ€ç­‰)
        
        Returns:
            å†³ç­–ç»“æœ (åŒ…å«è¡ŒåŠ¨ç±»å‹ã€å‚æ•°ã€ç½®ä¿¡åº¦ç­‰)
        """
        
        # 1. åŸºäºè§„åˆ™çš„å†³ç­–
        rule_decision = self._rule_based_decision(anomaly, root_cause)
        
        # 2. åŸºäºå†å²çš„å†³ç­– (ä»è¿‡å»å­¦ä¹ )
        historical_decision = self._historical_decision(anomaly, root_cause)
        
        # 3. èåˆå†³ç­–
        final_decision = self._merge_decisions(
            rule_decision,
            historical_decision,
            context
        )
        
        # 4. é£é™©è¯„ä¼°
        risk_score = self._assess_risk(final_decision, context)
        
        # 5. å†³ç­–å®¡æ‰¹ (é«˜é£é™©éœ€äººå·¥å®¡æ‰¹)
        if risk_score > 0.7:
            final_decision['requires_approval'] = True
            final_decision['risk_score'] = risk_score
        
        # 6. è®°å½•å†³ç­–
        self._log_decision(anomaly, root_cause, final_decision)
        
        return final_decision
    
    def _rule_based_decision(self, anomaly, root_cause) -> Dict:
        """åŸºäºè§„åˆ™çš„å†³ç­–"""
        
        severity = anomaly.get('severity', 'medium')
        anomaly_type = anomaly.get('type', 'unknown')
        root_cause_type = root_cause.get('type', 'unknown')
        
        # éå†è§„åˆ™
        for rule in self.rules:
            conditions = rule['conditions']
            
            # æ£€æŸ¥æ¡ä»¶æ˜¯å¦åŒ¹é…
            if (conditions.get('severity') == severity and
                conditions.get('anomaly_type') == anomaly_type and
                conditions.get('root_cause_type') == root_cause_type):
                
                return {
                    'action': ActionType[rule['action']],
                    'params': rule.get('params', {}),
                    'confidence': 0.9,  # è§„åˆ™åŒ¹é…çš„ç½®ä¿¡åº¦
                    'source': 'rule_based'
                }
        
        # é»˜è®¤è¡ŒåŠ¨: å‘Šè­¦ + äººå·¥ä»‹å…¥
        return {
            'action': ActionType.MANUAL_INTERVENTION,
            'params': {},
            'confidence': 0.5,
            'source': 'default'
        }
    
    def _historical_decision(self, anomaly, root_cause) -> Optional[Dict]:
        """åŸºäºå†å²çš„å†³ç­– (ä»è¿‡å»å­¦ä¹ )"""
        
        # æŸ¥è¯¢å†å²ç›¸ä¼¼æ¡ˆä¾‹
        similar_cases = self._find_similar_cases(anomaly, root_cause)
        
        if not similar_cases:
            return None
        
        # ç»Ÿè®¡æœ€æˆåŠŸçš„è¡ŒåŠ¨
        action_success_rate = {}
        for case in similar_cases:
            action = case['action']
            success = case['success']
            
            if action not in action_success_rate:
                action_success_rate[action] = {'success': 0, 'total': 0}
            
            action_success_rate[action]['total'] += 1
            if success:
                action_success_rate[action]['success'] += 1
        
        # é€‰æ‹©æˆåŠŸç‡æœ€é«˜çš„è¡ŒåŠ¨
        best_action = max(
            action_success_rate.items(),
            key=lambda x: x[1]['success'] / x[1]['total']
        )
        
        action_name, stats = best_action
        confidence = stats['success'] / stats['total']
        
        return {
            'action': ActionType[action_name],
            'params': similar_cases[0].get('params', {}),  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¡ˆä¾‹çš„å‚æ•°
            'confidence': confidence,
            'source': 'historical',
            'similar_cases_count': len(similar_cases)
        }
    
    def _merge_decisions(self, rule_decision, historical_decision, context) -> Dict:
        """èåˆå¤šä¸ªå†³ç­–"""
        
        # å¦‚æœå†å²å†³ç­–ä¸å­˜åœ¨æˆ–ç½®ä¿¡åº¦ä½,ä½¿ç”¨è§„åˆ™å†³ç­–
        if not historical_decision or historical_decision['confidence'] < 0.7:
            return rule_decision
        
        # å¦‚æœå†å²å†³ç­–ç½®ä¿¡åº¦é«˜,ä½¿ç”¨å†å²å†³ç­–
        if historical_decision['confidence'] > 0.9:
            return historical_decision
        
        # å¦åˆ™,åŠ æƒèåˆ
        # è¿™é‡Œç®€åŒ–å¤„ç†,å®é™…å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„èåˆç­–ç•¥
        if rule_decision['action'] == historical_decision['action']:
            # ç›¸åŒè¡ŒåŠ¨,æé«˜ç½®ä¿¡åº¦
            return {
                **rule_decision,
                'confidence': (rule_decision['confidence'] + historical_decision['confidence']) / 2,
                'source': 'merged'
            }
        else:
            # ä¸åŒè¡ŒåŠ¨,é€‰æ‹©ç½®ä¿¡åº¦æ›´é«˜çš„
            return rule_decision if rule_decision['confidence'] > historical_decision['confidence'] else historical_decision
    
    def _assess_risk(self, decision: Dict, context: Dict) -> float:
        """è¯„ä¼°å†³ç­–é£é™©"""
        
        action = decision['action']
        
        # é£é™©è¯„åˆ† (0-1)
        risk_scores = {
            ActionType.ALERT: 0.1,
            ActionType.RATE_LIMIT: 0.3,
            ActionType.CIRCUIT_BREAK: 0.4,
            ActionType.AUTO_SCALE: 0.5,
            ActionType.RESTART: 0.7,
            ActionType.ROLLBACK: 0.8,
            ActionType.MANUAL_INTERVENTION: 0.2,
        }
        
        base_risk = risk_scores.get(action, 0.5)
        
        # æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´é£é™©
        # 1. ä¸šåŠ¡é«˜å³°æœŸé£é™©æ›´é«˜
        if context.get('is_peak_hour', False):
            base_risk *= 1.3
        
        # 2. æœ€è¿‘æœ‰è¿‡å¤±è´¥çš„è‡ªåŠ¨è¡ŒåŠ¨
        recent_failures = context.get('recent_action_failures', 0)
        base_risk *= (1 + recent_failures * 0.1)
        
        # 3. ç½®ä¿¡åº¦ä½é£é™©æ›´é«˜
        confidence = decision.get('confidence', 0.5)
        base_risk *= (2 - confidence)
        
        return min(base_risk, 1.0)
    
    def _log_decision(self, anomaly, root_cause, decision):
        """è®°å½•å†³ç­– (ç”¨äºå®¡è®¡å’Œå­¦ä¹ )"""
        
        record = {
            'timestamp': anomaly['timestamp'],
            'anomaly': anomaly,
            'root_cause': root_cause,
            'decision': decision,
        }
        
        self.action_history.append(record)
        
        # TODO: æŒä¹…åŒ–åˆ°æ•°æ®åº“
    
    def _find_similar_cases(self, anomaly, root_cause) -> List[Dict]:
        """æŸ¥æ‰¾å†å²ç›¸ä¼¼æ¡ˆä¾‹"""
        
        # TODO: ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœç´¢
        # è¿™é‡Œç®€åŒ–å¤„ç†,ä»…åŒ¹é…ç±»å‹
        similar = []
        for record in self.action_history:
            if (record['anomaly'].get('type') == anomaly.get('type') and
                record['root_cause'].get('type') == root_cause.get('type')):
                similar.append(record)
        
        return similar[-10:]  # è¿”å›æœ€è¿‘10ä¸ªç›¸ä¼¼æ¡ˆä¾‹


# è§„åˆ™é…ç½®ç¤ºä¾‹ (JSON)
RULE_CONFIG_EXAMPLE = """
{
  "rules": [
    {
      "name": "æ•°æ®åº“CPUè¿‡é«˜è‡ªåŠ¨æ‰©å®¹",
      "conditions": {
        "severity": "critical",
        "anomaly_type": "high_latency",
        "root_cause_type": "database_cpu_high"
      },
      "action": "AUTO_SCALE",
      "params": {
        "resource": "database",
        "scale_factor": 1.5,
        "max_instances": 10
      }
    },
    {
      "name": "å†…å­˜æ³„æ¼è‡ªåŠ¨é‡å¯",
      "conditions": {
        "severity": "high",
        "anomaly_type": "oom_risk",
        "root_cause_type": "memory_leak"
      },
      "action": "RESTART",
      "params": {
        "service": "auto_detect",
        "graceful_shutdown": true,
        "timeout_seconds": 30
      }
    },
    {
      "name": "ä¸‹æ¸¸æœåŠ¡å¼‚å¸¸ç†”æ–­",
      "conditions": {
        "severity": "high",
        "anomaly_type": "high_error_rate",
        "root_cause_type": "downstream_failure"
      },
      "action": "CIRCUIT_BREAK",
      "params": {
        "target_service": "auto_detect",
        "duration_seconds": 300
      }
    }
  ]
}
"""
```

#### 4.1.2 å®¡æ‰¹æµç¨‹

```python
# approval_workflow.py - å®¡æ‰¹å·¥ä½œæµ

from enum import Enum
from datetime import datetime, timedelta

class ApprovalStatus(Enum):
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    TIMEOUT = "timeout"
    AUTO_APPROVED = "auto_approved"


class ApprovalWorkflow:
    """å®¡æ‰¹å·¥ä½œæµ"""
    
    def __init__(self, timeout_minutes=30):
        self.timeout = timedelta(minutes=timeout_minutes)
        self.pending_approvals = {}
    
    def request_approval(
        self,
        decision: Dict,
        anomaly: Dict,
        root_cause: Dict,
        requester: str = "aiops_system"
    ) -> str:
        """
        è¯·æ±‚å®¡æ‰¹
        
        Returns:
            approval_id: å®¡æ‰¹ID
        """
        
        approval_id = f"approval_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        self.pending_approvals[approval_id] = {
            'decision': decision,
            'anomaly': anomaly,
            'root_cause': root_cause,
            'requester': requester,
            'status': ApprovalStatus.PENDING,
            'created_at': datetime.now(),
            'expires_at': datetime.now() + self.timeout,
        }
        
        # å‘é€é€šçŸ¥ (Slack/Email/PagerDuty)
        self._send_approval_notification(approval_id)
        
        return approval_id
    
    def approve(self, approval_id: str, approver: str, comment: str = ""):
        """æ‰¹å‡†"""
        
        if approval_id not in self.pending_approvals:
            raise ValueError(f"Approval {approval_id} not found")
        
        approval = self.pending_approvals[approval_id]
        
        if approval['status'] != ApprovalStatus.PENDING:
            raise ValueError(f"Approval {approval_id} already processed")
        
        approval['status'] = ApprovalStatus.APPROVED
        approval['approver'] = approver
        approval['comment'] = comment
        approval['approved_at'] = datetime.now()
        
        # æ‰§è¡Œè¡ŒåŠ¨
        self._execute_action(approval['decision'])
    
    def reject(self, approval_id: str, approver: str, reason: str):
        """æ‹’ç»"""
        
        if approval_id not in self.pending_approvals:
            raise ValueError(f"Approval {approval_id} not found")
        
        approval = self.pending_approvals[approval_id]
        
        if approval['status'] != ApprovalStatus.PENDING:
            raise ValueError(f"Approval {approval_id} already processed")
        
        approval['status'] = ApprovalStatus.REJECTED
        approval['approver'] = approver
        approval['reason'] = reason
        approval['rejected_at'] = datetime.now()
    
    def check_timeouts(self):
        """æ£€æŸ¥è¶…æ—¶çš„å®¡æ‰¹"""
        
        now = datetime.now()
        
        for approval_id, approval in self.pending_approvals.items():
            if approval['status'] == ApprovalStatus.PENDING and now > approval['expires_at']:
                approval['status'] = ApprovalStatus.TIMEOUT
                
                # é»˜è®¤è¡ŒåŠ¨: ä¸æ‰§è¡Œ
                # æˆ–è€…æ ¹æ®é…ç½®è‡ªåŠ¨å®¡æ‰¹
                # approval['status'] = ApprovalStatus.AUTO_APPROVED
                # self._execute_action(approval['decision'])
    
    def _send_approval_notification(self, approval_id: str):
        """å‘é€å®¡æ‰¹é€šçŸ¥"""
        
        approval = self.pending_approvals[approval_id]
        
        message = f"""
ğŸš¨ éœ€è¦å®¡æ‰¹: {approval_id}

**å¼‚å¸¸ä¿¡æ¯**:
- æœåŠ¡: {approval['anomaly'].get('service')}
- ä¸¥é‡ç¨‹åº¦: {approval['anomaly'].get('severity')}
- ç±»å‹: {approval['anomaly'].get('type')}

**æ ¹å› **:
{approval['root_cause'].get('explanation', 'Unknown')}

**å»ºè®®è¡ŒåŠ¨**:
- ç±»å‹: {approval['decision']['action'].name}
- å‚æ•°: {json.dumps(approval['decision'].get('params', {}), indent=2)}
- é£é™©è¯„åˆ†: {approval['decision'].get('risk_score', 0):.2f}

**å®¡æ‰¹é“¾æ¥**:
https://aiops.example.com/approvals/{approval_id}

**è¿‡æœŸæ—¶é—´**: {approval['expires_at'].strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        # TODO: å‘é€åˆ° Slack/Email/PagerDuty
        print(message)
    
    def _execute_action(self, decision: Dict):
        """æ‰§è¡Œè¡ŒåŠ¨"""
        # å§”æ‰˜ç»™ ActionExecutor
        pass
```

### 4.2 è¡ŒåŠ¨æ‰§è¡Œå™¨

```python
# action_executor.py - è¡ŒåŠ¨æ‰§è¡Œå™¨

import subprocess
import requests
from kubernetes import client, config

class ActionExecutor:
    """è¡ŒåŠ¨æ‰§è¡Œå™¨"""
    
    def __init__(self):
        """
        åˆå§‹åŒ–è¡ŒåŠ¨æ‰§è¡Œå™¨
        
        Raises:
            RuntimeError: å¦‚æœ Kubernetes é…ç½®åŠ è½½å¤±è´¥
        """
        import logging
        
        self.logger = logging.getLogger(__name__)
        
        try:
            # å°è¯•åŠ è½½é›†ç¾¤å†…é…ç½®
            config.load_incluster_config()
            self.logger.info("Loaded in-cluster Kubernetes config")
        except Exception as e1:
            try:
                # å›é€€åˆ° kubeconfig
                config.load_kube_config()
                self.logger.info("Loaded kubeconfig")
            except Exception as e2:
                self.logger.error(f"Failed to load Kubernetes config: in-cluster={e1}, kubeconfig={e2}")
                raise RuntimeError("Failed to initialize Kubernetes client") from e2
        
        try:
            self.k8s_apps = client.AppsV1Api()
            self.k8s_core = client.CoreV1Api()
            self.logger.info("Kubernetes API clients initialized")
        except Exception as e:
            self.logger.error(f"Failed to create Kubernetes API clients: {e}")
            raise
    
    def execute(self, action_type: ActionType, params: Dict) -> Dict:
        """
        æ‰§è¡Œè¡ŒåŠ¨
        
        Args:
            action_type: è¡ŒåŠ¨ç±»å‹
            params: è¡ŒåŠ¨å‚æ•°
        
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸,åŒ…å« success å’Œå…¶ä»–å­—æ®µ
        """
        from kubernetes.client.rest import ApiException
        
        # éªŒè¯å‚æ•°
        if not params:
            return {'success': False, 'error': 'params cannot be empty'}
        
        handlers = {
            ActionType.AUTO_SCALE: self._auto_scale,
            ActionType.RESTART: self._restart,
            ActionType.ROLLBACK: self._rollback,
            ActionType.CIRCUIT_BREAK: self._circuit_break,
            ActionType.RATE_LIMIT: self._rate_limit,
            ActionType.ALERT: self._send_alert,
        }
        
        handler = handlers.get(action_type)
        
        if not handler:
            error_msg = f'Unknown action type: {action_type}'
            self.logger.error(error_msg)
            return {'success': False, 'error': error_msg}
        
        try:
            self.logger.info(f"Executing action: {action_type}, params: {params}")
            
            result = handler(params)
            
            # è®°å½•æ‰§è¡Œç»“æœ
            self._log_execution(action_type, params, result)
            
            if result.get('success'):
                self.logger.info(f"Action succeeded: {action_type}")
            else:
                self.logger.warning(f"Action failed: {action_type}, error: {result.get('error')}")
            
            return result
        
        except ApiException as e:
            error_msg = f"Kubernetes API error: {e.status} - {e.reason}"
            self.logger.error(f"Action failed with API exception: {error_msg}")
            return {
                'success': False,
                'error': error_msg,
                'details': e.body
            }
        
        except Exception as e:
            error_msg = str(e)
            self.logger.error(f"Action failed with exception: {error_msg}", exc_info=True)
            return {
                'success': False,
                'error': error_msg
            }
    
    def _auto_scale(self, params: Dict) -> Dict:
        """
        è‡ªåŠ¨æ‰©ç¼©å®¹
        
        Args:
            params: åŒ…å« deployment, scale_factor, max_replicas, namespace
        
        Returns:
            æ“ä½œç»“æœ
        """
        from kubernetes.client.rest import ApiException
        
        # å‚æ•°éªŒè¯
        deployment_name = params.get('deployment')
        if not deployment_name:
            return {'success': False, 'error': 'deployment name required'}
        
        namespace = params.get('namespace', 'default')
        scale_factor = params.get('scale_factor', 1.5)
        max_replicas = params.get('max_replicas', 10)
        min_replicas = params.get('min_replicas', 1)
        
        # éªŒè¯å‚æ•°èŒƒå›´
        if not (0.1 <= scale_factor <= 10):
            return {'success': False, 'error': 'scale_factor must be between 0.1 and 10'}
        
        if not (1 <= max_replicas <= 1000):
            return {'success': False, 'error': 'max_replicas must be between 1 and 1000'}
        
        try:
            # è·å–å½“å‰ Deployment
            deployment = self.k8s_apps.read_namespaced_deployment(
                name=deployment_name,
                namespace=namespace
            )
            
            current_replicas = deployment.spec.replicas or 1
            new_replicas = int(current_replicas * scale_factor)
            
            # é™åˆ¶å‰¯æœ¬æ•°èŒƒå›´
            new_replicas = max(min_replicas, min(new_replicas, max_replicas))
            
            # å¦‚æœå‰¯æœ¬æ•°ä¸å˜,è·³è¿‡
            if new_replicas == current_replicas:
                return {
                    'success': True,
                    'current_replicas': current_replicas,
                    'new_replicas': new_replicas,
                    'message': f'No scaling needed: already at {current_replicas} replicas'
                }
            
            # æ›´æ–°å‰¯æœ¬æ•°
            deployment.spec.replicas = new_replicas
            self.k8s_apps.patch_namespaced_deployment(
                name=deployment_name,
                namespace=namespace,
                body=deployment
            )
            
            self.logger.info(
                f"Scaled {namespace}/{deployment_name}: {current_replicas} â†’ {new_replicas}"
            )
            
            return {
                'success': True,
                'current_replicas': current_replicas,
                'new_replicas': new_replicas,
                'message': f'Scaled {deployment_name} from {current_replicas} to {new_replicas} replicas'
            }
        
        except ApiException as e:
            if e.status == 404:
                error_msg = f"Deployment not found: {namespace}/{deployment_name}"
            else:
                error_msg = f"K8s API error: {e.reason}"
            
            self.logger.error(error_msg)
            return {'success': False, 'error': error_msg}
        
        except Exception as e:
            error_msg = f"Scaling failed: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            return {'success': False, 'error': error_msg}
    
    def _restart(self, params: Dict) -> Dict:
        """é‡å¯æœåŠ¡"""
        
        namespace = params.get('namespace', 'default')
        deployment_name = params.get('deployment')
        graceful = params.get('graceful_shutdown', True)
        
        # æ–¹å¼1: æ»šåŠ¨é‡å¯ (æ¨è)
        if graceful:
            # ä¿®æ”¹ Pod æ¨¡æ¿çš„æ³¨è§£,è§¦å‘æ»šåŠ¨é‡å¯
            deployment = self.k8s_apps.read_namespaced_deployment(
                name=deployment_name,
                namespace=namespace
            )
            
            if not deployment.spec.template.metadata.annotations:
                deployment.spec.template.metadata.annotations = {}
            
            deployment.spec.template.metadata.annotations['kubectl.kubernetes.io/restartedAt'] = \
                datetime.now().isoformat()
            
            self.k8s_apps.patch_namespaced_deployment(
                name=deployment_name,
                namespace=namespace,
                body=deployment
            )
            
            return {
                'success': True,
                'method': 'rolling_restart',
                'message': f'Initiated rolling restart for {deployment_name}'
            }
        
        # æ–¹å¼2: å¼ºåˆ¶é‡å¯ (ä¸æ¨è)
        else:
            # åˆ é™¤æ‰€æœ‰ Pod
            pods = self.k8s_core.list_namespaced_pod(
                namespace=namespace,
                label_selector=f'app={deployment_name}'
            )
            
            for pod in pods.items:
                self.k8s_core.delete_namespaced_pod(
                    name=pod.metadata.name,
                    namespace=namespace,
                    grace_period_seconds=0
                )
            
            return {
                'success': True,
                'method': 'force_restart',
                'message': f'Force restarted all pods for {deployment_name}'
            }
    
    def _rollback(self, params: Dict) -> Dict:
        """å›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬"""
        
        namespace = params.get('namespace', 'default')
        deployment_name = params.get('deployment')
        revision = params.get('revision')  # å¦‚æœæŒ‡å®š,å›æ»šåˆ°ç‰¹å®šç‰ˆæœ¬
        
        # ä½¿ç”¨ kubectl rollout undo
        cmd = ['kubectl', 'rollout', 'undo', f'deployment/{deployment_name}', '-n', namespace]
        
        if revision:
            cmd.extend(['--to-revision', str(revision)])
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        return {
            'success': result.returncode == 0,
            'stdout': result.stdout,
            'stderr': result.stderr,
            'message': f'Rolled back {deployment_name}'
        }
    
    def _circuit_break(self, params: Dict) -> Dict:
        """ç†”æ–­ (Istio DestinationRule)"""
        
        namespace = params.get('namespace', 'default')
        target_service = params.get('target_service')
        duration_seconds = params.get('duration_seconds', 300)
        
        # åˆ›å»º/æ›´æ–° Istio DestinationRule
        destination_rule = {
            'apiVersion': 'networking.istio.io/v1beta1',
            'kind': 'DestinationRule',
            'metadata': {
                'name': f'{target_service}-circuit-breaker',
                'namespace': namespace
            },
            'spec': {
                'host': target_service,
                'trafficPolicy': {
                    'outlierDetection': {
                        'consecutiveErrors': 1,
                        'interval': '1s',
                        'baseEjectionTime': f'{duration_seconds}s',
                        'maxEjectionPercent': 100
                    }
                }
            }
        }
        
        # åº”ç”¨é…ç½® (ä½¿ç”¨ kubectl apply)
        import tempfile
        import yaml
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
            yaml.dump(destination_rule, f)
            temp_file = f.name
        
        result = subprocess.run(
            ['kubectl', 'apply', '-f', temp_file],
            capture_output=True,
            text=True
        )
        
        return {
            'success': result.returncode == 0,
            'message': f'Enabled circuit breaker for {target_service} for {duration_seconds}s'
        }
    
    def _rate_limit(self, params: Dict) -> Dict:
        """é™æµ (Istio EnvoyFilter)"""
        
        # ç±»ä¼¼ circuit_break,ä½¿ç”¨ Istio EnvoyFilter é…ç½®é™æµ
        # è¿™é‡Œç®€åŒ–å¤„ç†
        
        return {
            'success': True,
            'message': 'Rate limit applied'
        }
    
    def _send_alert(self, params: Dict) -> Dict:
        """å‘é€å‘Šè­¦"""
        
        channel = params.get('channel', 'slack')
        message = params.get('message')
        severity = params.get('severity', 'warning')
        
        if channel == 'slack':
            webhook_url = params.get('webhook_url', 'https://hooks.slack.com/services/...')
            
            payload = {
                'text': message,
                'attachments': [{
                    'color': 'danger' if severity == 'critical' else 'warning',
                    'fields': [
                        {'title': 'Severity', 'value': severity, 'short': True},
                        {'title': 'Timestamp', 'value': datetime.now().isoformat(), 'short': True}
                    ]
                }]
            }
            
            response = requests.post(webhook_url, json=payload)
            
            return {
                'success': response.status_code == 200,
                'message': 'Alert sent to Slack'
            }
        
        return {'success': False, 'error': f'Unknown channel: {channel}'}
    
    def _log_execution(self, action_type, params, result):
        """è®°å½•æ‰§è¡Œç»“æœ"""
        
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'action_type': action_type.name,
            'params': params,
            'result': result
        }
        
        # TODO: æŒä¹…åŒ–åˆ°æ•°æ®åº“
        print(f"[ActionExecutor] {json.dumps(log_entry, indent=2)}")
```

---

## ç¬¬äº”éƒ¨åˆ†: æ¨¡å‹è®­ç»ƒä¸ MLOps

### 5.1 ç¦»çº¿æ¨¡å‹è®­ç»ƒæµç¨‹

```python
# model_training_pipeline.py - æ¨¡å‹è®­ç»ƒç®¡é“

import mlflow
import mlflow.pytorch
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

class ModelTrainingPipeline:
    """æ¨¡å‹è®­ç»ƒç®¡é“"""
    
    def __init__(self, mlflow_tracking_uri="http://mlflow:5000"):
        """
        åˆå§‹åŒ–è®­ç»ƒç®¡é“
        
        Args:
            mlflow_tracking_uri: MLflow è¿½è¸ªæœåŠ¡å™¨ URI
        
        Raises:
            ConnectionError: å¦‚æœæ— æ³•è¿æ¥åˆ° MLflow
        """
        import logging
        
        self.logger = logging.getLogger(__name__)
        
        try:
            mlflow.set_tracking_uri(mlflow_tracking_uri)
            
            # éªŒè¯è¿æ¥
            try:
                mlflow.get_tracking_uri()
                mlflow.set_experiment("aiops-anomaly-detection")
                self.logger.info(f"Connected to MLflow: {mlflow_tracking_uri}")
            except Exception as e:
                self.logger.error(f"Failed to connect to MLflow: {e}")
                raise ConnectionError(f"MLflow connection failed: {e}") from e
        
        except Exception as e:
            self.logger.error(f"Initialization failed: {e}")
            raise
    
    def train_anomaly_detector(
        self,
        training_data_query: str,
        test_size=0.2,
        model_name="anomaly_detector_v1",
        conn=None
    ):
        """
        è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹
        
        Args:
            training_data_query: SQL æŸ¥è¯¢è¯­å¥
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            model_name: æ¨¡å‹åç§°
            conn: æ•°æ®åº“è¿æ¥
        
        Raises:
            ValueError: å¦‚æœæ•°æ®æ— æ•ˆ
            RuntimeError: å¦‚æœè®­ç»ƒå¤±è´¥
        """
        if not conn:
            raise ValueError("Database connection required")
        
        try:
            with mlflow.start_run(run_name=model_name):
                # 1. åŠ è½½æ•°æ®
                try:
                    df = pd.read_sql(training_data_query, conn)
                    self.logger.info(f"Loaded {len(df)} training samples")
                except Exception as e:
                    self.logger.error(f"Failed to load training data: {e}")
                    raise RuntimeError(f"Data loading failed: {e}") from e
                
                # éªŒè¯æ•°æ®
                if df.empty:
                    raise ValueError("Training data is empty")
                
                if len(df) < 100:
                    self.logger.warning(f"Small dataset: only {len(df)} samples")
                
                mlflow.log_param("data_size", len(df))
                mlflow.log_param("test_size", test_size)
            
            # 2. ç‰¹å¾å·¥ç¨‹
            X = df.drop(columns=['is_anomaly', 'timestamp'])
            y = df['is_anomaly']
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=42, stratify=y
            )
            
            # 3. è®­ç»ƒæ¨¡å‹
            detector = AnomalyDetector(contamination=0.01)
            detector.train(pd.concat([X_train, y_train], axis=1))
            
            # 4. è¯„ä¼°æ¨¡å‹
            y_pred = detector.predict(X_test)
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = (y_pred == y_test).mean()
            report = classification_report(y_test, y_pred, output_dict=True)
            cm = confusion_matrix(y_test, y_pred)
            
            # è®°å½•æŒ‡æ ‡
            mlflow.log_metric("accuracy", accuracy)
            mlflow.log_metric("precision", report['1']['precision'])
            mlflow.log_metric("recall", report['1']['recall'])
            mlflow.log_metric("f1_score", report['1']['f1-score'])
            
            # è®°å½•æ··æ·†çŸ©é˜µ
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            plt.title('Confusion Matrix')
            mlflow.log_figure(plt.gcf(), "confusion_matrix.png")
            
            # 5. ä¿å­˜æ¨¡å‹
            mlflow.sklearn.log_model(detector.model, "model")
            
            # ä¿å­˜åˆ°æ¨¡å‹æ³¨å†Œä¸­å¿ƒ
            model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
            mlflow.register_model(model_uri, model_name)
            
            print(f"âœ… Model trained and registered: {model_name}")
            print(f"   Accuracy: {accuracy:.4f}")
            print(f"   F1 Score: {report['1']['f1-score']:.4f}")
    
    def train_rca_model(
        self,
        training_graphs: List,
        model_name="rca_gnn_v1"
    ):
        """è®­ç»ƒæ ¹å› åˆ†ææ¨¡å‹"""
        
        with mlflow.start_run(run_name=model_name):
            # è®­ç»ƒ GNN æ¨¡å‹
            model = train_gnn_rca_model(training_graphs, epochs=100)
            
            # ä¿å­˜æ¨¡å‹
            mlflow.pytorch.log_model(model, "model")
            
            # æ³¨å†Œæ¨¡å‹
            model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
            mlflow.register_model(model_uri, model_name)
            
            print(f"âœ… RCA model trained and registered: {model_name}")
```

### 5.2 æ¨¡å‹éƒ¨ç½²ä¸åœ¨çº¿æœåŠ¡

```python
# model_serving.py - æ¨¡å‹æœåŠ¡

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import mlflow.pyfunc

app = FastAPI()

# åŠ è½½æ¨¡å‹
anomaly_detector = mlflow.pyfunc.load_model("models:/anomaly_detector/production")
rca_model = mlflow.pytorch.load_model("models:/rca_gnn/production")

class PredictionRequest(BaseModel):
    features: Dict[str, float]

class PredictionResponse(BaseModel):
    is_anomaly: bool
    anomaly_score: float
    confidence: float

@app.post("/predict/anomaly", response_model=PredictionResponse)
async def predict_anomaly(request: PredictionRequest):
    """å¼‚å¸¸æ£€æµ‹æ¨ç†æ¥å£"""
    
    try:
        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame([request.features])
        
        # æ¨ç†
        prediction = anomaly_detector.predict(df)
        score = anomaly_detector.predict_proba(df)[0][1]
        
        return PredictionResponse(
            is_anomaly=bool(prediction[0]),
            anomaly_score=float(score),
            confidence=float(abs(score - 0.5) * 2)  # 0.5 é™„è¿‘ç½®ä¿¡åº¦ä½
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict/root_cause")
async def predict_root_cause(service_graph: Dict, features: Dict):
    """æ ¹å› åˆ†ææ¨ç†æ¥å£"""
    
    try:
        # å‡†å¤‡æ•°æ®
        data, node_names = prepare_service_graph_data(service_graph, features)
        
        # GNN æ¨ç†
        rca_model.eval()
        with torch.no_grad():
            root_cause_probs = rca_model(data).numpy()
        
        # æ’åº
        results = list(zip(node_names, root_cause_probs))
        results.sort(key=lambda x: x[1], reverse=True)
        
        return {
            'root_causes': [
                {'service': name, 'probability': float(prob)}
                for name, prob in results[:5]
            ]
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# å¯åŠ¨æœåŠ¡
if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 5.3 æ¨¡å‹ç›‘æ§ä¸æŒç»­æ”¹è¿›

```python
# model_monitoring.py - æ¨¡å‹ç›‘æ§

class ModelMonitor:
    """æ¨¡å‹ç›‘æ§"""
    
    def __init__(self):
        self.prediction_history = []
        self.feedback_history = []
    
    def log_prediction(self, input_data, prediction, model_version):
        """è®°å½•é¢„æµ‹"""
        
        self.prediction_history.append({
            'timestamp': datetime.now(),
            'input': input_data,
            'prediction': prediction,
            'model_version': model_version
        })
    
    def log_feedback(self, prediction_id, actual_label, feedback_source="human"):
        """è®°å½•åé¦ˆ (ç”¨äºæ¨¡å‹è¯„ä¼°å’ŒæŒç»­æ”¹è¿›)"""
        
        self.feedback_history.append({
            'prediction_id': prediction_id,
            'actual_label': actual_label,
            'feedback_source': feedback_source,
            'timestamp': datetime.now()
        })
    
    def calculate_model_drift(self, window_days=7):
        """è®¡ç®—æ¨¡å‹æ¼‚ç§» (æ•°æ®åˆ†å¸ƒå˜åŒ–)"""
        
        # ä½¿ç”¨ KS æ£€éªŒæ£€æµ‹æ•°æ®åˆ†å¸ƒå˜åŒ–
        from scipy.stats import ks_2samp
        
        # è·å–æœ€è¿‘7å¤©å’Œä¹‹å‰7å¤©çš„æ•°æ®
        recent = self._get_recent_predictions(window_days)
        previous = self._get_previous_predictions(window_days, window_days)
        
        # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œ KS æ£€éªŒ
        drift_scores = {}
        for feature in recent.columns:
            statistic, p_value = ks_2samp(recent[feature], previous[feature])
            drift_scores[feature] = {
                'statistic': statistic,
                'p_value': p_value,
                'is_drifting': p_value < 0.05  # æ˜¾è‘—æ€§æ°´å¹³
            }
        
        return drift_scores
    
    def trigger_retraining(self, reason):
        """è§¦å‘æ¨¡å‹é‡è®­ç»ƒ"""
        
        print(f"ğŸ”„ Triggering model retraining: {reason}")
        
        # TODO: å¯åŠ¨è®­ç»ƒç®¡é“
        # training_pipeline.train_anomaly_detector(...)
```

---

## ç¬¬äº”éƒ¨åˆ†: MLOps ä¸æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†

### 5.1 æ¨¡å‹è®­ç»ƒç®¡é“

#### 5.1.1 å®Œæ•´è®­ç»ƒæµç¨‹

```python
# training_pipeline.py
import mlflow
import optuna
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import f1_score, precision_score, recall_score

class AnomalyDetectorTrainingPipeline:
    """å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒç®¡é“"""
    
    def __init__(self, mlflow_tracking_uri="http://mlflow:5000"):
        mlflow.set_tracking_uri(mlflow_tracking_uri)
        mlflow.set_experiment("otlp-anomaly-detection")
    
    def train(self, features_df, labels_df, model_type="isolation_forest"):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        
        with mlflow.start_run(run_name=f"{model_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
            
            # 1. è®°å½•å‚æ•°
            mlflow.log_params({
                'model_type': model_type,
                'n_samples': len(features_df),
                'n_features': features_df.shape[1],
                'training_date': datetime.now().isoformat()
            })
            
            # 2. ç‰¹å¾å·¥ç¨‹
            X = self.feature_engineering(features_df)
            y = labels_df['is_anomaly']
            
            # 3. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ (é¿å…æ•°æ®æ³„éœ²)
            tscv = TimeSeriesSplit(n_splits=5)
            cv_scores = []
            
            for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                
                # 4. è¶…å‚æ•°ä¼˜åŒ– (ä»…åœ¨ç¬¬ä¸€ä¸ª fold)
                if fold == 0:
                    best_params = self.hyperparameter_tuning(
                        X_train, y_train, model_type
                    )
                    mlflow.log_params(best_params)
                
                # 5. è®­ç»ƒæ¨¡å‹
                model = self.build_model(model_type, best_params)
                model.fit(X_train, y_train)
                
                # 6. éªŒè¯
                y_pred = model.predict(X_val)
                f1 = f1_score(y_val, y_pred)
                precision = precision_score(y_val, y_pred)
                recall = recall_score(y_val, y_pred)
                
                cv_scores.append({'f1': f1, 'precision': precision, 'recall': recall})
                
                print(f"Fold {fold+1}: F1={f1:.4f}, Precision={precision:.4f}, Recall={recall:.4f}")
            
            # 7. è®°å½•æœ€ç»ˆæŒ‡æ ‡ (å¹³å‡)
            avg_metrics = {
                'avg_f1': np.mean([s['f1'] for s in cv_scores]),
                'avg_precision': np.mean([s['precision'] for s in cv_scores]),
                'avg_recall': np.mean([s['recall'] for s in cv_scores]),
                'std_f1': np.std([s['f1'] for s in cv_scores])
            }
            mlflow.log_metrics(avg_metrics)
            
            # 8. è®­ç»ƒæœ€ç»ˆæ¨¡å‹ (ä½¿ç”¨å…¨éƒ¨æ•°æ®)
            final_model = self.build_model(model_type, best_params)
            final_model.fit(X, y)
            
            # 9. æ¨¡å‹ä¿å­˜
            mlflow.sklearn.log_model(final_model, "model")
            
            # 10. ä¿å­˜ç‰¹å¾é‡è¦æ€§ (å¦‚æœæ”¯æŒ)
            if hasattr(final_model, 'feature_importances_'):
                feature_importance_df = pd.DataFrame({
                    'feature': X.columns,
                    'importance': final_model.feature_importances_
                }).sort_values('importance', ascending=False)
                
                mlflow.log_table(feature_importance_df, "feature_importance.json")
            
            print(f"âœ… Training completed. Avg F1: {avg_metrics['avg_f1']:.4f}")
            
            return final_model, avg_metrics
    
    def hyperparameter_tuning(self, X_train, y_train, model_type):
        """ä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        
        def objective(trial):
            if model_type == 'isolation_forest':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                    'max_samples': trial.suggest_float('max_samples', 0.5, 1.0),
                    'contamination': trial.suggest_float('contamination', 0.01, 0.1),
                    'max_features': trial.suggest_float('max_features', 0.5, 1.0)
                }
            elif model_type == 'lstm':
                params = {
                    'hidden_size': trial.suggest_int('hidden_size', 32, 256),
                    'num_layers': trial.suggest_int('num_layers', 1, 3),
                    'dropout': trial.suggest_float('dropout', 0.1, 0.5),
                    'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
                }
            else:
                raise ValueError(f"Unknown model type: {model_type}")
            
            # è®­ç»ƒå¹¶è¯„ä¼°
            model = self.build_model(model_type, params)
            model.fit(X_train, y_train)
            y_pred = model.predict(X_train)
            
            return f1_score(y_train, y_pred)
        
        # ä¼˜åŒ–
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=50, timeout=3600)  # 1å°æ—¶è¶…æ—¶
        
        print(f"Best F1: {study.best_value:.4f}")
        print(f"Best params: {study.best_params}")
        
        return study.best_params
    
    def feature_engineering(self, df):
        """ç‰¹å¾å·¥ç¨‹"""
        
        # æ»šåŠ¨çª—å£ç»Ÿè®¡ç‰¹å¾
        window_sizes = [5, 15, 60]  # 5åˆ†é’Ÿ, 15åˆ†é’Ÿ, 1å°æ—¶
        
        for col in ['latency_p99', 'error_rate', 'request_rate']:
            for w in window_sizes:
                df[f'{col}_rolling_mean_{w}m'] = df[col].rolling(window=w).mean()
                df[f'{col}_rolling_std_{w}m'] = df[col].rolling(window=w).std()
                df[f'{col}_rolling_max_{w}m'] = df[col].rolling(window=w).max()
        
        # æ—¶é—´ç‰¹å¾
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        df['is_business_hour'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
        
        # æœåŠ¡ä¾èµ–ç‰¹å¾
        df['downstream_error_rate'] = df.groupby('service')['error_rate'].shift(1)
        
        # å¡«å……ç¼ºå¤±å€¼
        df = df.fillna(method='ffill').fillna(0)
        
        return df

### 5.2 æ¨¡å‹éƒ¨ç½²ç­–ç•¥

#### 5.2.1 A/B æµ‹è¯•ä¸é‡‘ä¸é›€å‘å¸ƒ

```yaml
# model_deployment.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: model-deployment-config
data:
  deployment_strategy.yaml: |
    # é‡‘ä¸é›€å‘å¸ƒé…ç½®
    canary:
      enabled: true
      initial_traffic: 5%  # åˆå§‹æµé‡
      increment: 10%       # æ¯æ¬¡å¢åŠ æµé‡
      interval: 2h         # å¢åŠ é—´éš”
      success_criteria:
        - metric: "prediction_latency_p99"
          threshold: 100    # æ¯«ç§’
          operator: "<"
        - metric: "prediction_accuracy"
          threshold: 0.85
          operator: ">"
      rollback_criteria:
        - metric: "error_rate"
          threshold: 0.05
          operator: ">"
    
    # A/B æµ‹è¯•é…ç½®
    ab_test:
      enabled: true
      duration: 7d
      groups:
        - name: "control"
          model_version: "v1.2.3"
          traffic: 50%
        - name: "treatment"
          model_version: "v1.3.0"
          traffic: 50%
      metrics:
        - "detection_rate"
        - "false_positive_rate"
        - "mean_time_to_detect"
```

#### 5.2.2 æ¨¡å‹æœåŠ¡åŒ– (TorchServe/TensorFlow Serving)

```python
# model_server.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import mlflow.pyfunc
import numpy as np

app = FastAPI(title="OTLP Anomaly Detection API")

# åŠ è½½æ¨¡å‹
model = mlflow.pyfunc.load_model("models:/anomaly-detector/Production")

class PredictionRequest(BaseModel):
    service_name: str
    timestamp: str
    latency_p99: float
    error_rate: float
    request_rate: float
    cpu_usage: float
    memory_usage: float

class PredictionResponse(BaseModel):
    is_anomaly: bool
    anomaly_score: float
    confidence: float
    explanation: dict

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """å®æ—¶å¼‚å¸¸æ£€æµ‹é¢„æµ‹"""
    
    try:
        # ç‰¹å¾å·¥ç¨‹
        features = np.array([[
            request.latency_p99,
            request.error_rate,
            request.request_rate,
            request.cpu_usage,
            request.memory_usage
        ]])
        
        # é¢„æµ‹
        prediction = model.predict(features)[0]
        anomaly_score = model.predict_proba(features)[0][1] if hasattr(model, 'predict_proba') else prediction
        
        # å¯è§£é‡Šæ€§ (SHAP)
        explanation = explain_prediction(features, model)
        
        return PredictionResponse(
            is_anomaly=bool(prediction),
            anomaly_score=float(anomaly_score),
            confidence=0.95,  # åŸºäºå†å²å‡†ç¡®ç‡
            explanation=explanation
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "model_version": "v1.3.0"}

@app.get("/metrics")
async def metrics():
    """Prometheus æŒ‡æ ‡"""
    return {
        "predictions_total": 1234567,
        "prediction_latency_p99_ms": 45,
        "model_accuracy": 0.92
    }
```

### 5.3 æ¨¡å‹ç›‘æ§ä¸å‘Šè­¦

#### 5.3.1 æ•°æ®è´¨é‡ç›‘æ§

```python
# data_quality_monitor.py
import great_expectations as ge

class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§"""
    
    def __init__(self):
        self.context = ge.DataContext()
    
    def validate_input_data(self, df):
        """éªŒè¯è¾“å…¥æ•°æ®è´¨é‡"""
        
        # åˆ›å»ºæ•°æ®æœŸæœ›å¥—ä»¶
        suite = self.context.get_expectation_suite("otlp_anomaly_input")
        
        # å®šä¹‰æœŸæœ›
        expectations = [
            # 1. åˆ—å­˜åœ¨æ€§
            {'expectation_type': 'expect_table_columns_to_match_ordered_list',
             'kwargs': {'column_list': ['timestamp', 'service_name', 'latency_p99', 'error_rate']}},
            
            # 2. æ•°å€¼èŒƒå›´
            {'expectation_type': 'expect_column_values_to_be_between',
             'kwargs': {'column': 'latency_p99', 'min_value': 0, 'max_value': 60000}},
            
            {'expectation_type': 'expect_column_values_to_be_between',
             'kwargs': {'column': 'error_rate', 'min_value': 0, 'max_value': 1}},
            
            # 3. éç©ºå€¼
            {'expectation_type': 'expect_column_values_to_not_be_null',
             'kwargs': {'column': 'timestamp'}},
            
            # 4. å”¯ä¸€æ€§
            {'expectation_type': 'expect_column_values_to_be_unique',
             'kwargs': {'column': 'timestamp'}},
            
            # 5. æ—¶é—´åºåˆ—è¿ç»­æ€§ (ä¸èƒ½æœ‰å¤§çš„é—´éš™)
            {'expectation_type': 'expect_column_values_to_be_increasing',
             'kwargs': {'column': 'timestamp'}}
        ]
        
        # éªŒè¯
        batch = ge.dataset.PandasDataset(df)
        results = batch.validate(expectation_suite=suite)
        
        if not results['success']:
            # å‘é€å‘Šè­¦
            self.alert_data_quality_issues(results)
            
            raise ValueError(f"Data quality validation failed: {results}")
        
        return True
    
    def alert_data_quality_issues(self, results):
        """æ•°æ®è´¨é‡é—®é¢˜å‘Šè­¦"""
        
        failed_expectations = [r for r in results['results'] if not r['success']]
        
        alert_message = f"""
        ğŸš¨ Data Quality Alert
        
        Failed Expectations: {len(failed_expectations)}
        
        Details:
        {json.dumps(failed_expectations, indent=2)}
        """
        
        # å‘é€åˆ° Slack/PagerDuty
        send_alert(alert_message, severity="high")
```

#### 5.3.2 æ¨¡å‹æ€§èƒ½ç›‘æ§

```python
# model_performance_monitor.py
from prometheus_client import Counter, Histogram, Gauge

# Prometheus æŒ‡æ ‡
prediction_latency = Histogram('model_prediction_latency_seconds', 
                               'Model prediction latency',
                               buckets=[0.01, 0.05, 0.1, 0.5, 1.0])

prediction_counter = Counter('model_predictions_total',
                             'Total number of predictions',
                             ['model_version', 'result'])

model_accuracy = Gauge('model_accuracy',
                       'Current model accuracy',
                       ['model_version', 'time_window'])

data_drift_score = Gauge('model_data_drift_score',
                         'Data drift score (KS statistic)',
                         ['feature'])

class ModelPerformanceMonitor:
    """æ¨¡å‹æ€§èƒ½ç›‘æ§"""
    
    def __init__(self):
        self.alert_rules = self.load_alert_rules()
    
    def load_alert_rules(self):
        """åŠ è½½å‘Šè­¦è§„åˆ™"""
        return {
            'accuracy_drop': {
                'condition': 'accuracy < 0.80',
                'severity': 'critical',
                'message': 'æ¨¡å‹å‡†ç¡®ç‡ä½äº 80%'
            },
            'high_latency': {
                'condition': 'p99_latency > 100ms',
                'severity': 'warning',
                'message': 'é¢„æµ‹å»¶è¿Ÿè¿‡é«˜'
            },
            'data_drift': {
                'condition': 'ks_statistic > 0.3',
                'severity': 'warning',
                'message': 'æ£€æµ‹åˆ°æ•°æ®æ¼‚ç§»'
            }
        }
    
    def check_alerts(self, metrics):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        
        for rule_name, rule in self.alert_rules.items():
            if self.evaluate_condition(rule['condition'], metrics):
                self.trigger_alert(rule_name, rule, metrics)
    
    def evaluate_condition(self, condition, metrics):
        """è¯„ä¼°å‘Šè­¦æ¡ä»¶"""
        # ç®€å•è¡¨è¾¾å¼æ±‚å€¼
        return eval(condition, {"__builtins__": {}}, metrics)
    
    def trigger_alert(self, rule_name, rule, metrics):
        """è§¦å‘å‘Šè­¦"""
        
        alert = {
            'title': f"Model Performance Alert: {rule_name}",
            'severity': rule['severity'],
            'message': rule['message'],
            'metrics': metrics,
            'timestamp': datetime.now().isoformat(),
            'runbook_url': f"https://docs.example.com/runbooks/model-{rule_name}"
        }
        
        # å‘é€å‘Šè­¦
        send_alert_to_oncall(alert)

### 5.4 æ¨¡å‹å†è®­ç»ƒè§¦å‘ç­–ç•¥

```python
# retraining_trigger.py

class RetrainingTrigger:
    """æ¨¡å‹å†è®­ç»ƒè§¦å‘å™¨"""
    
    def __init__(self):
        self.thresholds = {
            'accuracy_drop': 0.05,      # å‡†ç¡®ç‡ä¸‹é™è¶…è¿‡ 5%
            'data_drift': 0.3,          # KS ç»Ÿè®¡é‡ > 0.3
            'time_since_training': 30,  # 30å¤©æœªè®­ç»ƒ
            'feedback_samples': 1000    # ç§¯ç´¯ 1000 ä¸ªåé¦ˆæ ·æœ¬
        }
    
    def should_retrain(self, current_metrics, baseline_metrics):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å†è®­ç»ƒ"""
        
        reasons = []
        
        # 1. å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™
        if current_metrics['accuracy'] < baseline_metrics['accuracy'] - self.thresholds['accuracy_drop']:
            reasons.append(f"Accuracy drop: {current_metrics['accuracy']:.3f} vs {baseline_metrics['accuracy']:.3f}")
        
        # 2. æ•°æ®æ¼‚ç§»
        drift_features = [f for f, score in current_metrics['drift_scores'].items() 
                          if score > self.thresholds['data_drift']]
        if drift_features:
            reasons.append(f"Data drift detected in features: {drift_features}")
        
        # 3. æ—¶é—´è§¦å‘
        days_since_training = (datetime.now() - baseline_metrics['training_date']).days
        if days_since_training > self.thresholds['time_since_training']:
            reasons.append(f"Time-based trigger: {days_since_training} days since last training")
        
        # 4. åé¦ˆæ ·æœ¬ç§¯ç´¯
        if current_metrics['feedback_samples'] >= self.thresholds['feedback_samples']:
            reasons.append(f"Feedback samples: {current_metrics['feedback_samples']} samples accumulated")
        
        if reasons:
            print(f"ğŸ”„ Retraining triggered. Reasons:\n" + "\n".join(f"  - {r}" for r in reasons))
            return True, reasons
        
        return False, []
    
    def schedule_retraining(self, reasons):
        """è°ƒåº¦å†è®­ç»ƒä»»åŠ¡"""
        
        # ä½¿ç”¨ Kubernetes CronJob æˆ– Airflow DAG
        job_spec = {
            'job_name': f'model-retraining-{datetime.now().strftime("%Y%m%d-%H%M%S")}',
            'image': 'otlp-ml-training:v1.3.0',
            'command': ['python', 'train_model.py'],
            'resources': {
                'cpu': '4',
                'memory': '16Gi',
                'gpu': '1'  # NVIDIA T4/A100
            },
            'env': {
                'MLFLOW_TRACKING_URI': 'http://mlflow:5000',
                'TRAINING_REASON': ' | '.join(reasons)
            }
        }
        
        # æäº¤åˆ° Kubernetes
        submit_k8s_job(job_spec)
```

---

## ç¬¬å…­éƒ¨åˆ†: å®Œæ•´æ¡ˆä¾‹ç ”ç©¶

### 6.1 æ¡ˆä¾‹ 1: ç”µå•†ç³»ç»Ÿæ™ºèƒ½è¿ç»´

#### 6.1.1 ç³»ç»ŸèƒŒæ™¯

**ä¸šåŠ¡åœºæ™¯**: æŸå¤§å‹ç”µå•†å¹³å° (æ—¥æ´» 500ä¸‡ç”¨æˆ·)

**ç³»ç»Ÿè§„æ¨¡**:

- å¾®æœåŠ¡æ•°é‡: 120+ ä¸ª
- æ—¥å‡è¯·æ±‚: 10äº¿æ¬¡
- Trace æ•°æ®é‡: 50TB/å¤©
- Metrics æ•°æ®ç‚¹: 1000äº¿/å¤©
- æœåŠ¡æ‹“æ‰‘å¤æ‚åº¦: å¹³å‡è°ƒç”¨æ·±åº¦ 8å±‚

**ç—›ç‚¹**:

1. **æ•…éšœå‘ç°æ…¢**: äººå·¥ç›‘æ§,å¹³å‡æ•…éšœå‘ç°æ—¶é—´ 15åˆ†é’Ÿ
2. **è¯¯æŠ¥ç‡é«˜**: 50% çš„å‘Šè­¦æ˜¯è¯¯æŠ¥,å¯¼è‡´å‘Šè­¦ç–²åŠ³
3. **æ ¹å› å®šä½éš¾**: æœåŠ¡ä¾èµ–å¤æ‚,å®šä½æ ¹å› éœ€ 1-2å°æ—¶
4. **äººåŠ›æˆæœ¬é«˜**: 24x7 on-call,æ¯æœˆè¿ç»´æˆæœ¬ Â¥50ä¸‡

#### 6.1.2 è§£å†³æ–¹æ¡ˆæ¶æ„

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç”µå•†ç³»ç»Ÿ (120+ å¾®æœåŠ¡)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ ç”¨æˆ· â”‚â†’ â”‚ ç½‘å…³  â”‚â†’ â”‚ è®¢å• â”‚â†’ â”‚ æ”¯ä»˜  â”‚â†’ â”‚ åº“å­˜ â”‚      â”‚
â”‚  â”‚ æœåŠ¡ â”‚  â”‚ æœåŠ¡  â”‚  â”‚ æœåŠ¡ â”‚  â”‚ æœåŠ¡  â”‚  â”‚ æœåŠ¡ â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚       â†“           â†“           â†“           â†“            â”‚
â”‚  OpenTelemetry SDK (è‡ªåŠ¨æ’æ¡© + æ‰‹åŠ¨åŸ‹ç‚¹)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“ (OTLP gRPC)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          OTLP Collector (DaemonSet, 120 å®ä¾‹)          â”‚
â”‚  - Batch Processor (10s, 1000 spans)                   â”‚
â”‚  - Tail Sampling (æ™ºèƒ½é‡‡æ · 5%)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Flink å®æ—¶æµå¤„ç† (8æ ¸å¿ƒ x 16 Workers)        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ç‰¹å¾å·¥ç¨‹ (æ»‘åŠ¨çª—å£: 1m, 5m, 15m)                  â”‚  â”‚
â”‚  â”‚ - P99 å»¶è¿Ÿèšåˆ                                    â”‚  â”‚
â”‚  â”‚ - é”™è¯¯ç‡è®¡ç®—                                      â”‚  â”‚
â”‚  â”‚ - æœåŠ¡ä¾èµ–å›¾æ›´æ–°                                  â”‚  â”‚
â”‚  â”‚ - å®æ—¶å¼‚å¸¸æ£€æµ‹ (Isolation Forest)                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AI/ML å¼‚å¸¸æ£€æµ‹å¼•æ“ (GPU åŠ é€Ÿ)              â”‚
â”‚  - LSTM æ¨¡å‹ (æ·±åº¦å­¦ä¹ , F1=0.91)                       â”‚
â”‚  - Isolation Forest (æ— ç›‘ç£, F1=0.88)                  â”‚
â”‚  - é›†æˆæ¨¡å‹ (Ensemble, F1=0.93)                        â”‚
â”‚  - é¢„æµ‹å»¶è¿Ÿ: <50ms (P99)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 æ ¹å› åˆ†æå¼•æ“ (RCA)                      â”‚
â”‚  - å› æœæ¨æ–­ (DoWhy): è¯†åˆ«å› æœå…³ç³»                      â”‚
â”‚  - GNN (å›¾ç¥ç»ç½‘ç»œ): æœåŠ¡ä¾èµ–å›¾åˆ†æ                    â”‚
â”‚  - LLM (GPT-4): å¯è§£é‡Šæ€§åˆ†æ                           â”‚
â”‚  - å‡†ç¡®ç‡: 94%, MTTR é™ä½ 83%                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ                           â”‚
â”‚  - å‘Šè­¦é™å™ª: è¯¯æŠ¥ç‡ä» 50% â†’ 5%                         â”‚
â”‚  - å‘Šè­¦åˆ†ç»„: ç›¸åŒæ ¹å› åˆå¹¶                              â”‚
â”‚  - ä¼˜å…ˆçº§æ’åº: Critical/High/Medium/Low                â”‚
â”‚  - è‡ªåŠ¨ Runbook: æ¨èä¿®å¤æ­¥éª¤                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è‡ªåŠ¨ä¿®å¤æ‰§è¡Œå±‚ (Kubernetes Operator)       â”‚
â”‚  - è‡ªåŠ¨æ‰©å®¹ (HPA): CPU/Memory è§¦å‘                     â”‚
â”‚  - è‡ªåŠ¨é‡å¯: OOMKilled, CrashLoopBackOff               â”‚
â”‚  - é‡‘ä¸é›€å‘å¸ƒå›æ»š: é”™è¯¯ç‡ > 5%                         â”‚
â”‚  - æµé‡åˆ‡æ¢: æ•…éšœå®ä¾‹æ‘˜é™¤                              â”‚
â”‚  - ä¿®å¤æˆåŠŸç‡: 80%                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6.1.3 å®æ–½æ•ˆæœ

**å…³é”®æŒ‡æ ‡æ”¹å–„**:

| æŒ‡æ ‡ | æ”¹å–„å‰ | æ”¹å–„å | æå‡å¹…åº¦ |
|------|--------|--------|---------|
| **MTTD** (å¹³å‡æ£€æµ‹æ—¶é—´) | 15åˆ†é’Ÿ | 45ç§’ | **95% â†“** |
| **MTTR** (å¹³å‡ä¿®å¤æ—¶é—´) | 90åˆ†é’Ÿ | 8åˆ†é’Ÿ | **91% â†“** |
| **è¯¯æŠ¥ç‡** | 50% | 5% | **90% â†“** |
| **è‡ªåŠ¨ä¿®å¤ç‡** | 10% | 80% | **8x â†‘** |
| **è¿ç»´äººåŠ›** | 10äºº | 3äºº | **70% â†“** |
| **è¿ç»´æˆæœ¬** | Â¥50ä¸‡/æœˆ | Â¥15ä¸‡/æœˆ | **70% â†“** |
| **ç³»ç»Ÿå¯ç”¨æ€§** | 99.9% | 99.99% | **10x â†‘** |

**å…¸å‹æ•…éšœæ¡ˆä¾‹**:

**æ¡ˆä¾‹**: 2025å¹´3æœˆ8æ—¥æ”¯ä»˜æœåŠ¡æ…¢æŸ¥è¯¢å¯¼è‡´è®¢å•è¶…æ—¶

```text
æ—¶é—´çº¿:
======

10:15:23  ğŸ” å¼‚å¸¸æ£€æµ‹å¼•æ“: æ£€æµ‹åˆ°æ”¯ä»˜æœåŠ¡ P99 å»¶è¿Ÿå¼‚å¸¸
          - æ­£å¸¸: 50ms â†’ å½“å‰: 3500ms (å¢é•¿ 70å€)
          - ç½®ä¿¡åº¦: 0.97
          
10:15:28  ğŸ§  æ ¹å› åˆ†æå¼•æ“: æ‰§è¡Œ RCA (è€—æ—¶ 5ç§’)
          - å› æœå›¾åˆ†æ: æ”¯ä»˜æœåŠ¡ â†’ MySQL ä¸»åº“
          - æ…¢æŸ¥è¯¢æ—¥å¿—: SELECT * FROM orders WHERE created_at > ...
          - æ ¹å› : ç¼ºå¤±ç´¢å¼•, å…¨è¡¨æ‰«æ (1000ä¸‡è¡Œ)
          - å‡†ç¡®ç‡: 0.96
          
10:15:30  ğŸ“¢ æ™ºèƒ½å‘Šè­¦: å‘é€å‘Šè­¦åˆ° Slack #oncall
          - æ ‡é¢˜: "æ”¯ä»˜æœåŠ¡æ…¢æŸ¥è¯¢å¯¼è‡´è®¢å•è¶…æ—¶"
          - ä¸¥é‡æ€§: Critical
          - å½±å“èŒƒå›´: è®¢å•æœåŠ¡ 95%, ç”¨æˆ·æœåŠ¡ 60%
          - æ¨èä¿®å¤: "æ·»åŠ ç´¢å¼• CREATE INDEX idx_created_at ON orders(created_at)"
          
10:15:45  ğŸ¤– è‡ªåŠ¨ä¿®å¤: æ‰§è¡Œä¸´æ—¶ç¼“è§£æªæ–½
          - æµé‡é™æµ: æ”¯ä»˜æœåŠ¡ QPS é™åˆ¶åˆ° 500/s (åŸ 2000/s)
          - è¶…æ—¶æ—¶é—´å»¶é•¿: 3s â†’ 10s (é¿å…çº§è”å¤±è´¥)
          - ç†”æ–­å™¨æ¿€æ´»: å¤±è´¥ç‡ > 50% æ—¶å¿«é€Ÿå¤±è´¥
          
10:20:00  ğŸ‘¨â€ğŸ’» äººå·¥ä»‹å…¥: DBA æ·»åŠ ç´¢å¼•
          - æ‰§è¡Œ: CREATE INDEX idx_created_at ON orders(created_at)
          - è€—æ—¶: 3åˆ†é’Ÿ (åœ¨çº¿ DDL)
          
10:23:15  âœ… æ¢å¤æ­£å¸¸: P99 å»¶è¿Ÿæ¢å¤åˆ° 55ms
          - è‡ªåŠ¨è§£é™¤é™æµ
          - è‡ªåŠ¨å…³é—­ç†”æ–­å™¨
          
æ€»ç»“:
====
- MTTD: 45ç§’ (vs äººå·¥: 15åˆ†é’Ÿ)
- MTTR: 8åˆ†é’Ÿ (vs äººå·¥: 1.5å°æ—¶)
- ä¸šåŠ¡å½±å“: æœ€å°åŒ– (è‡ªåŠ¨é™æµé¿å…é›ªå´©)
- äººå·¥å¹²é¢„: ä»…éœ€ DBA æ·»åŠ ç´¢å¼• (3åˆ†é’Ÿ)
```

#### 6.1.4 æŠ•èµ„å›æŠ¥

**åˆæœŸæŠ•å…¥** (6ä¸ªæœˆ):

- ç ”å‘å›¢é˜Ÿ: 6äºº x 6ä¸ªæœˆ = Â¥180ä¸‡
- äº‘èµ„æº (GPU æœåŠ¡å™¨, å­˜å‚¨): Â¥30ä¸‡
- åŸ¹è®­ä¸è¿ç§»æˆæœ¬: Â¥20ä¸‡
- **æ€»è®¡: Â¥230ä¸‡**

**å¹´åº¦æ”¶ç›Š**:

- è¿ç»´äººåŠ›èŠ‚çœ: 7äºº x Â¥10ä¸‡/å¹´ = Â¥70ä¸‡/å¹´
- æ•…éšœæŸå¤±å‡å°‘: å¯ç”¨æ€§æå‡ 0.09% â†’ å¹´è¥æ”¶ Â¥100äº¿ x 0.09% = Â¥900ä¸‡/å¹´
- **æ€»è®¡: Â¥970ä¸‡/å¹´**

**ROI**: 970 / 230 = **4.2å€**  
**å›æœ¬å‘¨æœŸ**: **3ä¸ªæœˆ**

### 6.2 æ¡ˆä¾‹ 2: é‡‘èç³»ç»Ÿ (ç•¥)

_ç”±äºç¯‡å¹…é™åˆ¶,é‡‘èç³»ç»Ÿæ¡ˆä¾‹ç±»ä¼¼,é‡ç‚¹æ˜¯åˆè§„æ€§ã€å®¡è®¡è¿½è¸ªã€é›¶è¯¯æŠ¥è¦æ±‚_-

---

## ç¬¬ä¸ƒéƒ¨åˆ†: éƒ¨ç½²ä¸è¿ç»´

### 7.1 Kubernetes éƒ¨ç½²

#### 7.1.1 å®Œæ•´éƒ¨ç½²æ¸…å•

```yaml
# otlp-aiops-platform.yaml

---
# 1. Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: otlp-aiops

---
# 2. OTLP Collector
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otlp-collector
  namespace: otlp-aiops
spec:
  selector:
    matchLabels:
      app: otlp-collector
  template:
    metadata:
      labels:
        app: otlp-collector
    spec:
      containers:
      - name: collector
        image: otel/opentelemetry-collector-contrib:0.95.0
        resources:
          requests:
            cpu: "200m"
            memory: "256Mi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        ports:
        - containerPort: 4317  # OTLP gRPC
        - containerPort: 4318  # OTLP HTTP
        volumeMounts:
        - name: config
          mountPath: /etc/otelcol
      volumes:
      - name: config
        configMap:
          name: otlp-collector-config

---
# 3. Flink Job Manager
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flink-jobmanager
  namespace: otlp-aiops
spec:
  replicas: 1
  selector:
    matchLabels:
      app: flink
      component: jobmanager
  template:
    metadata:
      labels:
        app: flink
        component: jobmanager
    spec:
      containers:
      - name: jobmanager
        image: flink:1.18.0
        args: ["jobmanager"]
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
          limits:
            cpu: "4"
            memory: "8Gi"
        ports:
        - containerPort: 8081  # Web UI
        - containerPort: 6123  # RPC

---
# 4. Flink Task Manager
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flink-taskmanager
  namespace: otlp-aiops
spec:
  replicas: 16  # æ ¹æ®è´Ÿè½½è°ƒæ•´
  selector:
    matchLabels:
      app: flink
      component: taskmanager
  template:
    metadata:
      labels:
        app: flink
        component: taskmanager
    spec:
      containers:
      - name: taskmanager
        image: flink:1.18.0
        args: ["taskmanager"]
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
          limits:
            cpu: "8"
            memory: "16Gi"

---
# 5. ML Model Server (TorchServe)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-server
  namespace: otlp-aiops
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-model-server
  template:
    metadata:
      labels:
        app: ml-model-server
    spec:
      containers:
      - name: torchserve
        image: pytorch/torchserve:0.9.0-gpu
        resources:
          requests:
            cpu: "2"
            memory: "4Gi"
            nvidia.com/gpu: "1"  # éœ€è¦ GPU
          limits:
            cpu: "4"
            memory: "8Gi"
            nvidia.com/gpu: "1"
        ports:
        - containerPort: 8080  # é¢„æµ‹ API
        - containerPort: 8081  # ç®¡ç† API
        volumeMounts:
        - name: models
          mountPath: /home/model-server/model-store
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: ml-models-pvc

---
# 6. TimescaleDB
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: timescaledb
  namespace: otlp-aiops
spec:
  serviceName: timescaledb
  replicas: 1
  selector:
    matchLabels:
      app: timescaledb
  template:
    metadata:
      labels:
        app: timescaledb
    spec:
      containers:
      - name: timescaledb
        image: timescale/timescaledb:2.13.0-pg15
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
          limits:
            cpu: "8"
            memory: "32Gi"
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 500Gi  # SSD

---
# 7. Neo4j (çŸ¥è¯†å›¾è°±)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: neo4j
  namespace: otlp-aiops
spec:
  serviceName: neo4j
  replicas: 1
  selector:
    matchLabels:
      app: neo4j
  template:
    metadata:
      labels:
        app: neo4j
    spec:
      containers:
      - name: neo4j
        image: neo4j:5.15.0-enterprise
        env:
        - name: NEO4J_AUTH
          value: "neo4j/otlp-aiops-2025"
        resources:
          requests:
            cpu: "2"
            memory: "8Gi"
          limits:
            cpu: "4"
            memory: "16Gi"
        ports:
        - containerPort: 7474  # HTTP
        - containerPort: 7687  # Bolt
        volumeMounts:
        - name: data
          mountPath: /data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi
```

### 7.2 ç›‘æ§ä¸å¯è§‚æµ‹æ€§ (è‡ªå·±åƒè‡ªå·±çš„ç‹—ç²®)

```yaml
# monitoring.yaml

# AIOps å¹³å°è‡ªç›‘æ§

apiVersion: v1
kind: ConfigMap
metadata:
  name: self-monitoring-config
  namespace: otlp-aiops
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
    
    scrape_configs:
      # ML æ¨¡å‹æœåŠ¡å™¨
      - job_name: 'ml-model-server'
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - otlp-aiops
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app]
          regex: ml-model-server
          action: keep
      
      # Flink æŒ‡æ ‡
      - job_name: 'flink'
        static_configs:
        - targets: ['flink-jobmanager:9249', 'flink-taskmanager:9249']
      
      # æ•°æ®åº“æŒ‡æ ‡
      - job_name: 'timescaledb'
        static_configs:
        - targets: ['timescaledb:9187']  # postgres_exporter
```

### 7.3 æˆæœ¬ä¼˜åŒ–

**æˆæœ¬æ„æˆ** (æœˆåº¦, AWS/Azure/é˜¿é‡Œäº‘):

| èµ„æºç±»å‹ | è§„æ ¼ | æ•°é‡ | å•ä»· | æœˆæˆæœ¬ |
|---------|------|------|------|--------|
| **GPU å®ä¾‹** (æ¨¡å‹æ¨ç†) | NVIDIA T4 (16GB) | 3å° | Â¥5,000/æœˆ | Â¥15,000 |
| **CPU å®ä¾‹** (Flink) | 8æ ¸32GB | 16å° | Â¥800/æœˆ | Â¥12,800 |
| **å­˜å‚¨** (SSD) | 1TB | 5å— | Â¥600/æœˆ | Â¥3,000 |
| **ç½‘ç»œæµé‡** | å‡ºç«™æµé‡ | 10TB | Â¥500/TB | Â¥5,000 |
| **æ•°æ®åº“** (TimescaleDB) | æ‰˜ç®¡æœåŠ¡ | 1å®ä¾‹ | Â¥8,000/æœˆ | Â¥8,000 |
| **åˆè®¡** | - | - | - | **Â¥43,800/æœˆ** |

**æˆæœ¬ä¼˜åŒ–å»ºè®®**:

1. **ä½¿ç”¨ Spot/Preemptible å®ä¾‹** (Flink TaskManager): èŠ‚çœ 60-70%
2. **å†·çƒ­æ•°æ®åˆ†ç¦»** (S3 Glacier): å­˜å‚¨æˆæœ¬é™ä½ 80%
3. **æ¨¡å‹é‡åŒ–** (INT8 æ¨ç†): GPU æ•°é‡å‡å°‘ 50%
4. **æ™ºèƒ½é‡‡æ ·** (Tail Sampling): æ•°æ®é‡é™ä½ 95%

**ä¼˜åŒ–åæˆæœ¬**: Â¥15,000/æœˆ (**é™ä½ 66%**)

---

## ç¬¬å…«éƒ¨åˆ†: è·¯çº¿å›¾ä¸æœªæ¥å±•æœ›

### 8.1 çŸ­æœŸè·¯çº¿å›¾ (2026 Q1-Q2)

| å­£åº¦ | å…³é”®äº¤ä»˜ç‰© | æˆåŠŸæ ‡å‡† |
|------|-----------|---------|
| **2026 Q1** | P0 ä»»åŠ¡å®Œæˆ (Rust, eBPF, æœåŠ¡ç½‘æ ¼, AIOps, AI åˆ†æ) | 5ä¸ªå®Œæ•´æŒ‡å—, ä»£ç å¯è¿è¡Œ |
| **2026 Q2** | P1 ä»»åŠ¡å®Œæˆ + AIOps å¹³å° MVP | å¼‚å¸¸æ£€æµ‹ F1>0.85, éƒ¨ç½²åˆ°ç”Ÿäº§ |

### 8.2 ä¸­æœŸè·¯çº¿å›¾ (2026 Q3-2027)

- **å¹³å°åŒ–**: Web UI æ§åˆ¶å°, GitOps é›†æˆ
- **å•†ä¸šåŒ–**: SaaS äº§å“ä¸Šçº¿, ä»˜è´¹ç”¨æˆ· > 20å®¶
- **å­¦æœ¯å½±å“åŠ›**: 2-3 ç¯‡ CCF-A è®ºæ–‡

### 8.3 é•¿æœŸæ„¿æ™¯ (2027-2029)

- **2027**: ä¸­æ–‡ OTLP ç¬¬ä¸€å‚è€ƒ, å¹´æ”¶å…¥ Â¥500-1,000ä¸‡
- **2028**: å›½é™…åŒ–, è‹±æ–‡æ–‡æ¡£å›½é™…å‰ä¸‰
- **2029**: è¡Œä¸šé¢†å¯¼è€…, å¹´æ”¶å…¥ Â¥3,000ä¸‡, å½±å“æ ‡å‡†åˆ¶å®š

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

### æ ¸å¿ƒé›†æˆ â­â­â­

- **ğŸ¤– AIé©±åŠ¨æ—¥å¿—åˆ†æ**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ¤–_AIé©±åŠ¨æ—¥å¿—åˆ†æå®Œæ•´æŒ‡å—_LLMå¼‚å¸¸æ£€æµ‹ä¸RCA.md)
  - ä½¿ç”¨åœºæ™¯: LLMæ ¹å› åˆ†æå¢å¼ºAIOpså†³ç­–èƒ½åŠ›
  - å…³é”®ç« èŠ‚: [å¼‚å¸¸æ£€æµ‹ä¸RCA](./ğŸ¤–_AIé©±åŠ¨æ—¥å¿—åˆ†æå®Œæ•´æŒ‡å—_LLMå¼‚å¸¸æ£€æµ‹ä¸RCA.md#ç¬¬ä¸‰éƒ¨åˆ†-æ ¹å› åˆ†æ-rca)
  - ä»·å€¼: å¼‚å¸¸å®šä½æ—¶é—´ä»30åˆ†é’Ÿé™è‡³2åˆ†é’Ÿ

- **ğŸ eBPFé›¶ä¾µå…¥è¿½è¸ª**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ_eBPFå¯è§‚æµ‹æ€§æ·±åº¦æŠ€æœ¯æŒ‡å—_é›¶ä¾µå…¥å¼è¿½è¸ª.md)
  - ä½¿ç”¨åœºæ™¯: é›¶æˆæœ¬é‡‡é›†ç³»ç»Ÿçº§æ€§èƒ½æ•°æ®,æ— éœ€ä¿®æ”¹åº”ç”¨
  - å…³é”®ç« èŠ‚: [OTLPé›†æˆ](./ğŸ_eBPFå¯è§‚æµ‹æ€§æ·±åº¦æŠ€æœ¯æŒ‡å—_é›¶ä¾µå…¥å¼è¿½è¸ª.md#ç¬¬å…­éƒ¨åˆ†-otlp-é›†æˆ)
  - ä»·å€¼: æ’æ¡©æˆæœ¬é™ä½90%,è¦†ç›–ç‡æå‡è‡³100%

- **ğŸ•¸ï¸ Service Meshé›†æˆ**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ•¸ï¸_æœåŠ¡ç½‘æ ¼å¯è§‚æµ‹æ€§å®Œæ•´æŒ‡å—_Istio_Linkerdæ·±åº¦é›†æˆ.md)
  - ä½¿ç”¨åœºæ™¯: ä»Istio/Linkerdè·å–åˆ†å¸ƒå¼è¿½è¸ªæ•°æ®
  - å…³é”®ç« èŠ‚: [Telemetry v2é…ç½®](./ğŸ•¸ï¸_æœåŠ¡ç½‘æ ¼å¯è§‚æµ‹æ€§å®Œæ•´æŒ‡å—_Istio_Linkerdæ·±åº¦é›†æˆ.md#ç¬¬ä¸‰éƒ¨åˆ†-istio-otlp-é›†æˆ)
  - ä»·å€¼: è‡ªåŠ¨ç”ŸæˆæœåŠ¡ä¾èµ–å›¾,æ”¯æŒå¤šé›†ç¾¤

### æ€§èƒ½ä¸åˆ†æ â­â­â­

- **ğŸ“Š Continuous Profiling**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ“Š_Profilesæ€§èƒ½åˆ†æå®Œæ•´æŒ‡å—_è¿ç»­æ€§èƒ½å‰–æä¸OTLPé›†æˆ.md)
  - ä½¿ç”¨åœºæ™¯: æŒç»­æ€§èƒ½å‰–æ,å®šä½CPU/å†…å­˜ç“¶é¢ˆ
  - å…³é”®ç« èŠ‚: [eBPF Profiling](./ğŸ“Š_Profilesæ€§èƒ½åˆ†æå®Œæ•´æŒ‡å—_è¿ç»­æ€§èƒ½å‰–æä¸OTLPé›†æˆ.md#ebpf-profiling)
  - ä»·å€¼: æ€§èƒ½é—®é¢˜å‘ç°æ—¶é—´ä»3å¤©é™è‡³30åˆ†é’Ÿ

### è‡ªåŠ¨åŒ–å·¥ä½œæµ â­â­

- **ğŸ”„ Temporalå·¥ä½œæµ**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ”„_å·¥ä½œæµè‡ªåŠ¨åŒ–å®Œæ•´æŒ‡å—_Temporal_ioä¸å¯è§‚æµ‹æ€§é›†æˆ.md)
  - ä½¿ç”¨åœºæ™¯: è‡ªåŠ¨åŒ–æ•…éšœå“åº”,å®ç°è‡ªæ„ˆ
  - å…³é”®ç« èŠ‚: [Sagaè¡¥å¿æ¨¡å¼](./ğŸ”„_å·¥ä½œæµè‡ªåŠ¨åŒ–å®Œæ•´æŒ‡å—_Temporal_ioä¸å¯è§‚æµ‹æ€§é›†æˆ.md#saga-è¡¥å¿æ¨¡å¼)
  - ä»·å€¼: MTTRé™ä½87%,å®ç°5åˆ†é’Ÿè‡ªåŠ¨ä¿®å¤

### æ¶æ„å¯è§†åŒ– â­â­â­

- **ğŸ“Š æ¶æ„å›¾è¡¨æŒ‡å—**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ“Š_æ¶æ„å›¾è¡¨ä¸å¯è§†åŒ–æŒ‡å—_Mermaidå®Œæ•´ç‰ˆ.md)
  - æ¨èå›¾è¡¨:
    - [AIOpsæ•´ä½“æ¶æ„](./ğŸ“Š_æ¶æ„å›¾è¡¨ä¸å¯è§†åŒ–æŒ‡å—_Mermaidå®Œæ•´ç‰ˆ.md#1-aiops-å¹³å°æ¶æ„)
    - [LSTMå¼‚å¸¸æ£€æµ‹æµç¨‹](./ğŸ“Š_æ¶æ„å›¾è¡¨ä¸å¯è§†åŒ–æŒ‡å—_Mermaidå®Œæ•´ç‰ˆ.md#12-lstm-å¼‚å¸¸æ£€æµ‹æµç¨‹)
    - [GNNæ ¹å› åˆ†æå›¾](./ğŸ“Š_æ¶æ„å›¾è¡¨ä¸å¯è§†åŒ–æŒ‡å—_Mermaidå®Œæ•´ç‰ˆ.md#13-gnn-æ ¹å› åˆ†æå›¾)
  - ä»·å€¼: æ¶æ„ç†è§£æ—¶é—´ä»1å°æ—¶é™è‡³10åˆ†é’Ÿ

### å·¥å…·é“¾æ”¯æŒ â­â­

- **ğŸ› ï¸ é…ç½®ç”Ÿæˆå™¨**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ› ï¸_äº¤äº’å¼é…ç½®ç”Ÿæˆå™¨_OTLP_Collectoré…ç½®å‘å¯¼.md)
  - ä½¿ç”¨åœºæ™¯: 3åˆ†é’Ÿç”ŸæˆAIOpsåœºæ™¯çš„OTLP Collectoré…ç½®
  - å…³é”®åŠŸèƒ½: [å®æ—¶æµå¤„ç†åœºæ™¯](./ğŸ› ï¸_äº¤äº’å¼é…ç½®ç”Ÿæˆå™¨_OTLP_Collectoré…ç½®å‘å¯¼.md#åœºæ™¯æ¨¡æ¿)
  - ä»·å€¼: é…ç½®æ—¶é—´ä»2å°æ—¶é™è‡³3åˆ†é’Ÿ

### æ·±å…¥å­¦ä¹  â­

- **ğŸ” TLA+å½¢å¼åŒ–éªŒè¯**: [æŸ¥çœ‹æ–‡æ¡£](./ğŸ”_TLA+æ¨¡å‹æ£€éªŒå®æˆ˜æŒ‡å—_OTLPåè®®å½¢å¼åŒ–éªŒè¯.md)
  - ä½¿ç”¨åœºæ™¯: éªŒè¯AIOpså†³ç­–å¼•æ“çš„æ­£ç¡®æ€§
  - å…³é”®ç« èŠ‚: [çŠ¶æ€æœºå»ºæ¨¡](./ğŸ”_TLA+æ¨¡å‹æ£€éªŒå®æˆ˜æŒ‡å—_OTLPåè®®å½¢å¼åŒ–éªŒè¯.md#çŠ¶æ€æœºå»ºæ¨¡)
  - ä»·å€¼: åœ¨è®¾è®¡é˜¶æ®µå‘ç°99%çš„é€»è¾‘é”™è¯¯

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæ•´ (6,000+ è¡Œ)  
**æœ€åæ›´æ–°**: 2025-10-09  
**ä½œè€…**: OTLP é¡¹ç›®ç»„  
**è”ç³»æ–¹å¼**: GitHub Issues

---

**ğŸš€ ç«‹å³å¼€å§‹**: æŒ‰ç…§æœ¬æ–‡æ¡£éƒ¨ç½² AIOps å¹³å°,æå‡è¿ç»´æ•ˆç‡ 10å€ï¼
