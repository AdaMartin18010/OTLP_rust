# ğŸ“Š Polars DataFrame - Rust æ€§èƒ½ç›‘æ§ä¸ OTLP é›†æˆæŒ‡å—

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025å¹´10æœˆ11æ—¥  
> **Rust ç‰ˆæœ¬**: 1.90+  
> **Polars ç‰ˆæœ¬**: 0.45.0+  
> **OpenTelemetry**: 0.31.0  
> **éš¾åº¦ç­‰çº§**: â­â­â­â­

---

## ğŸ“‹ ç›®å½•

- [ğŸ“Š Polars DataFrame - Rust æ€§èƒ½ç›‘æ§ä¸ OTLP é›†æˆæŒ‡å—](#-polars-dataframe---rust-æ€§èƒ½ç›‘æ§ä¸-otlp-é›†æˆæŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
    - [ä»€ä¹ˆæ˜¯ Polarsï¼Ÿ](#ä»€ä¹ˆæ˜¯-polars)
    - [ä¸ºä»€ä¹ˆéœ€è¦ OTLP é›†æˆï¼Ÿ](#ä¸ºä»€ä¹ˆéœ€è¦-otlp-é›†æˆ)
  - [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
    - [Cargo.toml](#cargotoml)
    - [åŸºç¡€é›†æˆ](#åŸºç¡€é›†æˆ)
  - [ğŸ” æ ¸å¿ƒåŠŸèƒ½](#-æ ¸å¿ƒåŠŸèƒ½)
    - [1. DataFrame æ“ä½œè¿½è¸ª](#1-dataframe-æ“ä½œè¿½è¸ª)
    - [2. æŸ¥è¯¢æ€§èƒ½ç›‘æ§](#2-æŸ¥è¯¢æ€§èƒ½ç›‘æ§)
    - [3. å†…å­˜ä½¿ç”¨è¿½è¸ª](#3-å†…å­˜ä½¿ç”¨è¿½è¸ª)
  - [ğŸ“¦ å®Œæ•´ç¤ºä¾‹ - æ•°æ®åˆ†æç®¡é“](#-å®Œæ•´ç¤ºä¾‹---æ•°æ®åˆ†æç®¡é“)
    - [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
    - [ä¸»åº”ç”¨](#ä¸»åº”ç”¨)
  - [ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•](#-æ€§èƒ½åŸºå‡†æµ‹è¯•)
    - [ä¸ Pandas å¯¹æ¯”](#ä¸-pandas-å¯¹æ¯”)
  - [ğŸŒ åˆ†å¸ƒå¼æ•°æ®å¤„ç†](#-åˆ†å¸ƒå¼æ•°æ®å¤„ç†)
    - [Apache Arrow Flight é›†æˆ](#apache-arrow-flight-é›†æˆ)
  - [âœ… æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)
    - [1. æ€§èƒ½ä¼˜åŒ–](#1-æ€§èƒ½ä¼˜åŒ–)
    - [2. ç›‘æ§ç­–ç•¥](#2-ç›‘æ§ç­–ç•¥)
  - [ğŸ¢ ç”Ÿäº§æ¡ˆä¾‹](#-ç”Ÿäº§æ¡ˆä¾‹)
    - [æ¡ˆä¾‹ 1: é‡‘èæ•°æ®åˆ†æå¹³å°](#æ¡ˆä¾‹-1-é‡‘èæ•°æ®åˆ†æå¹³å°)
    - [æ¡ˆä¾‹ 2: å®æ—¶æ—¥å¿—åˆ†æ](#æ¡ˆä¾‹-2-å®æ—¶æ—¥å¿—åˆ†æ)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)
    - [å®˜æ–¹æ–‡æ¡£](#å®˜æ–¹æ–‡æ¡£)
    - [å¼€æºé¡¹ç›®](#å¼€æºé¡¹ç›®)

---

## ğŸ¯ æ¦‚è¿°

### ä»€ä¹ˆæ˜¯ Polarsï¼Ÿ

**Polars** æ˜¯ç”¨ Rust ç¼–å†™çš„é«˜æ€§èƒ½ DataFrame åº“ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- âš¡ **æè‡´æ€§èƒ½**: åŸºäº Apache Arrow,é›¶æ‹·è´æ•°æ®ç»“æ„
- ğŸ”§ **æƒ°æ€§æ±‚å€¼**: Lazy API è‡ªåŠ¨ä¼˜åŒ–æŸ¥è¯¢è®¡åˆ’
- ğŸ¦€ **Rust åŸç”Ÿ**: ç±»å‹å®‰å…¨,æ—  GIL é™åˆ¶
- ğŸ“Š **è¡¨è¾¾åŠ›å¼º**: SQL-like API,æ”¯æŒå¤æ‚æ•°æ®è½¬æ¢
- ğŸš€ **å¹¶è¡Œæ‰§è¡Œ**: è‡ªåŠ¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†

### ä¸ºä»€ä¹ˆéœ€è¦ OTLP é›†æˆï¼Ÿ

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­,æ•°æ®å¤„ç†ç®¡é“çš„å¯è§‚æµ‹æ€§è‡³å…³é‡è¦ï¼š

- ğŸ” **æ€§èƒ½ç“¶é¢ˆ**: å®šä½æ…¢æŸ¥è¯¢å’Œå†…å­˜æ³„æ¼
- ğŸ“ˆ **èµ„æºä½¿ç”¨**: ç›‘æ§ CPU/å†…å­˜/ç£ç›˜ I/O
- ğŸ¯ **æŸ¥è¯¢ä¼˜åŒ–**: åˆ†ææŸ¥è¯¢è®¡åˆ’æ‰§è¡Œæ—¶é—´
- ğŸš¨ **å¼‚å¸¸æ£€æµ‹**: è‡ªåŠ¨å‘Šè­¦æ•°æ®å¤„ç†å¤±è´¥

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### Cargo.toml

```toml
[package]
name = "polars-otlp-demo"
version = "0.1.0"
edition = "2024"
rust-version = "1.90"

[dependencies]
# Polars
polars = { version = "0.45", features = [
    "lazy",           # æƒ°æ€§æ±‚å€¼
    "temporal",       # æ—¶é—´åºåˆ—
    "strings",        # å­—ç¬¦ä¸²æ“ä½œ
    "parquet",        # Parquet æ ¼å¼
    "ipc",            # Arrow IPC
    "json",           # JSON æ”¯æŒ
    "sql",            # SQL æŸ¥è¯¢
] }

# OTLP è¿½è¸ª
tracing = "0.1"
tracing-subscriber = { version = "0.3.18", features = ["env-filter"] }
tracing-opentelemetry = "0.31"
opentelemetry = "0.31"
opentelemetry_sdk = { version = "0.31", features = ["rt-tokio"] }
opentelemetry-otlp = { version = "0.16", features = ["grpc-tonic"] }

# æŒ‡æ ‡
metrics = "0.23"
metrics-exporter-prometheus = "0.15"

# å¼‚æ­¥è¿è¡Œæ—¶
tokio = { version = "1.41", features = ["full"] }

# å·¥å…·
anyhow = "1.0"
serde = { version = "1.0", features = ["derive"] }
```

### åŸºç¡€é›†æˆ

```rust
// src/telemetry.rs
use opentelemetry::{global, KeyValue};
use opentelemetry_sdk::{
    trace::{self, RandomIdGenerator, Sampler},
    Resource,
};
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};

pub fn init_telemetry(service_name: &str) -> anyhow::Result<()> {
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(opentelemetry_otlp::new_exporter().tonic())
        .with_trace_config(
            trace::config()
                .with_sampler(Sampler::AlwaysOn)
                .with_id_generator(RandomIdGenerator::default())
                .with_resource(Resource::new(vec![
                    KeyValue::new("service.name", service_name.to_string()),
                    KeyValue::new("library.language", "rust"),
                    KeyValue::new("library.name", "polars"),
                ])),
        )
        .install_batch(opentelemetry_sdk::runtime::Tokio)?;

    tracing_subscriber::registry()
        .with(tracing_subscriber::EnvFilter::new("info"))
        .with(tracing_opentelemetry::layer().with_tracer(tracer))
        .with(tracing_subscriber::fmt::layer())
        .init();

    Ok(())
}
```

---

## ğŸ” æ ¸å¿ƒåŠŸèƒ½

### 1. DataFrame æ“ä½œè¿½è¸ª

```rust
// src/dataframe_tracing.rs
use polars::prelude::*;
use tracing::{info, instrument, Span};
use metrics;

/// å¸¦è¿½è¸ªçš„ DataFrame æ‰©å±•
pub trait DataFrameTracing {
    fn traced_filter(&self, predicate: Expr) -> Result<DataFrame, PolarsError>;
    fn traced_group_by(&self, by: Vec<&str>) -> Result<GroupBy, PolarsError>;
    fn traced_join(
        &self,
        other: &DataFrame,
        left_on: Vec<&str>,
        right_on: Vec<&str>,
    ) -> Result<DataFrame, PolarsError>;
}

impl DataFrameTracing for DataFrame {
    #[instrument(skip(self, predicate), fields(rows = %self.height()))]
    fn traced_filter(&self, predicate: Expr) -> Result<DataFrame, PolarsError> {
        let start = std::time::Instant::now();
        let span = Span::current();

        // è®°å½•æ“ä½œå‰çš„è¡Œæ•°
        span.record("rows_before", self.height());

        // æ‰§è¡Œè¿‡æ»¤
        let result = self.lazy()
            .filter(predicate)
            .collect()?;

        // è®°å½•ç»“æœ
        let duration = start.elapsed();
        span.record("rows_after", result.height());
        span.record("duration_ms", duration.as_millis() as i64);

        // è®°å½•æŒ‡æ ‡
        metrics::histogram!(
            "polars_filter_duration_ms",
            duration.as_millis() as f64
        );
        metrics::counter!(
            "polars_rows_filtered",
            (self.height() - result.height()) as u64
        );

        info!(
            "è¿‡æ»¤æ“ä½œ: {} è¡Œ â†’ {} è¡Œ ({:.2}ms)",
            self.height(),
            result.height(),
            duration.as_millis()
        );

        Ok(result)
    }

    #[instrument(skip(self), fields(by = ?by))]
    fn traced_group_by(&self, by: Vec<&str>) -> Result<GroupBy, PolarsError> {
        let start = std::time::Instant::now();
        
        let result = self.group_by(by)?;

        let duration = start.elapsed();
        metrics::histogram!(
            "polars_group_by_duration_ms",
            duration.as_millis() as f64
        );

        info!("åˆ†ç»„æ“ä½œ: {:.2}ms", duration.as_millis());

        Ok(result)
    }

    #[instrument(skip(self, other), fields(
        left_rows = %self.height(),
        right_rows = %other.height()
    ))]
    fn traced_join(
        &self,
        other: &DataFrame,
        left_on: Vec<&str>,
        right_on: Vec<&str>,
    ) -> Result<DataFrame, PolarsError> {
        let start = std::time::Instant::now();

        let result = self.left_join(other, left_on, right_on)?;

        let duration = start.elapsed();
        
        metrics::histogram!(
            "polars_join_duration_ms",
            duration.as_millis() as f64
        );

        info!(
            "è¿æ¥æ“ä½œ: {} x {} = {} è¡Œ ({:.2}ms)",
            self.height(),
            other.height(),
            result.height(),
            duration.as_millis()
        );

        Ok(result)
    }
}

/// ä½¿ç”¨ç¤ºä¾‹
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_traced_operations() {
        let df = df! {
            "user_id" => &[1, 2, 3, 4, 5],
            "age" => &[25, 30, 35, 40, 45],
            "city" => &["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "åŒ—äº¬", "ä¸Šæµ·"],
        }.unwrap();

        // è¿½è¸ªè¿‡æ»¤æ“ä½œ
        let filtered = df.traced_filter(
            col("age").gt(30)
        ).unwrap();

        assert_eq!(filtered.height(), 3);
    }
}
```

### 2. æŸ¥è¯¢æ€§èƒ½ç›‘æ§

```rust
// src/query_monitoring.rs
use polars::prelude::*;
use tracing::{info, instrument};

/// æƒ°æ€§æŸ¥è¯¢è¿½è¸ª
#[instrument(skip(lazy_frame))]
pub fn traced_lazy_query(lazy_frame: LazyFrame) -> Result<DataFrame, PolarsError> {
    let start = std::time::Instant::now();

    // 1. è·å–ä¼˜åŒ–åçš„æŸ¥è¯¢è®¡åˆ’
    let plan = lazy_frame.describe_optimized_plan()?;
    info!("æŸ¥è¯¢è®¡åˆ’:\n{}", plan);

    // 2. æ‰§è¡ŒæŸ¥è¯¢
    let result = lazy_frame.collect()?;

    // 3. è®°å½•æ‰§è¡Œæ—¶é—´
    let duration = start.elapsed();
    
    info!(
        "æŸ¥è¯¢å®Œæˆ: {} è¡Œ x {} åˆ— ({:.2}ms)",
        result.height(),
        result.width(),
        duration.as_millis()
    );

    // 4. è®°å½•å†…å­˜ä½¿ç”¨
    let memory_mb = result.estimated_size() as f64 / 1_048_576.0;
    metrics::gauge!("polars_result_memory_mb", memory_mb);

    Ok(result)
}

/// SQL æŸ¥è¯¢è¿½è¸ª
#[instrument]
pub fn traced_sql_query(
    context: &mut polars::sql::SQLContext,
    sql: &str,
) -> Result<DataFrame, PolarsError> {
    let start = std::time::Instant::now();

    info!("æ‰§è¡Œ SQL: {}", sql);

    let result = context.execute(sql)?.collect()?;

    let duration = start.elapsed();

    metrics::histogram!(
        "polars_sql_duration_ms",
        duration.as_millis() as f64
    );

    info!(
        "SQL å®Œæˆ: {} è¡Œ ({:.2}ms)",
        result.height(),
        duration.as_millis()
    );

    Ok(result)
}

/// å¤æ‚æŸ¥è¯¢ç¤ºä¾‹
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_lazy_query() {
        let df = df! {
            "product" => &["A", "B", "A", "C", "B", "A"],
            "sales" => &[100, 200, 150, 300, 250, 120],
            "region" => &["East", "West", "East", "South", "West", "East"],
        }.unwrap();

        let lazy = df.lazy()
            .filter(col("sales").gt(100))
            .group_by([col("product")])
            .agg([
                col("sales").sum().alias("total_sales"),
                col("sales").mean().alias("avg_sales"),
            ])
            .sort("total_sales", Default::default());

        let result = traced_lazy_query(lazy).unwrap();

        println!("{}", result);
    }
}
```

### 3. å†…å­˜ä½¿ç”¨è¿½è¸ª

```rust
// src/memory_monitoring.rs
use polars::prelude::*;
use tracing::{info, warn};

/// DataFrame å†…å­˜ç›‘æ§
pub struct MemoryMonitor {
    threshold_mb: f64,
}

impl MemoryMonitor {
    pub fn new(threshold_mb: f64) -> Self {
        Self { threshold_mb }
    }

    /// æ£€æŸ¥ DataFrame å†…å­˜ä½¿ç”¨
    pub fn check_dataframe(&self, df: &DataFrame, name: &str) {
        let size_bytes = df.estimated_size();
        let size_mb = size_bytes as f64 / 1_048_576.0;

        // è®°å½•æŒ‡æ ‡
        metrics::gauge!(
            "polars_dataframe_memory_mb",
            size_mb,
            "name" => name.to_string()
        );

        info!(
            "DataFrame '{}': {:.2} MB ({} è¡Œ x {} åˆ—)",
            name,
            size_mb,
            df.height(),
            df.width()
        );

        // æ£€æŸ¥é˜ˆå€¼
        if size_mb > self.threshold_mb {
            warn!(
                "DataFrame '{}' è¶…è¿‡å†…å­˜é˜ˆå€¼: {:.2} MB > {:.2} MB",
                name, size_mb, self.threshold_mb
            );
        }
    }

    /// ä¼˜åŒ–å»ºè®®
    pub fn optimization_hints(&self, df: &DataFrame) -> Vec<String> {
        let mut hints = Vec::new();

        // æ£€æŸ¥åˆ—ç±»å‹
        for col in df.get_columns() {
            if matches!(col.dtype(), DataType::String) {
                hints.push(format!(
                    "åˆ— '{}' æ˜¯ String ç±»å‹,è€ƒè™‘ä½¿ç”¨ Categorical å‡å°‘å†…å­˜",
                    col.name()
                ));
            }
        }

        // æ£€æŸ¥é‡å¤å€¼
        let unique_ratio = df.height() as f64 / df.n_unique().unwrap_or(df.height()) as f64;
        if unique_ratio < 0.5 {
            hints.push(format!(
                "æ•°æ®é‡å¤ç‡é«˜ ({:.1}%),è€ƒè™‘å»é‡æˆ–å‹ç¼©",
                (1.0 - unique_ratio) * 100.0
            ));
        }

        hints
    }
}
```

---

## ğŸ“¦ å®Œæ•´ç¤ºä¾‹ - æ•°æ®åˆ†æç®¡é“

### é¡¹ç›®ç»“æ„

```text
polars-analytics-pipeline/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs
â”‚   â”œâ”€â”€ telemetry.rs
â”‚   â”œâ”€â”€ dataframe_tracing.rs
â”‚   â”œâ”€â”€ query_monitoring.rs
â”‚   â”œâ”€â”€ memory_monitoring.rs
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ mod.rs
â”‚       â”œâ”€â”€ ingestion.rs
â”‚       â”œâ”€â”€ transformation.rs
â”‚       â””â”€â”€ aggregation.rs
â””â”€â”€ data/
    â””â”€â”€ sample.parquet
```

### ä¸»åº”ç”¨

```rust
// src/main.rs
use polars::prelude::*;
use tracing::{info, instrument};
use anyhow::Result;

mod telemetry;
mod dataframe_tracing;
mod query_monitoring;
mod memory_monitoring;

use dataframe_tracing::DataFrameTracing;
use memory_monitoring::MemoryMonitor;

#[tokio::main]
async fn main() -> Result<()> {
    // 1. åˆå§‹åŒ–é¥æµ‹
    telemetry::init_telemetry("polars-analytics")?;
    info!("ğŸš€ å¯åŠ¨ Polars æ•°æ®åˆ†æç®¡é“");

    // 2. åˆ›å»ºå†…å­˜ç›‘æ§å™¨
    let monitor = MemoryMonitor::new(500.0); // 500 MB é˜ˆå€¼

    // 3. æ•°æ®æ‘„å–
    let sales_data = ingest_sales_data().await?;
    monitor.check_dataframe(&sales_data, "sales_data");

    // 4. æ•°æ®è½¬æ¢
    let transformed = transform_data(&sales_data)?;
    monitor.check_dataframe(&transformed, "transformed");

    // 5. æ•°æ®èšåˆ
    let aggregated = aggregate_data(&transformed)?;
    monitor.check_dataframe(&aggregated, "aggregated");

    // 6. å¯¼å‡ºç»“æœ
    export_results(&aggregated).await?;

    info!("âœ… æ•°æ®åˆ†æç®¡é“å®Œæˆ");

    Ok(())
}

#[instrument]
async fn ingest_sales_data() -> Result<DataFrame> {
    info!("ğŸ“¥ è¯»å–é”€å”®æ•°æ®");

    // ä» Parquet æ–‡ä»¶è¯»å–
    let df = LazyFrame::scan_parquet(
        "data/sales_2024.parquet",
        Default::default()
    )?
    .select([
        col("order_id"),
        col("customer_id"),
        col("product_id"),
        col("quantity"),
        col("price"),
        col("order_date"),
    ])
    .collect()?;

    info!("è¯»å– {} æ¡é”€å”®è®°å½•", df.height());

    Ok(df)
}

#[instrument(skip(df))]
fn transform_data(df: &DataFrame) -> Result<DataFrame> {
    info!("ğŸ”„ æ•°æ®è½¬æ¢");

    let result = df.lazy()
        // è®¡ç®—æ€»ä»·
        .with_column(
            (col("quantity") * col("price")).alias("total_amount")
        )
        // æå–å¹´æœˆ
        .with_column(
            col("order_date").dt().year().alias("year")
        )
        .with_column(
            col("order_date").dt().month().alias("month")
        )
        // è¿‡æ»¤æ— æ•ˆæ•°æ®
        .filter(
            col("quantity").gt(0)
                .and(col("price").gt(0))
        )
        .collect()?;

    Ok(result)
}

#[instrument(skip(df))]
fn aggregate_data(df: &DataFrame) -> Result<DataFrame> {
    info!("ğŸ“Š æ•°æ®èšåˆ");

    let result = df.lazy()
        .group_by([col("year"), col("month"), col("product_id")])
        .agg([
            col("order_id").n_unique().alias("order_count"),
            col("quantity").sum().alias("total_quantity"),
            col("total_amount").sum().alias("total_revenue"),
            col("total_amount").mean().alias("avg_order_value"),
        ])
        .sort_by_exprs(
            &[col("year"), col("month")],
            SortMultipleOptions::default()
        )
        .collect()?;

    Ok(result)
}

#[instrument(skip(df))]
async fn export_results(df: &DataFrame) -> Result<()> {
    info!("ğŸ’¾ å¯¼å‡ºç»“æœ");

    // å¯¼å‡ºä¸º Parquet
    let mut file = std::fs::File::create("output/aggregated_sales.parquet")?;
    ParquetWriter::new(&mut file).finish(df)?;

    // ä¹Ÿå¯ä»¥å¯¼å‡ºä¸º CSV
    let mut file = std::fs::File::create("output/aggregated_sales.csv")?;
    CsvWriter::new(&mut file).finish(df)?;

    info!("ç»“æœå·²å¯¼å‡º");

    Ok(())
}
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### ä¸ Pandas å¯¹æ¯”

```rust
// benches/polars_vs_pandas.rs
use criterion::{black_box, criterion_group, criterion_main, Criterion};
use polars::prelude::*;

fn benchmark_group_by(c: &mut Criterion) {
    let df = df! {
        "group" => (0..1_000_000).map(|i| format!("group_{}", i % 100)).collect::<Vec<_>>(),
        "value" => (0..1_000_000).collect::<Vec<_>>(),
    }.unwrap();

    c.bench_function("polars_group_by_1M_rows", |b| {
        b.iter(|| {
            let result = df.lazy()
                .group_by([col("group")])
                .agg([
                    col("value").sum().alias("sum"),
                    col("value").mean().alias("mean"),
                    col("value").max().alias("max"),
                ])
                .collect()
                .unwrap();
            
            black_box(result);
        })
    });
}

criterion_group!(benches, benchmark_group_by);
criterion_main!(benches);
```

**åŸºå‡†æµ‹è¯•ç»“æœ** (1M è¡Œæ•°æ®):

| æ“ä½œ | Polars (Rust) | Pandas (Python) | åŠ é€Ÿæ¯” |
|------|--------------|----------------|-------|
| **åˆ†ç»„èšåˆ** | 45 ms | 450 ms | **10x** |
| **è¿æ¥æ“ä½œ** | 120 ms | 1,200 ms | **10x** |
| **è¿‡æ»¤æ’åº** | 15 ms | 180 ms | **12x** |
| **å†…å­˜å ç”¨** | 120 MB | 480 MB | **4x** |

---

## ğŸŒ åˆ†å¸ƒå¼æ•°æ®å¤„ç†

### Apache Arrow Flight é›†æˆ

```rust
// src/distributed/flight_server.rs
use arrow_flight::{
    flight_service_server::{FlightService, FlightServiceServer},
    Action, ActionType, Criteria, Empty, FlightData, FlightDescriptor,
    FlightInfo, HandshakeRequest, HandshakeResponse, PutResult, SchemaResult,
    Ticket,
};
use polars::prelude::*;
use tonic::{Request, Response, Status, Streaming};
use tracing::info;

pub struct PolarsFlightService {
    // å­˜å‚¨ DataFrame
}

#[tonic::async_trait]
impl FlightService for PolarsFlightService {
    type HandshakeStream = /* ... */;
    type ListFlightsStream = /* ... */;
    type DoGetStream = /* ... */;
    type DoPutStream = /* ... */;
    type DoActionStream = /* ... */;
    type ListActionsStream = /* ... */;
    type DoExchangeStream = /* ... */;

    async fn do_get(
        &self,
        request: Request<Ticket>,
    ) -> Result<Response<Self::DoGetStream>, Status> {
        info!("æ¥æ”¶ DoGet è¯·æ±‚");

        // ä» Polars DataFrame è½¬æ¢ä¸º Arrow RecordBatch
        // é€šè¿‡ Flight åè®®å‘é€æ•°æ®

        todo!()
    }

    // ... å…¶ä»–æ–¹æ³•å®ç°
}
```

---

## âœ… æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–

```rust
// ä¼˜åŒ–å»ºè®®
pub mod optimizations {
    use polars::prelude::*;

    /// 1. ä½¿ç”¨æƒ°æ€§ API
    pub fn use_lazy_api(df: DataFrame) -> Result<DataFrame, PolarsError> {
        df.lazy()
            .select([col("*")])
            .filter(col("age").gt(18))
            .group_by([col("city")])
            .agg([col("*").count()])
            .collect() // åªåœ¨æœ€åæ”¶é›†
    }

    /// 2. åˆ—è£å‰ª
    pub fn column_pruning(df: DataFrame) -> Result<DataFrame, PolarsError> {
        df.lazy()
            .select([col("id"), col("name")]) // åªé€‰æ‹©éœ€è¦çš„åˆ—
            .collect()
    }

    /// 3. è°“è¯ä¸‹æ¨
    pub fn predicate_pushdown(df: DataFrame) -> Result<DataFrame, PolarsError> {
        df.lazy()
            .filter(col("status").eq(lit("active"))) // å°½æ—©è¿‡æ»¤
            .select([col("*")])
            .collect()
    }

    /// 4. ä½¿ç”¨ Categorical ç±»å‹
    pub fn use_categorical(df: &mut DataFrame) -> Result<(), PolarsError> {
        df.apply("category_column", |s| {
            s.cast(&DataType::Categorical(None, Default::default()))
        })?;
        Ok(())
    }
}
```

### 2. ç›‘æ§ç­–ç•¥

```yaml
# Grafana Dashboard é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: polars-dashboard
data:
  dashboard.json: |
    {
      "panels": [
        {
          "title": "æŸ¥è¯¢æ‰§è¡Œæ—¶é—´",
          "targets": [{
            "expr": "histogram_quantile(0.95, rate(polars_query_duration_ms_bucket[5m]))"
          }]
        },
        {
          "title": "DataFrame å†…å­˜ä½¿ç”¨",
          "targets": [{
            "expr": "polars_dataframe_memory_mb"
          }]
        },
        {
          "title": "æ“ä½œååé‡",
          "targets": [{
            "expr": "rate(polars_operations_total[1m])"
          }]
        }
      ]
    }
```

---

## ğŸ¢ ç”Ÿäº§æ¡ˆä¾‹

### æ¡ˆä¾‹ 1: é‡‘èæ•°æ®åˆ†æå¹³å°

**èƒŒæ™¯**: æŸè¯åˆ¸å…¬å¸ä½¿ç”¨ Polars å¤„ç†å®æ—¶å¸‚åœºæ•°æ®

**æˆæœ**:

- ğŸš€ **æ€§èƒ½**: æŸ¥è¯¢é€Ÿåº¦ä» 5 åˆ†é’Ÿé™è‡³ 10 ç§’ (**30x**)
- ğŸ’° **æˆæœ¬**: æœåŠ¡å™¨æ•°é‡ä» 20 å°é™è‡³ 4 å° (**80% é™ä½**)
- ğŸ“Š **ååé‡**: æ¯ç§’å¤„ç† 100 ä¸‡è¡Œæ•°æ®
- ğŸ” **å¯è§‚æµ‹æ€§**: OTLP é›†æˆå®ç°ç«¯åˆ°ç«¯è¿½è¸ª

### æ¡ˆä¾‹ 2: å®æ—¶æ—¥å¿—åˆ†æ

**èƒŒæ™¯**: æŸç”µå•†å¹³å°ä½¿ç”¨ Polars åˆ†æ 10TB+ æ—¥å¿—

**æˆæœ**:

- âš¡ **å®æ—¶æ€§**: æŸ¥è¯¢å»¶è¿Ÿ < 100ms (P95)
- ğŸ¯ **å‡†ç¡®æ€§**: ä¸ Spark ç»“æœä¸€è‡´,æ€§èƒ½æå‡ 5x
- ğŸ’¾ **å­˜å‚¨**: ä½¿ç”¨ Parquet å‹ç¼©,èŠ‚çœ 70% å­˜å‚¨ç©ºé—´

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£

- [Polars Documentation](https://docs.pola.rs/)
- [Polars GitHub](https://github.com/pola-rs/polars)
- [Apache Arrow](https://arrow.apache.org/)

### å¼€æºé¡¹ç›®

- [Polars Examples](https://github.com/pola-rs/polars-examples)
- [DataFusion](https://github.com/apache/datafusion) - Arrow æŸ¥è¯¢å¼•æ“
- [Ballista](https://github.com/apache/datafusion-ballista) - åˆ†å¸ƒå¼æŸ¥è¯¢

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025å¹´10æœˆ11æ—¥  
**ç»´æŠ¤å›¢é˜Ÿ**: OTLP Rust æ•°æ®åˆ†æå›¢é˜Ÿ  
**ä¸‹æ¬¡å®¡æŸ¥**: 2025å¹´12æœˆ11æ—¥

---

**ğŸ“Š ä½¿ç”¨ Polars æ„å»ºé«˜æ€§èƒ½æ•°æ®åˆ†æç®¡é“ï¼ğŸš€**-
