# OTLP传输层 - gRPC详解

> **协议版本**: gRPC over HTTP/2  
> **OTLP版本**: v1.0.0 (Stable)  
> **默认端口**: 4317  
> **最后更新**: 2025年10月8日

---

## 目录

- [OTLP传输层 - gRPC详解](#otlp传输层---grpc详解)
  - [目录](#目录)
  - [1. 概念定义](#1-概念定义)
    - [1.1 正式定义](#11-正式定义)
    - [1.2 gRPC核心特性](#12-grpc核心特性)
    - [1.3 为什么选择gRPC](#13-为什么选择grpc)
  - [2. 服务定义](#2-服务定义)
    - [2.1 TraceService](#21-traceservice)
    - [2.2 MetricsService](#22-metricsservice)
    - [2.3 LogsService](#23-logsservice)
  - [3. 消息结构](#3-消息结构)
    - [3.1 Traces请求/响应](#31-traces请求响应)
    - [3.2 Metrics请求/响应](#32-metrics请求响应)
    - [3.3 Logs请求/响应](#33-logs请求响应)
  - [4. 传输机制](#4-传输机制)
    - [4.1 单向RPC (Unary RPC)](#41-单向rpc-unary-rpc)
    - [4.2 HTTP/2特性](#42-http2特性)
    - [4.3 连接管理](#43-连接管理)
  - [5. 元数据与头部](#5-元数据与头部)
    - [5.1 标准元数据](#51-标准元数据)
    - [5.2 自定义元数据](#52-自定义元数据)
    - [5.3 认证头部](#53-认证头部)
  - [6. 错误处理](#6-错误处理)
    - [6.1 gRPC状态码](#61-grpc状态码)
    - [6.2 错误详情](#62-错误详情)
    - [6.3 部分成功](#63-部分成功)
  - [7. 性能优化](#7-性能优化)
    - [7.1 压缩](#71-压缩)
    - [7.2 流量控制](#72-流量控制)
    - [7.3 连接池](#73-连接池)
  - [8. 安全性](#8-安全性)
    - [8.1 TLS配置](#81-tls配置)
    - [8.2 证书管理](#82-证书管理)
    - [8.3 认证方式](#83-认证方式)
  - [9. 实现指南](#9-实现指南)
    - [9.1 客户端实现](#91-客户端实现)
    - [9.2 服务器实现](#92-服务器实现)
    - [9.3 最佳实践](#93-最佳实践)
  - [10. 性能基准](#10-性能基准)
    - [10.1 延迟分析](#101-延迟分析)
    - [10.2 吞吐量分析](#102-吞吐量分析)
    - [10.3 资源消耗](#103-资源消耗)
  - [11. 故障处理](#11-故障处理)
    - [11.1 重连策略](#111-重连策略)
    - [11.2 超时设置](#112-超时设置)
    - [11.3 断路器](#113-断路器)
  - [12. 监控与调试](#12-监控与调试)
    - [12.1 内置监控](#121-内置监控)
    - [12.2 调试工具](#122-调试工具)
    - [12.3 日志记录](#123-日志记录)
  - [13. 对比HTTP传输](#13-对比http传输)
  - [14. 参考资源](#14-参考资源)
    - [14.1 gRPC官方](#141-grpc官方)
    - [14.2 OTLP规范](#142-otlp规范)
    - [14.3 实现参考](#143-实现参考)

## 1. 概念定义

### 1.1 正式定义

**gRPC (gRPC Remote Procedure Call)** 在OTLP中的形式化定义：

```text
gRPC_OTLP = (S, M, T, E)

其中:
- S: Services = {TraceService, MetricsService, LogsService}
  服务集合
  
- M: Methods = {Export: Request → Response}
  方法定义（目前仅Export单向RPC）
  
- T: Transport = HTTP/2
  底层传输协议
  
- E: Encoding = Protocol Buffers v3
  消息编码格式

通信模型:
Client --[gRPC Request]--> Server
       <--[gRPC Response]--

基于HTTP/2的双向通信能力，虽然目前仅使用单向RPC
```

### 1.2 gRPC核心特性

```text
1. 高性能
   - 基于HTTP/2：多路复用、流式传输
   - Protocol Buffers：高效二进制编码
   - 连接复用：减少握手开销

2. 强类型
   - .proto定义：明确的接口契约
   - 编译时检查：减少运行时错误
   - 代码生成：自动化客户端/服务器stub

3. 跨语言
   - 支持10+种语言
   - 一致的API设计
   - 统一的错误处理

4. 流式传输（虽然OTLP未使用）
   - 客户端流
   - 服务器流
   - 双向流
```

### 1.3 为什么选择gRPC

**对比其他协议的优势**：

| 特性 | gRPC | HTTP/1.1 | WebSocket |
|------|------|----------|-----------|
| **性能** | 高 (HTTP/2+Protobuf) | 中 (文本协议) | 高 (二进制) |
| **多路复用** | ✅ 原生支持 | ❌ 需HTTP/2 | ❌ 单连接 |
| **流式** | ✅ 原生支持 | ❌ | ✅ |
| **类型安全** | ✅ 强类型 | ❌ 弱类型 | ❌ 弱类型 |
| **生态** | ✅ 丰富 | ✅ 最广泛 | ⚠️ 中等 |
| **负载均衡** | ✅ 内置 | ✅ 成熟 | ⚠️ 复杂 |

**OTLP选择gRPC的理由**：

```text
1. 性能要求
   - 遥测数据量大，需要高吞吐
   - 低延迟传输
   - 高效编码

2. 互操作性
   - 跨语言一致性
   - 明确的契约
   - 标准化生态

3. 可扩展性
   - 支持流式（未来扩展）
   - 元数据机制
   - 版本演进
```

---

## 2. 服务定义

### 2.1 TraceService

**Protocol Buffers定义**：

```protobuf
syntax = "proto3";

package opentelemetry.proto.collector.trace.v1;

import "opentelemetry/proto/trace/v1/trace.proto";

// TraceService服务定义
service TraceService {
  // 导出追踪数据（单向RPC）
  rpc Export(ExportTraceServiceRequest) 
    returns (ExportTraceServiceResponse) {}
}

// 请求消息
message ExportTraceServiceRequest {
  // 资源spans数组
  repeated opentelemetry.proto.trace.v1.ResourceSpans resource_spans = 1;
}

// 响应消息
message ExportTraceServiceResponse {
  // 部分成功信息（可选）
  ExportTracePartialSuccess partial_success = 1;
}

// 部分成功详情
message ExportTracePartialSuccess {
  // 被拒绝的spans数量
  int64 rejected_spans = 1;
  
  // 错误消息
  string error_message = 2;
}
```

**服务端点**：

```text
完整路径: /opentelemetry.proto.collector.trace.v1.TraceService/Export
方法: POST (HTTP/2)
默认端口: 4317
Content-Type: application/grpc+proto
```

### 2.2 MetricsService

**Protocol Buffers定义**：

```protobuf
syntax = "proto3";

package opentelemetry.proto.collector.metrics.v1;

import "opentelemetry/proto/metrics/v1/metrics.proto";

service MetricsService {
  rpc Export(ExportMetricsServiceRequest) 
    returns (ExportMetricsServiceResponse) {}
}

message ExportMetricsServiceRequest {
  repeated opentelemetry.proto.metrics.v1.ResourceMetrics resource_metrics = 1;
}

message ExportMetricsServiceResponse {
  ExportMetricsPartialSuccess partial_success = 1;
}

message ExportMetricsPartialSuccess {
  int64 rejected_data_points = 1;
  string error_message = 2;
}
```

**服务端点**：

```text
完整路径: /opentelemetry.proto.collector.metrics.v1.MetricsService/Export
方法: POST (HTTP/2)
默认端口: 4317
```

### 2.3 LogsService

**Protocol Buffers定义**：

```protobuf
syntax = "proto3";

package opentelemetry.proto.collector.logs.v1;

import "opentelemetry/proto/logs/v1/logs.proto";

service LogsService {
  rpc Export(ExportLogsServiceRequest) 
    returns (ExportLogsServiceResponse) {}
}

message ExportLogsServiceRequest {
  repeated opentelemetry.proto.logs.v1.ResourceLogs resource_logs = 1;
}

message ExportLogsServiceResponse {
  ExportLogsPartialSuccess partial_success = 1;
}

message ExportLogsPartialSuccess {
  int64 rejected_log_records = 1;
  string error_message = 2;
}
```

---

## 3. 消息结构

### 3.1 Traces请求/响应

**请求结构层次**：

```text
ExportTraceServiceRequest
└── resource_spans: []ResourceSpans
    ├── resource: Resource
    │   └── attributes: []KeyValue
    ├── scope_spans: []ScopeSpans
    │   ├── scope: InstrumentationScope
    │   └── spans: []Span
    │       ├── trace_id: bytes (16 bytes)
    │       ├── span_id: bytes (8 bytes)
    │       ├── parent_span_id: bytes (8 bytes)
    │       ├── name: string
    │       ├── kind: SpanKind
    │       ├── start_time_unix_nano: fixed64
    │       ├── end_time_unix_nano: fixed64
    │       ├── attributes: []KeyValue
    │       ├── events: []Event
    │       ├── links: []Link
    │       └── status: Status
    └── schema_url: string
```

**响应结构**：

```text
ExportTraceServiceResponse
└── partial_success: ExportTracePartialSuccess (可选)
    ├── rejected_spans: int64
    └── error_message: string

成功场景:
- partial_success未设置 → 完全成功
- partial_success.rejected_spans = 0 → 完全成功
- partial_success.rejected_spans > 0 → 部分成功
```

### 3.2 Metrics请求/响应

**请求结构**：

```text
ExportMetricsServiceRequest
└── resource_metrics: []ResourceMetrics
    ├── resource: Resource
    ├── scope_metrics: []ScopeMetrics
    │   ├── scope: InstrumentationScope
    │   └── metrics: []Metric
    │       ├── name: string
    │       ├── description: string
    │       ├── unit: string
    │       └── data: oneof {
    │           gauge: Gauge,
    │           sum: Sum,
    │           histogram: Histogram,
    │           exponential_histogram: ExponentialHistogram,
    │           summary: Summary
    │       }
    └── schema_url: string
```

### 3.3 Logs请求/响应

**请求结构**：

```text
ExportLogsServiceRequest
└── resource_logs: []ResourceLogs
    ├── resource: Resource
    ├── scope_logs: []ScopeLogs
    │   ├── scope: InstrumentationScope
    │   └── log_records: []LogRecord
    │       ├── time_unix_nano: fixed64
    │       ├── severity_number: SeverityNumber
    │       ├── severity_text: string
    │       ├── body: AnyValue
    │       ├── attributes: []KeyValue
    │       ├── trace_id: bytes (16 bytes, 可选)
    │       └── span_id: bytes (8 bytes, 可选)
    └── schema_url: string
```

---

## 4. 传输机制

### 4.1 单向RPC (Unary RPC)

**通信流程**：

```text
Client                          Server
  │                               │
  ├──── HTTP/2 HEADERS ─────────>│
  │     :method = POST            │
  │     :path = /Service/Export   │
  │     content-type =            │
  │       application/grpc+proto  │
  │                               │
  ├──── HTTP/2 DATA ────────────>│
  │     <serialized request>      │
  │                               │
  ├──── HTTP/2 END_STREAM ──────>│
  │                               │
  │<──── HTTP/2 HEADERS ──────────┤
  │     :status = 200              │
  │     content-type =             │
  │       application/grpc+proto   │
  │                               │
  │<──── HTTP/2 DATA ─────────────┤
  │     <serialized response>      │
  │                               │
  │<──── HTTP/2 TRAILERS ─────────┤
  │     grpc-status = 0 (OK)       │
  │     grpc-message = (optional)  │
```

**请求帧结构**：

```text
gRPC Message Frame:
┌─────────────┬──────────────┬─────────────────┐
│ Compressed  │ Message      │ Message         │
│ Flag (1B)   │ Length (4B)  │ Data (Variable) │
├─────────────┼──────────────┼─────────────────┤
│ 0 or 1      │ Big Endian   │ Protobuf bytes  │
└─────────────┴──────────────┴─────────────────┘

Compressed Flag:
- 0: 未压缩
- 1: 使用grpc-encoding头部指定的压缩算法

Message Length:
- 4字节，大端序
- 不包含5字节帧头

Message Data:
- Protocol Buffers序列化的消息
```

### 4.2 HTTP/2特性

**多路复用 (Multiplexing)**：

```text
单TCP连接上的多个并发请求:

TCP Connection
├── Stream 1: Export Traces Request
├── Stream 3: Export Metrics Request
├── Stream 5: Export Logs Request
└── Stream 7: Another Traces Request

优势:
- 消除队头阻塞
- 减少连接建立开销
- 提高带宽利用率

Stream ID规则:
- 客户端发起: 奇数 (1, 3, 5, ...)
- 服务器发起: 偶数 (2, 4, 6, ...)
- Stream ID单调递增
```

**流量控制 (Flow Control)**：

```text
HTTP/2流量控制:

Connection Level:
- Initial window size: 65535 bytes (默认)
- 可通过SETTINGS frame调整

Stream Level:
- 每个stream独立窗口
- WINDOW_UPDATE frame更新窗口

gRPC建议:
- Connection window: 1MB+
- Stream window: 128KB-1MB
- 避免窗口耗尽导致的阻塞
```

**头部压缩 (HPACK)**：

```text
HPACK压缩机制:

静态表 (Static Table):
- 预定义常用头部
- 例: :method: POST = Index 3

动态表 (Dynamic Table):
- 连接级缓存
- 存储频繁使用的头部
- FIFO淘汰策略

压缩效果:
- 首次请求: ~300 bytes headers
- 后续请求: ~50 bytes headers
- 压缩率: 80-85%
```

### 4.3 连接管理

**连接生命周期**：

```text
连接状态机:

IDLE (初始)
  │
  ├─ Connect() ─> CONNECTING
  │                  │
  │                  ├─ Success ─> READY
  │                  │               │
  │                  │               ├─ RPC ─> READY (继续)
  │                  │               │
  │                  │               ├─ Idle Timeout ─> IDLE
  │                  │               │
  │                  │               └─ Error ─> TRANSIENT_FAILURE
  │                  │
  │                  └─ Failure ─> TRANSIENT_FAILURE
  │                                    │
  │                                    └─ Reconnect ─> CONNECTING
  │
  └─ Shutdown() ─> SHUTDOWN
```

**Keep-Alive机制**：

```text
gRPC Keep-Alive参数:

Client-side:
- keepalive_time: 30s (默认禁用)
  每30秒发送PING，如果连接空闲
  
- keepalive_timeout: 20s
  等待PING ACK的超时时间
  
- keepalive_permit_without_calls: false
  是否在无active RPC时发送PING

Server-side:
- keepalive_time: 2h (默认)
- keepalive_timeout: 20s
- keepalive_max_idle: Infinity
  最大空闲时间后关闭连接
- keepalive_permit_without_calls: false

推荐配置 (OTLP):
Client:
  keepalive_time: 30s
  keepalive_timeout: 10s
  keepalive_permit_without_calls: true

Server:
  keepalive_time: 1h
  keepalive_timeout: 20s
  keepalive_min_time: 5s (防止客户端过于频繁)
```

---

## 5. 元数据与头部

### 5.1 标准元数据

**gRPC标准头部**：

```text
必需头部:
- :method: POST
- :scheme: http 或 https
- :path: /package.service/method
- :authority: host:port
- content-type: application/grpc+proto
- te: trailers (必须存在)

可选头部:
- grpc-timeout: 1S, 100m, 30H
  格式: <value><unit>
  单位: H (hour), M (minute), S (second),
        m (millisecond), u (microsecond), n (nanosecond)
  
- grpc-encoding: gzip, deflate, snappy
  压缩算法
  
- grpc-accept-encoding: gzip, deflate
  客户端支持的压缩算法

响应Trailer:
- grpc-status: 0 (必需)
  gRPC状态码
  
- grpc-message: <error_message> (可选)
  错误描述
  
- grpc-status-details-bin: <base64> (可选)
  详细错误信息 (Protobuf编码)
```

### 5.2 自定义元数据

**OTLP推荐元数据**：

```text
版本信息:
- otlp-version: 1.0.0
  OTLP协议版本

客户端信息:
- user-agent: <sdk>/<version>
  例: opentelemetry-go/1.20.0

追踪上下文 (可选):
- traceparent: 00-<trace-id>-<span-id>-<flags>
  W3C Trace Context
  
- tracestate: <key>=<value>[,...]
  W3C Trace State

自定义扩展:
- <prefix>-<key>: <value>
  使用前缀避免冲突
  例: mycompany-region: us-west-1
```

### 5.3 认证头部

**常见认证方式**：

```text
1. Bearer Token:
   authorization: Bearer <token>

2. API Key:
   api-key: <key>

3. Custom Auth:
   x-custom-auth: <credentials>

4. mTLS:
   (通过TLS证书，无需额外头部)

示例 (Go):
md := metadata.New(map[string]string{
    "authorization": "Bearer " + token,
    "otlp-version": "1.0.0",
})
ctx = metadata.NewOutgoingContext(ctx, md)
```

---

## 6. 错误处理

### 6.1 gRPC状态码

**OTLP使用的状态码**：

| gRPC Code | 值 | 含义 | OTLP处理 |
|-----------|---|------|---------|
| **OK** | 0 | 成功 | 继续发送 |
| **CANCELLED** | 1 | 客户端取消 | 不重试 |
| **INVALID_ARGUMENT** | 3 | 参数无效 | 不重试，修复请求 |
| **DEADLINE_EXCEEDED** | 4 | 超时 | 重试 |
| **NOT_FOUND** | 5 | 端点不存在 | 检查配置 |
| **ALREADY_EXISTS** | 6 | 已存在 | 不适用OTLP |
| **PERMISSION_DENIED** | 7 | 权限不足 | 不重试，检查认证 |
| **RESOURCE_EXHAUSTED** | 8 | 限流/配额 | 指数退避重试 |
| **FAILED_PRECONDITION** | 9 | 前置条件失败 | 不重试 |
| **ABORTED** | 10 | 请求中止 | 重试 |
| **OUT_OF_RANGE** | 11 | 超出范围 | 不重试 |
| **UNIMPLEMENTED** | 12 | 未实现 | 不重试，检查版本 |
| **INTERNAL** | 13 | 内部错误 | 重试 |
| **UNAVAILABLE** | 14 | 服务不可用 | 重试 |
| **DATA_LOSS** | 15 | 数据丢失 | 不重试 |
| **UNAUTHENTICATED** | 16 | 未认证 | 不重试，添加认证 |

**重试决策树**：

```text
收到gRPC错误
  │
  ├─ DEADLINE_EXCEEDED?
  │    └─ 是 → 重试 (增加timeout)
  │
  ├─ RESOURCE_EXHAUSTED?
  │    └─ 是 → 指数退避重试
  │
  ├─ UNAVAILABLE?
  │    └─ 是 → 重试 (可能是临时故障)
  │
  ├─ INTERNAL?
  │    └─ 是 → 重试 (可能是临时错误)
  │
  ├─ ABORTED?
  │    └─ 是 → 重试
  │
  └─ 其他?
       └─ 否 → 不重试，记录错误
```

### 6.2 错误详情

**grpc-status-details-bin**：

```protobuf
// google/rpc/status.proto
message Status {
  int32 code = 1;           // gRPC status code
  string message = 2;        // 错误消息
  repeated google.protobuf.Any details = 3;  // 详细信息
}

// 可能的details类型
message RetryInfo {
  google.protobuf.Duration retry_delay = 1;
}

message QuotaFailure {
  repeated Violation violations = 1;
  message Violation {
    string subject = 1;
    string description = 2;
  }
}
```

**解析示例 (Go)**：

```go
func parseError(err error) {
    st, ok := status.FromError(err)
    if !ok {
        return
    }
    
    fmt.Printf("Code: %v\n", st.Code())
    fmt.Printf("Message: %s\n", st.Message())
    
    for _, detail := range st.Details() {
        switch t := detail.(type) {
        case *errdetails.RetryInfo:
            fmt.Printf("Retry after: %v\n", t.GetRetryDelay())
        case *errdetails.QuotaFailure:
            fmt.Printf("Quota violations: %v\n", t.GetViolations())
        }
    }
}
```

### 6.3 部分成功

**部分成功语义**：

```text
场景: 批量数据中部分数据被拒绝

响应:
ExportTraceServiceResponse {
  partial_success: {
    rejected_spans: 5,
    error_message: "Invalid span IDs for 5 spans"
  }
}

gRPC状态: OK (0)

客户端处理:
1. gRPC层面视为成功 (status = OK)
2. 应用层面检查partial_success
3. 记录被拒绝的数据
4. 不重试(数据本身有问题)
5. 可选: 过滤无效数据后重试

对比完全失败:
gRPC状态: INVALID_ARGUMENT (3)
响应: 空
客户端: 整个批次失败，需要修复数据
```

---

## 7. 性能优化

### 7.1 压缩

**支持的压缩算法**：

```text
gzip (推荐):
- 压缩率: 60-80%
- CPU开销: 中等
- 广泛支持

deflate:
- 压缩率: 类似gzip
- CPU开销: 中等
- 兼容性好

snappy:
- 压缩率: 40-60%
- CPU开销: 极低
- 速度快

配置示例 (Go):
grpc.Dial(addr,
    grpc.WithDefaultCallOptions(
        grpc.UseCompressor("gzip"),
    ),
)
```

**压缩权衡**：

| 场景 | 推荐算法 | 理由 |
|------|---------|------|
| 高吞吐 | gzip | 平衡压缩率和CPU |
| 低延迟 | 无压缩或snappy | 减少CPU开销 |
| 高延迟网络 | gzip | 减少传输时间 |
| 本地网络 | 无压缩 | CPU>带宽瓶颈 |

### 7.2 流量控制

**窗口大小优化**：

```text
默认配置:
- Initial window: 65KB
- 问题: 高吞吐场景下不足

优化配置:
Connection window: 10MB
Stream window: 1MB

Go示例:
grpc.Dial(addr,
    grpc.WithInitialWindowSize(1 << 20),      // 1MB
    grpc.WithInitialConnWindowSize(10 << 20), // 10MB
)

效果:
- 减少WINDOW_UPDATE帧数量
- 提高大批量数据传输效率
- 典型吞吐提升: 2-3倍
```

### 7.3 连接池

**连接管理策略**：

```text
场景1: 低并发 (< 10 RPC/s)
策略: 单连接
理由: HTTP/2多路复用足够

场景2: 中并发 (10-100 RPC/s)
策略: 2-5个连接
理由: 负载分散，避免单连接瓶颈

场景3: 高并发 (> 100 RPC/s)
策略: 10+个连接
理由: 充分利用多核，避免队头阻塞

连接池示例 (Go):
type ConnectionPool struct {
    conns []*grpc.ClientConn
    size  int
    idx   uint32
}

func (p *ConnectionPool) Get() *grpc.ClientConn {
    i := atomic.AddUint32(&p.idx, 1) % uint32(p.size)
    return p.conns[i]
}
```

---

## 8. 安全性

### 8.1 TLS配置

**客户端TLS**：

```go
// 最小安全配置
tlsConfig := &tls.Config{
    MinVersion: tls.VersionTLS12,
    CipherSuites: []uint16{
        tls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,
        tls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,
    },
}

creds := credentials.NewTLS(tlsConfig)

conn, err := grpc.Dial(addr,
    grpc.WithTransportCredentials(creds),
)
```

**服务器TLS**：

```go
cert, err := tls.LoadX509KeyPair("server.crt", "server.key")
if err != nil {
    log.Fatal(err)
}

tlsConfig := &tls.Config{
    Certificates: []tls.Certificate{cert},
    MinVersion:   tls.VersionTLS12,
    ClientAuth:   tls.RequireAndVerifyClientCert, // mTLS
}

creds := credentials.NewTLS(tlsConfig)

server := grpc.NewServer(
    grpc.Creds(creds),
)
```

### 8.2 证书管理

**证书类型**：

```text
1. 自签名证书 (开发环境)
   $ openssl req -x509 -newkey rsa:4096 \
       -keyout server.key -out server.crt \
       -days 365 -nodes

2. CA签发证书 (生产环境)
   - Let's Encrypt (免费)
   - 商业CA
   - 内部CA

3. mTLS证书
   - 服务器证书
   - 客户端证书
   - 双向验证
```

### 8.3 认证方式

**Per-RPC Credentials**：

```go
type TokenAuth struct {
    token string
}

func (t *TokenAuth) GetRequestMetadata(ctx context.Context, 
    uri ...string) (map[string]string, error) {
    return map[string]string{
        "authorization": "Bearer " + t.token,
    }, nil
}

func (t *TokenAuth) RequireTransportSecurity() bool {
    return true
}

// 使用
auth := &TokenAuth{token: "my-token"}
conn, err := grpc.Dial(addr,
    grpc.WithPerRPCCredentials(auth),
)
```

---

## 9. 实现指南

### 9.1 客户端实现

**完整示例 (Go)**：

```go
package main

import (
    "context"
    "log"
    "time"
    
    "google.golang.org/grpc"
    "google.golang.org/grpc/credentials"
    "google.golang.org/grpc/keepalive"
    
    tracepb "go.opentelemetry.io/proto/otlp/collector/trace/v1"
)

func main() {
    // TLS credentials
    creds, err := credentials.NewClientTLSFromFile(
        "ca.crt", "example.com")
    if err != nil {
        log.Fatal(err)
    }
    
    // 连接配置
    conn, err := grpc.Dial(
        "localhost:4317",
        // 安全
        grpc.WithTransportCredentials(creds),
        
        // Keep-Alive
        grpc.WithKeepaliveParams(keepalive.ClientParameters{
            Time:                30 * time.Second,
            Timeout:             10 * time.Second,
            PermitWithoutStream: true,
        }),
        
        // 压缩
        grpc.WithDefaultCallOptions(
            grpc.UseCompressor("gzip"),
        ),
        
        // 流量控制
        grpc.WithInitialWindowSize(1 << 20),
        grpc.WithInitialConnWindowSize(10 << 20),
        
        // 超时
        grpc.WithBlock(),
        grpc.WithTimeout(5*time.Second),
    )
    if err != nil {
        log.Fatal(err)
    }
    defer conn.Close()
    
    // 创建客户端
    client := tracepb.NewTraceServiceClient(conn)
    
    // 导出数据
    ctx, cancel := context.WithTimeout(context.Background(), 
        10*time.Second)
    defer cancel()
    
    req := &tracepb.ExportTraceServiceRequest{
        // ... 填充数据
    }
    
    resp, err := client.Export(ctx, req)
    if err != nil {
        log.Printf("Export failed: %v", err)
        return
    }
    
    // 检查部分成功
    if ps := resp.GetPartialSuccess(); ps != nil {
        if ps.RejectedSpans > 0 {
            log.Printf("Partial success: %d spans rejected: %s",
                ps.RejectedSpans, ps.ErrorMessage)
        }
    }
    
    log.Println("Export successful")
}
```

### 9.2 服务器实现

**完整示例 (Go)**：

```go
package main

import (
    "context"
    "log"
    "net"
    "time"
    
    "google.golang.org/grpc"
    "google.golang.org/grpc/credentials"
    "google.golang.org/grpc/keepalive"
    
    tracepb "go.opentelemetry.io/proto/otlp/collector/trace/v1"
)

type traceServer struct {
    tracepb.UnimplementedTraceServiceServer
}

func (s *traceServer) Export(ctx context.Context, 
    req *tracepb.ExportTraceServiceRequest) (
    *tracepb.ExportTraceServiceResponse, error) {
    
    // 处理spans
    totalSpans := 0
    for _, rs := range req.ResourceSpans {
        for _, ss := range rs.ScopeSpans {
            totalSpans += len(ss.Spans)
            for _, span := range ss.Spans {
                // 处理每个span
                log.Printf("Received span: %s", span.Name)
            }
        }
    }
    
    log.Printf("Received %d spans", totalSpans)
    
    // 返回成功
    return &tracepb.ExportTraceServiceResponse{}, nil
}

func main() {
    // TLS配置
    cert, err := credentials.NewServerTLSFromFile(
        "server.crt", "server.key")
    if err != nil {
        log.Fatal(err)
    }
    
    // 创建服务器
    server := grpc.NewServer(
        // 安全
        grpc.Creds(cert),
        
        // Keep-Alive
        grpc.KeepaliveParams(keepalive.ServerParameters{
            Time:    1 * time.Hour,
            Timeout: 20 * time.Second,
        }),
        grpc.KeepaliveEnforcementPolicy(keepalive.EnforcementPolicy{
            MinTime:             5 * time.Second,
            PermitWithoutStream: true,
        }),
        
        // 流量控制
        grpc.InitialWindowSize(1 << 20),
        grpc.InitialConnWindowSize(10 << 20),
        
        // 最大消息大小
        grpc.MaxRecvMsgSize(4 << 20), // 4MB
    )
    
    // 注册服务
    tracepb.RegisterTraceServiceServer(server, &traceServer{})
    
    // 监听
    lis, err := net.Listen("tcp", ":4317")
    if err != nil {
        log.Fatal(err)
    }
    
    log.Println("Server listening on :4317")
    if err := server.Serve(lis); err != nil {
        log.Fatal(err)
    }
}
```

### 9.3 最佳实践

```text
1. 连接管理
   ✅ 复用连接，避免频繁创建
   ✅ 使用连接池 (高并发场景)
   ✅ 配置Keep-Alive
   ❌ 不要为每个请求创建新连接

2. 错误处理
   ✅ 区分可重试和不可重试错误
   ✅ 实现指数退避
   ✅ 设置合理的超时
   ❌ 不要无限重试

3. 性能优化
   ✅ 启用压缩 (网络为瓶颈时)
   ✅ 批量发送数据
   ✅ 调整窗口大小
   ❌ 不要过小的批次

4. 安全
   ✅ 生产环境必须使用TLS
   ✅ 验证服务器证书
   ✅ 使用强加密套件
   ❌ 不要禁用证书验证

5. 监控
   ✅ 记录gRPC状态码
   ✅ 监控延迟和吞吐量
   ✅ 追踪重试次数
   ❌ 不要忽略错误
```

---

## 10. 性能基准

### 10.1 延迟分析

**典型延迟分解**：

```text
总延迟 = T_client + T_network + T_server

T_client:
  - 序列化: 0.1-0.5ms (取决于数据大小)
  - 压缩: 0.5-2ms (如果启用)
  - 总计: 0.6-2.5ms

T_network:
  - 本地网络: 0.1-1ms
  - 同区域: 1-10ms
  - 跨区域: 50-200ms
  - RTT占主导

T_server:
  - 反序列化: 0.1-0.5ms
  - 解压: 0.5-2ms
  - 处理: 1-10ms (取决于业务逻辑)
  - 总计: 1.6-12.5ms

实际测量 (本地网络):
- P50: 2-5ms
- P95: 5-15ms
- P99: 15-50ms
```

### 10.2 吞吐量分析

**单连接吞吐量**：

```text
测试场景:
- Span大小: 1KB (典型)
- 批次大小: 100 spans
- 网络: 1Gbps
- 压缩: gzip

单连接结果:
- 无压缩: 10,000-15,000 spans/s
- gzip压缩: 15,000-25,000 spans/s

多连接结果 (10个连接):
- 无压缩: 80,000-120,000 spans/s
- gzip压缩: 120,000-200,000 spans/s

瓶颈分析:
- CPU密集 (序列化/压缩)
- 批量大小影响显著
- 连接数线性扩展 (至CPU核数)
```

### 10.3 资源消耗

**内存使用**：

```text
客户端 (每连接):
- 基础: ~50KB
- 发送缓冲: 1-10MB (取决于窗口大小)
- 典型: 2-5MB/连接

服务器 (每连接):
- 基础: ~100KB
- 接收缓冲: 1-10MB
- 处理队列: 视业务而定
- 典型: 3-10MB/连接

CPU使用:
- 序列化/反序列化: 20-30%
- 压缩/解压: 30-50% (如启用)
- 网络I/O: 10-20%
- 业务逻辑: 10-30%
```

---

## 11. 故障处理

### 11.1 重连策略

**指数退避算法**：

```go
type Backoff struct {
    min    time.Duration
    max    time.Duration
    factor float64
    jitter bool
    
    attempt int
}

func (b *Backoff) Next() time.Duration {
    if b.attempt == 0 {
        b.attempt = 1
        return b.min
    }
    
    delay := float64(b.min) * math.Pow(b.factor, float64(b.attempt))
    if delay > float64(b.max) {
        delay = float64(b.max)
    }
    
    if b.jitter {
        delay = delay/2 + rand.Float64()*delay/2
    }
    
    b.attempt++
    return time.Duration(delay)
}

// 使用
backoff := &Backoff{
    min:    1 * time.Second,
    max:    120 * time.Second,
    factor: 2.0,
    jitter: true,
}

for {
    err := connect()
    if err == nil {
        break
    }
    
    delay := backoff.Next()
    log.Printf("Reconnect after %v", delay)
    time.Sleep(delay)
}
```

### 11.2 超时设置

**多层超时策略**：

```text
1. RPC超时 (每个请求)
   context.WithTimeout(ctx, 10*time.Second)
   
   合理范围:
   - 本地: 1-5s
   - 同区域: 5-10s
   - 跨区域: 10-30s

2. 连接超时
   grpc.WithTimeout(5*time.Second)
   
   合理范围: 3-10s

3. Keep-Alive超时
   keepalive.ClientParameters{
       Timeout: 10*time.Second,
   }
   
   合理范围: 10-30s

4. 整体超时 (业务层)
   context.WithDeadline(ctx, deadline)
   
   考虑重试次数和延迟
```

### 11.3 断路器

**断路器实现**：

```go
type CircuitBreaker struct {
    maxFailures int
    resetTimeout time.Duration
    
    failures int
    lastFailTime time.Time
    state string // "closed", "open", "half-open"
    mu sync.Mutex
}

func (cb *CircuitBreaker) Call(fn func() error) error {
    cb.mu.Lock()
    defer cb.mu.Unlock()
    
    switch cb.state {
    case "open":
        if time.Since(cb.lastFailTime) > cb.resetTimeout {
            cb.state = "half-open"
            cb.failures = 0
        } else {
            return errors.New("circuit breaker open")
        }
    }
    
    err := fn()
    if err != nil {
        cb.failures++
        cb.lastFailTime = time.Now()
        
        if cb.failures >= cb.maxFailures {
            cb.state = "open"
        }
        return err
    }
    
    cb.failures = 0
    cb.state = "closed"
    return nil
}
```

---

## 12. 监控与调试

### 12.1 内置监控

**gRPC指标**：

```text
客户端指标:
- grpc_client_started_total
  RPC启动总数
  
- grpc_client_handled_total{code}
  按状态码分类的完成总数
  
- grpc_client_msg_received_total
  接收消息总数
  
- grpc_client_msg_sent_total
  发送消息总数

服务器指标:
- grpc_server_started_total
- grpc_server_handled_total{code}
- grpc_server_msg_received_total
- grpc_server_msg_sent_total

延迟指标:
- grpc_client_handling_seconds
  客户端请求耗时
  
- grpc_server_handling_seconds
  服务器处理耗时
```

### 12.2 调试工具

**grpcurl (命令行测试)**：

```bash
# 列出服务
grpcurl -plaintext localhost:4317 list

# 列出方法
grpcurl -plaintext localhost:4317 \
    list opentelemetry.proto.collector.trace.v1.TraceService

# 调用方法
grpcurl -plaintext \
    -d @ \
    localhost:4317 \
    opentelemetry.proto.collector.trace.v1.TraceService/Export \
    < request.json
```

**grpc_cli (交互式测试)**：

```bash
# 调用
grpc_cli call localhost:4317 Export "..."

# 查看类型
grpc_cli type localhost:4317 \
    opentelemetry.proto.trace.v1.Span
```

### 12.3 日志记录

**日志级别建议**：

```text
DEBUG:
- 每个RPC请求/响应
- 序列化/反序列化时间
- 压缩前后大小

INFO:
- 连接建立/断开
- 重连尝试
- 批次统计 (spans数量、大小)

WARN:
- 重试
- 部分成功
- Keep-Alive超时

ERROR:
- RPC失败
- 连接错误
- 认证失败
```

---

## 13. 对比HTTP传输

| 特性 | gRPC | HTTP/1.1 |
|------|------|----------|
| **性能** | 更高 | 较低 |
| **延迟** | 更低 (HTTP/2) | 较高 |
| **多路复用** | ✅ 原生 | ❌ |
| **流式** | ✅ | ❌ |
| **压缩** | ✅ 内置 | ✅ 需配置 |
| **类型安全** | ✅ 强类型 | ❌ |
| **浏览器支持** | ⚠️ 需grpc-web | ✅ |
| **防火墙友好** | ⚠️ 可能被阻止 | ✅ |
| **负载均衡** | 复杂 (L7) | 简单 (L4) |
| **调试** | 困难 (二进制) | 容易 (文本) |

**选择建议**：

```text
优先gRPC:
✅ 高性能要求
✅ 低延迟要求
✅ 服务器到服务器
✅ 内部网络

优先HTTP/1.1:
✅ 浏览器客户端
✅ 严格的防火墙环境
✅ 简单的负载均衡
✅ 调试友好性重要
```

---

## 14. 参考资源

### 14.1 gRPC官方

- **官网**: <https://grpc.io>
- **GitHub**: <https://github.com/grpc/grpc>
- **规范**: <https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md>

### 14.2 OTLP规范

- **OTLP Spec**: <https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md>
- **Proto定义**: <https://github.com/open-telemetry/opentelemetry-proto>

### 14.3 实现参考

- **Go**: <https://github.com/open-telemetry/opentelemetry-go>
- **Java**: <https://github.com/open-telemetry/opentelemetry-java>
- **Python**: <https://github.com/open-telemetry/opentelemetry-python>

---

**文档状态**: ✅ 完成  
**审核状态**: 待审核  
**下一步**: [03_传输层_HTTP.md](./03_传输层_HTTP.md)
