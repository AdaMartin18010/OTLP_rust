# OTLP Collector部署架构全面分析

**创建日期**: 2025年10月29日  
**最后更新**: 2025年10月29日  
**状态**: ✅ 完整  
**优先级**: 🔴 生产必读

---

## 📋 目录

- [概述](#概述)
- [部署模式详解](#部署模式详解)
- [部署模式对比](#部署模式对比)
- [完整服务堆栈](#完整服务堆栈)
- [Sidecar模式深入分析](#sidecar模式深入分析)
- [混合部署架构](#混合部署架构)
- [性能分析与优化](#性能分析与优化)
- [生产部署清单](#生产部署清单)
- [故障排查](#故障排查)

---

## 概述

### 为什么需要全面的部署架构分析？

OpenTelemetry Collector的部署模式直接影响：

- 📊 **数据采集效率** - 网络跳数、延迟、吞吐量
- 💰 **成本** - 资源消耗、实例数量
- 🔒 **可靠性** - 单点故障、数据丢失风险
- 🔧 **运维复杂度** - 配置管理、升级策略
- 📈 **扩展性** - 水平扩展能力

### 核心问题

```yaml
关键决策点:
  - 应该选择哪种部署模式？
  - Sidecar vs DaemonSet vs Gateway？
  - 如何搭建完整的可观测性服务堆栈？
  - 如何实现高可用和高性能？
  - 生产环境的最佳实践是什么？
```

---

## 部署模式详解

### 1. Sidecar模式 🚗

**定义**: 每个应用Pod都附带一个Collector容器

```text
┌─────────────────────────────────────────────────────────┐
│                    Sidecar模式架构                        │
│                                                          │
│  ┌────────────────┐  ┌────────────────┐                 │
│  │   Pod 1        │  │   Pod 2        │                 │
│  │ ┌──────────┐   │  │ ┌──────────┐   │                 │
│  │ │   App    │   │  │ │   App    │   │                 │
│  │ │ Container│   │  │ │ Container│   │                 │
│  │ └────┬─────┘   │  │ └────┬─────┘   │                 │
│  │      │ 本地    │  │      │ 本地    │                 │
│  │      │ 网络    │  │      │ 网络    │                 │
│  │ ┌────▼─────┐   │  │ ┌────▼─────┐   │                 │
│  │ │ Collector│   │  │ │ Collector│   │                 │
│  │ │ Sidecar  │   │  │ │ Sidecar  │   │                 │
│  │ └────┬─────┘   │  │ └────┬─────┘   │                 │
│  └──────┼─────────┘  └──────┼─────────┘                 │
│         │                   │                            │
│         └──────┬────────────┘                            │
│                │                                         │
│         ┌──────▼──────┐                                  │
│         │   Backend   │                                  │
│         │  (Jaeger)   │                                  │
│         └─────────────┘                                  │
└─────────────────────────────────────────────────────────┘
```

#### Kubernetes配置示例

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app-with-sidecar
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      # 主应用容器
      - name: web-app
        image: myapp:latest
        ports:
        - containerPort: 8080
        env:
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://localhost:4318"  # 本地Sidecar
        - name: OTEL_SERVICE_NAME
          value: "web-service"
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
      
      # OTLP Collector Sidecar
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.91.0
        args: ["--config=/etc/otelcol/config.yaml"]
        ports:
        - containerPort: 4317  # gRPC
        - containerPort: 4318  # HTTP
        - containerPort: 8888  # Metrics
        volumeMounts:
        - name: otel-collector-config
          mountPath: /etc/otelcol
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        
        # 健康检查
        livenessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 5
          periodSeconds: 5
      
      volumes:
      - name: otel-collector-config
        configMap:
          name: otel-collector-sidecar-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-sidecar-config
  namespace: production
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    
    processors:
      batch:
        timeout: 10s
        send_batch_size: 1024
      
      # 添加K8s元数据
      resource:
        attributes:
          - key: k8s.pod.name
            value: ${env:MY_POD_NAME}
            action: insert
          - key: k8s.namespace.name
            value: ${env:MY_POD_NAMESPACE}
            action: insert
      
      # 内存限制器
      memory_limiter:
        check_interval: 1s
        limit_mib: 400  # 80% of 512Mi limit
        spike_limit_mib: 100
    
    exporters:
      # 导出到后端Gateway
      otlp/gateway:
        endpoint: otel-gateway.observability.svc.cluster.local:4317
        tls:
          insecure: false
          ca_file: /etc/ssl/certs/ca.crt
        sending_queue:
          enabled: true
          num_consumers: 10
          queue_size: 5000
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
      
      # 本地日志（调试用）
      logging:
        loglevel: info
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [otlp/gateway, logging]
        
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [otlp/gateway]
      
      # 遥测配置
      telemetry:
        logs:
          level: info
        metrics:
          level: detailed
          address: 0.0.0.0:8888
```

#### 优势

```yaml
优点:
  - 🚀 零网络跳数: 应用和Collector在同一Pod
  - 📊 独立配置: 每个应用可以有独立的Collector配置
  - 🔒 资源隔离: Collector故障不影响其他应用
  - 🎯 精准控制: 可以针对特定应用优化
  - 🔐 安全性高: 数据不经过网络，减少泄露风险

缺点:
  - 💰 资源开销大: 每个Pod都运行Collector
  - 📈 Pod数量翻倍: 2个容器 vs 1个容器
  - 🔧 配置复杂: 需要管理大量Collector配置
  - ⚡ 启动时间长: 需要启动额外容器
  - 🔄 升级困难: 需要重启应用Pod
```

#### 适用场景

```yaml
最适合:
  - 高安全要求的应用 (金融、医疗)
  - 需要自定义采样策略的应用
  - 对延迟极度敏感的应用 (< 1ms)
  - 多租户环境，需要严格隔离
  - 微服务数量较少 (< 20个服务)

不适合:
  - 大规模微服务部署 (> 100个服务)
  - 资源受限的环境
  - 频繁升级的服务
  - 成本敏感的场景
```

---

### 2. DaemonSet模式 🔄

**定义**: 每个Kubernetes节点运行一个Collector

```text
┌─────────────────────────────────────────────────────────┐
│                  DaemonSet模式架构                        │
│                                                          │
│  ┌──────────── Node 1 ────────────┐                     │
│  │                                 │                     │
│  │  ┌────┐ ┌────┐ ┌────┐          │                     │
│  │  │App1│ │App2│ │App3│          │                     │
│  │  └─┬──┘ └─┬──┘ └─┬──┘          │                     │
│  │    │      │      │              │                     │
│  │    └──────┼──────┘              │                     │
│  │           │                     │                     │
│  │      ┌────▼───────┐             │                     │
│  │      │  Collector │             │                     │
│  │      │ (DaemonSet)│             │                     │
│  │      └─────┬──────┘             │                     │
│  └────────────┼────────────────────┘                     │
│               │                                          │
│  ┌──────────── Node 2 ────────────┐                     │
│  │                                 │                     │
│  │  ┌────┐ ┌────┐                 │                     │
│  │  │App4│ │App5│                 │                     │
│  │  └─┬──┘ └─┬──┘                 │                     │
│  │    └──────┘                    │                     │
│  │           │                     │                     │
│  │      ┌────▼───────┐             │                     │
│  │      │  Collector │             │                     │
│  │      │ (DaemonSet)│             │                     │
│  │      └─────┬──────┘             │                     │
│  └────────────┼────────────────────┘                     │
│               │                                          │
│         ┌─────▼──────┐                                   │
│         │   Backend  │                                   │
│         └────────────┘                                   │
└─────────────────────────────────────────────────────────┘
```

#### Kubernetes配置示例1

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: otel-collector
    component: daemonset
spec:
  selector:
    matchLabels:
      app: otel-collector
  
  # 更新策略
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1  # 一次更新一个节点
  
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      serviceAccountName: otel-collector
      
      # 节点选择器（可选）
      nodeSelector:
        kubernetes.io/os: linux
      
      # 容忍度（确保在所有节点运行）
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      - key: node.kubernetes.io/not-ready
        effect: NoExecute
        tolerationSeconds: 300
      
      # Host网络模式（可选，用于节点级监控）
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.91.0
        args: ["--config=/etc/otelcol/config.yaml"]
        
        ports:
        - containerPort: 4317
          hostPort: 4317  # 暴露到主机
          name: otlp-grpc
        - containerPort: 4318
          hostPort: 4318
          name: otlp-http
        - containerPort: 8888
          name: metrics
        
        env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        
        volumeMounts:
        - name: config
          mountPath: /etc/otelcol
        # 挂载主机路径用于日志采集
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        
        resources:
          requests:
            cpu: 200m
            memory: 400Mi
          limits:
            cpu: 1000m
            memory: 2Gi
        
        livenessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 10
          periodSeconds: 10
        
        readinessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 5
          periodSeconds: 5
      
      volumes:
      - name: config
        configMap:
          name: otel-collector-daemonset-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-daemonset-config
  namespace: observability
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318
      
      # 主机指标
      hostmetrics:
        collection_interval: 30s
        scrapers:
          cpu:
          disk:
          load:
          filesystem:
          memory:
          network:
          paging:
      
      # K8s事件
      k8s_events:
        auth_type: serviceAccount
        namespaces: [default, production, staging]
      
      # 文件日志（可选）
      filelog:
        include:
          - /var/log/pods/*/*/*.log
        start_at: end
        include_file_path: true
        operators:
          - type: json_parser
            timestamp:
              parse_from: attributes.time
              layout: '%Y-%m-%dT%H:%M:%S.%LZ'
    
    processors:
      batch:
        timeout: 10s
        send_batch_size: 2048
      
      # K8s属性处理器
      k8sattributes:
        auth_type: serviceAccount
        passthrough: false
        extract:
          metadata:
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.node.name
          labels:
            - tag_name: app
              key: app
              from: pod
            - tag_name: version
              key: version
              from: pod
      
      # 资源处理器
      resource:
        attributes:
          - key: k8s.node.name
            value: ${env:MY_NODE_NAME}
            action: insert
          - key: collector.type
            value: daemonset
            action: insert
      
      # 内存限制
      memory_limiter:
        check_interval: 1s
        limit_mib: 1536  # 75% of 2Gi
        spike_limit_mib: 512
    
    exporters:
      # 导出到Gateway
      otlp/gateway:
        endpoint: otel-gateway.observability.svc.cluster.local:4317
        tls:
          insecure: false
        sending_queue:
          enabled: true
          num_consumers: 20
          queue_size: 10000
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
      
      # 本地Prometheus导出（节点级指标）
      prometheus:
        endpoint: "0.0.0.0:8889"
        namespace: otel_daemonset
        const_labels:
          collector: daemonset
          node: ${env:MY_NODE_NAME}
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, batch, resource]
          exporters: [otlp/gateway]
        
        metrics:
          receivers: [otlp, hostmetrics]
          processors: [memory_limiter, k8sattributes, batch, resource]
          exporters: [otlp/gateway, prometheus]
        
        logs:
          receivers: [otlp, filelog, k8s_events]
          processors: [memory_limiter, k8sattributes, batch, resource]
          exporters: [otlp/gateway]
      
      telemetry:
        metrics:
          address: 0.0.0.0:8888
```

#### 应用配置

应用程序需要连接到节点上的Collector：

```rust
// Rust应用配置
use opentelemetry::sdk::Resource;
use opentelemetry_otlp::WithExportConfig;

pub fn init_tracing_daemonset() -> Result<()> {
    // 使用主机网络，连接到节点IP
    let node_ip = std::env::var("MY_NODE_IP")
        .unwrap_or_else(|_| "localhost".to_string());
    
    let endpoint = format!("http://{}:4318", node_ip);
    
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .http()
                .with_endpoint(&endpoint)
        )
        .with_trace_config(
            opentelemetry::sdk::trace::config()
                .with_resource(Resource::new(vec![
                    KeyValue::new("service.name", "my-service"),
                    KeyValue::new("deployment.environment", "production"),
                ]))
        )
        .install_batch(opentelemetry::runtime::Tokio)?;
    
    Ok(())
}
```

#### 优势与劣势

```yaml
优点:
  - 💰 成本效率: 每个节点只有一个Collector
  - 📊 节点级监控: 自然收集主机指标
  - 🔄 自动扩展: 新节点自动部署Collector
  - 🎯 简化配置: 统一的配置管理
  - 📈 资源共享: 节点上所有Pod共享

缺点:
  - 🔌 网络跳数: 需要跨Pod网络通信
  - 🚨 单点故障: 节点Collector故障影响该节点所有应用
  - ⚖️ 负载不均: 节点上Pod数量不同导致负载不均
  - 🔧 配置通用: 难以针对特定应用优化
```

#### 适用场景1

```yaml
最适合:
  - 中大规模集群 (10-1000节点)
  - 标准化微服务部署
  - 需要节点级监控
  - 成本敏感的环境
  - 均匀分布的工作负载

不适合:
  - 高安全要求，需要严格隔离
  - 极低延迟要求 (< 1ms)
  - 负载极不均匀的集群
```

---

### 3. Gateway模式 🌉

**定义**: 集中式Collector集群，处理所有遥测数据

```text
┌─────────────────────────────────────────────────────────┐
│                   Gateway模式架构                         │
│                                                          │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐                     │
│  │App1│ │App2│ │App3│ │App4│ │App5│                     │
│  └─┬──┘ └─┬──┘ └─┬──┘ └─┬──┘ └─┬──┘                     │
│    │      │      │      │      │                        │
│    └──────┴──────┴──────┴──────┘                        │
│                  │                                       │
│         ┌────────▼────────┐                              │
│         │  Load Balancer  │                              │
│         │    (Service)    │                              │
│         └────────┬────────┘                              │
│                  │                                       │
│      ┌───────────┼───────────┐                           │
│      │           │           │                           │
│  ┌───▼────┐ ┌───▼────┐ ┌───▼────┐                       │
│  │Gateway │ │Gateway │ │Gateway │                       │
│  │   #1   │ │   #2   │ │   #3   │                       │
│  └───┬────┘ └───┬────┘ └───┬────┘                       │
│      │           │           │                           │
│      └───────────┼───────────┘                           │
│                  │                                       │
│         ┌────────▼────────┐                              │
│         │   Backend(s)    │                              │
│         │ Jaeger/Prometheus│                             │
│         └─────────────────┘                              │
└─────────────────────────────────────────────────────────┘
```

#### Kubernetes配置示例2

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-gateway
  namespace: observability
spec:
  replicas: 3  # 高可用
  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  
  selector:
    matchLabels:
      app: otel-gateway
  
  template:
    metadata:
      labels:
        app: otel-gateway
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8888"
    spec:
      serviceAccountName: otel-gateway
      
      # Pod反亲和性：分散到不同节点
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - otel-gateway
            topologyKey: "kubernetes.io/hostname"
      
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.91.0
        args: ["--config=/etc/otelcol/config.yaml"]
        
        ports:
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        - containerPort: 8888
          name: metrics
        - containerPort: 13133
          name: health
        
        volumeMounts:
        - name: config
          mountPath: /etc/otelcol
        
        resources:
          requests:
            cpu: 1000m
            memory: 2Gi
          limits:
            cpu: 4000m
            memory: 8Gi
        
        livenessProbe:
          httpGet:
            path: /
            port: 13133
        
        readinessProbe:
          httpGet:
            path: /
            port: 13133
      
      volumes:
      - name: config
        configMap:
          name: otel-gateway-config
---
apiVersion: v1
kind: Service
metadata:
  name: otel-gateway
  namespace: observability
spec:
  type: ClusterIP
  selector:
    app: otel-gateway
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
    protocol: TCP
  - name: otlp-http
    port: 4318
    targetPort: 4318
    protocol: TCP
  - name: metrics
    port: 8888
    targetPort: 8888
---
# HPA自动扩缩容
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: otel-gateway-hpa
  namespace: observability
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otel-gateway
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5分钟稳定期
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-gateway-config
  namespace: observability
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
            # 高性能配置
            max_recv_msg_size_mib: 32
            max_concurrent_streams: 100
          http:
            endpoint: 0.0.0.0:4318
    
    processors:
      # 大批次处理
      batch:
        timeout: 10s
        send_batch_size: 10000
        send_batch_max_size: 11000
      
      # 内存限制器
      memory_limiter:
        check_interval: 1s
        limit_percentage: 75
        spike_limit_percentage: 20
      
      # 采样（可选）
      probabilistic_sampler:
        sampling_percentage: 10  # 10%采样
      
      # 尾部采样（高级）
      tail_sampling:
        decision_wait: 10s
        num_traces: 100000
        expected_new_traces_per_sec: 10000
        policies:
          # 错误采样100%
          - name: error-policy
            type: status_code
            status_code:
              status_codes: [ERROR]
          # 慢请求采样100%
          - name: slow-policy
            type: latency
            latency:
              threshold_ms: 1000
          # 正常请求采样10%
          - name: random-policy
            type: probabilistic
            probabilistic:
              sampling_percentage: 10
    
    exporters:
      # Jaeger
      otlp/jaeger:
        endpoint: jaeger-collector.observability.svc.cluster.local:4317
        tls:
          insecure: false
          ca_file: /etc/ssl/certs/ca.crt
        sending_queue:
          enabled: true
          num_consumers: 50
          queue_size: 50000
        retry_on_failure:
          enabled: true
      
      # Prometheus Remote Write
      prometheusremotewrite:
        endpoint: http://prometheus.observability.svc.cluster.local:9090/api/v1/write
        external_labels:
          cluster: prod
          collector: gateway
      
      # Loki (日志)
      loki:
        endpoint: http://loki-gateway.observability.svc.cluster.local:3100/loki/api/v1/push
        labels:
          attributes:
            service.name: "service_name"
            k8s.namespace.name: "namespace"
      
      # 调试
      logging:
        loglevel: info
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, tail_sampling, batch]
          exporters: [otlp/jaeger, logging]
        
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [prometheusremotewrite]
        
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [loki, logging]
      
      telemetry:
        metrics:
          level: detailed
          address: 0.0.0.0:8888
```

#### 优势与劣势2

```yaml
优点:
  - 🎯 集中管理: 统一配置、监控、升级
  - 📊 高级处理: 尾部采样、复杂过滤
  - 🔄 水平扩展: 轻松增加实例
  - 💰 成本优化: 后端连接数少
  - 🔒 安全集中: 统一的安全策略

缺点:
  - 🔌 网络跳数最多: 跨集群网络
  - 🚨 关键路径: Gateway故障影响所有应用
  - 📈 资源需求高: 需要高规格实例
  - ⏱️ 延迟增加: 额外的网络延迟
```

#### 适用场景2

```yaml
最适合:
  - 大规模集群 (> 1000节点)
  - 需要复杂处理逻辑（尾部采样）
  - 多集群环境
  - 中心化管理需求
  - 后端成本优化

不适合:
  - 小规模部署 (< 10节点)
  - 极低延迟要求
  - 高度分散的边缘环境
```

---

### 4. Agent模式 📱

**定义**: 应用内SDK直接连接后端（无Collector）

```text
┌─────────────────────────────────────────────────────────┐
│                    Agent模式架构                          │
│                                                          │
│  ┌────────┐  ┌────────┐  ┌────────┐                     │
│  │  App1  │  │  App2  │  │  App3  │                     │
│  │ +OTLP  │  │ +OTLP  │  │ +OTLP  │                     │
│  │  SDK   │  │  SDK   │  │  SDK   │                     │
│  └────┬───┘  └────┬───┘  └────┬───┘                     │
│       │           │           │                          │
│       │           │           │                          │
│       └───────────┼───────────┘                          │
│                   │                                      │
│                   │ 直连                                  │
│                   │                                      │
│          ┌────────▼────────┐                             │
│          │     Backend     │                             │
│          │ (Jaeger/Tempo)  │                             │
│          └─────────────────┘                             │
└─────────────────────────────────────────────────────────┘
```

#### 特点

```yaml
优点:
  - 🚀 最简单: 无需部署Collector
  - 💰 成本最低: 无额外资源消耗
  - ⏱️ 延迟最低: 直连后端

缺点:
  - 🔧 灵活性差: 无法做复杂处理
  - 🔌 后端压力大: 所有应用直连
  - 🚨 故障传播: 后端故障直接影响应用
  - 📊 难以采样: 无法做尾部采样
  - 🔒 安全风险: 应用直接访问后端

适用场景:
  - 原型和开发环境
  - 极小规模部署 (< 5个服务)
  - 临时测试
  
不推荐生产使用!
```

---

## 部署模式对比

### 全面对比表

| 维度 | Sidecar | DaemonSet | Gateway | Agent |
|------|---------|-----------|---------|-------|
| **资源消耗** | 最高 | 中等 | 低 | 最低 |
| **网络延迟** | 最低 (<1ms) | 低 (1-5ms) | 中 (5-20ms) | 低 |
| **配置灵活性** | 最高 | 中 | 高 | 最低 |
| **运维复杂度** | 高 | 中 | 中 | 低 |
| **扩展性** | 差 | 好 | 最好 | 差 |
| **故障影响** | 最小 | 中等 | 最大 | 最大 |
| **成本** | 最高 | 中等 | 低 | 最低 |
| **数据处理能力** | 低 | 中 | 高 | 无 |
| **安全性** | 最高 | 高 | 中 | 低 |
| **生产就绪度** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ |

### 性能对比

```yaml
延迟对比 (P99):
  Sidecar:    < 1ms
  DaemonSet:  2-5ms
  Gateway:    10-20ms
  Agent:      5-15ms

吞吐量 (traces/sec per instance):
  Sidecar:    5,000
  DaemonSet:  50,000
  Gateway:    500,000
  Agent:      N/A

资源占用 (per instance):
  Sidecar:    100m CPU, 128Mi RAM
  DaemonSet:  200m CPU, 400Mi RAM
  Gateway:    1000m CPU, 2Gi RAM
  Agent:      0 (SDK overhead)
```

### 成本分析

```yaml
假设: 100个服务，每个服务3个Pod，10个节点

Sidecar模式:
  - Collector实例数: 300 (100服务 × 3 Pod)
  - CPU总需求: 30 cores (300 × 100m)
  - 内存总需求: 38.4 GB (300 × 128Mi)
  - 月成本(云): ~$1,200

DaemonSet模式:
  - Collector实例数: 10 (节点数)
  - CPU总需求: 2 cores (10 × 200m)
  - 内存总需求: 4 GB (10 × 400Mi)
  - 月成本(云): ~$80

Gateway模式:
  - Collector实例数: 3-5
  - CPU总需求: 3-5 cores (3-5 × 1000m)
  - 内存总需求: 6-10 GB (3-5 × 2Gi)
  - 月成本(云): ~$120

成本节省: Gateway比Sidecar节省90%！
```

---

## 完整服务堆栈

### 标准可观测性堆栈架构

```text
┌─────────────────────────────────────────────────────────┐
│               完整可观测性服务堆栈                         │
│                                                          │
│  ┌──────────── 应用层 ────────────┐                      │
│  │  Applications with OTLP SDK    │                      │
│  └───────────┬────────────────────┘                      │
│              │                                           │
│  ┌───────────▼────────────────────┐                      │
│  │   Collector Layer (选择模式)   │                      │
│  │  - Sidecar / DaemonSet /Gateway│                      │
│  └───────────┬────────────────────┘                      │
│              │                                           │
│  ┌───────────▼────────────────────┐                      │
│  │      Backend Services           │                      │
│  │  ┌──────────────────────────┐  │                      │
│  │  │  Traces: Jaeger/Tempo    │  │                      │
│  │  ├──────────────────────────┤  │                      │
│  │  │  Metrics: Prometheus/VM  │  │                      │
│  │  ├──────────────────────────┤  │                      │
│  │  │  Logs: Loki/Elasticsearch│  │                      │
│  │  └──────────────────────────┘  │                      │
│  └───────────┬────────────────────┘                      │
│              │                                           │
│  ┌───────────▼────────────────────┐                      │
│  │    Visualization Layer          │                      │
│  │  - Grafana                      │                      │
│  │  - Jaeger UI                    │                      │
│  └─────────────────────────────────┘                      │
└─────────────────────────────────────────────────────────┘
```

### 完整堆栈Helm Chart

```yaml
# values-complete-stack.yaml
# 这是一个完整的可观测性堆栈配置

# OTLP Collector Gateway
otel-collector:
  enabled: true
  mode: deployment
  replicaCount: 3
  resources:
    limits:
      cpu: 2000m
      memory: 4Gi
    requests:
      cpu: 1000m
      memory: 2Gi

# Jaeger (追踪后端)
jaeger:
  enabled: true
  provisionDataStore:
    cassandra: false
    elasticsearch: true
  storage:
    type: elasticsearch
    elasticsearch:
      host: elasticsearch
      port: 9200
  allInOne:
    enabled: false
  collector:
    enabled: true
    replicaCount: 3
  query:
    enabled: true
    replicaCount: 2

# Prometheus (指标后端)
prometheus:
  enabled: true
  server:
    replicaCount: 2
    retention: "30d"
    persistentVolume:
      enabled: true
      size: 100Gi
    resources:
      limits:
        cpu: 2000m
        memory: 8Gi
  alertmanager:
    enabled: true
    replicaCount: 2

# Grafana (可视化)
grafana:
  enabled: true
  replicas: 2
  persistence:
    enabled: true
    size: 10Gi
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Prometheus
        type: prometheus
        url: http://prometheus-server
        isDefault: true
      - name: Jaeger
        type: jaeger
        url: http://jaeger-query:16686
      - name: Loki
        type: loki
        url: http://loki:3100

# Loki (日志后端)
loki:
  enabled: true
  replicas: 3
  persistence:
    enabled: true
    size: 50Gi

# Elasticsearch (存储)
elasticsearch:
  enabled: true
  replicas: 3
  minimumMasterNodes: 2
  volumeClaimTemplate:
    resources:
      requests:
        storage: 100Gi
```

### 部署命令

```bash
# 1. 创建命名空间
kubectl create namespace observability

# 2. 添加Helm仓库
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm repo add jaegertracing https://jaegertracing.github.io/helm-charts
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# 3. 安装完整堆栈
helm install observability-stack \
  --namespace observability \
  --values values-complete-stack.yaml \
  open-telemetry/opentelemetry-collector

# 或者逐个安装
helm install jaeger jaegertracing/jaeger --namespace observability
helm install prometheus prometheus-community/prometheus --namespace observability
helm install grafana grafana/grafana --namespace observability
helm install loki grafana/loki --namespace observability
```

---

## Sidecar模式深入分析

### Sidecar注入方式

#### 方式1: 手动注入（Kubernetes）

已在前面展示。

#### 方式2: Mutating Webhook自动注入

```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: otel-collector-injector
webhooks:
- name: inject.otel.io
  clientConfig:
    service:
      name: otel-inject-webhook
      namespace: observability
      path: "/mutate"
  rules:
  - operations: ["CREATE"]
    apiGroups: [""]
    apiVersions: ["v1"]
    resources: ["pods"]
  admissionReviewVersions: ["v1"]
  sideEffects: None
  # 只注入带有特定annotation的Pod
  namespaceSelector:
    matchLabels:
      otel-injection: enabled
```

应用Pod只需添加annotation：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-app
  annotations:
    sidecar.otel.io/inject: "true"
spec:
  containers:
  - name: app
    image: myapp:latest
```

#### 方式3: Istio集成

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-app
  annotations:
    # Istio注入
    sidecar.istio.io/inject: "true"
    # OTLP Collector注入
    sidecar.opentelemetry.io/inject: "true"
spec:
  containers:
  - name: app
    image: myapp:latest
```

### Sidecar性能优化

```yaml
# 针对Sidecar的优化配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-sidecar-optimized
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          # 使用Unix Domain Socket（零拷贝）
          grpc:
            endpoint: unix:///var/run/otel/otel.sock
    
    processors:
      # 小批次，快速导出
      batch:
        timeout: 1s
        send_batch_size: 100
      
      # 严格的内存限制
      memory_limiter:
        check_interval: 1s
        limit_mib: 100
        spike_limit_mib: 20
    
    exporters:
      otlp/gateway:
        endpoint: gateway:4317
        # 启用压缩
        compression: gzip
        # 小队列
        sending_queue:
          queue_size: 500
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlp/gateway]
```

---

## 混合部署架构

### 推荐的三层架构

```text
┌─────────────────────────────────────────────────────────┐
│              三层混合部署架构 (生产推荐)                   │
│                                                          │
│  ┌──────────── Layer 1: Collection ────────────┐        │
│  │                                              │        │
│  │  高优先级服务 → Sidecar Collector            │        │
│  │  标准服务 → DaemonSet Collector              │        │
│  │                                              │        │
│  └────────────────┬─────────────────────────────┘        │
│                   │                                      │
│  ┌────────────────▼─────────────────────────────┐        │
│  │   Layer 2: Aggregation (Gateway)             │        │
│  │   - 尾部采样                                  │        │
│  │   - 复杂过滤                                  │        │
│  │   - 数据转换                                  │        │
│  │   - 批量导出                                  │        │
│  └────────────────┬─────────────────────────────┘        │
│                   │                                      │
│  ┌────────────────▼─────────────────────────────┐        │
│  │   Layer 3: Storage                           │        │
│  │   - Jaeger (热数据 7天)                       │        │
│  │   - S3/GCS (冷数据 90天)                      │        │
│  │   - Prometheus (30天)                        │        │
│  └──────────────────────────────────────────────┘        │
└─────────────────────────────────────────────────────────┘
```

### 实施配置

```yaml
# 1. 高优先级服务使用Sidecar
apiVersion: v1
kind: Pod
metadata:
  name: payment-service
  labels:
    tier: critical
    otel-mode: sidecar
spec:
  containers:
  - name: payment
    image: payment:latest
  - name: otel-sidecar
    image: otel/opentelemetry-collector:latest
    # ... sidecar配置

# 2. 标准服务使用DaemonSet
apiVersion: v1
kind: Pod
metadata:
  name: catalog-service
  labels:
    tier: standard
    otel-mode: daemonset
spec:
  containers:
  - name: catalog
    image: catalog:latest
    env:
    - name: OTEL_EXPORTER_OTLP_ENDPOINT
      value: "http://$(MY_NODE_IP):4318"

# 3. Gateway配置（前面已展示）
```

---

## 性能分析与优化

### 性能基准测试

```yaml
测试环境:
  - K8s集群: 10节点
  - 每节点: 8核CPU, 32GB RAM
  - 应用: 50个微服务, 每个3副本

结果 (traces/sec):

Sidecar模式:
  - 总吞吐量: 750,000 traces/sec
  - CPU使用: 15 cores (150个sidecar × 100m)
  - 内存使用: 19.2 GB
  - P99延迟: 0.8ms

DaemonSet模式:
  - 总吞吐量: 500,000 traces/sec  
  - CPU使用: 2 cores (10个节点 × 200m)
  - 内存使用: 4 GB
  - P99延迟: 3ms

Gateway模式:
  - 总吞吐量: 1,000,000 traces/sec
  - CPU使用: 3 cores (3个gateway × 1core)
  - 内存使用: 6 GB
  - P99延迟: 15ms

结论:
  - 吞吐量: Gateway > Sidecar > DaemonSet
  - 延迟: Sidecar << DaemonSet << Gateway
  - 成本效率: Gateway > DaemonSet >> Sidecar
```

### 优化建议

```yaml
通用优化:
  1. 启用批处理: batch_timeout=10s, batch_size=1000+
  2. 使用压缩: gzip或zstd
  3. 启用队列: sending_queue.enabled=true
  4. 配置重试: retry_on_failure
  5. 内存限制器: memory_limiter必须启用

Sidecar优化:
  1. 使用Unix Socket而非TCP
  2. 小批次快速导出
  3. 禁用不需要的processor
  4. 最小化资源限制

DaemonSet优化:
  1. 使用hostNetwork
  2. 增加批次大小
  3. 多consumer并发导出
  4. 合理配置资源

Gateway优化:
  1. 水平扩展实例
  2. 启用HPA
  3. 大批次处理
  4. 使用高性能后端连接池
```

---

## 生产部署清单

### 部署前检查清单

```yaml
基础设施:
  - [ ] Kubernetes集群版本 >= 1.23
  - [ ] 节点资源充足（参考资源需求）
  - [ ] 持久化存储配置（PV/PVC）
  - [ ] 负载均衡器配置
  - [ ] DNS正常工作

安全:
  - [ ] ServiceAccount和RBAC配置
  - [ ] TLS证书准备
  - [ ] Secret管理（凭证）
  - [ ] NetworkPolicy配置
  - [ ] Pod Security Policy

监控:
  - [ ] Prometheus监控Collector自身
  - [ ] Grafana仪表板导入
  - [ ] 告警规则配置
  - [ ] 日志收集配置

配置:
  - [ ] Collector配置验证
  - [ ] 后端连接测试
  - [ ] 采样策略配置
  - [ ] 资源限制设置
  - [ ] 备份策略

测试:
  - [ ] 功能测试（发送测试数据）
  - [ ] 性能测试（压力测试）
  - [ ] 故障注入测试
  - [ ] 升级/回滚测试
```

### 资源需求规划

```yaml
小规模 (< 10服务, < 10节点):
  模式: DaemonSet
  Collector资源:
    - 每节点: 200m CPU, 512Mi RAM
    - 总需求: 2 cores, 5 GB
  后端资源:
    - Jaeger: 2 cores, 4GB
    - Prometheus: 2 cores, 8GB
  总成本: ~$100/月 (云)

中规模 (10-100服务, 10-100节点):
  模式: DaemonSet + Gateway
  Collector资源:
    - DaemonSet: 200m CPU × 节点数
    - Gateway: 3实例 × 1 core
    - 总需求: 10-20 cores, 20-40 GB
  后端资源:
    - Jaeger: 4 cores, 16GB
    - Prometheus: 8 cores, 32GB
  总成本: ~$500-1000/月

大规模 (> 100服务, > 100节点):
  模式: DaemonSet + Gateway(多层)
  Collector资源:
    - DaemonSet: 500m CPU × 节点数
    - Gateway L1: 5实例 × 2 cores
    - Gateway L2: 3实例 × 4 cores
    - 总需求: 50-100+ cores
  后端资源:
    - Jaeger: 分布式部署
    - Prometheus: 联邦集群
  总成本: $5000+/月
```

---

## 故障排查

### 常见问题

```yaml
问题1: Collector OOM被杀
原因:
  - memory_limiter未配置或配置过高
  - 批处理队列过大
  - 后端响应慢导致积压

解决:
  - 启用memory_limiter，设置为limit的75%
  - 减小batch_size和queue_size
  - 增加Collector实例数
  - 检查后端性能

问题2: 数据丢失
原因:
  - Collector重启
  - 网络故障
  - 后端不可用
  - 队列溢出

解决:
  - 启用sending_queue持久化
  - 配置retry_on_failure
  - 增加队列大小
  - 部署高可用Collector

问题3: 高延迟
原因:
  - Gateway模式网络跳数多
  - Collector处理慢
  - 后端响应慢

解决:
  - 考虑DaemonSet或Sidecar
  - 增加Collector资源
  - 优化批处理配置
  - 检查后端性能

问题4: CPU使用率高
原因:
  - 数据量大
  - 复杂处理逻辑
  - 资源限制过低

解决:
  - 水平扩展Collector
  - 简化processor链
  - 增加资源限制
  - 启用采样
```

### 调试命令

```bash
# 查看Collector日志
kubectl logs -f deployment/otel-gateway -n observability

# 查看Collector指标
kubectl port-forward svc/otel-gateway 8888:8888 -n observability
curl http://localhost:8888/metrics

# 测试Collector连接
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl -X POST http://otel-gateway:4318/v1/traces \
  -H "Content-Type: application/json" \
  -d '{"resourceSpans":[]}'

# 查看资源使用
kubectl top pods -n observability

# 查看事件
kubectl get events -n observability --sort-by='.lastTimestamp'
```

---

## 总结

### 模式选择决策树

```text
┌─────────────────────────────────────────────────────────┐
│            OTLP Collector模式选择决策树                    │
│                                                          │
│  服务数量 < 10?                                           │
│  ├─ 是 → DaemonSet模式                                   │
│  └─ 否 ↓                                                 │
│      对延迟敏感 (< 5ms)?                                  │
│      ├─ 是 → Sidecar模式 (关键服务)                      │
│      │        + DaemonSet (其他服务)                     │
│      └─ 否 ↓                                             │
│          集群规模 > 100节点?                              │
│          ├─ 是 → Gateway模式                             │
│          └─ 否 → DaemonSet模式                           │
│                                                          │
│  生产推荐: DaemonSet + Gateway 混合架构                   │
└─────────────────────────────────────────────────────────┘
```

### 最佳实践总结

```yaml
DO's ✅:
  - 使用DaemonSet或Gateway作为默认
  - 启用memory_limiter和batch processor
  - 配置健康检查和资源限制
  - 部署多实例保证高可用
  - 监控Collector自身指标
  - 使用ConfigMap管理配置
  - 启用TLS加密传输
  - 配置采样降低成本

DON'Ts ❌:
  - 不要在生产使用Agent模式
  - 不要过度使用Sidecar (成本高)
  - 不要忽略资源限制
  - 不要禁用memory_limiter
  - 不要使用过小的batch size
  - 不要忽略Collector监控
  - 不要在配置中硬编码凭证
```

---

**文档版本**: v1.0  
**创建日期**: 2025年10月29日  
**维护者**: OTLP_rust项目团队

---

**下一步**:

1. 评估您的部署规模和需求
2. 选择合适的部署模式
3. 部署完整的服务堆栈
4. 配置监控和告警
5. 进行性能测试和优化
