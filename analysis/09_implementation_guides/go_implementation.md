# Goè¯­è¨€å®ç°æŒ‡å—

## ğŸ“‹ ç›®å½•

- [Goè¯­è¨€å®ç°æŒ‡å—](#goè¯­è¨€å®ç°æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [æ¦‚è¿°](#æ¦‚è¿°)
  - [1. é¡¹ç›®ç»“æ„](#1-é¡¹ç›®ç»“æ„)
  - [2. æ ¸å¿ƒç»„ä»¶å®ç°](#2-æ ¸å¿ƒç»„ä»¶å®ç°)
    - [2.1 OTLPæ”¶é›†å™¨](#21-otlpæ”¶é›†å™¨)
    - [2.2 æ•°æ®å¤„ç†ç®¡é“](#22-æ•°æ®å¤„ç†ç®¡é“)
  - [3. æ€§èƒ½ä¼˜åŒ–](#3-æ€§èƒ½ä¼˜åŒ–)
    - [3.1 å†…å­˜æ± ](#31-å†…å­˜æ± )
    - [3.2 æ‰¹é‡å¤„ç†](#32-æ‰¹é‡å¤„ç†)
  - [4. é…ç½®ç®¡ç†](#4-é…ç½®ç®¡ç†)
    - [4.1 é…ç½®ç»“æ„](#41-é…ç½®ç»“æ„)
    - [4.2 é…ç½®éªŒè¯](#42-é…ç½®éªŒè¯)
  - [5. æµ‹è¯•ç­–ç•¥](#5-æµ‹è¯•ç­–ç•¥)
    - [5.1 å•å…ƒæµ‹è¯•](#51-å•å…ƒæµ‹è¯•)
    - [5.2 é›†æˆæµ‹è¯•](#52-é›†æˆæµ‹è¯•)
  - [6. éƒ¨ç½²ä¸è¿ç»´](#6-éƒ¨ç½²ä¸è¿ç»´)
    - [6.1 Dockeréƒ¨ç½²](#61-dockeréƒ¨ç½²)
    - [6.2 Kuberneteséƒ¨ç½²](#62-kuberneteséƒ¨ç½²)
  - [7. ç›‘æ§ä¸å¯è§‚æµ‹æ€§](#7-ç›‘æ§ä¸å¯è§‚æµ‹æ€§)
    - [7.1 æŒ‡æ ‡æ”¶é›†](#71-æŒ‡æ ‡æ”¶é›†)
    - [7.2 å¥åº·æ£€æŸ¥](#72-å¥åº·æ£€æŸ¥)
  - [8. æœ€ä½³å®è·µ](#8-æœ€ä½³å®è·µ)
    - [8.1 é”™è¯¯å¤„ç†](#81-é”™è¯¯å¤„ç†)
    - [8.2 æ—¥å¿—è®°å½•](#82-æ—¥å¿—è®°å½•)

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›OTLPç³»ç»Ÿåœ¨Goè¯­è¨€ä¸­çš„å®ç°æŒ‡å—ï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€æ ¸å¿ƒç»„ä»¶ã€æ€§èƒ½ä¼˜åŒ–ã€æµ‹è¯•ç­–ç•¥ç­‰ï¼Œä¸ºGoå¼€å‘è€…æä¾›å®Œæ•´çš„å®ç°å‚è€ƒã€‚

## 1. é¡¹ç›®ç»“æ„

```text
otlp-go/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ collector/
â”‚   â”‚   â””â”€â”€ main.go
â”‚   â”œâ”€â”€ exporter/
â”‚   â”‚   â””â”€â”€ main.go
â”‚   â””â”€â”€ processor/
â”‚       â””â”€â”€ main.go
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ collector/
â”‚   â”œâ”€â”€ exporter/
â”‚   â”œâ”€â”€ processor/
â”‚   â”œâ”€â”€ storage/
â”‚   â””â”€â”€ config/
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ otlp/
â”‚   â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ traces/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ proto/
â”œâ”€â”€ examples/
â”œâ”€â”€ tests/
â”œâ”€â”€ docs/
â”œâ”€â”€ go.mod
â”œâ”€â”€ go.sum
â””â”€â”€ Makefile
```

## 2. æ ¸å¿ƒç»„ä»¶å®ç°

### 2.1 OTLPæ”¶é›†å™¨

```go
package collector

import (
    "context"
    "fmt"
    "sync"

    "go.opentelemetry.io/collector/component"
    "go.opentelemetry.io/collector/config"
    "go.opentelemetry.io/collector/consumer"
    "go.opentelemetry.io/collector/pdata/pmetric"
    "go.opentelemetry.io/collector/pdata/ptrace"
    "go.opentelemetry.io/collector/pdata/plog"
)

type OtlpCollector struct {
    config     *Config
    receivers  map[component.ID]component.Receiver
    processors map[component.ID]component.Processor
    exporters  map[component.ID]component.Exporter
    pipelines  map[component.ID]*Pipeline

    mu sync.RWMutex
}

type Config struct {
    Receivers  map[component.ID]config.Receiver  `mapstructure:"receivers"`
    Processors map[component.ID]config.Processor `mapstructure:"processors"`
    Exporters  map[component.ID]config.Exporter  `mapstructure:"exporters"`
    Service    ServiceConfig                     `mapstructure:"service"`
}

type ServiceConfig struct {
    Pipelines map[component.ID]PipelineConfig `mapstructure:"pipelines"`
    Extensions []component.ID                  `mapstructure:"extensions"`
}

type PipelineConfig struct {
    Receivers  []component.ID `mapstructure:"receivers"`
    Processors []component.ID `mapstructure:"processors"`
    Exporters  []component.ID `mapstructure:"exporters"`
}

func NewOtlpCollector(config *Config) *OtlpCollector {
    return &OtlpCollector{
        config:     config,
        receivers:  make(map[component.ID]component.Receiver),
        processors: make(map[component.ID]component.Processor),
        exporters:  make(map[component.ID]component.Exporter),
        pipelines:  make(map[component.ID]*Pipeline),
    }
}

func (c *OtlpCollector) Start(ctx context.Context) error {
    c.mu.Lock()
    defer c.mu.Unlock()

    // å¯åŠ¨æ¥æ”¶å™¨
    for id, receiver := range c.receivers {
        if err := receiver.Start(ctx, c); err != nil {
            return fmt.Errorf("failed to start receiver %s: %w", id, err)
        }
    }

    // å¯åŠ¨å¤„ç†å™¨
    for id, processor := range c.processors {
        if err := processor.Start(ctx, c); err != nil {
            return fmt.Errorf("failed to start processor %s: %w", id, err)
        }
    }

    // å¯åŠ¨å¯¼å‡ºå™¨
    for id, exporter := range c.exporters {
        if err := exporter.Start(ctx, c); err != nil {
            return fmt.Errorf("failed to start exporter %s: %w", id, err)
        }
    }

    // å¯åŠ¨ç®¡é“
    for id, pipeline := range c.pipelines {
        if err := pipeline.Start(ctx); err != nil {
            return fmt.Errorf("failed to start pipeline %s: %w", id, err)
        }
    }

    return nil
}

func (c *OtlpCollector) Shutdown(ctx context.Context) error {
    c.mu.Lock()
    defer c.mu.Unlock()

    // å…³é—­ç®¡é“
    for id, pipeline := range c.pipelines {
        if err := pipeline.Shutdown(ctx); err != nil {
            return fmt.Errorf("failed to shutdown pipeline %s: %w", id, err)
        }
    }

    // å…³é—­å¯¼å‡ºå™¨
    for id, exporter := range c.exporters {
        if err := exporter.Shutdown(ctx); err != nil {
            return fmt.Errorf("failed to shutdown exporter %s: %w", id, err)
        }
    }

    // å…³é—­å¤„ç†å™¨
    for id, processor := range c.processors {
        if err := processor.Shutdown(ctx); err != nil {
            return fmt.Errorf("failed to shutdown processor %s: %w", id, err)
        }
    }

    // å…³é—­æ¥æ”¶å™¨
    for id, receiver := range c.receivers {
        if err := receiver.Shutdown(ctx); err != nil {
            return fmt.Errorf("failed to shutdown receiver %s: %w", id, err)
        }
    }

    return nil
}
```

### 2.2 æ•°æ®å¤„ç†ç®¡é“

```go
package pipeline

import (
    "context"
    "sync"

    "go.opentelemetry.io/collector/consumer"
    "go.opentelemetry.io/collector/pdata/pmetric"
    "go.opentelemetry.io/collector/pdata/ptrace"
    "go.opentelemetry.io/collector/pdata/plog"
)

type Pipeline struct {
    receivers  []consumer.Metrics
    processors []consumer.Metrics
    exporters  []consumer.Metrics

    traceReceivers  []consumer.Traces
    traceProcessors []consumer.Traces
    traceExporters  []consumer.Traces

    logReceivers  []consumer.Logs
    logProcessors []consumer.Logs
    logExporters  []consumer.Logs

    mu sync.RWMutex
}

func (p *Pipeline) ConsumeMetrics(ctx context.Context, md pmetric.Metrics) error {
    p.mu.RLock()
    defer p.mu.RUnlock()

    // é€šè¿‡å¤„ç†å™¨é“¾å¤„ç†æŒ‡æ ‡
    current := md
    for _, processor := range p.processors {
        if err := processor.ConsumeMetrics(ctx, current); err != nil {
            return err
        }
    }

    // å‘é€åˆ°å¯¼å‡ºå™¨
    for _, exporter := range p.exporters {
        if err := exporter.ConsumeMetrics(ctx, current); err != nil {
            return err
        }
    }

    return nil
}

func (p *Pipeline) ConsumeTraces(ctx context.Context, td ptrace.Traces) error {
    p.mu.RLock()
    defer p.mu.RUnlock()

    // é€šè¿‡å¤„ç†å™¨é“¾å¤„ç†è¿½è¸ª
    current := td
    for _, processor := range p.traceProcessors {
        if err := processor.ConsumeTraces(ctx, current); err != nil {
            return err
        }
    }

    // å‘é€åˆ°å¯¼å‡ºå™¨
    for _, exporter := range p.traceExporters {
        if err := exporter.ConsumeTraces(ctx, current); err != nil {
            return err
        }
    }

    return nil
}

func (p *Pipeline) ConsumeLogs(ctx context.Context, ld plog.Logs) error {
    p.mu.RLock()
    defer p.mu.RUnlock()

    // é€šè¿‡å¤„ç†å™¨é“¾å¤„ç†æ—¥å¿—
    current := ld
    for _, processor := range p.logProcessors {
        if err := processor.ConsumeLogs(ctx, current); err != nil {
            return err
        }
    }

    // å‘é€åˆ°å¯¼å‡ºå™¨
    for _, exporter := range p.logExporters {
        if err := exporter.ConsumeLogs(ctx, current); err != nil {
            return err
        }
    }

    return nil
}
```

## 3. æ€§èƒ½ä¼˜åŒ–

### 3.1 å†…å­˜æ± 

```go
package pool

import (
    "sync"
    "go.opentelemetry.io/collector/pdata/pmetric"
)

type MetricsPool struct {
    pool sync.Pool
}

func NewMetricsPool() *MetricsPool {
    return &MetricsPool{
        pool: sync.Pool{
            New: func() interface{} {
                return pmetric.NewMetrics()
            },
        },
    }
}

func (p *MetricsPool) Get() pmetric.Metrics {
    return p.pool.Get().(pmetric.Metrics)
}

func (p *MetricsPool) Put(md pmetric.Metrics) {
    md.Reset()
    p.pool.Put(md)
}

type TracesPool struct {
    pool sync.Pool
}

func NewTracesPool() *TracesPool {
    return &TracesPool{
        pool: sync.Pool{
            New: func() interface{} {
                return ptrace.NewTraces()
            },
        },
    }
}

func (p *TracesPool) Get() ptrace.Traces {
    return p.pool.Get().(ptrace.Traces)
}

func (p *TracesPool) Put(td ptrace.Traces) {
    td.Reset()
    p.pool.Put(td)
}
```

### 3.2 æ‰¹é‡å¤„ç†

```go
package batch

import (
    "context"
    "sync"
    "time"

    "go.opentelemetry.io/collector/pdata/pmetric"
)

type BatchProcessor struct {
    batchSize     int
    flushInterval time.Duration
    buffer        []pmetric.Metrics
    mu            sync.Mutex
    consumer      consumer.Metrics
    done          chan struct{}
}

func NewBatchProcessor(batchSize int, flushInterval time.Duration, consumer consumer.Metrics) *BatchProcessor {
    return &BatchProcessor{
        batchSize:     batchSize,
        flushInterval: flushInterval,
        buffer:        make([]pmetric.Metrics, 0, batchSize),
        consumer:      consumer,
        done:          make(chan struct{}),
    }
}

func (bp *BatchProcessor) Start(ctx context.Context) error {
    go bp.flushLoop(ctx)
    return nil
}

func (bp *BatchProcessor) ConsumeMetrics(ctx context.Context, md pmetric.Metrics) error {
    bp.mu.Lock()
    defer bp.mu.Unlock()

    bp.buffer = append(bp.buffer, md)

    if len(bp.buffer) >= bp.batchSize {
        return bp.flush(ctx)
    }

    return nil
}

func (bp *BatchProcessor) flushLoop(ctx context.Context) {
    ticker := time.NewTicker(bp.flushInterval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            bp.mu.Lock()
            if len(bp.buffer) > 0 {
                bp.flush(ctx)
            }
            bp.mu.Unlock()
        case <-bp.done:
            return
        }
    }
}

func (bp *BatchProcessor) flush(ctx context.Context) error {
    if len(bp.buffer) == 0 {
        return nil
    }

    // åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
    combined := pmetric.NewMetrics()
    for _, md := range bp.buffer {
        md.ResourceMetrics().MoveAndAppendTo(combined.ResourceMetrics())
    }

    // å‘é€åˆ°æ¶ˆè´¹è€…
    err := bp.consumer.ConsumeMetrics(ctx, combined)

    // æ¸…ç©ºç¼“å†²åŒº
    bp.buffer = bp.buffer[:0]

    return err
}
```

## 4. é…ç½®ç®¡ç†

### 4.1 é…ç½®ç»“æ„

```go
package config

import (
    "fmt"
    "time"

    "go.opentelemetry.io/collector/component"
    "go.opentelemetry.io/collector/config/configgrpc"
    "go.opentelemetry.io/collector/config/confighttp"
)

type Config struct {
    Receivers  map[component.ID]config.Receiver  `mapstructure:"receivers"`
    Processors map[component.ID]config.Processor `mapstructure:"processors"`
    Exporters  map[component.ID]config.Exporter  `mapstructure:"exporters"`
    Service    ServiceConfig                     `mapstructure:"service"`
}

type OTLPReceiverConfig struct {
    config.ReceiverSettings `mapstructure:",squash"`
    GRPC                    *configgrpc.GRPCServerSettings `mapstructure:"grpc"`
    HTTP                    *confighttp.HTTPServerSettings `mapstructure:"http"`
}

type BatchProcessorConfig struct {
    config.ProcessorSettings `mapstructure:",squash"`
    Timeout                  time.Duration `mapstructure:"timeout"`
    SendBatchSize            uint32        `mapstructure:"send_batch_size"`
    SendBatchMaxSize         uint32        `mapstructure:"send_batch_max_size"`
}

type OTLPExporterConfig struct {
    config.ExporterSettings `mapstructure:",squash"`
    GRPCClientSettings      configgrpc.GRPCClientSettings `mapstructure:"grpc"`
    HTTPClientSettings      confighttp.HTTPClientSettings `mapstructure:"http"`
    QueueSettings           QueueSettings                 `mapstructure:"sending_queue"`
    RetrySettings           RetrySettings                 `mapstructure:"retry_on_failure"`
}

type QueueSettings struct {
    Enabled      bool          `mapstructure:"enabled"`
    NumConsumers int           `mapstructure:"num_consumers"`
    QueueSize    int           `mapstructure:"queue_size"`
    StorageID    *component.ID `mapstructure:"storage"`
}

type RetrySettings struct {
    Enabled         bool          `mapstructure:"enabled"`
    InitialInterval time.Duration `mapstructure:"initial_interval"`
    MaxInterval     time.Duration `mapstructure:"max_interval"`
    MaxElapsedTime  time.Duration `mapstructure:"max_elapsed_time"`
}
```

### 4.2 é…ç½®éªŒè¯

```go
package config

import (
    "fmt"
    "time"
)

func (cfg *Config) Validate() error {
    if err := cfg.Service.Validate(); err != nil {
        return fmt.Errorf("service config validation failed: %w", err)
    }

    for id, receiver := range cfg.Receivers {
        if err := receiver.Validate(); err != nil {
            return fmt.Errorf("receiver %s validation failed: %w", id, err)
        }
    }

    for id, processor := range cfg.Processors {
        if err := processor.Validate(); err != nil {
            return fmt.Errorf("processor %s validation failed: %w", id, err)
        }
    }

    for id, exporter := range cfg.Exporters {
        if err := exporter.Validate(); err != nil {
            return fmt.Errorf("exporter %s validation failed: %w", id, err)
        }
    }

    return nil
}

func (cfg *OTLPReceiverConfig) Validate() error {
    if cfg.GRPC == nil && cfg.HTTP == nil {
        return fmt.Errorf("either grpc or http must be configured")
    }

    if cfg.GRPC != nil {
        if err := cfg.GRPC.Validate(); err != nil {
            return fmt.Errorf("grpc config validation failed: %w", err)
        }
    }

    if cfg.HTTP != nil {
        if err := cfg.HTTP.Validate(); err != nil {
            return fmt.Errorf("http config validation failed: %w", err)
        }
    }

    return nil
}

func (cfg *BatchProcessorConfig) Validate() error {
    if cfg.Timeout <= 0 {
        return fmt.Errorf("timeout must be positive")
    }

    if cfg.SendBatchSize == 0 {
        return fmt.Errorf("send_batch_size must be positive")
    }

    if cfg.SendBatchMaxSize > 0 && cfg.SendBatchSize > cfg.SendBatchMaxSize {
        return fmt.Errorf("send_batch_size must be less than or equal to send_batch_max_size")
    }

    return nil
}
```

## 5. æµ‹è¯•ç­–ç•¥

### 5.1 å•å…ƒæµ‹è¯•

```go
package collector_test

import (
    "context"
    "testing"
    "time"

    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/require"
    "go.opentelemetry.io/collector/pdata/pmetric"
    "go.opentelemetry.io/collector/pdata/ptrace"
    "go.opentelemetry.io/collector/pdata/plog"
)

func TestOtlpCollector_Start(t *testing.T) {
    config := &Config{
        Receivers:  make(map[component.ID]config.Receiver),
        Processors: make(map[component.ID]config.Processor),
        Exporters:  make(map[component.ID]config.Exporter),
        Service: ServiceConfig{
            Pipelines: make(map[component.ID]PipelineConfig),
        },
    }

    collector := NewOtlpCollector(config)

    ctx := context.Background()
    err := collector.Start(ctx)

    assert.NoError(t, err)
    defer collector.Shutdown(ctx)
}

func TestBatchProcessor_ConsumeMetrics(t *testing.T) {
    consumer := &mockMetricsConsumer{}
    processor := NewBatchProcessor(2, 100*time.Millisecond, consumer)

    ctx := context.Background()
    err := processor.Start(ctx)
    require.NoError(t, err)
    defer processor.Shutdown(ctx)

    // å‘é€å•ä¸ªæŒ‡æ ‡
    md := pmetric.NewMetrics()
    err = processor.ConsumeMetrics(ctx, md)
    assert.NoError(t, err)

    // ç­‰å¾…æ‰¹é‡å¤„ç†
    time.Sleep(150 * time.Millisecond)

    assert.Equal(t, 1, consumer.ConsumeMetricsCallCount())
}

type mockMetricsConsumer struct {
    consumeMetricsCallCount int
    mu                     sync.Mutex
}

func (m *mockMetricsConsumer) ConsumeMetrics(ctx context.Context, md pmetric.Metrics) error {
    m.mu.Lock()
    defer m.mu.Unlock()
    m.consumeMetricsCallCount++
    return nil
}

func (m *mockMetricsConsumer) ConsumeMetricsCallCount() int {
    m.mu.Lock()
    defer m.mu.Unlock()
    return m.consumeMetricsCallCount
}
```

### 5.2 é›†æˆæµ‹è¯•

```go
package integration_test

import (
    "context"
    "testing"
    "time"

    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/require"
    "go.opentelemetry.io/collector/component"
    "go.opentelemetry.io/collector/pdata/pmetric"
)

func TestEndToEndFlow(t *testing.T) {
    // åˆ›å»ºæµ‹è¯•é…ç½®
    config := createTestConfig()

    // åˆ›å»ºæ”¶é›†å™¨
    collector := NewOtlpCollector(config)

    // å¯åŠ¨æ”¶é›†å™¨
    ctx := context.Background()
    err := collector.Start(ctx)
    require.NoError(t, err)
    defer collector.Shutdown(ctx)

    // å‘é€æµ‹è¯•æ•°æ®
    testData := createTestMetrics()
    err = collector.ConsumeMetrics(ctx, testData)
    assert.NoError(t, err)

    // ç­‰å¾…å¤„ç†å®Œæˆ
    time.Sleep(1 * time.Second)

    // éªŒè¯ç»“æœ
    assertMetricsExported(t)
}

func createTestConfig() *Config {
    return &Config{
        Receivers: map[component.ID]config.Receiver{
            component.NewID("otlp"): &OTLPReceiverConfig{
                GRPC: &configgrpc.GRPCServerSettings{
                    NetAddr: confignet.NetAddr{
                        Endpoint: "localhost:4317",
                    },
                },
            },
        },
        Processors: map[component.ID]config.Processor{
            component.NewID("batch"): &BatchProcessorConfig{
                Timeout:       1 * time.Second,
                SendBatchSize: 10,
            },
        },
        Exporters: map[component.ID]config.Exporter{
            component.NewID("otlp"): &OTLPExporterConfig{
                GRPCClientSettings: configgrpc.GRPCClientSettings{
                    Endpoint: "localhost:4318",
                },
            },
        },
        Service: ServiceConfig{
            Pipelines: map[component.ID]PipelineConfig{
                component.NewID("metrics"): {
                    Receivers:  []component.ID{component.NewID("otlp")},
                    Processors: []component.ID{component.NewID("batch")},
                    Exporters:  []component.ID{component.NewID("otlp")},
                },
            },
        },
    }
}
```

## 6. éƒ¨ç½²ä¸è¿ç»´

### 6.1 Dockeréƒ¨ç½²

```dockerfile
FROM golang:1.21-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -o otlp-collector ./cmd/collector

FROM alpine:latest
RUN apk --no-cache add ca-certificates
WORKDIR /root/

COPY --from=builder /app/otlp-collector .
COPY --from=builder /app/config.yaml .

EXPOSE 4317 4318 8888 8889

CMD ["./otlp-collector", "--config=config.yaml"]
```

### 6.2 Kuberneteséƒ¨ç½²

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-collector
spec:
  replicas: 3
  selector:
    matchLabels:
      app: otlp-collector
  template:
    metadata:
      labels:
        app: otlp-collector
    spec:
      containers:
      - name: otlp-collector
        image: otlp-collector:latest
        ports:
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        - containerPort: 8888
          name: metrics
        - containerPort: 8889
          name: health
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /
            port: 8889
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 8889
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: otlp-collector
spec:
  selector:
    app: otlp-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
  - name: otlp-http
    port: 4318
    targetPort: 4318
  - name: metrics
    port: 8888
    targetPort: 8888
  - name: health
    port: 8889
    targetPort: 8889
```

## 7. ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### 7.1 æŒ‡æ ‡æ”¶é›†

```go
package metrics

import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    metricsReceived = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "otlp_collector_metrics_received_total",
            Help: "Total number of metrics received",
        },
        []string{"receiver", "type"},
    )

    metricsProcessed = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "otlp_collector_metrics_processed_total",
            Help: "Total number of metrics processed",
        },
        []string{"processor", "type"},
    )

    metricsExported = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "otlp_collector_metrics_exported_total",
            Help: "Total number of metrics exported",
        },
        []string{"exporter", "type"},
    )

    processingDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "otlp_collector_processing_duration_seconds",
            Help: "Processing duration in seconds",
        },
        []string{"component", "type"},
    )
)
```

### 7.2 å¥åº·æ£€æŸ¥

```go
package health

import (
    "context"
    "net/http"
    "time"
)

type HealthChecker struct {
    components map[string]Component
}

type Component interface {
    HealthCheck(ctx context.Context) error
}

func (hc *HealthChecker) CheckHealth(ctx context.Context) map[string]error {
    results := make(map[string]error)

    for name, component := range hc.components {
        ctx, cancel := context.WithTimeout(ctx, 5*time.Second)
        defer cancel()

        results[name] = component.HealthCheck(ctx)
    }

    return results
}

func (hc *HealthChecker) ServeHTTP(w http.ResponseWriter, r *http.Request) {
    results := hc.CheckHealth(r.Context())

    status := http.StatusOK
    for _, err := range results {
        if err != nil {
            status = http.StatusServiceUnavailable
            break
        }
    }

    w.WriteHeader(status)

    if status == http.StatusOK {
        w.Write([]byte("OK"))
    } else {
        w.Write([]byte("Service Unavailable"))
    }
}
```

## 8. æœ€ä½³å®è·µ

### 8.1 é”™è¯¯å¤„ç†

```go
package errors

import (
    "fmt"
    "errors"
)

var (
    ErrInvalidConfig     = errors.New("invalid configuration")
    ErrComponentNotFound = errors.New("component not found")
    ErrStartupFailed     = errors.New("startup failed")
    ErrShutdownFailed    = errors.New("shutdown failed")
)

type ComponentError struct {
    Component string
    Operation string
    Err       error
}

func (e *ComponentError) Error() string {
    return fmt.Sprintf("component %s %s failed: %v", e.Component, e.Operation, e.Err)
}

func (e *ComponentError) Unwrap() error {
    return e.Err
}

func NewComponentError(component, operation string, err error) *ComponentError {
    return &ComponentError{
        Component: component,
        Operation: operation,
        Err:       err,
    }
}
```

### 8.2 æ—¥å¿—è®°å½•

```go
package logging

import (
    "go.uber.org/zap"
    "go.uber.org/zap/zapcore"
)

func NewLogger(level string) (*zap.Logger, error) {
    config := zap.NewProductionConfig()

    switch level {
    case "debug":
        config.Level = zap.NewAtomicLevelAt(zapcore.DebugLevel)
    case "info":
        config.Level = zap.NewAtomicLevelAt(zapcore.InfoLevel)
    case "warn":
        config.Level = zap.NewAtomicLevelAt(zapcore.WarnLevel)
    case "error":
        config.Level = zap.NewAtomicLevelAt(zapcore.ErrorLevel)
    default:
        config.Level = zap.NewAtomicLevelAt(zapcore.InfoLevel)
    }

    return config.Build()
}

func WithComponent(logger *zap.Logger, component string) *zap.Logger {
    return logger.With(zap.String("component", component))
}
```

---

_æœ¬æ–‡æ¡£æä¾›äº†OTLPç³»ç»Ÿåœ¨Goè¯­è¨€ä¸­çš„å®Œæ•´å®ç°æŒ‡å—ï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€æ ¸å¿ƒç»„ä»¶ã€æ€§èƒ½ä¼˜åŒ–ã€æµ‹è¯•ç­–ç•¥å’Œéƒ¨ç½²è¿ç»´ç­‰å„ä¸ªæ–¹é¢ã€‚_
