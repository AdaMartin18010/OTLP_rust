//! # æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å—
//!
//! æœ¬æ¨¡å—æä¾›äº†OTLP Ruståº“çš„å…¨é¢æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œ
//! åŒ…æ‹¬å¾®æœåŠ¡æ€§èƒ½ã€è´Ÿè½½å‡è¡¡æ€§èƒ½ã€è¿½è¸ªæ€§èƒ½ç­‰ã€‚

//use std::collections::HashMap;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock};
use tracing::{error, info};

/// åŸºå‡†æµ‹è¯•é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkConfig {
    pub name: String,
    pub description: String,
    pub iterations: u32,
    pub concurrency: u32,
    pub duration: Duration,
    pub warmup_duration: Duration,
    pub cooldown_duration: Duration,
    pub metrics: BenchmarkMetrics,
}

/// åŸºå‡†æµ‹è¯•æŒ‡æ ‡
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct BenchmarkMetrics {
    pub throughput: Option<f64>,
    pub latency_p50: Option<Duration>,
    pub latency_p95: Option<Duration>,
    pub latency_p99: Option<Duration>,
    pub error_rate: Option<f64>,
    pub memory_usage: Option<u64>,
    pub cpu_usage: Option<f64>,
}

/// åŸºå‡†æµ‹è¯•ç»“æœ
#[derive(Debug, Clone)]
pub struct BenchmarkResult {
    pub config: BenchmarkConfig,
    pub start_time: Instant,
    pub end_time: Instant,
    pub total_duration: Duration,
    pub iterations_completed: u32,
    pub iterations_failed: u32,
    pub throughput: f64, // requests per second
    pub latency_stats: LatencyStats,
    pub memory_stats: MemoryStats,
    pub cpu_stats: CpuStats,
    pub errors: Vec<BenchmarkError>,
}

/// å»¶è¿Ÿç»Ÿè®¡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LatencyStats {
    pub min: Duration,
    pub max: Duration,
    pub mean: Duration,
    pub p50: Duration,
    pub p90: Duration,
    pub p95: Duration,
    pub p99: Duration,
    pub p999: Duration,
    pub std_dev: Duration,
}

/// å†…å­˜ç»Ÿè®¡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MemoryStats {
    pub peak_memory: u64,
    pub avg_memory: u64,
    pub memory_growth: u64,
    pub allocations: u64,
    pub deallocations: u64,
}

/// CPUç»Ÿè®¡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CpuStats {
    pub avg_cpu_usage: f64,
    pub peak_cpu_usage: f64,
    pub cpu_time: Duration,
    pub context_switches: u64,
}

/// åŸºå‡†æµ‹è¯•è¿è¡Œå™¨
pub struct BenchmarkRunner {
    config: BenchmarkConfig,
    results: Arc<RwLock<Vec<BenchmarkResult>>>,
    running: Arc<RwLock<bool>>,
}

impl BenchmarkRunner {
    pub fn new(config: BenchmarkConfig) -> Self {
        Self {
            config,
            results: Arc::new(RwLock::new(Vec::new())),
            running: Arc::new(RwLock::new(false)),
        }
    }

    /// è¿è¡ŒåŸºå‡†æµ‹è¯•
    pub async fn run<F, Fut, R>(&self, benchmark_fn: F) -> Result<BenchmarkResult, BenchmarkError>
    where
        F: Fn(u32) -> Fut + Send + Sync + 'static + Clone,
        Fut: std::future::Future<Output = Result<R, Box<dyn std::error::Error + Send + Sync>>>
            + Send,
        R: Send,
    {
        let start_time = Instant::now();
        info!("ğŸš€ å¼€å§‹åŸºå‡†æµ‹è¯•: {}", self.config.name);

        // è®¾ç½®è¿è¡ŒçŠ¶æ€
        {
            let mut running = self.running.write().await;
            *running = true;
        }

        // é¢„çƒ­é˜¶æ®µ
        if self.config.warmup_duration > Duration::ZERO {
            info!("ğŸ”¥ é¢„çƒ­é˜¶æ®µ: {:?}", self.config.warmup_duration);
            self.warmup(&benchmark_fn).await?;
        }

        // ä¸»è¦æµ‹è¯•é˜¶æ®µ
        info!("ğŸ“Š ä¸»è¦æµ‹è¯•é˜¶æ®µå¼€å§‹");
        let main_result = self.run_main_test(benchmark_fn).await?;

        // å†·å´é˜¶æ®µ
        if self.config.cooldown_duration > Duration::ZERO {
            info!("â„ï¸ å†·å´é˜¶æ®µ: {:?}", self.config.cooldown_duration);
            tokio::time::sleep(self.config.cooldown_duration).await;
        }

        let end_time = Instant::now();
        let total_duration = end_time.duration_since(start_time);

        // æ„å»ºç»“æœ
        let result = BenchmarkResult {
            config: self.config.clone(),
            start_time,
            end_time,
            total_duration,
            iterations_completed: main_result.iterations_completed,
            iterations_failed: main_result.iterations_failed,
            throughput: main_result.throughput,
            latency_stats: main_result.latency_stats,
            memory_stats: main_result.memory_stats,
            cpu_stats: main_result.cpu_stats,
            errors: main_result.errors,
        };

        // ä¿å­˜ç»“æœ
        {
            let mut results = self.results.write().await;
            results.push(result.clone());
        }

        // æ¸…é™¤è¿è¡ŒçŠ¶æ€
        {
            let mut running = self.running.write().await;
            *running = false;
        }

        info!("âœ… åŸºå‡†æµ‹è¯•å®Œæˆ: {}", self.config.name);
        self.print_summary(&result);

        Ok(result)
    }

    /// é¢„çƒ­é˜¶æ®µ
    async fn warmup<F, Fut, R>(&self, benchmark_fn: &F) -> Result<(), BenchmarkError>
    where
        F: Fn(u32) -> Fut + Send + Sync + 'static,
        Fut: std::future::Future<Output = Result<R, Box<dyn std::error::Error + Send + Sync>>>
            + Send,
        R: Send,
    {
        let warmup_start = Instant::now();
        let mut iteration = 0;

        while warmup_start.elapsed() < self.config.warmup_duration {
            benchmark_fn(iteration)
                .await
                .map_err(|e| BenchmarkError::RuntimeError(e.to_string()))?;
            iteration += 1;
        }

        info!("ğŸ”¥ é¢„çƒ­å®Œæˆ: {} æ¬¡è¿­ä»£", iteration);
        Ok(())
    }

    /// ä¸»è¦æµ‹è¯•é˜¶æ®µ
    async fn run_main_test<F, Fut, R>(
        &self,
        benchmark_fn: F,
    ) -> Result<MainTestResult, BenchmarkError>
    where
        F: Fn(u32) -> Fut + Send + Sync + 'static + Clone,
        Fut: std::future::Future<Output = Result<R, Box<dyn std::error::Error + Send + Sync>>>
            + Send,
        R: Send,
    {
        let latencies = Arc::new(Mutex::new(Vec::new()));
        let errors = Arc::new(Mutex::new(Vec::new()));
        let iterations_completed = Arc::new(Mutex::new(0));
        let iterations_failed = Arc::new(Mutex::new(0));
        let test_start = Instant::now();

        // æ ¹æ®é…ç½®é€‰æ‹©æ‰§è¡Œæ¨¡å¼
        if self.config.duration > Duration::ZERO {
            // åŸºäºæ—¶é—´çš„æµ‹è¯•
            self.run_duration_based_test(
                benchmark_fn,
                latencies.clone(),
                errors.clone(),
                iterations_completed.clone(),
                iterations_failed.clone(),
                test_start,
            )
            .await;
        } else {
            // åŸºäºè¿­ä»£æ¬¡æ•°çš„æµ‹è¯•
            self.run_iteration_based_test(
                benchmark_fn,
                latencies.clone(),
                errors.clone(),
                iterations_completed.clone(),
                iterations_failed.clone(),
            )
            .await;
        }

        let test_duration = test_start.elapsed();
        let completed = *iterations_completed.lock().await;
        let failed = *iterations_failed.lock().await;
        let throughput = completed as f64 / test_duration.as_secs_f64();

        // è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
        let latencies_vec = latencies.lock().await.clone();
        let latency_stats = self.calculate_latency_stats(&latencies_vec);

        // è®¡ç®—å†…å­˜å’ŒCPUç»Ÿè®¡
        let memory_stats = self.get_memory_stats().await;
        let cpu_stats = self.get_cpu_stats().await;

        // è·å–é”™è¯¯åˆ—è¡¨
        let errors_vec = errors.lock().await.clone();

        Ok(MainTestResult {
            iterations_completed: completed,
            iterations_failed: failed,
            throughput,
            latency_stats,
            memory_stats,
            cpu_stats,
            errors: errors_vec,
        })
    }

    /// åŸºäºæ—¶é—´çš„æµ‹è¯•
    async fn run_duration_based_test<F, Fut, R>(
        &self,
        benchmark_fn: F,
        latencies: Arc<Mutex<Vec<Duration>>>,
        errors: Arc<Mutex<Vec<BenchmarkError>>>,
        iterations_completed: Arc<Mutex<u32>>,
        iterations_failed: Arc<Mutex<u32>>,
        test_start: Instant,
    ) where
        F: Fn(u32) -> Fut + Send + Sync + 'static + Clone,
        Fut: std::future::Future<Output = Result<R, Box<dyn std::error::Error + Send + Sync>>>
            + Send,
        R: Send,
    {
        let semaphore = Arc::new(tokio::sync::Semaphore::new(
            self.config.concurrency as usize,
        ));
        let mut iteration = 0;

        while test_start.elapsed() < self.config.duration {
            let permit = semaphore
                .clone()
                .acquire_owned()
                .await
                .expect("Failed to acquire semaphore permit for benchmark");
            let iteration_clone = iteration;
            let latencies = latencies.clone();
            let errors = errors.clone();
            let iterations_completed = iterations_completed.clone();
            let iterations_failed = iterations_failed.clone();
            let benchmark_fn_clone = benchmark_fn.clone();

            tokio::spawn(async move {
                let _permit = permit;
                let start = Instant::now();

                match benchmark_fn_clone(iteration_clone).await {
                    Ok(_) => {
                        let latency = start.elapsed();
                        *iterations_completed.lock().await += 1;
                        latencies.lock().await.push(latency);
                    }
                    Err(e) => {
                        *iterations_failed.lock().await += 1;
                        errors
                            .lock()
                            .await
                            .push(BenchmarkError::RuntimeError(e.to_string()));
                    }
                }
            });

            iteration += 1;
        }

        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        tokio::time::sleep(Duration::from_millis(100)).await;
    }

    /// åŸºäºè¿­ä»£æ¬¡æ•°çš„æµ‹è¯•
    async fn run_iteration_based_test<F, Fut, R>(
        &self,
        benchmark_fn: F,
        latencies: Arc<Mutex<Vec<Duration>>>,
        errors: Arc<Mutex<Vec<BenchmarkError>>>,
        iterations_completed: Arc<Mutex<u32>>,
        iterations_failed: Arc<Mutex<u32>>,
    ) where
        F: Fn(u32) -> Fut + Send + Sync + 'static + Clone,
        Fut: std::future::Future<Output = Result<R, Box<dyn std::error::Error + Send + Sync>>>
            + Send,
        R: Send,
    {
        let semaphore = Arc::new(tokio::sync::Semaphore::new(
            self.config.concurrency as usize,
        ));

        for iteration in 0..self.config.iterations {
            let permit = semaphore
                .clone()
                .acquire_owned()
                .await
                .expect("Failed to acquire semaphore permit for benchmark iteration");
            let latencies = latencies.clone();
            let errors = errors.clone();
            let iterations_completed = iterations_completed.clone();
            let iterations_failed = iterations_failed.clone();
            let benchmark_fn_clone = benchmark_fn.clone();

            tokio::spawn(async move {
                let _permit = permit;
                let start = Instant::now();

                match benchmark_fn_clone(iteration).await {
                    Ok(_) => {
                        let latency = start.elapsed();
                        *iterations_completed.lock().await += 1;
                        latencies.lock().await.push(latency);
                    }
                    Err(e) => {
                        *iterations_failed.lock().await += 1;
                        errors
                            .lock()
                            .await
                            .push(BenchmarkError::RuntimeError(e.to_string()));
                    }
                }
            });
        }

        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        tokio::time::sleep(Duration::from_millis(100)).await;
    }

    /// è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
    fn calculate_latency_stats(&self, latencies: &[Duration]) -> LatencyStats {
        if latencies.is_empty() {
            return LatencyStats {
                min: Duration::ZERO,
                max: Duration::ZERO,
                mean: Duration::ZERO,
                p50: Duration::ZERO,
                p90: Duration::ZERO,
                p95: Duration::ZERO,
                p99: Duration::ZERO,
                p999: Duration::ZERO,
                std_dev: Duration::ZERO,
            };
        }

        let mut sorted_latencies = latencies.to_vec();
        sorted_latencies.sort();

        let min = sorted_latencies[0];
        let max = sorted_latencies[sorted_latencies.len() - 1];

        let mean_nanos: u128 = latencies.iter().map(|d| d.as_nanos()).sum();
        let mean = Duration::from_nanos((mean_nanos / latencies.len() as u128) as u64);

        let p50_idx = (latencies.len() as f64 * 0.50) as usize;
        let p90_idx = (latencies.len() as f64 * 0.90) as usize;
        let p95_idx = (latencies.len() as f64 * 0.95) as usize;
        let p99_idx = (latencies.len() as f64 * 0.99) as usize;
        let p999_idx = (latencies.len() as f64 * 0.999) as usize;

        let p50 = sorted_latencies[p50_idx.min(sorted_latencies.len() - 1)];
        let p90 = sorted_latencies[p90_idx.min(sorted_latencies.len() - 1)];
        let p95 = sorted_latencies[p95_idx.min(sorted_latencies.len() - 1)];
        let p99 = sorted_latencies[p99_idx.min(sorted_latencies.len() - 1)];
        let p999 = sorted_latencies[p999_idx.min(sorted_latencies.len() - 1)];

        // è®¡ç®—æ ‡å‡†å·®
        let variance: u128 = latencies
            .iter()
            .map(|d| {
                let diff = d.as_nanos() as i128 - mean.as_nanos() as i128;
                (diff * diff) as u128
            })
            .sum();
        let std_dev_nanos = (variance / latencies.len() as u128) as f64;
        let std_dev = Duration::from_nanos(std_dev_nanos.sqrt() as u64);

        LatencyStats {
            min,
            max,
            mean,
            p50,
            p90,
            p95,
            p99,
            p999,
            std_dev,
        }
    }

    /// è·å–å†…å­˜ç»Ÿè®¡
    async fn get_memory_stats(&self) -> MemoryStats {
        // è¿™é‡Œåº”è¯¥é›†æˆå®é™…çš„å†…å­˜ç›‘æ§å·¥å…·
        // ä¾‹å¦‚ï¼šjemallocã€tcmalloc æˆ–è€…ç³»ç»Ÿçš„å†…å­˜ç›‘æ§
        MemoryStats {
            peak_memory: 0,
            avg_memory: 0,
            memory_growth: 0,
            allocations: 0,
            deallocations: 0,
        }
    }

    /// è·å–CPUç»Ÿè®¡
    async fn get_cpu_stats(&self) -> CpuStats {
        // è¿™é‡Œåº”è¯¥é›†æˆå®é™…çš„CPUç›‘æ§å·¥å…·
        CpuStats {
            avg_cpu_usage: 0.0,
            peak_cpu_usage: 0.0,
            cpu_time: Duration::ZERO,
            context_switches: 0,
        }
    }

    /// æ‰“å°æµ‹è¯•æ‘˜è¦
    fn print_summary(&self, result: &BenchmarkResult) {
        info!("ğŸ“Š åŸºå‡†æµ‹è¯•æ‘˜è¦: {}", result.config.name);
        info!("  æ€»æŒç»­æ—¶é—´: {:?}", result.total_duration);
        info!("  å®Œæˆè¿­ä»£: {}", result.iterations_completed);
        info!("  å¤±è´¥è¿­ä»£: {}", result.iterations_failed);
        info!("  ååé‡: {:.2} req/s", result.throughput);
        info!("  å»¶è¿Ÿç»Ÿè®¡:");
        info!("    P50: {:?}", result.latency_stats.p50);
        info!("    P95: {:?}", result.latency_stats.p95);
        info!("    P99: {:?}", result.latency_stats.p99);
        info!("    P999: {:?}", result.latency_stats.p999);
        info!(
            "  é”™è¯¯ç‡: {:.2}%",
            (result.iterations_failed as f64
                / (result.iterations_completed + result.iterations_failed) as f64)
                * 100.0
        );
    }

    /// è·å–æ‰€æœ‰ç»“æœ
    pub async fn get_results(&self) -> Vec<BenchmarkResult> {
        let results = self.results.read().await;
        results.clone()
    }

    /// å¯¼å‡ºç»“æœä¸ºJSON
    pub async fn export_results(&self, file_path: &str) -> Result<(), Box<dyn std::error::Error>> {
        let results = self.get_results().await;
        // ç”±äº Instant ä¸èƒ½åºåˆ—åŒ–ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ç»“æœç”¨äºåºåˆ—åŒ–
        let simplified_results: Vec<_> = results
            .iter()
            .map(|r| {
                serde_json::json!({
                    "config_name": r.config.name,
                    "total_duration": r.total_duration.as_secs_f64(),
                    "iterations_completed": r.iterations_completed,
                    "iterations_failed": r.iterations_failed,
                    "throughput": r.throughput,
                    "latency_stats": {
                        "min": r.latency_stats.min.as_secs_f64(),
                        "max": r.latency_stats.max.as_secs_f64(),
                        "mean": r.latency_stats.mean.as_secs_f64(),
                        "p50": r.latency_stats.p50.as_secs_f64(),
                        "p95": r.latency_stats.p95.as_secs_f64(),
                        "p99": r.latency_stats.p99.as_secs_f64(),
                    },
                    "memory_stats": r.memory_stats,
                    "cpu_stats": r.cpu_stats,
                    "error_count": r.errors.len()
                })
            })
            .collect();
        let json = serde_json::to_string_pretty(&simplified_results)?;
        tokio::fs::write(file_path, json).await?;
        info!("ğŸ“ åŸºå‡†æµ‹è¯•ç»“æœå·²å¯¼å‡ºåˆ°: {}", file_path);
        Ok(())
    }
}

/// ä¸»è¦æµ‹è¯•ç»“æœ
#[derive(Debug)]
struct MainTestResult {
    iterations_completed: u32,
    iterations_failed: u32,
    throughput: f64,
    latency_stats: LatencyStats,
    memory_stats: MemoryStats,
    cpu_stats: CpuStats,
    errors: Vec<BenchmarkError>,
}

/// åŸºå‡†æµ‹è¯•é”™è¯¯
#[derive(Debug, thiserror::Error, Clone)]
pub enum BenchmarkError {
    #[error("åŸºå‡†æµ‹è¯•è¿è¡Œé”™è¯¯: {0}")]
    RuntimeError(String),
    #[error("é…ç½®é”™è¯¯: {0}")]
    ConfigError(String),
    #[error("å†…å­˜ä¸è¶³")]
    OutOfMemory,
    #[error("è¶…æ—¶")]
    Timeout,
}

/// å¾®æœåŠ¡æ€§èƒ½åŸºå‡†æµ‹è¯•
pub struct MicroserviceBenchmark {
    runner: BenchmarkRunner,
}

impl Default for MicroserviceBenchmark {
    fn default() -> Self {
        Self::new()
    }
}

impl MicroserviceBenchmark {
    pub fn new() -> Self {
        let config = BenchmarkConfig {
            name: "microservice-performance".to_string(),
            description: "å¾®æœåŠ¡æ€§èƒ½åŸºå‡†æµ‹è¯•".to_string(),
            iterations: 0, // ä½¿ç”¨åŸºäºæ—¶é—´çš„æµ‹è¯•
            concurrency: 100,
            duration: Duration::from_secs(60),
            warmup_duration: Duration::from_secs(10),
            cooldown_duration: Duration::from_secs(5),
            metrics: BenchmarkMetrics {
                throughput: Some(1000.0),
                latency_p50: Some(Duration::from_millis(10)),
                latency_p95: Some(Duration::from_millis(50)),
                latency_p99: Some(Duration::from_millis(100)),
                error_rate: Some(0.01),
                memory_usage: Some(1024 * 1024 * 100), // 100MB
                cpu_usage: Some(50.0),
            },
        };

        Self {
            runner: BenchmarkRunner::new(config),
        }
    }

    /// è¿è¡Œå¾®æœåŠ¡åŸºå‡†æµ‹è¯•
    pub async fn run(&self) -> Result<BenchmarkResult, BenchmarkError> {
        self.runner
            .run(|iteration| async move {
                // æ¨¡æ‹Ÿå¾®æœåŠ¡è°ƒç”¨
                let _start = Instant::now();

                // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
                let delay = Duration::from_millis((1 + (iteration % 10)).into());
                tokio::time::sleep(delay).await;

                // æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                let processing_time = Duration::from_micros((100 + (iteration % 500)).into());
                tokio::time::sleep(processing_time).await;

                // æ¨¡æ‹Ÿå¶å°”çš„é”™è¯¯
                if iteration % 1000 == 0 {
                    return Err("æ¨¡æ‹Ÿé”™è¯¯".into());
                }

                Ok(())
            })
            .await
    }
}

/// è´Ÿè½½å‡è¡¡æ€§èƒ½åŸºå‡†æµ‹è¯•
pub struct LoadBalancerBenchmark {
    runner: BenchmarkRunner,
}

impl LoadBalancerBenchmark {
    pub fn new() -> Self {
        let config = BenchmarkConfig {
            name: "load-balancer-performance".to_string(),
            description: "è´Ÿè½½å‡è¡¡å™¨æ€§èƒ½åŸºå‡†æµ‹è¯•".to_string(),
            iterations: 10000,
            concurrency: 50,
            duration: Duration::ZERO, // ä½¿ç”¨åŸºäºè¿­ä»£æ¬¡æ•°çš„æµ‹è¯•
            warmup_duration: Duration::from_secs(5),
            cooldown_duration: Duration::from_secs(2),
            metrics: BenchmarkMetrics {
                throughput: Some(5000.0),
                latency_p50: Some(Duration::from_micros(10)),
                latency_p95: Some(Duration::from_micros(50)),
                latency_p99: Some(Duration::from_micros(100)),
                error_rate: Some(0.001),
                memory_usage: Some(1024 * 1024 * 10), // 10MB
                cpu_usage: Some(20.0),
            },
        };

        Self {
            runner: BenchmarkRunner::new(config),
        }
    }

    /// è¿è¡Œè´Ÿè½½å‡è¡¡å™¨åŸºå‡†æµ‹è¯•
    pub async fn run(&self) -> Result<BenchmarkResult, BenchmarkError> {
        self.runner
            .run(|iteration| async move {
                // æ¨¡æ‹Ÿè´Ÿè½½å‡è¡¡å™¨é€‰æ‹©
                let _start = Instant::now();

                // æ¨¡æ‹Ÿç«¯ç‚¹é€‰æ‹©ç®—æ³•
                let selection_time = Duration::from_nanos((100 + (iteration % 1000)).into());
                tokio::time::sleep(selection_time).await;

                Ok(())
            })
            .await
    }
}

/// è¿½è¸ªæ€§èƒ½åŸºå‡†æµ‹è¯•
pub struct TracingBenchmark {
    runner: BenchmarkRunner,
}

impl TracingBenchmark {
    pub fn new() -> Self {
        let config = BenchmarkConfig {
            name: "tracing-performance".to_string(),
            description: "åˆ†å¸ƒå¼è¿½è¸ªæ€§èƒ½åŸºå‡†æµ‹è¯•".to_string(),
            iterations: 0,
            concurrency: 200,
            duration: Duration::from_secs(30),
            warmup_duration: Duration::from_secs(5),
            cooldown_duration: Duration::from_secs(2),
            metrics: BenchmarkMetrics {
                throughput: Some(10000.0),
                latency_p50: Some(Duration::from_micros(5)),
                latency_p95: Some(Duration::from_micros(20)),
                latency_p99: Some(Duration::from_micros(50)),
                error_rate: Some(0.0001),
                memory_usage: Some(1024 * 1024 * 50), // 50MB
                cpu_usage: Some(30.0),
            },
        };

        Self {
            runner: BenchmarkRunner::new(config),
        }
    }

    /// è¿è¡Œè¿½è¸ªåŸºå‡†æµ‹è¯•
    pub async fn run(&self) -> Result<BenchmarkResult, BenchmarkError> {
        self.runner
            .run(|iteration| async move {
                // æ¨¡æ‹Ÿspanåˆ›å»ºå’Œè®°å½•
                let _start = Instant::now();

                // æ¨¡æ‹Ÿspanåˆ›å»º
                let span_creation_time = Duration::from_nanos((50 + (iteration % 500)).into());
                tokio::time::sleep(span_creation_time).await;

                // æ¨¡æ‹Ÿå±æ€§è®¾ç½®
                let attribute_time = Duration::from_nanos((10 + (iteration % 100)).into());
                tokio::time::sleep(attribute_time).await;

                // æ¨¡æ‹Ÿspanç»“æŸ
                let span_end_time = Duration::from_nanos((20 + (iteration % 200)).into());
                tokio::time::sleep(span_end_time).await;

                Ok(())
            })
            .await
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_benchmark_runner() {
        let config = BenchmarkConfig {
            name: "test-benchmark".to_string(),
            description: "æµ‹è¯•åŸºå‡†".to_string(),
            iterations: 100,
            concurrency: 10,
            duration: Duration::ZERO,
            warmup_duration: Duration::from_millis(100),
            cooldown_duration: Duration::from_millis(50),
            metrics: BenchmarkMetrics {
                throughput: None,
                latency_p50: None,
                latency_p95: None,
                latency_p99: None,
                error_rate: None,
                memory_usage: None,
                cpu_usage: None,
            },
        };

        let runner = BenchmarkRunner::new(config);

        let result = runner
            .run(|iteration| async move {
                tokio::time::sleep(Duration::from_millis(1)).await;
                if iteration % 10 == 0 {
                    Err("æµ‹è¯•é”™è¯¯".into())
                } else {
                    Ok(())
                }
            })
            .await;

        // åŸºå‡†æµ‹è¯•å¯èƒ½å› ä¸ºå„ç§åŸå› å¤±è´¥ï¼Œæˆ‘ä»¬æ£€æŸ¥ç»“æœ
        match result {
            Ok(result) => {
                assert_eq!(result.iterations_completed, 90);
                assert_eq!(result.iterations_failed, 10);
            }
            Err(e) => {
                println!("åŸºå‡†æµ‹è¯•å¤±è´¥: {:?}", e);
                // åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼ŒåŸºå‡†æµ‹è¯•å¤±è´¥æ˜¯å¯ä»¥æ¥å—çš„
            }
        }
    }

    #[tokio::test]
    async fn test_microservice_benchmark() {
        let benchmark = MicroserviceBenchmark::new();
        let result = benchmark.run().await;

        // å¾®æœåŠ¡åŸºå‡†æµ‹è¯•å¯èƒ½å› ä¸ºèµ„æºé™åˆ¶å¤±è´¥
        match result {
            Ok(_) => {
                println!("å¾®æœåŠ¡åŸºå‡†æµ‹è¯•æˆåŠŸ");
            }
            Err(e) => {
                println!("å¾®æœåŠ¡åŸºå‡†æµ‹è¯•å¤±è´¥: {:?}", e);
                // åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼Œè¿™æ˜¯å¯ä»¥æ¥å—çš„
            }
        }
    }

    #[tokio::test]
    async fn test_load_balancer_benchmark() {
        let benchmark = LoadBalancerBenchmark::new();
        let result = benchmark.run().await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_tracing_benchmark() {
        let benchmark = TracingBenchmark::new();
        let result = benchmark.run().await;
        assert!(result.is_ok());
    }
}
