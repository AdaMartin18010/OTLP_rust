# é¡¹ç›®å®æ–½ä¸éªŒè¯å®Œæ•´æŒ‡å—

> **ç‰ˆæœ¬**: 1.0  
> **æ—¥æœŸ**: 2025å¹´10æœˆ17æ—¥  
> **çŠ¶æ€**: âœ… å®Œæ•´ç‰ˆ

---

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®å®æ–½ä¸éªŒè¯å®Œæ•´æŒ‡å—](#é¡¹ç›®å®æ–½ä¸éªŒè¯å®Œæ•´æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ç¬¬ä¸€éƒ¨åˆ†: å®æ–½è§„åˆ’](#ç¬¬ä¸€éƒ¨åˆ†-å®æ–½è§„åˆ’)
    - [1.1 å®æ–½é˜¶æ®µ](#11-å®æ–½é˜¶æ®µ)
    - [1.2 è§„åˆ’æ£€æŸ¥æ¸…å•](#12-è§„åˆ’æ£€æŸ¥æ¸…å•)
    - [1.3 æŠ€æœ¯é€‰å‹å†³ç­–](#13-æŠ€æœ¯é€‰å‹å†³ç­–)
  - [ç¬¬äºŒéƒ¨åˆ†: ç¯å¢ƒå‡†å¤‡](#ç¬¬äºŒéƒ¨åˆ†-ç¯å¢ƒå‡†å¤‡)
    - [2.1 åŸºç¡€è®¾æ–½éœ€æ±‚](#21-åŸºç¡€è®¾æ–½éœ€æ±‚)
      - [2.1.1 ç¡¬ä»¶èµ„æº](#211-ç¡¬ä»¶èµ„æº)
      - [2.1.2 ç½‘ç»œé…ç½®](#212-ç½‘ç»œé…ç½®)
    - [2.2 è½¯ä»¶å®‰è£…](#22-è½¯ä»¶å®‰è£…)
      - [2.2.1 Dockerç¯å¢ƒ](#221-dockerç¯å¢ƒ)
      - [2.2.2 Kubernetesç¯å¢ƒ](#222-kubernetesç¯å¢ƒ)
  - [ç¬¬ä¸‰éƒ¨åˆ†: Collectoréƒ¨ç½²](#ç¬¬ä¸‰éƒ¨åˆ†-collectoréƒ¨ç½²)
    - [3.1 Docker Composeéƒ¨ç½²](#31-docker-composeéƒ¨ç½²)
      - [3.1.1 å®Œæ•´ç¤ºä¾‹](#311-å®Œæ•´ç¤ºä¾‹)
    - [3.2 Kuberneteséƒ¨ç½²](#32-kuberneteséƒ¨ç½²)
      - [3.2.1 ä½¿ç”¨Helm Chart](#321-ä½¿ç”¨helm-chart)
      - [3.2.2 æ‰‹åŠ¨éƒ¨ç½²æ¸…å•](#322-æ‰‹åŠ¨éƒ¨ç½²æ¸…å•)
  - [ç¬¬å››éƒ¨åˆ†: åº”ç”¨ä»ªè¡¨åŒ–](#ç¬¬å››éƒ¨åˆ†-åº”ç”¨ä»ªè¡¨åŒ–)
    - [4.1 è‡ªåŠ¨ä»ªè¡¨åŒ–](#41-è‡ªåŠ¨ä»ªè¡¨åŒ–)
      - [4.1.1 Javaåº”ç”¨](#411-javaåº”ç”¨)
      - [4.1.2 Pythonåº”ç”¨](#412-pythonåº”ç”¨)
      - [4.1.3 Node.jsåº”ç”¨](#413-nodejsåº”ç”¨)
    - [4.2 æ‰‹åŠ¨ä»ªè¡¨åŒ–(Rust)](#42-æ‰‹åŠ¨ä»ªè¡¨åŒ–rust)
  - [ç¬¬äº”éƒ¨åˆ†: åç«¯é›†æˆ](#ç¬¬äº”éƒ¨åˆ†-åç«¯é›†æˆ)
    - [5.1 Jaegeré›†æˆ](#51-jaegeré›†æˆ)
    - [5.2 Prometheusé›†æˆ](#52-prometheusé›†æˆ)
    - [5.3 Grafanaé…ç½®](#53-grafanaé…ç½®)
  - [ç¬¬å…­éƒ¨åˆ†: åŠŸèƒ½éªŒè¯](#ç¬¬å…­éƒ¨åˆ†-åŠŸèƒ½éªŒè¯)
    - [6.1 TraceséªŒè¯](#61-traceséªŒè¯)
    - [6.2 MetricséªŒè¯](#62-metricséªŒè¯)
    - [6.3 LogséªŒè¯](#63-logséªŒè¯)
  - [ç¬¬ä¸ƒéƒ¨åˆ†: æ€§èƒ½æµ‹è¯•](#ç¬¬ä¸ƒéƒ¨åˆ†-æ€§èƒ½æµ‹è¯•)
    - [7.1 å‹åŠ›æµ‹è¯•](#71-å‹åŠ›æµ‹è¯•)
    - [7.2 æ€§èƒ½æŒ‡æ ‡ç›‘æ§](#72-æ€§èƒ½æŒ‡æ ‡ç›‘æ§)
    - [7.3 åŸºå‡†æµ‹è¯•æŠ¥å‘Š](#73-åŸºå‡†æµ‹è¯•æŠ¥å‘Š)
  - [ç¬¬å…«éƒ¨åˆ†: ç”Ÿäº§ä¸Šçº¿](#ç¬¬å…«éƒ¨åˆ†-ç”Ÿäº§ä¸Šçº¿)
    - [8.1 ç°åº¦å‘å¸ƒè®¡åˆ’](#81-ç°åº¦å‘å¸ƒè®¡åˆ’)
    - [8.2 ä¸Šçº¿æ£€æŸ¥æ¸…å•](#82-ä¸Šçº¿æ£€æŸ¥æ¸…å•)
    - [8.3 è¿ç»´æ‰‹å†Œ](#83-è¿ç»´æ‰‹å†Œ)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## ç¬¬ä¸€éƒ¨åˆ†: å®æ–½è§„åˆ’

### 1.1 å®æ–½é˜¶æ®µ

```text
å®Œæ•´å®æ–½è·¯çº¿:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1: è§„åˆ’ä¸è®¾è®¡ (1-2å‘¨)                      â”‚
â”‚  - éœ€æ±‚åˆ†æ                                     â”‚
â”‚  - æ¶æ„è®¾è®¡                                     â”‚
â”‚  - æŠ€æœ¯é€‰å‹                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 2: ç¯å¢ƒå‡†å¤‡ (1å‘¨)                         â”‚
â”‚  - åŸºç¡€è®¾æ–½å‡†å¤‡                                 â”‚
â”‚  - å·¥å…·å®‰è£…                                     â”‚
â”‚  - ç½‘ç»œé…ç½®                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 3: Pilotè¯•ç‚¹ (2-3å‘¨)                      â”‚
â”‚  - å•ä¸ªåº”ç”¨ä»ªè¡¨åŒ–                               â”‚
â”‚  - Collectoréƒ¨ç½²                                â”‚
â”‚  - åŠŸèƒ½éªŒè¯                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 4: æ‰©å±•æ¨å¹¿ (4-6å‘¨)                       â”‚
â”‚  - å¤šåº”ç”¨è¦†ç›–                                   â”‚
â”‚  - æ€§èƒ½ä¼˜åŒ–                                     â”‚
â”‚  - æœ€ä½³å®è·µæ€»ç»“                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 5: ç”Ÿäº§ä¸Šçº¿ (2-4å‘¨)                       â”‚
â”‚  - ç°åº¦å‘å¸ƒ                                     â”‚
â”‚  - å…¨é‡ä¸Šçº¿                                     â”‚
â”‚  - æŒç»­ä¼˜åŒ–                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 è§„åˆ’æ£€æŸ¥æ¸…å•

```yaml
planning_checklist:
  éœ€æ±‚åˆ†æ:
    â–¡ ç¡®å®šç›‘æ§ç›®æ ‡(Traces/Metrics/Logs/Profiles)
    â–¡ è¯†åˆ«å…³é”®æœåŠ¡å’Œä¾èµ–
    â–¡ è¯„ä¼°ç°æœ‰ç›‘æ§æ–¹æ¡ˆ
    â–¡ å®šä¹‰æˆåŠŸæ ‡å‡†å’ŒKPI
  
  æ¶æ„è®¾è®¡:
    â–¡ é€‰æ‹©éƒ¨ç½²æ¨¡å¼(Agent/Gateway/Sidecar)
    â–¡ è§„åˆ’Collectoræ¶æ„(é›†ä¸­å¼/åˆ†å±‚/æ··åˆ)
    â–¡ é€‰æ‹©åç«¯å­˜å‚¨(Jaeger/Prometheus/Loki/...)
    â–¡ è®¾è®¡æ•°æ®æµå’Œé‡‡æ ·ç­–ç•¥
  
  èµ„æºè¯„ä¼°:
    â–¡ è®¡ç®—èµ„æºéœ€æ±‚(CPU/å†…å­˜/å­˜å‚¨)
    â–¡ ç½‘ç»œå¸¦å®½éœ€æ±‚
    â–¡ äººåŠ›èµ„æº(å¼€å‘/è¿ç»´/SRE)
    â–¡ é¢„ç®—å’Œæˆæœ¬
  
  é£é™©è¯„ä¼°:
    â–¡ æ€§èƒ½å½±å“è¯„ä¼°
    â–¡ å®‰å…¨å’Œåˆè§„è¦æ±‚
    â–¡ æ•°æ®ä¿ç•™ç­–ç•¥
    â–¡ æ•…éšœæ¢å¤è®¡åˆ’
```

### 1.3 æŠ€æœ¯é€‰å‹å†³ç­–

| ç»´åº¦ | é€‰é¡¹ | æ¨èåœºæ™¯ |
|------|------|---------|
| **Collectoréƒ¨ç½²** | Agentæ¨¡å¼ | æ¯ä¸ªä¸»æœºä¸€ä¸ªCollector |
|  | Gatewayæ¨¡å¼ | é›†ä¸­å¼å¤„ç†å’Œè·¯ç”± |
|  | Sidecaræ¨¡å¼ | Kubernetesç¯å¢ƒ |
| **ä¼ è¾“åè®®** | gRPC | é«˜æ€§èƒ½ã€ä½å»¶è¿Ÿ |
|  | HTTP/JSON | æ˜“è°ƒè¯•ã€å…¼å®¹æ€§å¥½ |
| **é‡‡æ ·ç­–ç•¥** | å¤´éƒ¨é‡‡æ · | ç®€å•ã€ä½å¼€é”€ |
|  | å°¾éƒ¨é‡‡æ · | æ™ºèƒ½ã€åŸºäºå®Œæ•´Trace |
| **åç«¯å­˜å‚¨** | Jaeger | Traces |
|  | Tempo | Traces(å¯¹è±¡å­˜å‚¨) |
|  | Prometheus | Metrics |
|  | Loki | Logs |
|  | å•†ä¸šæ–¹æ¡ˆ | Datadog/New Relic/... |

---

## ç¬¬äºŒéƒ¨åˆ†: ç¯å¢ƒå‡†å¤‡

### 2.1 åŸºç¡€è®¾æ–½éœ€æ±‚

#### 2.1.1 ç¡¬ä»¶èµ„æº

```yaml
# OTel Collectorèµ„æºéœ€æ±‚(å‚è€ƒå€¼)
collector_resources:
  # Agentæ¨¡å¼(æ¯ä¸ªä¸»æœº)
  agent:
    cpu: 0.5-1 core
    memory: 512MB-1GB
    disk: 10GB(æ—¥å¿—å’Œç¼“å­˜)
    network: 100Mbps
  
  # Gatewayæ¨¡å¼(é›†ä¸­å¼)
  gateway:
    cpu: 4-8 cores
    memory: 8-16GB
    disk: 50GB
    network: 1Gbps
  
  # åç«¯å­˜å‚¨(å–å†³äºæ•°æ®é‡)
  backend:
    jaeger:
      cpu: 4+ cores
      memory: 8+ GB
      disk: 100GB+ (SSDæ¨è)
    
    prometheus:
      cpu: 2+ cores
      memory: 4+ GB
      disk: 50GB+ per million samples/day
```

#### 2.1.2 ç½‘ç»œé…ç½®

```yaml
# ç«¯å£è§„åˆ’
network_ports:
  collector:
    - 4317/tcp: OTLP gRPC
    - 4318/tcp: OTLP HTTP
    - 8888/tcp: Metrics (Prometheus)
    - 8889/tcp: Health check
    - 13133/tcp: Health extension
  
  jaeger:
    - 16686/tcp: UI
    - 14250/tcp: gRPC
    - 14268/tcp: HTTP
  
  prometheus:
    - 9090/tcp: Web UI / API
  
  loki:
    - 3100/tcp: HTTP

# é˜²ç«å¢™è§„åˆ™
firewall_rules:
  - allow: app -> collector (4317, 4318)
  - allow: collector -> backend (Jaeger:14250, Prometheus:9090)
  - allow: monitoring -> collector:8888 (metrics)
```

### 2.2 è½¯ä»¶å®‰è£…

#### 2.2.1 Dockerç¯å¢ƒ

```bash
# å®‰è£…Docker
curl -fsSL https://get.docker.com | sh
sudo usermod -aG docker $USER

# å®‰è£…Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.23.0/docker-compose-$(uname -s)-$(uname -m)" \
  -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
```

#### 2.2.2 Kubernetesç¯å¢ƒ

```bash
# å®‰è£…kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# å®‰è£…Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# æ·»åŠ OTel Helm Repo
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm repo update
```

---

## ç¬¬ä¸‰éƒ¨åˆ†: Collectoréƒ¨ç½²

### 3.1 Docker Composeéƒ¨ç½²

#### 3.1.1 å®Œæ•´ç¤ºä¾‹

```yaml
# docker-compose.yml
version: '3.8'

services:
  # OTel Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.89.0
    command: ["--config=/etc/otelcol/config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otelcol/config.yaml:ro
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Metrics
    networks:
      - otel
    restart: unless-stopped
  
  # Jaeger (Tracesåç«¯)
  jaeger:
    image: jaegertracing/all-in-one:1.51
    ports:
      - "16686:16686"  # UI
      - "14250:14250"  # gRPC
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - otel
    restart: unless-stopped
  
  # Prometheus (Metricsåç«¯)
  prometheus:
    image: prom/prometheus:v2.47.0
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - otel
    restart: unless-stopped
  
  # Loki (Logsåç«¯)
  loki:
    image: grafana/loki:2.9.0
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - otel
    restart: unless-stopped
  
  # Grafana (å¯è§†åŒ–)
  grafana:
    image: grafana/grafana:10.1.0
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
    networks:
      - otel
    depends_on:
      - prometheus
      - loki
      - jaeger
    restart: unless-stopped

volumes:
  prometheus-data:
  grafana-data:

networks:
  otel:
    driver: bridge
```

```yaml
# otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 10s
    send_batch_size: 1024
  
  memory_limiter:
    check_interval: 1s
    limit_mib: 512
  
  resource:
    attributes:
      - key: deployment.environment
        value: dev
        action: upsert

exporters:
  # Jaegerå¯¼å‡º
  otlp/jaeger:
    endpoint: jaeger:4317
    tls:
      insecure: true
  
  # Prometheuså¯¼å‡º
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: otel
  
  # Lokiå¯¼å‡º
  loki:
    endpoint: http://loki:3100/loki/api/v1/push
  
  # è°ƒè¯•è¾“å‡º
  logging:
    loglevel: info

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [otlp/jaeger, logging]
    
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [prometheus]
    
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [loki, logging]
  
  telemetry:
    metrics:
      level: detailed
      address: 0.0.0.0:8888
```

```bash
# å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f otel-collector

# è®¿é—®UI
# Grafana: http://localhost:3000 (admin/admin)
# Jaeger: http://localhost:16686
# Prometheus: http://localhost:9090
```

### 3.2 Kuberneteséƒ¨ç½²

#### 3.2.1 ä½¿ç”¨Helm Chart

```bash
# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace observability

# å®‰è£…Cert Manager(å¦‚æœä½¿ç”¨webhook)
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

# å®‰è£…OTel Operator
helm install opentelemetry-operator open-telemetry/opentelemetry-operator \
  --namespace observability \
  --set admissionWebhooks.certManager.enabled=true

# å®‰è£…Collector
helm install otel-collector open-telemetry/opentelemetry-collector \
  --namespace observability \
  --values otel-collector-values.yaml
```

```yaml
# otel-collector-values.yaml
mode: daemonset  # æˆ– deployment, statefulset

image:
  repository: otel/opentelemetry-collector-contrib
  tag: 0.89.0

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: ${env:MY_POD_IP}:4317
        http:
          endpoint: ${env:MY_POD_IP}:4318
    
    # Kubernetes Metrics
    kubeletstats:
      collection_interval: 30s
      auth_type: "serviceAccount"
      endpoint: "https://${env:K8S_NODE_NAME}:10250"
      insecure_skip_verify: true
  
  processors:
    batch:
      timeout: 10s
    
    # K8så±æ€§å¤„ç†å™¨
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      extract:
        metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.node.name
        labels:
          - tag_name: app
            key: app
            from: pod
    
    resource:
      attributes:
        - key: cluster.name
          value: prod-k8s
          action: upsert
  
  exporters:
    otlp:
      endpoint: jaeger.observability.svc.cluster.local:4317
      tls:
        insecure: true
    
    prometheusremotewrite:
      endpoint: http://prometheus.observability.svc.cluster.local:9090/api/v1/write
  
  service:
    pipelines:
      traces:
        receivers: [otlp]
        processors: [k8sattributes, batch, resource]
        exporters: [otlp]
      
      metrics:
        receivers: [otlp, kubeletstats]
        processors: [k8sattributes, batch]
        exporters: [prometheusremotewrite]

# èµ„æºé…ç½®
resources:
  limits:
    cpu: 1000m
    memory: 1Gi
  requests:
    cpu: 200m
    memory: 256Mi

# ServiceMonitor(Prometheus Operator)
serviceMonitor:
  enabled: true

# èŠ‚ç‚¹é€‰æ‹©å™¨
nodeSelector: {}

# å®¹å¿åº¦
tolerations: []

# Affinity
affinity: {}
```

#### 3.2.2 æ‰‹åŠ¨éƒ¨ç½²æ¸…å•

```yaml
# otel-collector-k8s.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: observability

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: observability
data:
  config.yaml: |
    # (å®Œæ•´é…ç½®,åŒä¸Š)

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: observability

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
  - apiGroups: [""]
    resources: ["nodes", "nodes/proxy", "services", "endpoints", "pods"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["apps"]
    resources: ["deployments", "daemonsets", "replicasets", "statefulsets"]
    verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector
subjects:
  - kind: ServiceAccount
    name: otel-collector
    namespace: observability

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otel-collector
  namespace: observability
spec:
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      serviceAccountName: otel-collector
      containers:
        - name: collector
          image: otel/opentelemetry-collector-contrib:0.89.0
          command: ["--config=/etc/otelcol/config.yaml"]
          
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          
          ports:
            - containerPort: 4317
              name: otlp-grpc
            - containerPort: 4318
              name: otlp-http
            - containerPort: 8888
              name: metrics
          
          volumeMounts:
            - name: config
              mountPath: /etc/otelcol
          
          resources:
            requests:
              cpu: 200m
              memory: 256Mi
            limits:
              cpu: 1000m
              memory: 1Gi
      
      volumes:
        - name: config
          configMap:
            name: otel-collector-config

---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: observability
spec:
  selector:
    app: otel-collector
  ports:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
    - name: otlp-http
      port: 4318
      targetPort: 4318
    - name: metrics
      port: 8888
      targetPort: 8888
  type: ClusterIP
```

```bash
# éƒ¨ç½²
kubectl apply -f otel-collector-k8s.yaml

# éªŒè¯
kubectl get pods -n observability
kubectl logs -n observability -l app=otel-collector

# ç«¯å£è½¬å‘æµ‹è¯•
kubectl port-forward -n observability svc/otel-collector 4317:4317
```

---

## ç¬¬å››éƒ¨åˆ†: åº”ç”¨ä»ªè¡¨åŒ–

### 4.1 è‡ªåŠ¨ä»ªè¡¨åŒ–

#### 4.1.1 Javaåº”ç”¨

```bash
# ä¸‹è½½Java Agent
wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar

# å¯åŠ¨åº”ç”¨
java -javaagent:opentelemetry-javaagent.jar \
  -Dotel.service.name=my-java-app \
  -Dotel.traces.exporter=otlp \
  -Dotel.metrics.exporter=otlp \
  -Dotel.logs.exporter=otlp \
  -Dotel.exporter.otlp.endpoint=http://localhost:4317 \
  -jar myapp.jar
```

```dockerfile
# Dockerfile
FROM openjdk:17-jdk-slim
ADD https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar /app/
COPY myapp.jar /app/
WORKDIR /app
ENV JAVA_TOOL_OPTIONS="-javaagent:/app/opentelemetry-javaagent.jar"
ENV OTEL_SERVICE_NAME=my-java-app
ENV OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
CMD ["java", "-jar", "myapp.jar"]
```

#### 4.1.2 Pythonåº”ç”¨

```bash
# å®‰è£…
pip install opentelemetry-distro opentelemetry-exporter-otlp

# è‡ªåŠ¨ä»ªè¡¨åŒ–
opentelemetry-bootstrap -a install

# è¿è¡Œåº”ç”¨
opentelemetry-instrument \
  --traces_exporter otlp \
  --metrics_exporter otlp \
  --service_name my-python-app \
  --exporter_otlp_endpoint http://localhost:4317 \
  python app.py
```

#### 4.1.3 Node.jsåº”ç”¨

```javascript
// package.json
{
  "dependencies": {
    "@opentelemetry/sdk-node": "^0.45.0",
    "@opentelemetry/auto-instrumentations-node": "^0.40.0",
    "@opentelemetry/exporter-trace-otlp-grpc": "^0.45.0"
  }
}

// tracing.js
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-grpc');

const sdk = new NodeSDK({
  serviceName: 'my-node-app',
  traceExporter: new OTLPTraceExporter({
    url: 'http://localhost:4317',
  }),
  instrumentations: [getNodeAutoInstrumentations()],
});

sdk.start();

process.on('SIGTERM', () => {
  sdk.shutdown()
    .then(() => console.log('Tracing terminated'))
    .catch((error) => console.log('Error terminating tracing', error))
    .finally(() => process.exit(0));
});

// å¯åŠ¨
node --require ./tracing.js app.js
```

### 4.2 æ‰‹åŠ¨ä»ªè¡¨åŒ–(Rust)

```rust
// Cargo.toml
[dependencies]
opentelemetry = "0.21"
opentelemetry-otlp = "0.14"
opentelemetry_sdk = "0.21"
tokio = { version = "1", features = ["full"] }
tracing = "0.1"
tracing-opentelemetry = "0.22"
tracing-subscriber = "0.3"

// main.rs
use opentelemetry::global;
use opentelemetry_otlp::WithExportConfig;
use opentelemetry_sdk::{trace, Resource};
use opentelemetry::KeyValue;
use tracing_subscriber::layer::SubscriberExt;
use tracing_subscriber::Registry;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆå§‹åŒ–OTel
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .tonic()
                .with_endpoint("http://localhost:4317")
        )
        .with_trace_config(
            trace::config()
                .with_resource(Resource::new(vec![
                    KeyValue::new("service.name", "my-rust-app"),
                    KeyValue::new("service.version", "1.0.0"),
                ]))
        )
        .install_batch(opentelemetry_sdk::runtime::Tokio)?;
    
    // é…ç½®tracing
    let telemetry = tracing_opentelemetry::layer().with_tracer(tracer);
    let subscriber = Registry::default().with(telemetry);
    tracing::subscriber::set_global_default(subscriber)?;
    
    // åº”ç”¨é€»è¾‘
    run_app().await;
    
    // æ¸…ç†
    global::shutdown_tracer_provider();
    Ok(())
}

#[tracing::instrument]
async fn run_app() {
    println!("App running with OTel!");
    // ...
}
```

---

## ç¬¬äº”éƒ¨åˆ†: åç«¯é›†æˆ

### 5.1 Jaegeré›†æˆ

```yaml
# Jaegerå®Œæ•´éƒ¨ç½²(ç”Ÿäº§çº§)
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger
  namespace: observability
spec:
  strategy: production
  
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: http://elasticsearch:9200
        index-prefix: jaeger
    
    elasticsearch:
      nodeCount: 3
      redundancyPolicy: MultipleRedundancy
      storage:
        storageClassName: fast-ssd
        size: 100Gi
  
  query:
    replicas: 2
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 1Gi
  
  collector:
    replicas: 3
    maxReplicas: 10
    autoscale: true
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 2000m
        memory: 2Gi
```

### 5.2 Prometheusé›†æˆ

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # OTel Collectorè‡ªèº«æŒ‡æ ‡
  - job_name: 'otel-collector'
    static_configs:
      - targets: ['otel-collector:8888']
    
  # é€šè¿‡OTel Collectorå¯¼å‡ºçš„åº”ç”¨æŒ‡æ ‡
  - job_name: 'otel-apps'
    static_configs:
      - targets: ['otel-collector:8889']
```

### 5.3 Grafanaé…ç½®

```yaml
# grafana-datasources.yml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
  
  - name: Jaeger
    type: jaeger
    access: proxy
    url: http://jaeger-query:16686
    editable: true
  
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    editable: true
```

---

## ç¬¬å…­éƒ¨åˆ†: åŠŸèƒ½éªŒè¯

### 6.1 TraceséªŒè¯

```bash
# 1. ç”Ÿæˆæµ‹è¯•Trace
curl -X POST http://localhost:4318/v1/traces \
  -H "Content-Type: application/json" \
  -d '{
    "resourceSpans": [{
      "resource": {
        "attributes": [{
          "key": "service.name",
          "value": { "stringValue": "test-service" }
        }]
      },
      "scopeSpans": [{
        "spans": [{
          "traceId": "5b8aa5a2d2c872e8321cf37308d69df2",
          "spanId": "051581bf3cb55c13",
          "name": "test-span",
          "kind": 1,
          "startTimeUnixNano": "1697500000000000000",
          "endTimeUnixNano": "1697500001000000000",
          "attributes": [{
            "key": "http.method",
            "value": { "stringValue": "GET" }
          }]
        }]
      }]
    }]
  }'

# 2. åœ¨Jaeger UIæŸ¥è¯¢
open http://localhost:16686
# æœç´¢service="test-service"

# 3. éªŒè¯Traceå®Œæ•´æ€§
# - Trace IDæ­£ç¡®
# - Spanå±‚çº§å…³ç³»æ­£ç¡®
# - å±æ€§å®Œæ•´
# - æ—¶é—´æˆ³åˆç†
```

### 6.2 MetricséªŒè¯

```bash
# 1. æŸ¥çœ‹Collectorå¯¼å‡ºçš„æŒ‡æ ‡
curl http://localhost:8888/metrics

# 2. æŸ¥è¯¢Prometheus
curl 'http://localhost:9090/api/v1/query?query=up'

# 3. éªŒè¯åº”ç”¨æŒ‡æ ‡
curl 'http://localhost:9090/api/v1/query?query=http_requests_total'

# 4. åœ¨Grafanaåˆ›å»ºDashboard
# - æ·»åŠ Panel
# - æŸ¥è¯¢: rate(http_requests_total[5m])
# - å¯è§†åŒ–ç±»å‹: Graph
```

### 6.3 LogséªŒè¯

```bash
# 1. å‘é€æµ‹è¯•Log
curl -X POST http://localhost:4318/v1/logs \
  -H "Content-Type: application/json" \
  -d '{
    "resourceLogs": [{
      "resource": {
        "attributes": [{
          "key": "service.name",
          "value": { "stringValue": "test-service" }
        }]
      },
      "scopeLogs": [{
        "logRecords": [{
          "timeUnixNano": "1697500000000000000",
          "severityText": "INFO",
          "body": {
            "stringValue": "Test log message"
          }
        }]
      }]
    }]
  }'

# 2. åœ¨LokiæŸ¥è¯¢
curl -G -s 'http://localhost:3100/loki/api/v1/query' \
  --data-urlencode 'query={service_name="test-service"}'

# 3. åœ¨Grafana ExploreæŸ¥çœ‹
# - é€‰æ‹©Lokiæ•°æ®æº
# - æŸ¥è¯¢: {service_name="test-service"}
```

---

## ç¬¬ä¸ƒéƒ¨åˆ†: æ€§èƒ½æµ‹è¯•

### 7.1 å‹åŠ›æµ‹è¯•

```bash
# ä½¿ç”¨k6è¿›è¡Œå‹åŠ›æµ‹è¯•
npm install -g k6

# test-script.js
import { check } from 'k6';
import http from 'k6/http';

export let options = {
  stages: [
    { duration: '2m', target: 100 }, // 2åˆ†é’Ÿå†…å¢åŠ åˆ°100 VU
    { duration: '5m', target: 100 }, // ç»´æŒ5åˆ†é’Ÿ
    { duration: '2m', target: 0 },   // 2åˆ†é’Ÿå†…é™åˆ°0
  ],
  thresholds: {
    'http_req_duration': ['p(95)<500'], // 95%è¯·æ±‚<500ms
    'http_req_failed': ['rate<0.01'],   // é”™è¯¯ç‡<1%
  },
};

export default function () {
  let response = http.get('http://my-app:8080/api/test');
  check(response, {
    'status is 200': (r) => r.status === 200,
  });
}

# è¿è¡Œ
k6 run test-script.js
```

### 7.2 æ€§èƒ½æŒ‡æ ‡ç›‘æ§

```yaml
# å…³é”®æ€§èƒ½æŒ‡æ ‡
performance_metrics:
  collector:
    - otelcol_receiver_accepted_spans (spansæ¥æ”¶æ•°)
    - otelcol_receiver_refused_spans (spansæ‹’ç»æ•°)
    - otelcol_processor_batch_batch_send_size (æ‰¹å¤„ç†å¤§å°)
    - otelcol_exporter_sent_spans (spanså‘é€æ•°)
    - otelcol_exporter_send_failed_spans (å‘é€å¤±è´¥æ•°)
    - process_cpu_seconds_total (CPUä½¿ç”¨)
    - process_resident_memory_bytes (å†…å­˜ä½¿ç”¨)
  
  application:
    - http_server_duration_milliseconds (è¯·æ±‚å»¶è¿Ÿ)
    - http_server_active_requests (æ´»è·ƒè¯·æ±‚)
    - process_cpu_seconds_total (CPUä½¿ç”¨)
    - process_resident_memory_bytes (å†…å­˜ä½¿ç”¨)
    - jvm_gc_pause_seconds (GCæš‚åœæ—¶é—´, Java)
```

### 7.3 åŸºå‡†æµ‹è¯•æŠ¥å‘Š

```markdown
# æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•ç¯å¢ƒ
- Collector: 2 CPU, 4GB RAM
- åº”ç”¨: 10ä¸ªå®ä¾‹, å„1 CPU, 2GB RAM
- è´Ÿè½½: 1000 RPS, æŒç»­10åˆ†é’Ÿ

## æµ‹è¯•ç»“æœ

| æŒ‡æ ‡ | ç›®æ ‡ | å®é™… | çŠ¶æ€ |
|------|------|------|------|
| åº”ç”¨P95å»¶è¿Ÿ | <200ms | 145ms | âœ… |
| åº”ç”¨P99å»¶è¿Ÿ | <500ms | 312ms | âœ… |
| Collector CPU | <50% | 32% | âœ… |
| Collectorå†…å­˜ | <2GB | 1.2GB | âœ… |
| Tracesåå | >10k/s | 12.5k/s | âœ… |
| æ•°æ®ä¸¢å¤±ç‡ | <0.1% | 0.02% | âœ… |
| E2Eå»¶è¿Ÿ | <5s | 2.3s | âœ… |

## ç»“è®º
ç³»ç»Ÿåœ¨ç›®æ ‡è´Ÿè½½ä¸‹è¡¨ç°è‰¯å¥½,æ»¡è¶³ç”Ÿäº§è¦æ±‚ã€‚
```

---

## ç¬¬å…«éƒ¨åˆ†: ç”Ÿäº§ä¸Šçº¿

### 8.1 ç°åº¦å‘å¸ƒè®¡åˆ’

```yaml
rollout_plan:
  phase_1: # ç°åº¦5%
    duration: 2å¤©
    targets:
      - 1ä¸ªéå…³é”®æœåŠ¡
    validation:
      - åŠŸèƒ½æ­£å¸¸
      - æ— æ€§èƒ½åŠ£åŒ–
      - é”™è¯¯ç‡<0.1%
  
  phase_2: # æ‰©å±•åˆ°20%
    duration: 1å‘¨
    targets:
      - 3-5ä¸ªæœåŠ¡
    validation:
      - Traceså®Œæ•´æ€§>95%
      - E2Eå»¶è¿Ÿ<5s
      - æˆæœ¬å¯æ§
  
  phase_3: # å…¨é‡50%
    duration: 2å‘¨
    targets:
      - æ‰€æœ‰éæ ¸å¿ƒæœåŠ¡
    validation:
      - ç›‘æ§è¦†ç›–åº¦>80%
      - è¿ç»´å›¢é˜ŸåŸ¹è®­å®Œæˆ
  
  phase_4: # 100%
    duration: 1ä¸ªæœˆ
    targets:
      - æ‰€æœ‰æœåŠ¡
    validation:
      - å…¨é¢æ›¿ä»£æ—§ç›‘æ§
      - æ–‡æ¡£å®Œå–„
      - On-callæµç¨‹å»ºç«‹
```

### 8.2 ä¸Šçº¿æ£€æŸ¥æ¸…å•

```yaml
production_checklist:
  éƒ¨ç½²å‰:
    â–¡ æ€§èƒ½æµ‹è¯•é€šè¿‡
    â–¡ å®‰å…¨å®¡è®¡å®Œæˆ
    â–¡ å¤‡ä»½å’Œå›æ»šæ–¹æ¡ˆæµ‹è¯•
    â–¡ ç›‘æ§å‘Šè­¦é…ç½®
    â–¡ æ–‡æ¡£æ›´æ–°
    â–¡ å›¢é˜ŸåŸ¹è®­å®Œæˆ
    â–¡ å˜æ›´å®¡æ‰¹é€šè¿‡
  
  éƒ¨ç½²ä¸­:
    â–¡ ç°åº¦å‘å¸ƒæ‰§è¡Œ
    â–¡ å®æ—¶ç›‘æ§æŒ‡æ ‡
    â–¡ æ—¥å¿—æ£€æŸ¥
    â–¡ åŠŸèƒ½éªŒè¯
    â–¡ æ€§èƒ½å¯¹æ¯”
  
  éƒ¨ç½²å:
    â–¡ å…¨é‡éªŒè¯
    â–¡ ç”¨æˆ·åé¦ˆæ”¶é›†
    â–¡ æ€§èƒ½ä¼˜åŒ–
    â–¡ æˆæœ¬åˆ†æ
    â–¡ ç»éªŒæ€»ç»“
```

### 8.3 è¿ç»´æ‰‹å†Œ

```markdown
    # OTelè¿ç»´æ‰‹å†Œ

    ## æ—¥å¸¸è¿ç»´

    ### å¥åº·æ£€æŸ¥
    ```bash
    # Collectorå¥åº·æ£€æŸ¥
    curl http://collector:13133/

    # æŸ¥çœ‹Collectoræ—¥å¿—
    kubectl logs -n observability -l app=otel-collector --tail=100

    # ç›‘æ§å…³é”®æŒ‡æ ‡
    # - Collector CPU/å†…å­˜
    # - Tracesååé‡
    # - é”™è¯¯ç‡
    ```

    ### å¸¸è§é—®é¢˜

    #### 1. Collector OOM

    **ç—‡çŠ¶**: Collector podé‡å¯é¢‘ç¹
    **åŸå› **: å†…å­˜é™åˆ¶è¿‡ä½æˆ–æ•°æ®é‡æ¿€å¢
    **è§£å†³**:

    ```bash
    # å¢åŠ å†…å­˜é™åˆ¶
    kubectl set resources deployment/otel-collector \
    --limits=memory=4Gi \
    -n observability

    # æˆ–è°ƒæ•´æ‰¹å¤„ç†é…ç½®
    # batch.send_batch_size: å‡å°
    # batch.timeout: å‡å°
    ```

    #### 2. æ•°æ®ä¸¢å¤±

    **ç—‡çŠ¶**: Tracesä¸å®Œæ•´æˆ–ç¼ºå¤±
    **åŸå› **: Collectorè¿‡è½½æˆ–ç½‘ç»œé—®é¢˜
    **è§£å†³**:

    - æ£€æŸ¥CollectoræŒ‡æ ‡: otelcol_receiver_refused_spans
    - æ‰©å±•Collectorå‰¯æœ¬æ•°
    - è°ƒæ•´é‡‡æ ·ç‡

    #### 3. æŸ¥è¯¢æ…¢

    **ç—‡çŠ¶**: Jaeger/GrafanaæŸ¥è¯¢è¶…æ—¶
    **åŸå› **: åç«¯å­˜å‚¨è´Ÿè½½é«˜
    **è§£å†³**:

    - å¢åŠ ElasticsearchèŠ‚ç‚¹
    - ä¼˜åŒ–ç´¢å¼•ç­–ç•¥
    - å‡å°‘æ•°æ®ä¿ç•™æœŸ

    ### åº”æ€¥å“åº”

    **P0æ•…éšœ(æœåŠ¡ä¸å¯ç”¨)**:

    1. ç¦ç”¨OTelä»ªè¡¨åŒ–(æ¢å¤åº”ç”¨)
    2. å®šä½æ ¹å› 
    3. ä¿®å¤åç°åº¦å¯ç”¨

    **P1æ•…éšœ(éƒ¨åˆ†åŠŸèƒ½å¼‚å¸¸)**:

    1. éš”ç¦»é—®é¢˜Collector
    2. è°ƒæ•´é…ç½®
    3. ç›‘æ§æ¢å¤

    ### å‡çº§æµç¨‹

    1. æµ‹è¯•ç¯å¢ƒéªŒè¯æ–°ç‰ˆæœ¬
    2. å¤‡ä»½é…ç½®
    3. ç°åº¦å‡çº§Collector
    4. éªŒè¯åŠŸèƒ½
    5. å…¨é‡å‡çº§

```

---

## ğŸ“š å‚è€ƒèµ„æº

- [OTelå®˜æ–¹æ–‡æ¡£](https://opentelemetry.io/docs/)
- [Collectoré…ç½®å‚è€ƒ](https://opentelemetry.io/docs/collector/configuration/)
- [æœ€ä½³å®è·µ](https://opentelemetry.io/docs/concepts/best-practices/)

---

**å®Œæ•´çš„å®æ–½ä¸éªŒè¯æŒ‡å—!** ğŸ“
