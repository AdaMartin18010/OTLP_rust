# æ‰©å®¹å†³ç­–

## ç›®å½•

- [æ‰©å®¹å†³ç­–](#æ‰©å®¹å†³ç­–)
  - [ç›®å½•](#ç›®å½•)
  - [æ¦‚è¿°](#æ¦‚è¿°)
  - [æ‰©å®¹è§¦å‘æ¡ä»¶](#æ‰©å®¹è§¦å‘æ¡ä»¶)
    - [åŸºäºé˜ˆå€¼çš„è§¦å‘](#åŸºäºé˜ˆå€¼çš„è§¦å‘)
  - [æ‰©å®¹ç­–ç•¥](#æ‰©å®¹ç­–ç•¥)
    - [æ°´å¹³æ‰©å±• vs å‚ç›´æ‰©å±•](#æ°´å¹³æ‰©å±•-vs-å‚ç›´æ‰©å±•)
  - [è‡ªåŠ¨æ‰©ç¼©å®¹](#è‡ªåŠ¨æ‰©ç¼©å®¹)
    - [Kubernetes HPA é…ç½®](#kubernetes-hpa-é…ç½®)
    - [Rust å®ç°ï¼šè‡ªåŠ¨æ‰©ç¼©å®¹æ§åˆ¶å™¨](#rust-å®ç°è‡ªåŠ¨æ‰©ç¼©å®¹æ§åˆ¶å™¨)
  - [æ‰©å®¹å†³ç­–æ ‘](#æ‰©å®¹å†³ç­–æ ‘)
    - [å†³ç­–æµç¨‹å›¾](#å†³ç­–æµç¨‹å›¾)
      - [ğŸ“Š å®Œæ•´æ‰©å®¹å†³ç­–æµç¨‹ï¼ˆMermaidï¼‰](#-å®Œæ•´æ‰©å®¹å†³ç­–æµç¨‹mermaid)
      - [ğŸ”„ è‡ªåŠ¨æ‰©ç¼©å®¹çŠ¶æ€æœº](#-è‡ªåŠ¨æ‰©ç¼©å®¹çŠ¶æ€æœº)
      - [ğŸ“ˆ æ‰©å®¹å†³ç­–æ—¶åºå›¾](#-æ‰©å®¹å†³ç­–æ—¶åºå›¾)
    - [æ–‡æœ¬ç‰ˆå†³ç­–æµç¨‹ï¼ˆå…¼å®¹æ€§ï¼‰](#æ–‡æœ¬ç‰ˆå†³ç­–æµç¨‹å…¼å®¹æ€§)
    - [å†³ç­–çŸ©é˜µ](#å†³ç­–çŸ©é˜µ)
  - [é¢„æµ‹æ€§æ‰©å®¹](#é¢„æµ‹æ€§æ‰©å®¹)
    - [åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹](#åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹)
  - [æˆæœ¬ä¼˜åŒ–çš„æ‰©å®¹ç­–ç•¥](#æˆæœ¬ä¼˜åŒ–çš„æ‰©å®¹ç­–ç•¥)
    - [æˆæœ¬æ„ŸçŸ¥æ‰©å®¹](#æˆæœ¬æ„ŸçŸ¥æ‰©å®¹)
  - [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)
    - [æ¡ˆä¾‹1ï¼šæµé‡çªå¢çš„è‡ªåŠ¨æ‰©å®¹](#æ¡ˆä¾‹1æµé‡çªå¢çš„è‡ªåŠ¨æ‰©å®¹)
    - [æ¡ˆä¾‹2ï¼šæˆæœ¬ä¼˜åŒ–çš„æ™ºèƒ½ç¼©å®¹](#æ¡ˆä¾‹2æˆæœ¬ä¼˜åŒ–çš„æ™ºèƒ½ç¼©å®¹)
  - [æ‰©å®¹æœ€ä½³å®è·µ](#æ‰©å®¹æœ€ä½³å®è·µ)
    - [1. è®¾ç½®åˆç†çš„é˜ˆå€¼](#1-è®¾ç½®åˆç†çš„é˜ˆå€¼)
    - [2. é…ç½®å†·å´æœŸ](#2-é…ç½®å†·å´æœŸ)
    - [3. æ¸è¿›å¼æ‰©å®¹](#3-æ¸è¿›å¼æ‰©å®¹)

## æ¦‚è¿°

æ‰©å®¹å†³ç­–ç¡®å®šä½•æ—¶ä»¥åŠå¦‚ä½•å¢åŠ ç³»ç»Ÿèµ„æºä»¥æ»¡è¶³è´Ÿè½½éœ€æ±‚ã€‚

## æ‰©å®¹è§¦å‘æ¡ä»¶

### åŸºäºé˜ˆå€¼çš„è§¦å‘

```rust
pub struct ScalingTrigger {
    cpu_threshold: f64,
    memory_threshold: f64,
    latency_threshold_ms: f64,
}

impl ScalingTrigger {
    pub fn should_scale_up(&self, metrics: &SystemMetrics) -> bool {
        metrics.cpu_usage > self.cpu_threshold ||
        metrics.memory_usage > self.memory_threshold ||
        metrics.p99_latency_ms > self.latency_threshold_ms
    }

    pub fn should_scale_down(&self, metrics: &SystemMetrics) -> bool {
        metrics.cpu_usage < self.cpu_threshold * 0.5 &&
        metrics.memory_usage < self.memory_threshold * 0.5 &&
        metrics.p99_latency_ms < self.latency_threshold_ms * 0.5
    }
}

pub struct SystemMetrics {
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub p99_latency_ms: f64,
}
```

## æ‰©å®¹ç­–ç•¥

### æ°´å¹³æ‰©å±• vs å‚ç›´æ‰©å±•

```rust
pub enum ScalingStrategy {
    Horizontal { instances_to_add: usize },
    Vertical { cpu_increase: usize, memory_increase_gb: usize },
}

pub struct ScalingDecisionMaker;

impl ScalingDecisionMaker {
    pub fn decide_strategy(&self, metrics: &SystemMetrics) -> ScalingStrategy {
        // å¦‚æœ CPU æ˜¯ç“¶é¢ˆï¼Œè€ƒè™‘æ°´å¹³æ‰©å±•
        if metrics.cpu_usage > 80.0 {
            ScalingStrategy::Horizontal {
                instances_to_add: self.calculate_instances_needed(metrics),
            }
        } else {
            // å¦åˆ™è€ƒè™‘å‚ç›´æ‰©å±•
            ScalingStrategy::Vertical {
                cpu_increase: 2,
                memory_increase_gb: 4,
            }
        }
    }

    fn calculate_instances_needed(&self, metrics: &SystemMetrics) -> usize {
        let current_load = metrics.cpu_usage / 100.0;
        let target_load = 0.7; // ç›®æ ‡ 70% åˆ©ç”¨ç‡
        
        ((current_load / target_load).ceil() as usize).saturating_sub(1).max(1)
    }
}
```

## è‡ªåŠ¨æ‰©ç¼©å®¹

### Kubernetes HPA é…ç½®

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: otlp-collector-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otlp-collector
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: otlp_export_latency_p99
      target:
        type: AverageValue
        averageValue: "100m"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
```

### Rust å®ç°ï¼šè‡ªåŠ¨æ‰©ç¼©å®¹æ§åˆ¶å™¨

```rust
use tokio::time::{interval, Duration};

pub struct AutoScaler {
    trigger: ScalingTrigger,
    decision_maker: ScalingDecisionMaker,
    cooldown_period: Duration,
    last_scaling: Option<std::time::Instant>,
}

impl AutoScaler {
    pub fn new(cooldown_minutes: u64) -> Self {
        Self {
            trigger: ScalingTrigger {
                cpu_threshold: 70.0,
                memory_threshold: 80.0,
                latency_threshold_ms: 100.0,
            },
            decision_maker: ScalingDecisionMaker,
            cooldown_period: Duration::from_secs(cooldown_minutes * 60),
            last_scaling: None,
        }
    }

    pub async fn run(&mut self) {
        let mut ticker = interval(Duration::from_secs(30));

        loop {
            ticker.tick().await;

            // æ£€æŸ¥å†·å´æœŸ
            if let Some(last) = self.last_scaling {
                if last.elapsed() < self.cooldown_period {
                    continue;
                }
            }

            let metrics = self.collect_metrics().await;

            if self.trigger.should_scale_up(&metrics) {
                let strategy = self.decision_maker.decide_strategy(&metrics);
                self.execute_scaling(strategy).await;
                self.last_scaling = Some(std::time::Instant::now());
            } else if self.trigger.should_scale_down(&metrics) {
                self.execute_scale_down().await;
                self.last_scaling = Some(std::time::Instant::now());
            }
        }
    }

    async fn collect_metrics(&self) -> SystemMetrics {
        // ä» Prometheus æˆ–å…¶ä»–ç›‘æ§ç³»ç»Ÿæ”¶é›†æŒ‡æ ‡
        SystemMetrics {
            cpu_usage: 0.0,
            memory_usage: 0.0,
            p99_latency_ms: 0.0,
        }
    }

    async fn execute_scaling(&self, strategy: ScalingStrategy) {
        match strategy {
            ScalingStrategy::Horizontal { instances_to_add } => {
                println!("Scaling horizontally: adding {} instances", instances_to_add);
                // è°ƒç”¨ K8s API æˆ–äº‘æœåŠ¡ API
            }
            ScalingStrategy::Vertical { cpu_increase, memory_increase_gb } => {
                println!("Scaling vertically: +{} CPU, +{}GB memory", 
                    cpu_increase, memory_increase_gb);
            }
        }
    }

    async fn execute_scale_down(&self) {
        println!("Scaling down");
        // å®ç°ç¼©å®¹é€»è¾‘
    }
}
```

## æ‰©å®¹å†³ç­–æ ‘

### å†³ç­–æµç¨‹å›¾

#### ğŸ“Š å®Œæ•´æ‰©å®¹å†³ç­–æµç¨‹ï¼ˆMermaidï¼‰

```mermaid
flowchart TD
    Start([å¼€å§‹ç›‘æ§]) --> Collect[é‡‡é›†ç³»ç»ŸæŒ‡æ ‡]
    Collect --> Check{æ˜¯å¦è¶…è¿‡é˜ˆå€¼?}
    
    Check -->|å¦| Monitor[ç»§ç»­ç›‘æ§]
    Monitor --> Collect
    
    Check -->|æ˜¯| Analyze[åˆ†æç“¶é¢ˆç±»å‹]
    
    Analyze --> CPUCheck{CPUç“¶é¢ˆ?}
    Analyze --> MemCheck{å†…å­˜ç“¶é¢ˆ?}
    Analyze --> IOCheck{I/Oç“¶é¢ˆ?}
    Analyze --> LatencyCheck{å»¶è¿Ÿç“¶é¢ˆ?}
    
    CPUCheck -->|æ˜¯| CPUDecision{CPUå¯†é›†å‹?}
    CPUDecision -->|æ˜¯| Horizontal1[æ°´å¹³æ‰©å±•<br/>å¢åŠ å®ä¾‹]
    CPUDecision -->|å¦| Vertical1[å‚ç›´æ‰©å±•<br/>å¢åŠ CPUæ ¸å¿ƒ]
    
    MemCheck -->|æ˜¯| MemDecision{å†…å­˜æ³„æ¼?}
    MemDecision -->|æ˜¯| Fix[ä¿®å¤å†…å­˜æ³„æ¼]
    MemDecision -->|å¦| Vertical2[å‚ç›´æ‰©å±•<br/>å¢åŠ å†…å­˜]
    
    IOCheck -->|æ˜¯| IODecision{ç£ç›˜I/O?}
    IODecision -->|æ˜¯| Storage[å‡çº§å­˜å‚¨<br/>SSD/NVMe]
    IODecision -->|å¦| Network[ä¼˜åŒ–ç½‘ç»œ<br/>å¢åŠ å¸¦å®½]
    
    LatencyCheck -->|æ˜¯| LatencyDecision{è¯·æ±‚æ’é˜Ÿ?}
    LatencyDecision -->|æ˜¯| Horizontal2[æ°´å¹³æ‰©å±•<br/>å¢åŠ å®ä¾‹]
    LatencyDecision -->|å¦| Optimize[æ€§èƒ½ä¼˜åŒ–<br/>ä»£ç /æŸ¥è¯¢]
    
    Horizontal1 --> CostCheck{æˆæœ¬å¯æ¥å—?}
    Horizontal2 --> CostCheck
    Vertical1 --> CostCheck
    Vertical2 --> CostCheck
    Storage --> CostCheck
    Network --> CostCheck
    
    CostCheck -->|æ˜¯| Execute[æ‰§è¡Œæ‰©å®¹]
    CostCheck -->|å¦| Alternative[å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆ]
    Alternative --> Optimize
    
    Execute --> Verify[éªŒè¯æ•ˆæœ]
    Verify --> Success{æŒ‡æ ‡æ¢å¤æ­£å¸¸?}
    
    Success -->|æ˜¯| Record[è®°å½•å†³ç­–<br/>æ›´æ–°åŸºçº¿]
    Success -->|å¦| Rollback[å›æ»š<br/>é‡æ–°åˆ†æ]
    
    Record --> End([ç»“æŸ])
    Rollback --> Analyze
    
    Fix --> Verify
    Optimize --> Verify
    
    style Start fill:#90EE90
    style End fill:#FFB6C1
    style Execute fill:#FFD700
    style Verify fill:#87CEEB
    style CostCheck fill:#FFA500
```

#### ğŸ”„ è‡ªåŠ¨æ‰©ç¼©å®¹çŠ¶æ€æœº

```mermaid
stateDiagram-v2
    [*] --> Normal: ç³»ç»Ÿå¯åŠ¨
    
    Normal --> Monitoring: æŒç»­ç›‘æ§
    Monitoring --> Normal: æŒ‡æ ‡æ­£å¸¸
    
    Monitoring --> Alert: è¶…è¿‡é˜ˆå€¼
    Alert --> Analyzing: è§¦å‘åˆ†æ
    
    Analyzing --> ScalingUp: éœ€è¦æ‰©å®¹
    Analyzing --> ScalingDown: å¯ä»¥ç¼©å®¹
    Analyzing --> Optimizing: éœ€è¦ä¼˜åŒ–
    
    ScalingUp --> Cooldown: æ‰©å®¹å®Œæˆ
    ScalingDown --> Cooldown: ç¼©å®¹å®Œæˆ
    Optimizing --> Cooldown: ä¼˜åŒ–å®Œæˆ
    
    Cooldown --> Normal: å†·å´æœŸç»“æŸ
    
    Normal --> [*]: ç³»ç»Ÿå…³é—­
    
    note right of Alert
        è§¦å‘æ¡ä»¶ï¼š
        - CPU > 70%
        - Memory > 80%
        - Latency > 500ms
    end note
    
    note right of Cooldown
        å†·å´æœŸï¼š5åˆ†é’Ÿ
        é˜²æ­¢é¢‘ç¹æ‰©ç¼©å®¹
    end note
```

#### ğŸ“ˆ æ‰©å®¹å†³ç­–æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant M as ç›‘æ§ç³»ç»Ÿ
    participant A as å‘Šè­¦ç³»ç»Ÿ
    participant D as å†³ç­–å¼•æ“
    participant K as Kubernetes
    participant S as æœåŠ¡å®ä¾‹
    
    M->>M: é‡‡é›†æŒ‡æ ‡
    M->>A: æŒ‡æ ‡è¶…è¿‡é˜ˆå€¼
    A->>D: è§¦å‘æ‰©å®¹å†³ç­–
    
    D->>D: åˆ†æç“¶é¢ˆç±»å‹
    D->>D: è®¡ç®—æ‰©å®¹æ–¹æ¡ˆ
    D->>D: è¯„ä¼°æˆæœ¬å½±å“
    
    alt æˆæœ¬å¯æ¥å—
        D->>K: å‘é€æ‰©å®¹è¯·æ±‚
        K->>K: è°ƒåº¦æ–°å®ä¾‹
        K->>S: å¯åŠ¨æ–°Pod
        S-->>K: å°±ç»ªä¿¡å·
        K-->>D: æ‰©å®¹å®Œæˆ
        D->>M: æ›´æ–°ç›‘æ§åŸºçº¿
    else æˆæœ¬è¿‡é«˜
        D->>D: å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆ
        D->>S: æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–
        S-->>D: ä¼˜åŒ–å®Œæˆ
    end
    
    D->>M: è¿›å…¥å†·å´æœŸ
    M->>M: æŒç»­ç›‘æ§
```

### æ–‡æœ¬ç‰ˆå†³ç­–æµç¨‹ï¼ˆå…¼å®¹æ€§ï¼‰

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç›‘æ§æŒ‡æ ‡é‡‡é›†    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ˜¯å¦è¶…è¿‡é˜ˆå€¼ï¼Ÿ  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
     â”‚ å¦     â”‚ æ˜¯
     â†“        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç»§ç»­ç›‘æ§â”‚ â”‚ åˆ†æç“¶é¢ˆç±»å‹ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚         â”‚         â”‚
         â†“         â†“         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚CPUç“¶é¢ˆâ”‚â”‚å†…å­˜ç“¶é¢ˆâ”‚â”‚I/Oç“¶é¢ˆâ”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚         â”‚         â”‚
        â†“         â†“         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚æ°´å¹³æ‰©å±•â”‚â”‚å‚ç›´æ‰©å±•â”‚â”‚ä¼˜åŒ–ç­–ç•¥â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚         â”‚         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ æ‰§è¡Œæ‰©å®¹       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ éªŒè¯æ•ˆæœ       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å†³ç­–çŸ©é˜µ

```rust
/// æ‰©å®¹å†³ç­–çŸ©é˜µ
pub struct ScalingDecisionMatrix;

impl ScalingDecisionMatrix {
    /// åŸºäºå¤šç»´åº¦æŒ‡æ ‡åšå‡ºæ‰©å®¹å†³ç­–
    pub fn make_decision(
        &self,
        cpu_usage: f64,
        memory_usage: f64,
        io_wait: f64,
        network_bandwidth: f64,
        latency_p99: f64,
    ) -> ScalingDecision {
        // 1. CPU å¯†é›†å‹åœºæ™¯
        if cpu_usage > 80.0 && memory_usage < 70.0 {
            return ScalingDecision::HorizontalScale {
                reason: "CPU ç“¶é¢ˆ".to_string(),
                instances: self.calculate_cpu_based_instances(cpu_usage),
                priority: Priority::High,
            };
        }

        // 2. å†…å­˜å¯†é›†å‹åœºæ™¯
        if memory_usage > 85.0 && cpu_usage < 70.0 {
            return ScalingDecision::VerticalScale {
                reason: "å†…å­˜ç“¶é¢ˆ".to_string(),
                memory_increase_gb: self.calculate_memory_increase(memory_usage),
                priority: Priority::High,
            };
        }

        // 3. I/O å¯†é›†å‹åœºæ™¯
        if io_wait > 20.0 {
            return ScalingDecision::OptimizeAndScale {
                reason: "I/O ç“¶é¢ˆ".to_string(),
                optimization: vec![
                    "å¢åŠ  IOPS".to_string(),
                    "ä½¿ç”¨ SSD".to_string(),
                    "ä¼˜åŒ–æŸ¥è¯¢".to_string(),
                ],
                then_scale: Box::new(ScalingDecision::HorizontalScale {
                    reason: "I/O ä¼˜åŒ–åçš„æ°´å¹³æ‰©å±•".to_string(),
                    instances: 2,
                    priority: Priority::Medium,
                }),
            };
        }

        // 4. ç½‘ç»œç“¶é¢ˆ
        if network_bandwidth > 80.0 {
            return ScalingDecision::NetworkOptimization {
                reason: "ç½‘ç»œå¸¦å®½ç“¶é¢ˆ".to_string(),
                actions: vec![
                    "å‡çº§ç½‘ç»œæ¥å£".to_string(),
                    "å¯ç”¨å‹ç¼©".to_string(),
                    "ä¼˜åŒ–æ•°æ®ä¼ è¾“".to_string(),
                ],
            };
        }

        // 5. å»¶è¿Ÿé—®é¢˜
        if latency_p99 > 100.0 {
            return ScalingDecision::HybridScale {
                reason: "å»¶è¿Ÿè¿‡é«˜".to_string(),
                horizontal_instances: 2,
                vertical_cpu: 1,
                vertical_memory_gb: 2,
            };
        }

        ScalingDecision::NoAction {
            reason: "æŒ‡æ ‡æ­£å¸¸".to_string(),
        }
    }

    fn calculate_cpu_based_instances(&self, cpu_usage: f64) -> usize {
        let current_load = cpu_usage / 100.0;
        let target_load = 0.7;
        ((current_load / target_load).ceil() as usize).saturating_sub(1).max(1)
    }

    fn calculate_memory_increase(&self, memory_usage: f64) -> usize {
        if memory_usage > 95.0 {
            8
        } else if memory_usage > 90.0 {
            4
        } else {
            2
        }
    }
}

#[derive(Debug, Clone)]
pub enum ScalingDecision {
    HorizontalScale {
        reason: String,
        instances: usize,
        priority: Priority,
    },
    VerticalScale {
        reason: String,
        memory_increase_gb: usize,
        priority: Priority,
    },
    HybridScale {
        reason: String,
        horizontal_instances: usize,
        vertical_cpu: usize,
        vertical_memory_gb: usize,
    },
    OptimizeAndScale {
        reason: String,
        optimization: Vec<String>,
        then_scale: Box<ScalingDecision>,
    },
    NetworkOptimization {
        reason: String,
        actions: Vec<String>,
    },
    NoAction {
        reason: String,
    },
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Priority {
    High,
    Medium,
    Low,
}
```

## é¢„æµ‹æ€§æ‰©å®¹

### åŸºäºæœºå™¨å­¦ä¹ çš„é¢„æµ‹

```rust
/// é¢„æµ‹æ€§æ‰©å®¹å¼•æ“
pub struct PredictiveScaler {
    historical_data: Vec<TimeSeriesData>,
    prediction_window: Duration,
}

#[derive(Debug, Clone)]
pub struct TimeSeriesData {
    timestamp: u64,
    cpu_usage: f64,
    memory_usage: f64,
    request_rate: f64,
}

impl PredictiveScaler {
    pub fn new(prediction_window_minutes: u64) -> Self {
        Self {
            historical_data: Vec::new(),
            prediction_window: Duration::from_secs(prediction_window_minutes * 60),
        }
    }

    /// æ·»åŠ å†å²æ•°æ®ç‚¹
    pub fn add_data_point(&mut self, data: TimeSeriesData) {
        self.historical_data.push(data);
        
        // ä¿ç•™æœ€è¿‘ 7 å¤©çš„æ•°æ®
        let cutoff = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs() - (7 * 24 * 3600);
        
        self.historical_data.retain(|d| d.timestamp >= cutoff);
    }

    /// é¢„æµ‹æœªæ¥è´Ÿè½½
    pub fn predict_future_load(&self, minutes_ahead: u64) -> PredictedLoad {
        // ç®€åŒ–çš„çº¿æ€§å›å½’é¢„æµ‹
        if self.historical_data.len() < 10 {
            return PredictedLoad::default();
        }

        let recent = &self.historical_data[self.historical_data.len() - 10..];
        
        // è®¡ç®—è¶‹åŠ¿
        let cpu_trend = self.calculate_trend(
            recent.iter().map(|d| d.cpu_usage).collect()
        );
        let memory_trend = self.calculate_trend(
            recent.iter().map(|d| d.memory_usage).collect()
        );
        let request_trend = self.calculate_trend(
            recent.iter().map(|d| d.request_rate).collect()
        );

        // é¢„æµ‹æœªæ¥å€¼
        let current_cpu = recent.last().unwrap().cpu_usage;
        let current_memory = recent.last().unwrap().memory_usage;
        let current_requests = recent.last().unwrap().request_rate;

        PredictedLoad {
            predicted_cpu: (current_cpu + cpu_trend * minutes_ahead as f64).min(100.0),
            predicted_memory: (current_memory + memory_trend * minutes_ahead as f64).min(100.0),
            predicted_request_rate: (current_requests + request_trend * minutes_ahead as f64).max(0.0),
            confidence: self.calculate_confidence(),
        }
    }

    fn calculate_trend(&self, values: Vec<f64>) -> f64 {
        if values.len() < 2 {
            return 0.0;
        }

        let n = values.len() as f64;
        let sum_x: f64 = (0..values.len()).map(|i| i as f64).sum();
        let sum_y: f64 = values.iter().sum();
        let sum_xy: f64 = values.iter().enumerate()
            .map(|(i, &y)| i as f64 * y)
            .sum();
        let sum_x2: f64 = (0..values.len())
            .map(|i| (i as f64).powi(2))
            .sum();

        // çº¿æ€§å›å½’æ–œç‡
        (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x.powi(2))
    }

    fn calculate_confidence(&self) -> f64 {
        // åŸºäºæ•°æ®é‡å’Œæ–¹å·®è®¡ç®—ç½®ä¿¡åº¦
        let data_points = self.historical_data.len();
        if data_points < 100 {
            0.5
        } else if data_points < 1000 {
            0.7
        } else {
            0.9
        }
    }

    /// å†³å®šæ˜¯å¦éœ€è¦æå‰æ‰©å®¹
    pub fn should_preemptive_scale(&self) -> Option<ScalingDecision> {
        let prediction = self.predict_future_load(30); // é¢„æµ‹30åˆ†é’Ÿå

        if prediction.confidence < 0.6 {
            return None; // ç½®ä¿¡åº¦å¤ªä½ï¼Œä¸é‡‡å–è¡ŒåŠ¨
        }

        if prediction.predicted_cpu > 85.0 || prediction.predicted_memory > 90.0 {
            Some(ScalingDecision::HorizontalScale {
                reason: format!(
                    "é¢„æµ‹æ€§æ‰©å®¹ï¼šé¢„è®¡30åˆ†é’Ÿå CPU={:.1}%, Memory={:.1}%",
                    prediction.predicted_cpu,
                    prediction.predicted_memory
                ),
                instances: 2,
                priority: Priority::Medium,
            })
        } else {
            None
        }
    }
}

#[derive(Debug, Clone, Default)]
pub struct PredictedLoad {
    pub predicted_cpu: f64,
    pub predicted_memory: f64,
    pub predicted_request_rate: f64,
    pub confidence: f64,
}
```

## æˆæœ¬ä¼˜åŒ–çš„æ‰©å®¹ç­–ç•¥

### æˆæœ¬æ„ŸçŸ¥æ‰©å®¹

```rust
/// æˆæœ¬æ„ŸçŸ¥æ‰©å®¹å†³ç­–å™¨
pub struct CostAwareScaler {
    instance_cost_per_hour: f64,
    slo_violation_cost_per_minute: f64,
}

impl CostAwareScaler {
    pub fn new(instance_cost: f64, slo_cost: f64) -> Self {
        Self {
            instance_cost_per_hour: instance_cost,
            slo_violation_cost_per_minute: slo_cost,
        }
    }

    /// è®¡ç®—æ‰©å®¹çš„æˆæœ¬æ•ˆç›Š
    pub fn calculate_scaling_roi(
        &self,
        current_instances: usize,
        proposed_instances: usize,
        current_slo_violation_rate: f64,
        predicted_slo_violation_rate: f64,
    ) -> ScalingROI {
        // æ‰©å®¹æˆæœ¬
        let additional_instances = proposed_instances.saturating_sub(current_instances);
        let scaling_cost_per_hour = additional_instances as f64 * self.instance_cost_per_hour;

        // SLO è¿åæˆæœ¬èŠ‚çœ
        let violation_reduction = current_slo_violation_rate - predicted_slo_violation_rate;
        let cost_savings_per_hour = violation_reduction * 60.0 * self.slo_violation_cost_per_minute;

        // ROI è®¡ç®—
        let net_benefit = cost_savings_per_hour - scaling_cost_per_hour;
        let roi = if scaling_cost_per_hour > 0.0 {
            (cost_savings_per_hour / scaling_cost_per_hour - 1.0) * 100.0
        } else {
            0.0
        };

        ScalingROI {
            scaling_cost_per_hour,
            cost_savings_per_hour,
            net_benefit,
            roi_percentage: roi,
            recommendation: if net_benefit > 0.0 {
                "å»ºè®®æ‰©å®¹".to_string()
            } else {
                "ä¸å»ºè®®æ‰©å®¹ï¼Œæˆæœ¬è¿‡é«˜".to_string()
            },
        }
    }

    /// æ‰¾åˆ°æœ€ä¼˜å®ä¾‹æ•°
    pub fn find_optimal_instance_count(
        &self,
        current_instances: usize,
        max_instances: usize,
        current_slo_violation_rate: f64,
    ) -> usize {
        let mut best_instances = current_instances;
        let mut best_net_benefit = f64::MIN;

        for instances in current_instances..=max_instances {
            // ä¼°ç®—è¯¥å®ä¾‹æ•°ä¸‹çš„ SLO è¿åç‡
            let predicted_violation_rate = self.estimate_slo_violation_rate(
                current_instances,
                instances,
                current_slo_violation_rate,
            );

            let roi = self.calculate_scaling_roi(
                current_instances,
                instances,
                current_slo_violation_rate,
                predicted_violation_rate,
            );

            if roi.net_benefit > best_net_benefit {
                best_net_benefit = roi.net_benefit;
                best_instances = instances;
            }
        }

        best_instances
    }

    fn estimate_slo_violation_rate(
        &self,
        current_instances: usize,
        new_instances: usize,
        current_violation_rate: f64,
    ) -> f64 {
        // ç®€åŒ–æ¨¡å‹ï¼šå‡è®¾è¿åç‡ä¸å®ä¾‹æ•°æˆåæ¯”
        let scaling_factor = current_instances as f64 / new_instances as f64;
        (current_violation_rate * scaling_factor).max(0.0)
    }
}

#[derive(Debug)]
pub struct ScalingROI {
    pub scaling_cost_per_hour: f64,
    pub cost_savings_per_hour: f64,
    pub net_benefit: f64,
    pub roi_percentage: f64,
    pub recommendation: String,
}
```

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæµé‡çªå¢çš„è‡ªåŠ¨æ‰©å®¹

**åœºæ™¯æè¿°**ï¼š

- æ—¶é—´ï¼š2025-10-05 14:00
- äº‹ä»¶ï¼šè¥é”€æ´»åŠ¨å¯¼è‡´æµé‡æ¿€å¢ 5 å€
- åˆå§‹é…ç½®ï¼š3 ä¸ªå®ä¾‹ï¼Œæ¯ä¸ª 2 CPU / 4GB å†…å­˜

**æ‰©å®¹è¿‡ç¨‹**ï¼š

```text
14:00 - æµé‡å¼€å§‹ä¸Šå‡
14:02 - CPU ä½¿ç”¨ç‡è¾¾åˆ° 85%
14:03 - HPA è§¦å‘æ‰©å®¹ï¼Œç›®æ ‡ 6 ä¸ªå®ä¾‹
14:04 - æ–°å®ä¾‹å¯åŠ¨ä¸­ï¼ˆ2ä¸ªï¼‰
14:06 - 2 ä¸ªæ–°å®ä¾‹å°±ç»ªï¼ŒCPU é™è‡³ 70%
14:08 - CPU å†æ¬¡ä¸Šå‡è‡³ 82%
14:09 - HPA å†æ¬¡è§¦å‘ï¼Œç›®æ ‡ 9 ä¸ªå®ä¾‹
14:11 - 3 ä¸ªæ–°å®ä¾‹å°±ç»ª
14:13 - ç³»ç»Ÿç¨³å®šï¼ŒCPU 65%ï¼Œå»¶è¿Ÿæ­£å¸¸
```

**ç›‘æ§æ•°æ®**ï¼š

```rust
/// æ‰©å®¹æ•ˆæœåˆ†æ
pub struct ScalingEffectivenessAnalyzer;

impl ScalingEffectivenessAnalyzer {
    pub fn analyze_scaling_event(event: &ScalingEvent) -> ScalingAnalysis {
        ScalingAnalysis {
            trigger_time: event.trigger_time,
            completion_time: event.completion_time,
            duration_seconds: (event.completion_time - event.trigger_time).as_secs(),
            
            instances_before: event.instances_before,
            instances_after: event.instances_after,
            scaling_ratio: event.instances_after as f64 / event.instances_before as f64,
            
            cpu_before: event.cpu_before,
            cpu_after: event.cpu_after,
            cpu_improvement: event.cpu_before - event.cpu_after,
            
            latency_before_ms: event.latency_before_ms,
            latency_after_ms: event.latency_after_ms,
            latency_improvement_percent: 
                (event.latency_before_ms - event.latency_after_ms) / event.latency_before_ms * 100.0,
            
            success: event.cpu_after < 75.0 && event.latency_after_ms < 100.0,
        }
    }
}

#[derive(Debug)]
pub struct ScalingEvent {
    trigger_time: std::time::Instant,
    completion_time: std::time::Instant,
    instances_before: usize,
    instances_after: usize,
    cpu_before: f64,
    cpu_after: f64,
    latency_before_ms: f64,
    latency_after_ms: f64,
}

#[derive(Debug)]
pub struct ScalingAnalysis {
    trigger_time: std::time::Instant,
    completion_time: std::time::Instant,
    duration_seconds: u64,
    instances_before: usize,
    instances_after: usize,
    scaling_ratio: f64,
    cpu_before: f64,
    cpu_after: f64,
    cpu_improvement: f64,
    latency_before_ms: f64,
    latency_after_ms: f64,
    latency_improvement_percent: f64,
    success: bool,
}
```

### æ¡ˆä¾‹2ï¼šæˆæœ¬ä¼˜åŒ–çš„æ™ºèƒ½ç¼©å®¹

**åœºæ™¯æè¿°**ï¼š

- æ—¶é—´ï¼šå‡Œæ™¨ 2:00 - 6:00ï¼ˆä½å³°æœŸï¼‰
- å½“å‰ï¼š10 ä¸ªå®ä¾‹
- CPU ä½¿ç”¨ç‡ï¼šå¹³å‡ 25%
- ç›®æ ‡ï¼šåœ¨ä¿è¯ SLO çš„å‰æä¸‹é™ä½æˆæœ¬

**å†³ç­–è¿‡ç¨‹**ï¼š

```rust
/// æ™ºèƒ½ç¼©å®¹å†³ç­–ç¤ºä¾‹
pub fn smart_scale_down_example() {
    let scaler = CostAwareScaler::new(
        0.5,   // æ¯å®ä¾‹æ¯å°æ—¶ $0.5
        10.0,  // SLO è¿åæ¯åˆ†é’Ÿ $10
    );

    let current_instances = 10;
    let current_slo_violation_rate = 0.001; // 0.1%

    // è¯„ä¼°ç¼©å®¹åˆ°ä¸åŒå®ä¾‹æ•°çš„æ•ˆæœ
    for target_instances in (3..=current_instances).rev() {
        let predicted_violation_rate = 0.001 * (10.0 / target_instances as f64);
        
        let roi = scaler.calculate_scaling_roi(
            current_instances,
            target_instances,
            current_slo_violation_rate,
            predicted_violation_rate,
        );

        println!("ç¼©å®¹åˆ° {} ä¸ªå®ä¾‹:", target_instances);
        println!("  æˆæœ¬èŠ‚çœ: ${:.2}/å°æ—¶", -roi.scaling_cost_per_hour);
        println!("  SLO è¿åæˆæœ¬å¢åŠ : ${:.2}/å°æ—¶", roi.cost_savings_per_hour);
        println!("  å‡€æ”¶ç›Š: ${:.2}/å°æ—¶", roi.net_benefit);
        println!("  å»ºè®®: {}", roi.recommendation);
        println!();
    }

    // æ‰¾åˆ°æœ€ä¼˜å®ä¾‹æ•°
    let optimal = scaler.find_optimal_instance_count(
        current_instances,
        current_instances,
        current_slo_violation_rate,
    );

    println!("æœ€ä¼˜å®ä¾‹æ•°: {}", optimal);
}
```

**ç»“æœ**ï¼š

- å†³å®šç¼©å®¹åˆ° 5 ä¸ªå®ä¾‹
- æ¯å°æ—¶èŠ‚çœæˆæœ¬ï¼š$2.5
- SLO è¿åç‡ä» 0.1% å¢åŠ åˆ° 0.2%ï¼ˆå¯æ¥å—ï¼‰
- æ¯æœˆèŠ‚çœï¼š$1,800

## æ‰©å®¹æœ€ä½³å®è·µ

### 1. è®¾ç½®åˆç†çš„é˜ˆå€¼

```yaml
# æ¨èé˜ˆå€¼é…ç½®
scaling_thresholds:
  # CPU é˜ˆå€¼
  cpu:
    scale_up: 70%      # æ‰©å®¹é˜ˆå€¼
    scale_down: 30%    # ç¼©å®¹é˜ˆå€¼
    
  # å†…å­˜é˜ˆå€¼
  memory:
    scale_up: 80%
    scale_down: 40%
    
  # å»¶è¿Ÿé˜ˆå€¼
  latency_p99:
    scale_up: 100ms
    scale_down: 50ms
    
  # è¯·æ±‚é˜Ÿåˆ—
  queue_depth:
    scale_up: 1000
    scale_down: 100
```

### 2. é…ç½®å†·å´æœŸ

```rust
/// å†·å´æœŸç®¡ç†å™¨
pub struct CooldownManager {
    scale_up_cooldown: Duration,
    scale_down_cooldown: Duration,
    last_scale_up: Option<Instant>,
    last_scale_down: Option<Instant>,
}

impl CooldownManager {
    pub fn new() -> Self {
        Self {
            scale_up_cooldown: Duration::from_secs(60),    // 1åˆ†é’Ÿ
            scale_down_cooldown: Duration::from_secs(300), // 5åˆ†é’Ÿ
            last_scale_up: None,
            last_scale_down: None,
        }
    }

    pub fn can_scale_up(&self) -> bool {
        self.last_scale_up
            .map(|t| t.elapsed() >= self.scale_up_cooldown)
            .unwrap_or(true)
    }

    pub fn can_scale_down(&self) -> bool {
        self.last_scale_down
            .map(|t| t.elapsed() >= self.scale_down_cooldown)
            .unwrap_or(true)
    }

    pub fn record_scale_up(&mut self) {
        self.last_scale_up = Some(Instant::now());
    }

    pub fn record_scale_down(&mut self) {
        self.last_scale_down = Some(Instant::now());
    }
}
```

### 3. æ¸è¿›å¼æ‰©å®¹

```rust
/// æ¸è¿›å¼æ‰©å®¹ç­–ç•¥
pub struct GradualScaler {
    max_instances_per_step: usize,
    step_interval: Duration,
}

impl GradualScaler {
    pub fn calculate_scaling_steps(
        &self,
        current_instances: usize,
        target_instances: usize,
    ) -> Vec<ScalingStep> {
        let mut steps = Vec::new();
        let mut current = current_instances;

        while current < target_instances {
            let next = (current + self.max_instances_per_step).min(target_instances);
            steps.push(ScalingStep {
                from: current,
                to: next,
                wait_duration: self.step_interval,
            });
            current = next;
        }

        steps
    }
}

#[derive(Debug)]
pub struct ScalingStep {
    from: usize,
    to: usize,
    wait_duration: Duration,
}
```

---

**ç›¸å…³æ–‡æ¡£**ï¼š

- [å®¹é‡é¢„æµ‹æ¨¡å‹](./å®¹é‡é¢„æµ‹æ¨¡å‹.md)
- [èµ„æºä½¿ç”¨åˆ†æ](./èµ„æºä½¿ç”¨åˆ†æ.md)
- [æˆæœ¬ä¼˜åŒ–](./æˆæœ¬ä¼˜åŒ–.md)
