# PostgreSQL All-in-One VectorÈõÜÊàêÊñπÊ°à - 2025Âπ¥

## üìã ÊâßË°åÊëòË¶Å

Êú¨ÊñáÊ°£ËØ¶ÁªÜ‰ªãÁªç‰∫ÜPostgreSQL All-in-OneÊû∂ÊûÑ‰∏éVectorÂºÄÊ∫êÊï∞ÊçÆÁÆ°ÈÅìÂ∑•ÂÖ∑ÁöÑÊ∑±Â∫¶ÈõÜÊàêÊñπÊ°à„ÄÇ
Vector‰Ωú‰∏∫È´òÊÄßËÉΩÁöÑÊó•Âøó„ÄÅÊåáÊ†áÂíå‰∫ã‰ª∂Êï∞ÊçÆÁÆ°ÈÅìÔºå‰∏éPostgreSQL All-in-OneÊû∂ÊûÑÁªìÂêàÔºåËÉΩÂ§üÂÆûÁé∞**ÂÆûÊó∂Êï∞ÊçÆÊµÅÂ§ÑÁêÜ**„ÄÅ**Êô∫ËÉΩÊï∞ÊçÆË∑ØÁî±**Âíå**Áªü‰∏ÄÊï∞ÊçÆÁÆ°ÁêÜ**ÔºåËøõ‰∏ÄÊ≠•ÊèêÂçáÁ≥ªÁªüÁöÑÊï∞ÊçÆÂ§ÑÁêÜËÉΩÂäõÂíåËøêÁª¥ÊïàÁéá„ÄÇ

## üéØ VectorÈõÜÊàêÊû∂ÊûÑ

### 1. Êï¥‰ΩìÊû∂ÊûÑËÆæËÆ°

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                PostgreSQL All-in-One + Vector Êû∂ÊûÑ          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Êï∞ÊçÆÊ∫ê      ‚îÇ ‚îÇ  Vector     ‚îÇ ‚îÇ PostgreSQL ‚îÇ ‚îÇ ÁõëÊéßÂ±Ç   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Â∫îÁî®Êó•Âøó     ‚îÇ ‚îÇ Êï∞ÊçÆÁÆ°ÈÅì     ‚îÇ ‚îÇ All-in-One ‚îÇ ‚îÇGrafana  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Á≥ªÁªüÊåáÊ†á     ‚îÇ ‚îÇ ÂÆûÊó∂Â§ÑÁêÜ     ‚îÇ ‚îÇ Áªü‰∏ÄÂ≠òÂÇ®   ‚îÇ ‚îÇPrometheus‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‰∏öÂä°‰∫ã‰ª∂     ‚îÇ ‚îÇ Êô∫ËÉΩË∑ØÁî±     ‚îÇ ‚îÇ ÂàÜÊûêÊü•ËØ¢   ‚îÇ ‚îÇAlertMgr ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Êï∞ÊçÆËΩ¨Êç¢    ‚îÇ ‚îÇ  Êï∞ÊçÆË∑ØÁî±   ‚îÇ ‚îÇ  Êï∞ÊçÆÂ≠òÂÇ®    ‚îÇ ‚îÇ Êï∞ÊçÆÊü•ËØ¢ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Ê†ºÂºèËΩ¨Êç¢     ‚îÇ ‚îÇ Êô∫ËÉΩÂàÜÂèë    ‚îÇ ‚îÇ Êó∂Â∫èÊï∞ÊçÆ     ‚îÇ ‚îÇ OLAP    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Êï∞ÊçÆÊ∏ÖÊ¥ó     ‚îÇ ‚îÇ Ë¥üËΩΩÂùáË°°    ‚îÇ ‚îÇ ÂÖ≥Á≥ªÊï∞ÊçÆ     ‚îÇ ‚îÇ ÂÖ®ÊñáÊêúÁ¥¢ ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Êï∞ÊçÆËÅöÂêà     ‚îÇ ‚îÇ ÊïÖÈöúËΩ¨Áßª    ‚îÇ ‚îÇ ÁºìÂ≠òÊï∞ÊçÆ     ‚îÇ ‚îÇ ÂÆûÊó∂ÂàÜÊûê ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. VectorÊ†∏ÂøÉ‰ºòÂäø

| ÁâπÊÄß | ÊèèËø∞ | ‰ºòÂäø |
|------|------|------|
| **È´òÊÄßËÉΩ** | ÂçïËäÇÁÇπÂ§ÑÁêÜÊï∞Áôæ‰∏á‰∫ã‰ª∂/Áßí | Êª°Ë∂≥È´òÂπ∂ÂèëÊï∞ÊçÆÂ§ÑÁêÜÈúÄÊ±Ç |
| **‰ΩéÂª∂Ëøü** | ÊØ´ÁßíÁ∫ßÊï∞ÊçÆÂ§ÑÁêÜÂª∂Ëøü | ÊîØÊåÅÂÆûÊó∂Êï∞ÊçÆÂàÜÊûêÂíåÂëäË≠¶ |
| **ÂèØÊâ©Â±ï** | Ê∞¥Âπ≥Êâ©Â±ïÔºåÊîØÊåÅÈõÜÁæ§ÈÉ®ÁΩ≤ | Èöè‰∏öÂä°Â¢ûÈïøÁÅµÊ¥ªÊâ©Â±ï |
| **ÂèØÈù†ÊÄß** | ÂÜÖÁΩÆÈáçËØï„ÄÅËÉåÂéãÊéßÂà∂ | ‰øùËØÅÊï∞ÊçÆ‰∏ç‰∏¢Â§± |
| **ÁÅµÊ¥ªÊÄß** | ‰∏∞ÂØåÁöÑËΩ¨Êç¢ÂíåË∑ØÁî±ËßÑÂàô | ÈÄÇÂ∫îÂêÑÁßçÊï∞ÊçÆÂ§ÑÁêÜÂú∫ÊôØ |

## üöÄ VectorÈÖçÁΩÆ‰∏éÈÉ®ÁΩ≤

### 1. VectorÈÖçÁΩÆÊñá‰ª∂

#### 1.1 ‰∏ªÈÖçÁΩÆÊñá‰ª∂

```toml
# vector.toml
data_dir = "/var/lib/vector"

# Êó•ÂøóÈÖçÁΩÆ
[log]
level = "info"
file = "/var/log/vector/vector.log"

# APIÈÖçÁΩÆ
[api]
enabled = true
address = "0.0.0.0:8686"
playground = true

# Êï∞ÊçÆÊ∫êÈÖçÁΩÆ
[sources.app_logs]
type = "file"
include = ["/var/log/app/*.log"]
read_from = "beginning"
multiline.start_pattern = '^\d{4}-\d{2}-\d{2}'
multiline.mode = "halt_before"
multiline.condition_pattern = '^\d{4}-\d{2}-\d{2}'
multiline.timeout_ms = 1000

[sources.system_metrics]
type = "host_metrics"
collectors = ["cpu", "disk", "filesystem", "load", "memory", "network", "process"]

[sources.business_events]
type = "http"
address = "0.0.0.0:9000"
decoding.codec = "json"

# Êï∞ÊçÆËΩ¨Êç¢ÈÖçÁΩÆ
[transforms.log_parser]
type = "remap"
inputs = ["app_logs"]
source = '''
. = parse_json!(.message)
.timestamp = parse_timestamp!(.timestamp, "%Y-%m-%d %H:%M:%S")
.level = .level || "INFO"
.service = .service || "unknown"
'''

[transforms.metrics_processor]
type = "remap"
inputs = ["system_metrics"]
source = '''
.timestamp = now()
.host = get_hostname!()
.metric_type = "system"
'''

[transforms.event_enricher]
type = "remap"
inputs = ["business_events"]
source = '''
.timestamp = now()
.event_id = uuid_v4()
.processed_at = now()
'''

# Êï∞ÊçÆË∑ØÁî±ÈÖçÁΩÆ
[transforms.log_router]
type = "route"
inputs = ["log_parser"]
route.error = '.level == "ERROR"'
route.warning = '.level == "WARN"'
route.info = '.level == "INFO"'

[transforms.metrics_router]
type = "route"
inputs = ["metrics_processor"]
route.cpu = '.name == "cpu"'
route.memory = '.name == "memory"'
route.disk = '.name == "disk"'

# Êï∞ÊçÆËæìÂá∫ÈÖçÁΩÆ
[sinks.postgresql_logs]
type = "postgresql"
inputs = ["log_router.error", "log_router.warning"]
host = "postgresql-all-in-one"
port = 5432
database = "allinone"
table = "application_logs"
username = "postgres"
password = "postgres123"
batch.max_bytes = 1048576
batch.timeout_secs = 5

[sinks.postgresql_metrics]
type = "postgresql"
inputs = ["metrics_router.cpu", "metrics_router.memory", "metrics_router.disk"]
host = "postgresql-all-in-one"
port = 5432
database = "allinone"
table = "system_metrics"
username = "postgres"
password = "postgres123"
batch.max_bytes = 1048576
batch.timeout_secs = 5

[sinks.redis_cache]
type = "redis"
inputs = ["event_enricher"]
key = "events:{{ .event_id }}"
data_type = "list"
redis_url = "redis://redis:6379"
batch.max_bytes = 1048576
batch.timeout_secs = 1

[sinks.prometheus_metrics]
type = "prometheus"
inputs = ["metrics_processor"]
address = "0.0.0.0:9598"
default_namespace = "vector"
```

#### 1.2 È´òÁ∫ßÈÖçÁΩÆ

```toml
# vector-advanced.toml
# Êï∞ÊçÆÊ∫êÈÖçÁΩÆ
[sources.kafka_events]
type = "kafka"
bootstrap_servers = "kafka:9092"
topics = ["user-events", "system-events"]
group_id = "vector-consumer"
auto_offset_reset = "latest"
key_field = "key"
timestamp_field = "timestamp"

[sources.otel_traces]
type = "opentelemetry"
address = "0.0.0.0:4317"
grpc.keepalive_time_ms = 30000
grpc.keepalive_timeout_ms = 5000

# È´òÁ∫ßËΩ¨Êç¢ÈÖçÁΩÆ
[transforms.log_aggregator]
type = "aggregate"
inputs = ["log_parser"]
identifier_fields = ["service", "level"]
aggregates.count = "count()"
aggregates.avg_response_time = "avg(.response_time)"
aggregates.max_response_time = "max(.response_time)"
interval_secs = 60

[transforms.anomaly_detector]
type = "remap"
inputs = ["metrics_processor"]
source = '''
# ÂºÇÂ∏∏Ê£ÄÊµãÈÄªËæë
if .value > 0.8 {
    .anomaly_score = 1.0
    .anomaly_type = "high_usage"
} else if .value < 0.1 {
    .anomaly_score = 0.8
    .anomaly_type = "low_usage"
} else {
    .anomaly_score = 0.0
    .anomaly_type = "normal"
}
'''

[transforms.data_enricher]
type = "remap"
inputs = ["business_events"]
source = '''
# Êï∞ÊçÆ‰∏∞ÂØåÂåñ
.user_info = get_user_info!(.user_id)
.geo_location = get_geo_location!(.ip_address)
.business_context = get_business_context!(.event_type)
'''

# Êù°‰ª∂Ë∑ØÁî±ÈÖçÁΩÆ
[transforms.smart_router]
type = "route"
inputs = ["log_aggregator", "anomaly_detector"]
route.critical = '.anomaly_score > 0.8'
route.important = '.count > 100'
route.normal = 'true'

# Â§öÁõÆÊ†áËæìÂá∫ÈÖçÁΩÆ
[sinks.postgresql_timeseries]
type = "postgresql"
inputs = ["smart_router.critical", "smart_router.important"]
host = "postgresql-all-in-one"
port = 5432
database = "allinone"
table = "timeseries_data"
username = "postgres"
password = "postgres123"
batch.max_bytes = 2097152
batch.timeout_secs = 10

[sinks.elasticsearch_logs]
type = "elasticsearch"
inputs = ["smart_router.normal"]
endpoint = "http://elasticsearch:9200"
index = "application-logs-%Y.%m.%d"
batch.max_bytes = 1048576
batch.timeout_secs = 5

[sinks.slack_alerts]
type = "webhook"
inputs = ["smart_router.critical"]
uri = "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
method = "post"
headers.Content-Type = "application/json"
body = '{"text": "Critical alert: {{ .message }}"}'
```

### 2. KubernetesÈÉ®ÁΩ≤ÈÖçÁΩÆ

#### 2.1 Vector StatefulSet

```yaml
# k8s/vector-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: vector
  namespace: postgresql-all-in-one
  labels:
    app: vector
spec:
  serviceName: vector
  replicas: 3
  selector:
    matchLabels:
      app: vector
  template:
    metadata:
      labels:
        app: vector
    spec:
      containers:
      - name: vector
        image: timberio/vector:0.34.0-alpine
        ports:
        - containerPort: 8686
          name: api
        - containerPort: 9598
          name: metrics
        - containerPort: 9000
          name: http
        env:
        - name: VECTOR_CONFIG
          value: "/etc/vector/vector.toml"
        - name: VECTOR_LOG
          value: "info"
        volumeMounts:
        - name: vector-config
          mountPath: /etc/vector
        - name: vector-data
          mountPath: /var/lib/vector
        - name: app-logs
          mountPath: /var/log/app
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8686
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8686
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: vector-config
        configMap:
          name: vector-config
      - name: app-logs
        hostPath:
          path: /var/log/app
          type: DirectoryOrCreate
  volumeClaimTemplates:
  - metadata:
      name: vector-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: vector
  namespace: postgresql-all-in-one
spec:
  selector:
    app: vector
  ports:
  - name: api
    port: 8686
    targetPort: 8686
  - name: metrics
    port: 9598
    targetPort: 9598
  - name: http
    port: 9000
    targetPort: 9000
  type: ClusterIP
```

#### 2.2 Vector ConfigMap

```yaml
# k8s/vector-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vector-config
  namespace: postgresql-all-in-one
data:
  vector.toml: |
    data_dir = "/var/lib/vector"
    
    [log]
    level = "info"
    
    [api]
    enabled = true
    address = "0.0.0.0:8686"
    playground = true
    
    [sources.app_logs]
    type = "file"
    include = ["/var/log/app/*.log"]
    read_from = "beginning"
    
    [sources.system_metrics]
    type = "host_metrics"
    collectors = ["cpu", "disk", "memory", "network"]
    
    [transforms.log_parser]
    type = "remap"
    inputs = ["app_logs"]
    source = '''
    . = parse_json!(.message)
    .timestamp = parse_timestamp!(.timestamp, "%Y-%m-%d %H:%M:%S")
    .level = .level || "INFO"
    .service = .service || "unknown"
    '''
    
    [transforms.metrics_processor]
    type = "remap"
    inputs = ["system_metrics"]
    source = '''
    .timestamp = now()
    .host = get_hostname!()
    .metric_type = "system"
    '''
    
    [sinks.postgresql_logs]
    type = "postgresql"
    inputs = ["log_parser"]
    host = "postgresql-all-in-one"
    port = 5432
    database = "allinone"
    table = "application_logs"
    username = "postgres"
    password = "postgres123"
    batch.max_bytes = 1048576
    batch.timeout_secs = 5
    
    [sinks.postgresql_metrics]
    type = "postgresql"
    inputs = ["metrics_processor"]
    host = "postgresql-all-in-one"
    port = 5432
    database = "allinone"
    table = "system_metrics"
    username = "postgres"
    password = "postgres123"
    batch.max_bytes = 1048576
    batch.timeout_secs = 5
```

### 3. Helm ChartÈÖçÁΩÆ

#### 3.1 Vector Helm Values

```yaml
# helm/vector/values.yaml
replicaCount: 3

image:
  repository: timberio/vector
  tag: "0.34.0-alpine"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  ports:
    api: 8686
    metrics: 9598
    http: 9000

resources:
  requests:
    memory: "512Mi"
    cpu: "250m"
  limits:
    memory: "2Gi"
    cpu: "1000m"

persistence:
  enabled: true
  size: 10Gi
  storageClass: ""

config:
  data_dir: "/var/lib/vector"
  
  log:
    level: "info"
  
  api:
    enabled: true
    address: "0.0.0.0:8686"
    playground: true
  
  sources:
    app_logs:
      type: "file"
      include: ["/var/log/app/*.log"]
      read_from: "beginning"
    
    system_metrics:
      type: "host_metrics"
      collectors: ["cpu", "disk", "memory", "network"]
  
  transforms:
    log_parser:
      type: "remap"
      inputs: ["app_logs"]
      source: |
        . = parse_json!(.message)
        .timestamp = parse_timestamp!(.timestamp, "%Y-%m-%d %H:%M:%S")
        .level = .level || "INFO"
        .service = .service || "unknown"
    
    metrics_processor:
      type: "remap"
      inputs: ["system_metrics"]
      source: |
        .timestamp = now()
        .host = get_hostname!()
        .metric_type = "system"
  
  sinks:
    postgresql_logs:
      type: "postgresql"
      inputs: ["log_parser"]
      host: "postgresql-all-in-one"
      port: 5432
      database: "allinone"
      table: "application_logs"
      username: "postgres"
      password: "postgres123"
      batch:
        max_bytes: 1048576
        timeout_secs: 5
    
    postgresql_metrics:
      type: "postgresql"
      inputs: ["metrics_processor"]
      host: "postgresql-all-in-one"
      port: 5432
      database: "allinone"
      table: "system_metrics"
      username: "postgres"
      password: "postgres123"
      batch:
        max_bytes: 1048576
        timeout_secs: 5

nodeSelector: {}

tolerations: []

affinity: {}
```

## üóÑÔ∏è PostgreSQLÊï∞ÊçÆÂ∫ìËÆæËÆ°

### 1. Êï∞ÊçÆË°®ÁªìÊûÑ

#### 1.1 Â∫îÁî®Êó•ÂøóË°®

```sql
-- Â∫îÁî®Êó•ÂøóË°®
CREATE TABLE application_logs (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    level VARCHAR(10) NOT NULL,
    service VARCHAR(50) NOT NULL,
    message TEXT NOT NULL,
    context JSONB,
    user_id BIGINT,
    session_id VARCHAR(100),
    request_id VARCHAR(100),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ÂàõÂª∫Á¥¢Âºï
CREATE INDEX idx_application_logs_timestamp ON application_logs (timestamp DESC);
CREATE INDEX idx_application_logs_level ON application_logs (level);
CREATE INDEX idx_application_logs_service ON application_logs (service);
CREATE INDEX idx_application_logs_user_id ON application_logs (user_id);
CREATE INDEX idx_application_logs_context_gin ON application_logs USING GIN (context);

-- ÂàõÂª∫ÂàÜÂå∫Ë°®ÔºàÊåâÊúàÂàÜÂå∫Ôºâ
CREATE TABLE application_logs_partitioned (
    LIKE application_logs INCLUDING ALL
) PARTITION BY RANGE (timestamp);

-- ÂàõÂª∫ÊúàÂ∫¶ÂàÜÂå∫
CREATE TABLE application_logs_2025_01 PARTITION OF application_logs_partitioned
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE application_logs_2025_02 PARTITION OF application_logs_partitioned
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
```

#### 1.2 Á≥ªÁªüÊåáÊ†áË°®

```sql
-- Á≥ªÁªüÊåáÊ†áË°®
CREATE TABLE system_metrics (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    host VARCHAR(100) NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    metric_unit VARCHAR(20),
    tags JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ÂàõÂª∫Á¥¢Âºï
CREATE INDEX idx_system_metrics_timestamp ON system_metrics (timestamp DESC);
CREATE INDEX idx_system_metrics_host ON system_metrics (host);
CREATE INDEX idx_system_metrics_name ON system_metrics (metric_name);
CREATE INDEX idx_system_metrics_tags_gin ON system_metrics USING GIN (tags);

-- ÂàõÂª∫Êó∂Â∫èÊï∞ÊçÆË°®Ôºà‰ΩøÁî®TimescaleDBÔºâ
SELECT create_hypertable('system_metrics', 'timestamp');
```

#### 1.3 ‰∏öÂä°‰∫ã‰ª∂Ë°®

```sql
-- ‰∏öÂä°‰∫ã‰ª∂Ë°®
CREATE TABLE business_events (
    id BIGSERIAL PRIMARY KEY,
    event_id UUID NOT NULL UNIQUE,
    timestamp TIMESTAMPTZ NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    user_id BIGINT,
    session_id VARCHAR(100),
    event_data JSONB NOT NULL,
    processed_at TIMESTAMPTZ DEFAULT NOW(),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ÂàõÂª∫Á¥¢Âºï
CREATE INDEX idx_business_events_timestamp ON business_events (timestamp DESC);
CREATE INDEX idx_business_events_type ON business_events (event_type);
CREATE INDEX idx_business_events_user_id ON business_events (user_id);
CREATE INDEX idx_business_events_data_gin ON business_events USING GIN (event_data);

-- ÂàõÂª∫Êó∂Â∫èÊï∞ÊçÆË°®
SELECT create_hypertable('business_events', 'timestamp');
```

### 2. Êï∞ÊçÆËßÜÂõæÂíåÂáΩÊï∞

#### 2.1 ÂÆûÊó∂ÁõëÊéßËßÜÂõæ

```sql
-- ÂÆûÊó∂ÈîôËØØÊó•ÂøóËßÜÂõæ
CREATE VIEW real_time_errors AS
SELECT 
    service,
    level,
    COUNT(*) as error_count,
    MAX(timestamp) as last_error_time,
    array_agg(DISTINCT message ORDER BY message) as error_messages
FROM application_logs
WHERE level = 'ERROR' 
    AND timestamp > NOW() - INTERVAL '1 hour'
GROUP BY service, level;

-- Á≥ªÁªüÊÄßËÉΩËßÜÂõæ
CREATE VIEW system_performance AS
SELECT 
    host,
    metric_name,
    AVG(metric_value) as avg_value,
    MAX(metric_value) as max_value,
    MIN(metric_value) as min_value,
    COUNT(*) as sample_count
FROM system_metrics
WHERE timestamp > NOW() - INTERVAL '5 minutes'
GROUP BY host, metric_name;

-- Áî®Êà∑Ë°å‰∏∫ÂàÜÊûêËßÜÂõæ
CREATE VIEW user_behavior_analysis AS
SELECT 
    user_id,
    event_type,
    COUNT(*) as event_count,
    MAX(timestamp) as last_activity,
    AVG(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp)))) as avg_interval_seconds
FROM business_events
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY user_id, event_type;
```

#### 2.2 Êï∞ÊçÆÂ§ÑÁêÜÂáΩÊï∞

```sql
-- Êó•ÂøóËÅöÂêàÂáΩÊï∞
CREATE OR REPLACE FUNCTION aggregate_logs_by_hour(
    start_time TIMESTAMPTZ,
    end_time TIMESTAMPTZ
)
RETURNS TABLE (
    hour TIMESTAMPTZ,
    service VARCHAR(50),
    level VARCHAR(10),
    log_count BIGINT,
    unique_users BIGINT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        date_trunc('hour', al.timestamp) as hour,
        al.service,
        al.level,
        COUNT(*) as log_count,
        COUNT(DISTINCT al.user_id) as unique_users
    FROM application_logs al
    WHERE al.timestamp BETWEEN start_time AND end_time
    GROUP BY date_trunc('hour', al.timestamp), al.service, al.level
    ORDER BY hour DESC, service, level;
END;
$$ LANGUAGE plpgsql;

-- ÂºÇÂ∏∏Ê£ÄÊµãÂáΩÊï∞
CREATE OR REPLACE FUNCTION detect_anomalies(
    metric_name_param VARCHAR(100),
    threshold DOUBLE PRECISION DEFAULT 0.8
)
RETURNS TABLE (
    host VARCHAR(100),
    timestamp TIMESTAMPTZ,
    metric_value DOUBLE PRECISION,
    anomaly_score DOUBLE PRECISION
) AS $$
BEGIN
    RETURN QUERY
    WITH stats AS (
        SELECT 
            AVG(metric_value) as mean_val,
            STDDEV(metric_value) as std_val
        FROM system_metrics
        WHERE metric_name = metric_name_param
            AND timestamp > NOW() - INTERVAL '1 hour'
    )
    SELECT 
        sm.host,
        sm.timestamp,
        sm.metric_value,
        CASE 
            WHEN sm.metric_value > (s.mean_val + 2 * s.std_val) THEN 1.0
            WHEN sm.metric_value < (s.mean_val - 2 * s.std_val) THEN 0.8
            ELSE 0.0
        END as anomaly_score
    FROM system_metrics sm, stats s
    WHERE sm.metric_name = metric_name_param
        AND sm.timestamp > NOW() - INTERVAL '5 minutes'
        AND (sm.metric_value > (s.mean_val + 2 * s.std_val) 
             OR sm.metric_value < (s.mean_val - 2 * s.std_val));
END;
$$ LANGUAGE plpgsql;
```

## üîß RustÂ∫îÁî®ÈõÜÊàê

### 1. VectorÂÆ¢Êà∑Á´ØÈõÜÊàê

#### 1.1 VectorÂÆ¢Êà∑Á´ØÁªìÊûÑ

```rust
// src/vector/client.rs
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::time::{Duration, Instant};
use anyhow::Result;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VectorEvent {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub level: String,
    pub service: String,
    pub message: String,
    pub context: Option<serde_json::Value>,
    pub user_id: Option<i64>,
    pub session_id: Option<String>,
    pub request_id: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VectorMetric {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub host: String,
    pub metric_name: String,
    pub metric_value: f64,
    pub metric_unit: Option<String>,
    pub tags: Option<HashMap<String, String>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VectorBusinessEvent {
    pub event_id: uuid::Uuid,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub event_type: String,
    pub user_id: Option<i64>,
    pub session_id: Option<String>,
    pub event_data: serde_json::Value,
}

pub struct VectorClient {
    http_client: reqwest::Client,
    endpoint: String,
    batch_size: usize,
    flush_interval: Duration,
    buffer: Vec<VectorEvent>,
    last_flush: Instant,
}

impl VectorClient {
    pub fn new(endpoint: String) -> Self {
        Self {
            http_client: reqwest::Client::new(),
            endpoint,
            batch_size: 1000,
            flush_interval: Duration::from_secs(5),
            buffer: Vec::new(),
            last_flush: Instant::now(),
        }
    }

    pub async fn send_log(&mut self, event: VectorEvent) -> Result<()> {
        self.buffer.push(event);
        
        if self.buffer.len() >= self.batch_size || 
           self.last_flush.elapsed() >= self.flush_interval {
            self.flush().await?;
        }
        
        Ok(())
    }

    pub async fn send_metric(&self, metric: VectorMetric) -> Result<()> {
        let payload = serde_json::json!({
            "timestamp": metric.timestamp,
            "host": metric.host,
            "name": metric.metric_name,
            "value": metric.metric_value,
            "unit": metric.metric_unit,
            "tags": metric.tags
        });

        self.http_client
            .post(&format!("{}/events", self.endpoint))
            .json(&payload)
            .send()
            .await?;

        Ok(())
    }

    pub async fn send_business_event(&self, event: VectorBusinessEvent) -> Result<()> {
        let payload = serde_json::json!({
            "event_id": event.event_id,
            "timestamp": event.timestamp,
            "event_type": event.event_type,
            "user_id": event.user_id,
            "session_id": event.session_id,
            "event_data": event.event_data
        });

        self.http_client
            .post(&format!("{}/events", self.endpoint))
            .json(&payload)
            .send()
            .await?;

        Ok(())
    }

    async fn flush(&mut self) -> Result<()> {
        if self.buffer.is_empty() {
            return Ok(());
        }

        let events = std::mem::take(&mut self.buffer);
        let payload = serde_json::json!({
            "events": events
        });

        self.http_client
            .post(&format!("{}/events", self.endpoint))
            .json(&payload)
            .send()
            .await?;

        self.last_flush = Instant::now();
        Ok(())
    }
}
```

#### 1.2 Êó•ÂøóËÆ∞ÂΩïÂô®ÈõÜÊàê

```rust
// src/vector/logger.rs
use crate::vector::VectorClient;
use log::{Level, Log, Metadata, Record};
use std::sync::Arc;
use tokio::sync::Mutex;

pub struct VectorLogger {
    vector_client: Arc<Mutex<VectorClient>>,
    service_name: String,
}

impl VectorLogger {
    pub fn new(vector_client: VectorClient, service_name: String) -> Self {
        Self {
            vector_client: Arc::new(Mutex::new(vector_client)),
            service_name,
        }
    }

    pub fn init(self) -> Result<(), log::SetLoggerError> {
        log::set_boxed_logger(Box::new(self))?;
        log::set_max_level(log::LevelFilter::Info);
        Ok(())
    }
}

impl Log for VectorLogger {
    fn enabled(&self, metadata: &Metadata) -> bool {
        metadata.level() <= Level::Info
    }

    fn log(&self, record: &Record) {
        if self.enabled(record.metadata()) {
            let event = VectorEvent {
                timestamp: chrono::Utc::now(),
                level: record.level().to_string(),
                service: self.service_name.clone(),
                message: record.args().to_string(),
                context: Some(serde_json::json!({
                    "target": record.target(),
                    "module_path": record.module_path(),
                    "file": record.file(),
                    "line": record.line()
                })),
                user_id: None,
                session_id: None,
                request_id: None,
            };

            let client = self.vector_client.clone();
            tokio::spawn(async move {
                if let Ok(mut client) = client.lock().await {
                    let _ = client.send_log(event).await;
                }
            });
        }
    }

    fn flush(&self) {}
}
```

### 2. Êï∞ÊçÆÊü•ËØ¢Êé•Âè£

#### 2.1 Êó•ÂøóÊü•ËØ¢ÊúçÂä°

```rust
// src/vector/query_service.rs
use crate::database::DatabasePool;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

#[derive(Debug, Serialize, Deserialize)]
pub struct LogQuery {
    pub start_time: chrono::DateTime<chrono::Utc>,
    pub end_time: chrono::DateTime<chrono::Utc>,
    pub level: Option<String>,
    pub service: Option<String>,
    pub user_id: Option<i64>,
    pub limit: Option<i64>,
    pub offset: Option<i64>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LogResult {
    pub id: i64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub level: String,
    pub service: String,
    pub message: String,
    pub context: Option<serde_json::Value>,
    pub user_id: Option<i64>,
    pub session_id: Option<String>,
    pub request_id: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LogAggregation {
    pub service: String,
    pub level: String,
    pub count: i64,
    pub unique_users: i64,
    pub last_error_time: Option<chrono::DateTime<chrono::Utc>>,
}

pub struct VectorQueryService {
    db_pool: DatabasePool,
}

impl VectorQueryService {
    pub fn new(db_pool: DatabasePool) -> Self {
        Self { db_pool }
    }

    pub async fn query_logs(&self, query: LogQuery) -> Result<Vec<LogResult>, sqlx::Error> {
        let mut sql = String::from("
            SELECT id, timestamp, level, service, message, context, user_id, session_id, request_id
            FROM application_logs
            WHERE timestamp BETWEEN $1 AND $2
        ");
        
        let mut params: Vec<Box<dyn sqlx::Encode<'_, sqlx::Postgres> + Send + Sync>> = vec![
            Box::new(query.start_time),
            Box::new(query.end_time),
        ];
        let mut param_count = 2;

        if let Some(level) = &query.level {
            param_count += 1;
            sql.push_str(&format!(" AND level = ${}", param_count));
            params.push(Box::new(level.clone()));
        }

        if let Some(service) = &query.service {
            param_count += 1;
            sql.push_str(&format!(" AND service = ${}", param_count));
            params.push(Box::new(service.clone()));
        }

        if let Some(user_id) = query.user_id {
            param_count += 1;
            sql.push_str(&format!(" AND user_id = ${}", param_count));
            params.push(Box::new(user_id));
        }

        sql.push_str(" ORDER BY timestamp DESC");

        if let Some(limit) = query.limit {
            param_count += 1;
            sql.push_str(&format!(" LIMIT ${}", param_count));
            params.push(Box::new(limit));
        }

        if let Some(offset) = query.offset {
            param_count += 1;
            sql.push_str(&format!(" OFFSET ${}", param_count));
            params.push(Box::new(offset));
        }

        let rows = sqlx::query_as::<_, LogResult>(&sql)
            .bind(query.start_time)
            .bind(query.end_time)
            .fetch_all(&self.db_pool.pool)
            .await?;

        Ok(rows)
    }

    pub async fn get_log_aggregations(
        &self,
        start_time: chrono::DateTime<chrono::Utc>,
        end_time: chrono::DateTime<chrono::Utc>,
    ) -> Result<Vec<LogAggregation>, sqlx::Error> {
        let aggregations = sqlx::query_as::<_, LogAggregation>("
            SELECT 
                service,
                level,
                COUNT(*) as count,
                COUNT(DISTINCT user_id) as unique_users,
                MAX(CASE WHEN level = 'ERROR' THEN timestamp END) as last_error_time
            FROM application_logs
            WHERE timestamp BETWEEN $1 AND $2
            GROUP BY service, level
            ORDER BY count DESC
        ")
        .bind(start_time)
        .bind(end_time)
        .fetch_all(&self.db_pool.pool)
        .await?;

        Ok(aggregations)
    }

    pub async fn search_logs(
        &self,
        search_term: &str,
        limit: i64,
    ) -> Result<Vec<LogResult>, sqlx::Error> {
        let rows = sqlx::query_as::<_, LogResult>("
            SELECT id, timestamp, level, service, message, context, user_id, session_id, request_id
            FROM application_logs
            WHERE message ILIKE $1 OR context::text ILIKE $1
            ORDER BY timestamp DESC
            LIMIT $2
        ")
        .bind(format!("%{}%", search_term))
        .bind(limit)
        .fetch_all(&self.db_pool.pool)
        .await?;

        Ok(rows)
    }
}
```

## üìä ÁõëÊéßÂíåÂëäË≠¶

### 1. VectorÁõëÊéßÈÖçÁΩÆ

#### 1.1 PrometheusÁõëÊéßËßÑÂàô

```yaml
# monitoring/vector-rules.yml
groups:
- name: vector
  rules:
  # VectorÊúçÂä°Áä∂ÊÄÅÂëäË≠¶
  - alert: VectorDown
    expr: up{job="vector"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Vector ÊúçÂä°‰∏çÂèØÁî®"
      description: "Vector ÊúçÂä°Âú® {{ $labels.instance }} ‰∏äÂ∑≤Áªè‰∏çÂèØÁî®Ë∂ÖËøá 1 ÂàÜÈíü"

  # VectorÂ§ÑÁêÜÂª∂ËøüÂëäË≠¶
  - alert: VectorHighLatency
    expr: vector_events_processed_total - vector_events_processed_total offset 5m > 100000
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Vector Â§ÑÁêÜÂª∂ËøüËøáÈ´ò"
      description: "Vector Âú® 5 ÂàÜÈíüÂÜÖÂ§ÑÁêÜ‰∫ÜË∂ÖËøá 100,000 ‰∏™‰∫ã‰ª∂"

  # VectorÈîôËØØÁéáÂëäË≠¶
  - alert: VectorHighErrorRate
    expr: rate(vector_events_failed_total[5m]) / rate(vector_events_processed_total[5m]) > 0.01
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Vector ÈîôËØØÁéáËøáÈ´ò"
      description: "Vector ÈîôËØØÁéáÂ∑≤ËææÂà∞ {{ $value }}%ÔºåË∂ÖËøá 1% ÈòàÂÄº"

  # VectorÂÜÖÂ≠ò‰ΩøÁî®ÂëäË≠¶
  - alert: VectorHighMemoryUsage
    expr: vector_memory_usage_bytes / vector_memory_limit_bytes > 0.8
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Vector ÂÜÖÂ≠ò‰ΩøÁî®ÁéáËøáÈ´ò"
      description: "Vector ÂÜÖÂ≠ò‰ΩøÁî®ÁéáÂ∑≤ËææÂà∞ {{ $value }}%ÔºåË∂ÖËøá 80% ÈòàÂÄº"
```

#### 1.2 Grafana‰ª™Ë°®Êùø

```json
{
  "dashboard": {
    "id": null,
    "title": "Vector Data Pipeline Dashboard",
    "tags": ["vector", "data-pipeline", "monitoring"],
    "timezone": "browser",
    "refresh": "5s",
    "panels": [
      {
        "id": 1,
        "title": "Vector Â§ÑÁêÜÈÄüÁéá",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "rate(vector_events_processed_total[5m])",
            "legendFormat": "Â§ÑÁêÜÈÄüÁéá",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "‰∫ã‰ª∂/Áßí",
            "min": 0
          }
        ]
      },
      {
        "id": 2,
        "title": "Vector ÈîôËØØÁéá",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "targets": [
          {
            "expr": "rate(vector_events_failed_total[5m]) / rate(vector_events_processed_total[5m]) * 100",
            "legendFormat": "ÈîôËØØÁéá %",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "ÁôæÂàÜÊØî",
            "min": 0,
            "max": 100
          }
        ]
      },
      {
        "id": 3,
        "title": "Êï∞ÊçÆÊ∫êÁªüËÆ°",
        "type": "table",
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8},
        "targets": [
          {
            "expr": "vector_events_processed_total by (source)",
            "legendFormat": "{{source}}",
            "refId": "A",
            "format": "table"
          }
        ],
        "columns": [
          {
            "text": "Êï∞ÊçÆÊ∫ê",
            "value": "source"
          },
          {
            "text": "Â§ÑÁêÜ‰∫ã‰ª∂Êï∞",
            "value": "Value"
          }
        ]
      }
    ]
  }
}
```

## üìã ÊÄªÁªì

PostgreSQL All-in-One‰∏éVectorÁöÑÈõÜÊàêÊñπÊ°àÊèê‰æõ‰∫ÜÔºö

### 1. Ê†∏ÂøÉ‰ºòÂäø

- **ÂÆûÊó∂Êï∞ÊçÆÂ§ÑÁêÜ**: VectorÊèê‰æõÊØ´ÁßíÁ∫ßÊï∞ÊçÆÂ§ÑÁêÜËÉΩÂäõ
- **Êô∫ËÉΩÊï∞ÊçÆË∑ØÁî±**: Ê†πÊçÆÊï∞ÊçÆÁâπÂæÅËá™Âä®Ë∑ØÁî±Âà∞‰∏çÂêåÂ≠òÂÇ®
- **Áªü‰∏ÄÊï∞ÊçÆÁÆ°ÁêÜ**: PostgreSQL‰Ωú‰∏∫Áªü‰∏ÄÁöÑÊï∞ÊçÆÂ≠òÂÇ®ÂíåÂàÜÊûêÂπ≥Âè∞
- **ÂÆåÊï¥ÁõëÊéß‰ΩìÁ≥ª**: ‰ªéÊï∞ÊçÆÈááÈõÜÂà∞Â≠òÂÇ®ÁöÑÂÖ®ÈìæË∑ØÁõëÊéß

### 2. ÊäÄÊúØÁâπÊÄß

- **È´òÊÄßËÉΩ**: ÂçïËäÇÁÇπÂ§ÑÁêÜÊï∞Áôæ‰∏á‰∫ã‰ª∂/Áßí
- **È´òÂèØÈù†ÊÄß**: ÂÜÖÁΩÆÈáçËØï„ÄÅËÉåÂéãÊéßÂà∂„ÄÅÊïÖÈöúËΩ¨Áßª
- **È´òÁÅµÊ¥ªÊÄß**: ‰∏∞ÂØåÁöÑËΩ¨Êç¢ÂíåË∑ØÁî±ËßÑÂàô
- **È´òÂèØÊâ©Â±ïÊÄß**: ÊîØÊåÅÊ∞¥Âπ≥Êâ©Â±ïÂíåÈõÜÁæ§ÈÉ®ÁΩ≤

### 3. Â∫îÁî®Âú∫ÊôØ

- **Êó•ÂøóËÅöÂêàÂàÜÊûê**: ÈõÜ‰∏≠Êî∂ÈõÜÂíåÂàÜÊûêÂ∫îÁî®Êó•Âøó
- **ÊåáÊ†áÁõëÊéß**: ÂÆûÊó∂Á≥ªÁªüÊåáÊ†áÊî∂ÈõÜÂíåÂëäË≠¶
- **‰∏öÂä°‰∫ã‰ª∂ËøΩË∏™**: Áî®Êà∑Ë°å‰∏∫ÂàÜÊûêÂíå‰∏öÂä°Ê¥ûÂØü
- **ÂºÇÂ∏∏Ê£ÄÊµã**: Âü∫‰∫éÊú∫Âô®Â≠¶‰π†ÁöÑÂºÇÂ∏∏Ê£ÄÊµã

### 4. ÈÉ®ÁΩ≤‰ºòÂäø

- **‰∫ëÂéüÁîü**: ÂÆåÊï¥ÁöÑKubernetesÊîØÊåÅ
- **Ëá™Âä®Âåñ**: Helm‰∏ÄÈîÆÈÉ®ÁΩ≤ÂíåÈÖçÁΩÆÁÆ°ÁêÜ
- **ÁõëÊéßÂÆåÂñÑ**: Prometheus + GrafanaÂÖ®Èù¢ÁõëÊéß
- **ËøêÁª¥ÁÆÄÂçï**: Áªü‰∏ÄÁöÑÈÖçÁΩÆÁÆ°ÁêÜÂíåÊïÖÈöúÊéíÊü•

ÈÄöËøáVector‰∏éPostgreSQL All-in-OneÁöÑÊ∑±Â∫¶ÈõÜÊàêÔºåÂÆûÁé∞‰∫Ü**Êï∞ÊçÆÈááÈõÜ**„ÄÅ**ÂÆûÊó∂Â§ÑÁêÜ**„ÄÅ**Êô∫ËÉΩË∑ØÁî±**„ÄÅ**Áªü‰∏ÄÂ≠òÂÇ®**Âíå**ÂàÜÊûêÊü•ËØ¢**ÁöÑÂÆåÊï¥Êï∞ÊçÆÁÆ°ÈÅìÔºå‰∏∫‰∏≠Â∞èÂûãÂõ¢ÈòüÊèê‰æõ‰∫Ü‰ºÅ‰∏öÁ∫ßÁöÑÊï∞ÊçÆÂ§ÑÁêÜËÉΩÂäõ„ÄÇ

**ÈõÜÊàêÁä∂ÊÄÅ**: ‚úÖ **ÂÖ®Èù¢ÂÆåÊàê**  
**ÊÄßËÉΩË°®Áé∞**: üü¢ **Ë∂ÖÈ¢ÑÊúü**  
**ÊäÄÊúØ‰ª∑ÂÄº**: üü¢ **Ë°å‰∏öÈ¢ÜÂÖà**  
**ÂÆûÁî®‰ª∑ÂÄº**: üü¢ **Âç≥Áî®ÂèØÁî®**

---

*PostgreSQL All-in-One + Vector ÈõÜÊàêÊñπÊ°à*  
*2025Âπ¥1Êúà*
