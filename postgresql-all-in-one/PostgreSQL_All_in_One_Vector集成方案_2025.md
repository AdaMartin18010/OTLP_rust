# PostgreSQL All-in-One Vectoré›†æˆæ–¹æ¡ˆ - 2025å¹´

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†PostgreSQL All-in-Oneæ¶æ„ä¸Vectorå¼€æºæ•°æ®ç®¡é“å·¥å…·çš„æ·±åº¦é›†æˆæ–¹æ¡ˆã€‚
Vectorä½œä¸ºé«˜æ€§èƒ½çš„æ—¥å¿—ã€æŒ‡æ ‡å’Œäº‹ä»¶æ•°æ®ç®¡é“ï¼Œä¸PostgreSQL All-in-Oneæ¶æ„ç»“åˆï¼Œèƒ½å¤Ÿå®ç°**å®æ—¶æ•°æ®æµå¤„ç†**ã€**æ™ºèƒ½æ•°æ®è·¯ç”±**å’Œ**ç»Ÿä¸€æ•°æ®ç®¡ç†**ï¼Œè¿›ä¸€æ­¥æå‡ç³»ç»Ÿçš„æ•°æ®å¤„ç†èƒ½åŠ›å’Œè¿ç»´æ•ˆç‡ã€‚

## ğŸ¯ Vectoré›†æˆæ¶æ„

### 1. æ•´ä½“æ¶æ„è®¾è®¡

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PostgreSQL All-in-One + Vector æ¶æ„          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  æ•°æ®æº      â”‚ â”‚  Vector     â”‚ â”‚ PostgreSQL â”‚ â”‚ ç›‘æ§å±‚   â”‚ â”‚
â”‚  â”‚ åº”ç”¨æ—¥å¿—     â”‚ â”‚ æ•°æ®ç®¡é“     â”‚ â”‚ All-in-One â”‚ â”‚Grafana  â”‚ â”‚
â”‚  â”‚ ç³»ç»ŸæŒ‡æ ‡     â”‚ â”‚ å®æ—¶å¤„ç†     â”‚ â”‚ ç»Ÿä¸€å­˜å‚¨   â”‚ â”‚Prometheusâ”‚ â”‚
â”‚  â”‚ ä¸šåŠ¡äº‹ä»¶     â”‚ â”‚ æ™ºèƒ½è·¯ç”±     â”‚ â”‚ åˆ†ææŸ¥è¯¢   â”‚ â”‚AlertMgr â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  æ•°æ®è½¬æ¢    â”‚ â”‚  æ•°æ®è·¯ç”±   â”‚ â”‚  æ•°æ®å­˜å‚¨    â”‚ â”‚ æ•°æ®æŸ¥è¯¢ â”‚ â”‚
â”‚  â”‚ æ ¼å¼è½¬æ¢     â”‚ â”‚ æ™ºèƒ½åˆ†å‘    â”‚ â”‚ æ—¶åºæ•°æ®     â”‚ â”‚ OLAP    â”‚ â”‚
â”‚  â”‚ æ•°æ®æ¸…æ´—     â”‚ â”‚ è´Ÿè½½å‡è¡¡    â”‚ â”‚ å…³ç³»æ•°æ®     â”‚ â”‚ å…¨æ–‡æœç´¢ â”‚ â”‚
â”‚  â”‚ æ•°æ®èšåˆ     â”‚ â”‚ æ•…éšœè½¬ç§»    â”‚ â”‚ ç¼“å­˜æ•°æ®     â”‚ â”‚ å®æ—¶åˆ†æ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Vectoræ ¸å¿ƒä¼˜åŠ¿

| ç‰¹æ€§ | æè¿° | ä¼˜åŠ¿ |
|------|------|------|
| **é«˜æ€§èƒ½** | å•èŠ‚ç‚¹å¤„ç†æ•°ç™¾ä¸‡äº‹ä»¶/ç§’ | æ»¡è¶³é«˜å¹¶å‘æ•°æ®å¤„ç†éœ€æ±‚ |
| **ä½å»¶è¿Ÿ** | æ¯«ç§’çº§æ•°æ®å¤„ç†å»¶è¿Ÿ | æ”¯æŒå®æ—¶æ•°æ®åˆ†æå’Œå‘Šè­¦ |
| **å¯æ‰©å±•** | æ°´å¹³æ‰©å±•ï¼Œæ”¯æŒé›†ç¾¤éƒ¨ç½² | éšä¸šåŠ¡å¢é•¿çµæ´»æ‰©å±• |
| **å¯é æ€§** | å†…ç½®é‡è¯•ã€èƒŒå‹æ§åˆ¶ | ä¿è¯æ•°æ®ä¸ä¸¢å¤± |
| **çµæ´»æ€§** | ä¸°å¯Œçš„è½¬æ¢å’Œè·¯ç”±è§„åˆ™ | é€‚åº”å„ç§æ•°æ®å¤„ç†åœºæ™¯ |

## ğŸš€ Vectoré…ç½®ä¸éƒ¨ç½²

### 1. Vectoré…ç½®æ–‡ä»¶

#### 1.1 ä¸»é…ç½®æ–‡ä»¶

```toml
# vector.toml
data_dir = "/var/lib/vector"

# æ—¥å¿—é…ç½®
[log]
level = "info"
file = "/var/log/vector/vector.log"

# APIé…ç½®
[api]
enabled = true
address = "0.0.0.0:8686"
playground = true

# æ•°æ®æºé…ç½®
[sources.app_logs]
type = "file"
include = ["/var/log/app/*.log"]
read_from = "beginning"
multiline.start_pattern = '^\d{4}-\d{2}-\d{2}'
multiline.mode = "halt_before"
multiline.condition_pattern = '^\d{4}-\d{2}-\d{2}'
multiline.timeout_ms = 1000

[sources.system_metrics]
type = "host_metrics"
collectors = ["cpu", "disk", "filesystem", "load", "memory", "network", "process"]

[sources.business_events]
type = "http"
address = "0.0.0.0:9000"
decoding.codec = "json"

# æ•°æ®è½¬æ¢é…ç½®
[transforms.log_parser]
type = "remap"
inputs = ["app_logs"]
source = '''
. = parse_json!(.message)
.timestamp = parse_timestamp!(.timestamp, "%Y-%m-%d %H:%M:%S")
.level = .level || "INFO"
.service = .service || "unknown"
'''

[transforms.metrics_processor]
type = "remap"
inputs = ["system_metrics"]
source = '''
.timestamp = now()
.host = get_hostname!()
.metric_type = "system"
'''

[transforms.event_enricher]
type = "remap"
inputs = ["business_events"]
source = '''
.timestamp = now()
.event_id = uuid_v4()
.processed_at = now()
'''

# æ•°æ®è·¯ç”±é…ç½®
[transforms.log_router]
type = "route"
inputs = ["log_parser"]
route.error = '.level == "ERROR"'
route.warning = '.level == "WARN"'
route.info = '.level == "INFO"'

[transforms.metrics_router]
type = "route"
inputs = ["metrics_processor"]
route.cpu = '.name == "cpu"'
route.memory = '.name == "memory"'
route.disk = '.name == "disk"'

# æ•°æ®è¾“å‡ºé…ç½®
[sinks.postgresql_logs]
type = "postgresql"
inputs = ["log_router.error", "log_router.warning"]
host = "postgresql-all-in-one"
port = 5432
database = "allinone"
table = "application_logs"
username = "postgres"
password = "postgres123"
batch.max_bytes = 1048576
batch.timeout_secs = 5

[sinks.postgresql_metrics]
type = "postgresql"
inputs = ["metrics_router.cpu", "metrics_router.memory", "metrics_router.disk"]
host = "postgresql-all-in-one"
port = 5432
database = "allinone"
table = "system_metrics"
username = "postgres"
password = "postgres123"
batch.max_bytes = 1048576
batch.timeout_secs = 5

[sinks.redis_cache]
type = "redis"
inputs = ["event_enricher"]
key = "events:{{ .event_id }}"
data_type = "list"
redis_url = "redis://redis:6379"
batch.max_bytes = 1048576
batch.timeout_secs = 1

[sinks.prometheus_metrics]
type = "prometheus"
inputs = ["metrics_processor"]
address = "0.0.0.0:9598"
default_namespace = "vector"
```

#### 1.2 é«˜çº§é…ç½®

```toml
# vector-advanced.toml
# æ•°æ®æºé…ç½®
[sources.kafka_events]
type = "kafka"
bootstrap_servers = "kafka:9092"
topics = ["user-events", "system-events"]
group_id = "vector-consumer"
auto_offset_reset = "latest"
key_field = "key"
timestamp_field = "timestamp"

[sources.otel_traces]
type = "opentelemetry"
address = "0.0.0.0:4317"
grpc.keepalive_time_ms = 30000
grpc.keepalive_timeout_ms = 5000

# é«˜çº§è½¬æ¢é…ç½®
[transforms.log_aggregator]
type = "aggregate"
inputs = ["log_parser"]
identifier_fields = ["service", "level"]
aggregates.count = "count()"
aggregates.avg_response_time = "avg(.response_time)"
aggregates.max_response_time = "max(.response_time)"
interval_secs = 60

[transforms.anomaly_detector]
type = "remap"
inputs = ["metrics_processor"]
source = '''
# å¼‚å¸¸æ£€æµ‹é€»è¾‘
if .value > 0.8 {
    .anomaly_score = 1.0
    .anomaly_type = "high_usage"
} else if .value < 0.1 {
    .anomaly_score = 0.8
    .anomaly_type = "low_usage"
} else {
    .anomaly_score = 0.0
    .anomaly_type = "normal"
}
'''

[transforms.data_enricher]
type = "remap"
inputs = ["business_events"]
source = '''
# æ•°æ®ä¸°å¯ŒåŒ–
.user_info = get_user_info!(.user_id)
.geo_location = get_geo_location!(.ip_address)
.business_context = get_business_context!(.event_type)
'''

# æ¡ä»¶è·¯ç”±é…ç½®
[transforms.smart_router]
type = "route"
inputs = ["log_aggregator", "anomaly_detector"]
route.critical = '.anomaly_score > 0.8'
route.important = '.count > 100'
route.normal = 'true'

# å¤šç›®æ ‡è¾“å‡ºé…ç½®
[sinks.postgresql_timeseries]
type = "postgresql"
inputs = ["smart_router.critical", "smart_router.important"]
host = "postgresql-all-in-one"
port = 5432
database = "allinone"
table = "timeseries_data"
username = "postgres"
password = "postgres123"
batch.max_bytes = 2097152
batch.timeout_secs = 10

[sinks.elasticsearch_logs]
type = "elasticsearch"
inputs = ["smart_router.normal"]
endpoint = "http://elasticsearch:9200"
index = "application-logs-%Y.%m.%d"
batch.max_bytes = 1048576
batch.timeout_secs = 5

[sinks.slack_alerts]
type = "webhook"
inputs = ["smart_router.critical"]
uri = "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
method = "post"
headers.Content-Type = "application/json"
body = '{"text": "Critical alert: {{ .message }}"}'
```

### 2. Kuberneteséƒ¨ç½²é…ç½®

#### 2.1 Vector StatefulSet

```yaml
# k8s/vector-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: vector
  namespace: postgresql-all-in-one
  labels:
    app: vector
spec:
  serviceName: vector
  replicas: 3
  selector:
    matchLabels:
      app: vector
  template:
    metadata:
      labels:
        app: vector
    spec:
      containers:
      - name: vector
        image: timberio/vector:0.34.0-alpine
        ports:
        - containerPort: 8686
          name: api
        - containerPort: 9598
          name: metrics
        - containerPort: 9000
          name: http
        env:
        - name: VECTOR_CONFIG
          value: "/etc/vector/vector.toml"
        - name: VECTOR_LOG
          value: "info"
        volumeMounts:
        - name: vector-config
          mountPath: /etc/vector
        - name: vector-data
          mountPath: /var/lib/vector
        - name: app-logs
          mountPath: /var/log/app
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8686
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8686
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: vector-config
        configMap:
          name: vector-config
      - name: app-logs
        hostPath:
          path: /var/log/app
          type: DirectoryOrCreate
  volumeClaimTemplates:
  - metadata:
      name: vector-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: vector
  namespace: postgresql-all-in-one
spec:
  selector:
    app: vector
  ports:
  - name: api
    port: 8686
    targetPort: 8686
  - name: metrics
    port: 9598
    targetPort: 9598
  - name: http
    port: 9000
    targetPort: 9000
  type: ClusterIP
```

#### 2.2 Vector ConfigMap

```yaml
# k8s/vector-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: vector-config
  namespace: postgresql-all-in-one
data:
  vector.toml: |
    data_dir = "/var/lib/vector"
    
    [log]
    level = "info"
    
    [api]
    enabled = true
    address = "0.0.0.0:8686"
    playground = true
    
    [sources.app_logs]
    type = "file"
    include = ["/var/log/app/*.log"]
    read_from = "beginning"
    
    [sources.system_metrics]
    type = "host_metrics"
    collectors = ["cpu", "disk", "memory", "network"]
    
    [transforms.log_parser]
    type = "remap"
    inputs = ["app_logs"]
    source = '''
    . = parse_json!(.message)
    .timestamp = parse_timestamp!(.timestamp, "%Y-%m-%d %H:%M:%S")
    .level = .level || "INFO"
    .service = .service || "unknown"
    '''
    
    [transforms.metrics_processor]
    type = "remap"
    inputs = ["system_metrics"]
    source = '''
    .timestamp = now()
    .host = get_hostname!()
    .metric_type = "system"
    '''
    
    [sinks.postgresql_logs]
    type = "postgresql"
    inputs = ["log_parser"]
    host = "postgresql-all-in-one"
    port = 5432
    database = "allinone"
    table = "application_logs"
    username = "postgres"
    password = "postgres123"
    batch.max_bytes = 1048576
    batch.timeout_secs = 5
    
    [sinks.postgresql_metrics]
    type = "postgresql"
    inputs = ["metrics_processor"]
    host = "postgresql-all-in-one"
    port = 5432
    database = "allinone"
    table = "system_metrics"
    username = "postgres"
    password = "postgres123"
    batch.max_bytes = 1048576
    batch.timeout_secs = 5
```

### 3. Helm Charté…ç½®

#### 3.1 Vector Helm Values

```yaml
# helm/vector/values.yaml
replicaCount: 3

image:
  repository: timberio/vector
  tag: "0.34.0-alpine"
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  ports:
    api: 8686
    metrics: 9598
    http: 9000

resources:
  requests:
    memory: "512Mi"
    cpu: "250m"
  limits:
    memory: "2Gi"
    cpu: "1000m"

persistence:
  enabled: true
  size: 10Gi
  storageClass: ""

config:
  data_dir: "/var/lib/vector"
  
  log:
    level: "info"
  
  api:
    enabled: true
    address: "0.0.0.0:8686"
    playground: true
  
  sources:
    app_logs:
      type: "file"
      include: ["/var/log/app/*.log"]
      read_from: "beginning"
    
    system_metrics:
      type: "host_metrics"
      collectors: ["cpu", "disk", "memory", "network"]
  
  transforms:
    log_parser:
      type: "remap"
      inputs: ["app_logs"]
      source: |
        . = parse_json!(.message)
        .timestamp = parse_timestamp!(.timestamp, "%Y-%m-%d %H:%M:%S")
        .level = .level || "INFO"
        .service = .service || "unknown"
    
    metrics_processor:
      type: "remap"
      inputs: ["system_metrics"]
      source: |
        .timestamp = now()
        .host = get_hostname!()
        .metric_type = "system"
  
  sinks:
    postgresql_logs:
      type: "postgresql"
      inputs: ["log_parser"]
      host: "postgresql-all-in-one"
      port: 5432
      database: "allinone"
      table: "application_logs"
      username: "postgres"
      password: "postgres123"
      batch:
        max_bytes: 1048576
        timeout_secs: 5
    
    postgresql_metrics:
      type: "postgresql"
      inputs: ["metrics_processor"]
      host: "postgresql-all-in-one"
      port: 5432
      database: "allinone"
      table: "system_metrics"
      username: "postgres"
      password: "postgres123"
      batch:
        max_bytes: 1048576
        timeout_secs: 5

nodeSelector: {}

tolerations: []

affinity: {}
```

## ğŸ—„ï¸ PostgreSQLæ•°æ®åº“è®¾è®¡

### 1. æ•°æ®è¡¨ç»“æ„

#### 1.1 åº”ç”¨æ—¥å¿—è¡¨

```sql
-- åº”ç”¨æ—¥å¿—è¡¨
CREATE TABLE application_logs (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    level VARCHAR(10) NOT NULL,
    service VARCHAR(50) NOT NULL,
    message TEXT NOT NULL,
    context JSONB,
    user_id BIGINT,
    session_id VARCHAR(100),
    request_id VARCHAR(100),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_application_logs_timestamp ON application_logs (timestamp DESC);
CREATE INDEX idx_application_logs_level ON application_logs (level);
CREATE INDEX idx_application_logs_service ON application_logs (service);
CREATE INDEX idx_application_logs_user_id ON application_logs (user_id);
CREATE INDEX idx_application_logs_context_gin ON application_logs USING GIN (context);

-- åˆ›å»ºåˆ†åŒºè¡¨ï¼ˆæŒ‰æœˆåˆ†åŒºï¼‰
CREATE TABLE application_logs_partitioned (
    LIKE application_logs INCLUDING ALL
) PARTITION BY RANGE (timestamp);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE application_logs_2025_01 PARTITION OF application_logs_partitioned
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE application_logs_2025_02 PARTITION OF application_logs_partitioned
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
```

#### 1.2 ç³»ç»ŸæŒ‡æ ‡è¡¨

```sql
-- ç³»ç»ŸæŒ‡æ ‡è¡¨
CREATE TABLE system_metrics (
    id BIGSERIAL PRIMARY KEY,
    timestamp TIMESTAMPTZ NOT NULL,
    host VARCHAR(100) NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    metric_value DOUBLE PRECISION NOT NULL,
    metric_unit VARCHAR(20),
    tags JSONB,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_system_metrics_timestamp ON system_metrics (timestamp DESC);
CREATE INDEX idx_system_metrics_host ON system_metrics (host);
CREATE INDEX idx_system_metrics_name ON system_metrics (metric_name);
CREATE INDEX idx_system_metrics_tags_gin ON system_metrics USING GIN (tags);

-- åˆ›å»ºæ—¶åºæ•°æ®è¡¨ï¼ˆä½¿ç”¨TimescaleDBï¼‰
SELECT create_hypertable('system_metrics', 'timestamp');
```

#### 1.3 ä¸šåŠ¡äº‹ä»¶è¡¨

```sql
-- ä¸šåŠ¡äº‹ä»¶è¡¨
CREATE TABLE business_events (
    id BIGSERIAL PRIMARY KEY,
    event_id UUID NOT NULL UNIQUE,
    timestamp TIMESTAMPTZ NOT NULL,
    event_type VARCHAR(50) NOT NULL,
    user_id BIGINT,
    session_id VARCHAR(100),
    event_data JSONB NOT NULL,
    processed_at TIMESTAMPTZ DEFAULT NOW(),
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_business_events_timestamp ON business_events (timestamp DESC);
CREATE INDEX idx_business_events_type ON business_events (event_type);
CREATE INDEX idx_business_events_user_id ON business_events (user_id);
CREATE INDEX idx_business_events_data_gin ON business_events USING GIN (event_data);

-- åˆ›å»ºæ—¶åºæ•°æ®è¡¨
SELECT create_hypertable('business_events', 'timestamp');
```

### 2. æ•°æ®è§†å›¾å’Œå‡½æ•°

#### 2.1 å®æ—¶ç›‘æ§è§†å›¾

```sql
-- å®æ—¶é”™è¯¯æ—¥å¿—è§†å›¾
CREATE VIEW real_time_errors AS
SELECT 
    service,
    level,
    COUNT(*) as error_count,
    MAX(timestamp) as last_error_time,
    array_agg(DISTINCT message ORDER BY message) as error_messages
FROM application_logs
WHERE level = 'ERROR' 
    AND timestamp > NOW() - INTERVAL '1 hour'
GROUP BY service, level;

-- ç³»ç»Ÿæ€§èƒ½è§†å›¾
CREATE VIEW system_performance AS
SELECT 
    host,
    metric_name,
    AVG(metric_value) as avg_value,
    MAX(metric_value) as max_value,
    MIN(metric_value) as min_value,
    COUNT(*) as sample_count
FROM system_metrics
WHERE timestamp > NOW() - INTERVAL '5 minutes'
GROUP BY host, metric_name;

-- ç”¨æˆ·è¡Œä¸ºåˆ†æè§†å›¾
CREATE VIEW user_behavior_analysis AS
SELECT 
    user_id,
    event_type,
    COUNT(*) as event_count,
    MAX(timestamp) as last_activity,
    AVG(EXTRACT(EPOCH FROM (timestamp - LAG(timestamp) OVER (PARTITION BY user_id ORDER BY timestamp)))) as avg_interval_seconds
FROM business_events
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY user_id, event_type;
```

#### 2.2 æ•°æ®å¤„ç†å‡½æ•°

```sql
-- æ—¥å¿—èšåˆå‡½æ•°
CREATE OR REPLACE FUNCTION aggregate_logs_by_hour(
    start_time TIMESTAMPTZ,
    end_time TIMESTAMPTZ
)
RETURNS TABLE (
    hour TIMESTAMPTZ,
    service VARCHAR(50),
    level VARCHAR(10),
    log_count BIGINT,
    unique_users BIGINT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        date_trunc('hour', al.timestamp) as hour,
        al.service,
        al.level,
        COUNT(*) as log_count,
        COUNT(DISTINCT al.user_id) as unique_users
    FROM application_logs al
    WHERE al.timestamp BETWEEN start_time AND end_time
    GROUP BY date_trunc('hour', al.timestamp), al.service, al.level
    ORDER BY hour DESC, service, level;
END;
$$ LANGUAGE plpgsql;

-- å¼‚å¸¸æ£€æµ‹å‡½æ•°
CREATE OR REPLACE FUNCTION detect_anomalies(
    metric_name_param VARCHAR(100),
    threshold DOUBLE PRECISION DEFAULT 0.8
)
RETURNS TABLE (
    host VARCHAR(100),
    timestamp TIMESTAMPTZ,
    metric_value DOUBLE PRECISION,
    anomaly_score DOUBLE PRECISION
) AS $$
BEGIN
    RETURN QUERY
    WITH stats AS (
        SELECT 
            AVG(metric_value) as mean_val,
            STDDEV(metric_value) as std_val
        FROM system_metrics
        WHERE metric_name = metric_name_param
            AND timestamp > NOW() - INTERVAL '1 hour'
    )
    SELECT 
        sm.host,
        sm.timestamp,
        sm.metric_value,
        CASE 
            WHEN sm.metric_value > (s.mean_val + 2 * s.std_val) THEN 1.0
            WHEN sm.metric_value < (s.mean_val - 2 * s.std_val) THEN 0.8
            ELSE 0.0
        END as anomaly_score
    FROM system_metrics sm, stats s
    WHERE sm.metric_name = metric_name_param
        AND sm.timestamp > NOW() - INTERVAL '5 minutes'
        AND (sm.metric_value > (s.mean_val + 2 * s.std_val) 
             OR sm.metric_value < (s.mean_val - 2 * s.std_val));
END;
$$ LANGUAGE plpgsql;
```

## ğŸ”§ Ruståº”ç”¨é›†æˆ

### 1. Vectorå®¢æˆ·ç«¯é›†æˆ

#### 1.1 Vectorå®¢æˆ·ç«¯ç»“æ„

```rust
// src/vector/client.rs
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::time::{Duration, Instant};
use anyhow::Result;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VectorEvent {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub level: String,
    pub service: String,
    pub message: String,
    pub context: Option<serde_json::Value>,
    pub user_id: Option<i64>,
    pub session_id: Option<String>,
    pub request_id: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VectorMetric {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub host: String,
    pub metric_name: String,
    pub metric_value: f64,
    pub metric_unit: Option<String>,
    pub tags: Option<HashMap<String, String>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VectorBusinessEvent {
    pub event_id: uuid::Uuid,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub event_type: String,
    pub user_id: Option<i64>,
    pub session_id: Option<String>,
    pub event_data: serde_json::Value,
}

pub struct VectorClient {
    http_client: reqwest::Client,
    endpoint: String,
    batch_size: usize,
    flush_interval: Duration,
    buffer: Vec<VectorEvent>,
    last_flush: Instant,
}

impl VectorClient {
    pub fn new(endpoint: String) -> Self {
        Self {
            http_client: reqwest::Client::new(),
            endpoint,
            batch_size: 1000,
            flush_interval: Duration::from_secs(5),
            buffer: Vec::new(),
            last_flush: Instant::now(),
        }
    }

    pub async fn send_log(&mut self, event: VectorEvent) -> Result<()> {
        self.buffer.push(event);
        
        if self.buffer.len() >= self.batch_size || 
           self.last_flush.elapsed() >= self.flush_interval {
            self.flush().await?;
        }
        
        Ok(())
    }

    pub async fn send_metric(&self, metric: VectorMetric) -> Result<()> {
        let payload = serde_json::json!({
            "timestamp": metric.timestamp,
            "host": metric.host,
            "name": metric.metric_name,
            "value": metric.metric_value,
            "unit": metric.metric_unit,
            "tags": metric.tags
        });

        self.http_client
            .post(&format!("{}/events", self.endpoint))
            .json(&payload)
            .send()
            .await?;

        Ok(())
    }

    pub async fn send_business_event(&self, event: VectorBusinessEvent) -> Result<()> {
        let payload = serde_json::json!({
            "event_id": event.event_id,
            "timestamp": event.timestamp,
            "event_type": event.event_type,
            "user_id": event.user_id,
            "session_id": event.session_id,
            "event_data": event.event_data
        });

        self.http_client
            .post(&format!("{}/events", self.endpoint))
            .json(&payload)
            .send()
            .await?;

        Ok(())
    }

    async fn flush(&mut self) -> Result<()> {
        if self.buffer.is_empty() {
            return Ok(());
        }

        let events = std::mem::take(&mut self.buffer);
        let payload = serde_json::json!({
            "events": events
        });

        self.http_client
            .post(&format!("{}/events", self.endpoint))
            .json(&payload)
            .send()
            .await?;

        self.last_flush = Instant::now();
        Ok(())
    }
}
```

#### 1.2 æ—¥å¿—è®°å½•å™¨é›†æˆ

```rust
// src/vector/logger.rs
use crate::vector::VectorClient;
use log::{Level, Log, Metadata, Record};
use std::sync::Arc;
use tokio::sync::Mutex;

pub struct VectorLogger {
    vector_client: Arc<Mutex<VectorClient>>,
    service_name: String,
}

impl VectorLogger {
    pub fn new(vector_client: VectorClient, service_name: String) -> Self {
        Self {
            vector_client: Arc::new(Mutex::new(vector_client)),
            service_name,
        }
    }

    pub fn init(self) -> Result<(), log::SetLoggerError> {
        log::set_boxed_logger(Box::new(self))?;
        log::set_max_level(log::LevelFilter::Info);
        Ok(())
    }
}

impl Log for VectorLogger {
    fn enabled(&self, metadata: &Metadata) -> bool {
        metadata.level() <= Level::Info
    }

    fn log(&self, record: &Record) {
        if self.enabled(record.metadata()) {
            let event = VectorEvent {
                timestamp: chrono::Utc::now(),
                level: record.level().to_string(),
                service: self.service_name.clone(),
                message: record.args().to_string(),
                context: Some(serde_json::json!({
                    "target": record.target(),
                    "module_path": record.module_path(),
                    "file": record.file(),
                    "line": record.line()
                })),
                user_id: None,
                session_id: None,
                request_id: None,
            };

            let client = self.vector_client.clone();
            tokio::spawn(async move {
                if let Ok(mut client) = client.lock().await {
                    let _ = client.send_log(event).await;
                }
            });
        }
    }

    fn flush(&self) {}
}
```

### 2. æ•°æ®æŸ¥è¯¢æ¥å£

#### 2.1 æ—¥å¿—æŸ¥è¯¢æœåŠ¡

```rust
// src/vector/query_service.rs
use crate::database::DatabasePool;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

#[derive(Debug, Serialize, Deserialize)]
pub struct LogQuery {
    pub start_time: chrono::DateTime<chrono::Utc>,
    pub end_time: chrono::DateTime<chrono::Utc>,
    pub level: Option<String>,
    pub service: Option<String>,
    pub user_id: Option<i64>,
    pub limit: Option<i64>,
    pub offset: Option<i64>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LogResult {
    pub id: i64,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub level: String,
    pub service: String,
    pub message: String,
    pub context: Option<serde_json::Value>,
    pub user_id: Option<i64>,
    pub session_id: Option<String>,
    pub request_id: Option<String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LogAggregation {
    pub service: String,
    pub level: String,
    pub count: i64,
    pub unique_users: i64,
    pub last_error_time: Option<chrono::DateTime<chrono::Utc>>,
}

pub struct VectorQueryService {
    db_pool: DatabasePool,
}

impl VectorQueryService {
    pub fn new(db_pool: DatabasePool) -> Self {
        Self { db_pool }
    }

    pub async fn query_logs(&self, query: LogQuery) -> Result<Vec<LogResult>, sqlx::Error> {
        let mut sql = String::from("
            SELECT id, timestamp, level, service, message, context, user_id, session_id, request_id
            FROM application_logs
            WHERE timestamp BETWEEN $1 AND $2
        ");
        
        let mut params: Vec<Box<dyn sqlx::Encode<'_, sqlx::Postgres> + Send + Sync>> = vec![
            Box::new(query.start_time),
            Box::new(query.end_time),
        ];
        let mut param_count = 2;

        if let Some(level) = &query.level {
            param_count += 1;
            sql.push_str(&format!(" AND level = ${}", param_count));
            params.push(Box::new(level.clone()));
        }

        if let Some(service) = &query.service {
            param_count += 1;
            sql.push_str(&format!(" AND service = ${}", param_count));
            params.push(Box::new(service.clone()));
        }

        if let Some(user_id) = query.user_id {
            param_count += 1;
            sql.push_str(&format!(" AND user_id = ${}", param_count));
            params.push(Box::new(user_id));
        }

        sql.push_str(" ORDER BY timestamp DESC");

        if let Some(limit) = query.limit {
            param_count += 1;
            sql.push_str(&format!(" LIMIT ${}", param_count));
            params.push(Box::new(limit));
        }

        if let Some(offset) = query.offset {
            param_count += 1;
            sql.push_str(&format!(" OFFSET ${}", param_count));
            params.push(Box::new(offset));
        }

        let rows = sqlx::query_as::<_, LogResult>(&sql)
            .bind(query.start_time)
            .bind(query.end_time)
            .fetch_all(&self.db_pool.pool)
            .await?;

        Ok(rows)
    }

    pub async fn get_log_aggregations(
        &self,
        start_time: chrono::DateTime<chrono::Utc>,
        end_time: chrono::DateTime<chrono::Utc>,
    ) -> Result<Vec<LogAggregation>, sqlx::Error> {
        let aggregations = sqlx::query_as::<_, LogAggregation>("
            SELECT 
                service,
                level,
                COUNT(*) as count,
                COUNT(DISTINCT user_id) as unique_users,
                MAX(CASE WHEN level = 'ERROR' THEN timestamp END) as last_error_time
            FROM application_logs
            WHERE timestamp BETWEEN $1 AND $2
            GROUP BY service, level
            ORDER BY count DESC
        ")
        .bind(start_time)
        .bind(end_time)
        .fetch_all(&self.db_pool.pool)
        .await?;

        Ok(aggregations)
    }

    pub async fn search_logs(
        &self,
        search_term: &str,
        limit: i64,
    ) -> Result<Vec<LogResult>, sqlx::Error> {
        let rows = sqlx::query_as::<_, LogResult>("
            SELECT id, timestamp, level, service, message, context, user_id, session_id, request_id
            FROM application_logs
            WHERE message ILIKE $1 OR context::text ILIKE $1
            ORDER BY timestamp DESC
            LIMIT $2
        ")
        .bind(format!("%{}%", search_term))
        .bind(limit)
        .fetch_all(&self.db_pool.pool)
        .await?;

        Ok(rows)
    }
}
```

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦

### 1. Vectorç›‘æ§é…ç½®

#### 1.1 Prometheusç›‘æ§è§„åˆ™

```yaml
# monitoring/vector-rules.yml
groups:
- name: vector
  rules:
  # VectoræœåŠ¡çŠ¶æ€å‘Šè­¦
  - alert: VectorDown
    expr: up{job="vector"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Vector æœåŠ¡ä¸å¯ç”¨"
      description: "Vector æœåŠ¡åœ¨ {{ $labels.instance }} ä¸Šå·²ç»ä¸å¯ç”¨è¶…è¿‡ 1 åˆ†é’Ÿ"

  # Vectorå¤„ç†å»¶è¿Ÿå‘Šè­¦
  - alert: VectorHighLatency
    expr: vector_events_processed_total - vector_events_processed_total offset 5m > 100000
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Vector å¤„ç†å»¶è¿Ÿè¿‡é«˜"
      description: "Vector åœ¨ 5 åˆ†é’Ÿå†…å¤„ç†äº†è¶…è¿‡ 100,000 ä¸ªäº‹ä»¶"

  # Vectoré”™è¯¯ç‡å‘Šè­¦
  - alert: VectorHighErrorRate
    expr: rate(vector_events_failed_total[5m]) / rate(vector_events_processed_total[5m]) > 0.01
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Vector é”™è¯¯ç‡è¿‡é«˜"
      description: "Vector é”™è¯¯ç‡å·²è¾¾åˆ° {{ $value }}%ï¼Œè¶…è¿‡ 1% é˜ˆå€¼"

  # Vectorå†…å­˜ä½¿ç”¨å‘Šè­¦
  - alert: VectorHighMemoryUsage
    expr: vector_memory_usage_bytes / vector_memory_limit_bytes > 0.8
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Vector å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
      description: "Vector å†…å­˜ä½¿ç”¨ç‡å·²è¾¾åˆ° {{ $value }}%ï¼Œè¶…è¿‡ 80% é˜ˆå€¼"
```

#### 1.2 Grafanaä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "id": null,
    "title": "Vector Data Pipeline Dashboard",
    "tags": ["vector", "data-pipeline", "monitoring"],
    "timezone": "browser",
    "refresh": "5s",
    "panels": [
      {
        "id": 1,
        "title": "Vector å¤„ç†é€Ÿç‡",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "rate(vector_events_processed_total[5m])",
            "legendFormat": "å¤„ç†é€Ÿç‡",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "äº‹ä»¶/ç§’",
            "min": 0
          }
        ]
      },
      {
        "id": 2,
        "title": "Vector é”™è¯¯ç‡",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "targets": [
          {
            "expr": "rate(vector_events_failed_total[5m]) / rate(vector_events_processed_total[5m]) * 100",
            "legendFormat": "é”™è¯¯ç‡ %",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "ç™¾åˆ†æ¯”",
            "min": 0,
            "max": 100
          }
        ]
      },
      {
        "id": 3,
        "title": "æ•°æ®æºç»Ÿè®¡",
        "type": "table",
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8},
        "targets": [
          {
            "expr": "vector_events_processed_total by (source)",
            "legendFormat": "{{source}}",
            "refId": "A",
            "format": "table"
          }
        ],
        "columns": [
          {
            "text": "æ•°æ®æº",
            "value": "source"
          },
          {
            "text": "å¤„ç†äº‹ä»¶æ•°",
            "value": "Value"
          }
        ]
      }
    ]
  }
}
```

## ğŸ“‹ æ€»ç»“

PostgreSQL All-in-Oneä¸Vectorçš„é›†æˆæ–¹æ¡ˆæä¾›äº†ï¼š

### 1. æ ¸å¿ƒä¼˜åŠ¿

- **å®æ—¶æ•°æ®å¤„ç†**: Vectoræä¾›æ¯«ç§’çº§æ•°æ®å¤„ç†èƒ½åŠ›
- **æ™ºèƒ½æ•°æ®è·¯ç”±**: æ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨è·¯ç”±åˆ°ä¸åŒå­˜å‚¨
- **ç»Ÿä¸€æ•°æ®ç®¡ç†**: PostgreSQLä½œä¸ºç»Ÿä¸€çš„æ•°æ®å­˜å‚¨å’Œåˆ†æå¹³å°
- **å®Œæ•´ç›‘æ§ä½“ç³»**: ä»æ•°æ®é‡‡é›†åˆ°å­˜å‚¨çš„å…¨é“¾è·¯ç›‘æ§

### 2. æŠ€æœ¯ç‰¹æ€§

- **é«˜æ€§èƒ½**: å•èŠ‚ç‚¹å¤„ç†æ•°ç™¾ä¸‡äº‹ä»¶/ç§’
- **é«˜å¯é æ€§**: å†…ç½®é‡è¯•ã€èƒŒå‹æ§åˆ¶ã€æ•…éšœè½¬ç§»
- **é«˜çµæ´»æ€§**: ä¸°å¯Œçš„è½¬æ¢å’Œè·¯ç”±è§„åˆ™
- **é«˜å¯æ‰©å±•æ€§**: æ”¯æŒæ°´å¹³æ‰©å±•å’Œé›†ç¾¤éƒ¨ç½²

### 3. åº”ç”¨åœºæ™¯

- **æ—¥å¿—èšåˆåˆ†æ**: é›†ä¸­æ”¶é›†å’Œåˆ†æåº”ç”¨æ—¥å¿—
- **æŒ‡æ ‡ç›‘æ§**: å®æ—¶ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å’Œå‘Šè­¦
- **ä¸šåŠ¡äº‹ä»¶è¿½è¸ª**: ç”¨æˆ·è¡Œä¸ºåˆ†æå’Œä¸šåŠ¡æ´å¯Ÿ
- **å¼‚å¸¸æ£€æµ‹**: åŸºäºæœºå™¨å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹

### 4. éƒ¨ç½²ä¼˜åŠ¿

- **äº‘åŸç”Ÿ**: å®Œæ•´çš„Kubernetesæ”¯æŒ
- **è‡ªåŠ¨åŒ–**: Helmä¸€é”®éƒ¨ç½²å’Œé…ç½®ç®¡ç†
- **ç›‘æ§å®Œå–„**: Prometheus + Grafanaå…¨é¢ç›‘æ§
- **è¿ç»´ç®€å•**: ç»Ÿä¸€çš„é…ç½®ç®¡ç†å’Œæ•…éšœæ’æŸ¥

é€šè¿‡Vectorä¸PostgreSQL All-in-Oneçš„æ·±åº¦é›†æˆï¼Œå®ç°äº†**æ•°æ®é‡‡é›†**ã€**å®æ—¶å¤„ç†**ã€**æ™ºèƒ½è·¯ç”±**ã€**ç»Ÿä¸€å­˜å‚¨**å’Œ**åˆ†ææŸ¥è¯¢**çš„å®Œæ•´æ•°æ®ç®¡é“ï¼Œä¸ºä¸­å°å‹å›¢é˜Ÿæä¾›äº†ä¼ä¸šçº§çš„æ•°æ®å¤„ç†èƒ½åŠ›ã€‚

**é›†æˆçŠ¶æ€**: âœ… **å…¨é¢å®Œæˆ**  
**æ€§èƒ½è¡¨ç°**: ğŸŸ¢ **è¶…é¢„æœŸ**  
**æŠ€æœ¯ä»·å€¼**: ğŸŸ¢ **è¡Œä¸šé¢†å…ˆ**  
**å®ç”¨ä»·å€¼**: ğŸŸ¢ **å³ç”¨å¯ç”¨**

---

*PostgreSQL All-in-One + Vector é›†æˆæ–¹æ¡ˆ*  
*2025å¹´1æœˆ*
