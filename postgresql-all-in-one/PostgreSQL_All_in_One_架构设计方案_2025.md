# PostgreSQL All-in-One æ¶æ„è®¾è®¡æ–¹æ¡ˆ - 2025å¹´

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æäº†PostgreSQL All-in-Oneæ¶æ„çš„è®¾è®¡ç†å¿µã€æŠ€æœ¯å®ç°å’Œæ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆã€‚ç»“åˆRust 1.90æ–°ç‰¹æ€§å’Œæœ€æ–°å¼€æºè½¯ä»¶å †æ ˆï¼Œä¸ºä¸­å°å‹å›¢é˜Ÿæä¾›ä¸€å¥—å®Œæ•´çš„æ•°æ®å¤„ç†è§£å†³æ–¹æ¡ˆï¼Œå®ç°OLTPã€OLAPå’Œå…¨æ–‡æ£€ç´¢çš„ç»Ÿä¸€æ¶æ„ã€‚

## ğŸ¯ æ¶æ„è®¾è®¡ç›®æ ‡

### æ ¸å¿ƒç›®æ ‡

1. **ç®€åŒ–æ¶æ„**: å•ä¸€æ•°æ®åº“å®ä¾‹å¤„ç†æ‰€æœ‰æ•°æ®éœ€æ±‚
2. **é™ä½æˆæœ¬**: ç›¸æ¯”åˆ†å¸ƒå¼æ¶æ„èŠ‚çœ60-80%æˆæœ¬
3. **é™ä½å¤æ‚åº¦**: å‡å°‘è¿ç»´ç»„ä»¶ï¼Œæå‡å¼€å‘æ•ˆç‡
4. **ä¿è¯æ€§èƒ½**: æ»¡è¶³ç§’çº§æŸ¥è¯¢å“åº”éœ€æ±‚
5. **ç¡®ä¿å¯é æ€§**: 99.9%ç³»ç»Ÿå¯ç”¨æ€§

### é€‚ç”¨åœºæ™¯

- å›¢é˜Ÿè§„æ¨¡: 5-50äºº
- æ•°æ®è§„æ¨¡: < 10TB
- æŸ¥è¯¢å»¶è¿Ÿ: å¯æ¥å—ç§’çº§
- å¹¶å‘ç”¨æˆ·: < 10,000 QPS
- é¢„ç®—é™åˆ¶: è¿½æ±‚æˆæœ¬æ•ˆç›Š

## ğŸ—ï¸ æ•´ä½“æ¶æ„è®¾è®¡

### 1. æ¶æ„åˆ†å±‚

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åº”ç”¨å±‚ (Application Layer)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Web App   â”‚ â”‚  Mobile App â”‚ â”‚   APIæœåŠ¡   â”‚ â”‚ æŠ¥è¡¨ç³»ç»Ÿ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    æœåŠ¡å±‚ (Service Layer)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ä¸šåŠ¡é€»è¾‘    â”‚ â”‚  æ•°æ®å¤„ç†   â”‚ â”‚  ç¼“å­˜æœåŠ¡    â”‚ â”‚ ç›‘æ§å‘Šè­¦ â”‚ â”‚
â”‚  â”‚ Business    â”‚ â”‚ Processing  â”‚ â”‚   Cache     â”‚ â”‚Monitoringâ”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  PostgreSQL All-in-One                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   OLTP      â”‚ â”‚   OLAP      â”‚ â”‚  å…¨æ–‡æ£€ç´¢    â”‚ â”‚ æ—¶åºæ•°æ® â”‚ â”‚
â”‚  â”‚ äº‹åŠ¡å¤„ç†     â”‚ â”‚ åˆ†æå¤„ç†    â”‚ â”‚ Full Text   â”‚ â”‚TimeSeriesâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    å­˜å‚¨å±‚ (Storage Layer)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ä¸»å­˜å‚¨      â”‚ â”‚  å¤‡ä»½å­˜å‚¨   â”‚ â”‚  å½’æ¡£å­˜å‚¨    â”‚ â”‚ æ—¥å¿—å­˜å‚¨ â”‚ â”‚
â”‚  â”‚   SSD       â”‚ â”‚   Backup    â”‚ â”‚  Archive    â”‚ â”‚  Logs   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. æ ¸å¿ƒç»„ä»¶è®¾è®¡

#### 2.1 PostgreSQL æ ¸å¿ƒé…ç½®

```sql
-- PostgreSQL æ ¸å¿ƒé…ç½®ä¼˜åŒ–
-- postgresql.conf

# å†…å­˜é…ç½®
shared_buffers = 8GB                    -- 25% of RAM
effective_cache_size = 24GB             -- 75% of RAM
work_mem = 256MB                        -- å·¥ä½œå†…å­˜
maintenance_work_mem = 2GB              -- ç»´æŠ¤å†…å­˜

# è¿æ¥é…ç½®
max_connections = 200                   -- æœ€å¤§è¿æ¥æ•°
shared_preload_libraries = 'timescaledb,pg_stat_statements'

# æŸ¥è¯¢ä¼˜åŒ–
random_page_cost = 1.1                  -- SSDä¼˜åŒ–
effective_io_concurrency = 200          -- å¹¶å‘I/O
max_parallel_workers_per_gather = 4     -- å¹¶è¡ŒæŸ¥è¯¢
max_parallel_workers = 8                -- æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹

# æ—¥å¿—é…ç½®
log_statement = 'all'                   -- è®°å½•æ‰€æœ‰SQL
log_min_duration_statement = 1000       -- è®°å½•æ…¢æŸ¥è¯¢
log_checkpoints = on                    -- è®°å½•æ£€æŸ¥ç‚¹
log_connections = on                    -- è®°å½•è¿æ¥
log_disconnections = on                 -- è®°å½•æ–­å¼€è¿æ¥

# WALé…ç½®
wal_level = replica                     -- WALçº§åˆ«
max_wal_size = 4GB                      -- æœ€å¤§WALå¤§å°
min_wal_size = 1GB                      -- æœ€å°WALå¤§å°
checkpoint_completion_target = 0.9      -- æ£€æŸ¥ç‚¹å®Œæˆç›®æ ‡
```

#### 2.2 æ•°æ®æ¨¡å‹è®¾è®¡

```sql
-- æ ¸å¿ƒä¸šåŠ¡è¡¨è®¾è®¡
CREATE TABLE users (
    id BIGSERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    profile JSONB,                      -- ç”¨æˆ·é…ç½®æ–‡ä»¶
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- åˆ†åŒºè¡¨è®¾è®¡ï¼ˆæŒ‰æ—¶é—´åˆ†åŒºï¼‰
CREATE TABLE events (
    id BIGSERIAL,
    user_id BIGINT REFERENCES users(id),
    event_type VARCHAR(50) NOT NULL,
    event_data JSONB,                   -- äº‹ä»¶æ•°æ®
    timestamp TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW()
) PARTITION BY RANGE (timestamp);

-- åˆ›å»ºåˆ†åŒº
CREATE TABLE events_2025_01 PARTITION OF events
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
CREATE TABLE events_2025_02 PARTITION OF events
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');

-- æ—¶åºæ•°æ®è¡¨ï¼ˆä½¿ç”¨TimescaleDBï¼‰
CREATE TABLE metrics (
    time TIMESTAMPTZ NOT NULL,
    device_id VARCHAR(50) NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    value DOUBLE PRECISION NOT NULL,
    tags JSONB
);

-- è½¬æ¢ä¸ºæ—¶åºè¡¨
SELECT create_hypertable('metrics', 'time');

-- ç´¢å¼•è®¾è®¡
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_profile_gin ON users USING GIN(profile);
CREATE INDEX idx_events_user_id ON events(user_id);
CREATE INDEX idx_events_timestamp ON events(timestamp);
CREATE INDEX idx_events_event_data_gin ON events USING GIN(event_data);
CREATE INDEX idx_metrics_device_time ON metrics(device_id, time DESC);
CREATE INDEX idx_metrics_name_time ON metrics(metric_name, time DESC);
CREATE INDEX idx_metrics_tags_gin ON metrics USING GIN(tags);
```

#### 2.3 å…¨æ–‡æ£€ç´¢é…ç½®

```sql
-- å…¨æ–‡æ£€ç´¢é…ç½®
-- åˆ›å»ºå…¨æ–‡æœç´¢é…ç½®
CREATE TEXT SEARCH CONFIGURATION chinese_zh (COPY = simple);

-- åˆ›å»ºä¸­æ–‡åˆ†è¯æ‰©å±•ï¼ˆéœ€è¦å®‰è£…zhparserï¼‰
-- CREATE EXTENSION zhparser;
-- CREATE TEXT SEARCH CONFIGURATION chinese_zh (COPY = simple);
-- ALTER TEXT SEARCH CONFIGURATION chinese_zh ALTER MAPPING FOR hword, hword_part, word WITH zhparser;

-- åˆ›å»ºå…¨æ–‡æœç´¢è¡¨
CREATE TABLE documents (
    id BIGSERIAL PRIMARY KEY,
    title VARCHAR(200) NOT NULL,
    content TEXT NOT NULL,
    metadata JSONB,
    search_vector tsvector,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- åˆ›å»ºå…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX idx_documents_search ON documents USING GIN(search_vector);
CREATE INDEX idx_documents_metadata_gin ON documents USING GIN(metadata);

-- åˆ›å»ºè§¦å‘å™¨è‡ªåŠ¨æ›´æ–°æœç´¢å‘é‡
CREATE OR REPLACE FUNCTION update_document_search_vector()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector := to_tsvector('chinese_zh', COALESCE(NEW.title, '') || ' ' || COALESCE(NEW.content, ''));
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_update_document_search_vector
    BEFORE INSERT OR UPDATE ON documents
    FOR EACH ROW EXECUTE FUNCTION update_document_search_vector();
```

## ğŸ”§ æŠ€æœ¯å®ç°æ–¹æ¡ˆ

### 1. Rust 1.90 é›†æˆæ–¹æ¡ˆ

#### 1.1 æ•°æ®åº“è¿æ¥æ± 

```rust
// src/database/pool.rs
use sqlx::{PgPool, PgConnection, Row};
use std::time::Duration;
use tokio::time::timeout;

#[derive(Clone)]
pub struct DatabasePool {
    pool: PgPool,
    config: PoolConfig,
}

#[derive(Clone)]
pub struct PoolConfig {
    pub max_connections: u32,
    pub min_connections: u32,
    pub acquire_timeout: Duration,
    pub idle_timeout: Duration,
    pub max_lifetime: Duration,
}

impl DatabasePool {
    pub async fn new(database_url: &str, config: PoolConfig) -> Result<Self, sqlx::Error> {
        let pool = PgPool::builder()
            .max_connections(config.max_connections)
            .min_connections(config.min_connections)
            .acquire_timeout(config.acquire_timeout)
            .idle_timeout(config.idle_timeout)
            .max_lifetime(config.max_lifetime)
            .build(database_url)
            .await?;

        Ok(Self { pool, config })
    }

    // ä½¿ç”¨Rust 1.90çš„å¼‚æ­¥é—­åŒ…ç‰¹æ€§
    pub async fn with_connection<F, Fut, R>(&self, f: F) -> Result<R, sqlx::Error>
    where
        F: FnOnce(PgConnection) -> Fut,
        Fut: std::future::Future<Output = Result<R, sqlx::Error>> + Send + 'static,
        R: Send,
    {
        let mut conn = self.pool.acquire().await?;
        f(conn).await
    }

    // äº‹åŠ¡å¤„ç†
    pub async fn with_transaction<F, Fut, R>(&self, f: F) -> Result<R, sqlx::Error>
    where
        F: FnOnce(sqlx::Transaction<'_, sqlx::Postgres>) -> Fut,
        Fut: std::future::Future<Output = Result<R, sqlx::Error>> + Send + 'static,
        R: Send,
    {
        let mut tx = self.pool.begin().await?;
        let result = f(tx).await?;
        tx.commit().await?;
        Ok(result)
    }
}
```

#### 1.2 é«˜æ€§èƒ½æŸ¥è¯¢å¼•æ“

```rust
// src/query/engine.rs
use sqlx::{PgPool, Row};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::sync::RwLock;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QueryResult {
    pub rows: Vec<HashMap<String, serde_json::Value>>,
    pub execution_time: Duration,
    pub row_count: usize,
}

pub struct QueryEngine {
    pool: PgPool,
    cache: Arc<RwLock<HashMap<String, QueryResult>>>,
    config: QueryConfig,
}

#[derive(Clone)]
pub struct QueryConfig {
    pub cache_ttl: Duration,
    pub max_cache_size: usize,
    pub query_timeout: Duration,
    pub enable_parallel: bool,
}

impl QueryEngine {
    pub fn new(pool: PgPool, config: QueryConfig) -> Self {
        Self {
            pool,
            cache: Arc::new(RwLock::new(HashMap::new())),
            config,
        }
    }

    // æ‰§è¡ŒOLTPæŸ¥è¯¢
    pub async fn execute_oltp_query(&self, sql: &str, params: &[&dyn sqlx::Encode<'_, sqlx::Postgres>]) -> Result<QueryResult, sqlx::Error> {
        let start = std::time::Instant::now();
        
        let rows = sqlx::query(sql)
            .bind_all(params)
            .fetch_all(&self.pool)
            .await?;

        let mut result_rows = Vec::new();
        for row in rows {
            let mut map = HashMap::new();
            for (i, column) in row.columns().iter().enumerate() {
                let value: serde_json::Value = row.try_get(i)?;
                map.insert(column.name().to_string(), value);
            }
            result_rows.push(map);
        }

        Ok(QueryResult {
            rows: result_rows,
            execution_time: start.elapsed(),
            row_count: result_rows.len(),
        })
    }

    // æ‰§è¡ŒOLAPæŸ¥è¯¢ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰
    pub async fn execute_olap_query(&self, sql: &str) -> Result<QueryResult, sqlx::Error> {
        let cache_key = format!("olap:{}", sql);
        
        // æ£€æŸ¥ç¼“å­˜
        if let Ok(cache) = self.cache.read().await {
            if let Some(cached_result) = cache.get(&cache_key) {
                return Ok(cached_result.clone());
            }
        }

        let start = std::time::Instant::now();
        
        // æ‰§è¡ŒæŸ¥è¯¢
        let rows = sqlx::query(sql)
            .fetch_all(&self.pool)
            .await?;

        let mut result_rows = Vec::new();
        for row in rows {
            let mut map = HashMap::new();
            for (i, column) in row.columns().iter().enumerate() {
                let value: serde_json::Value = row.try_get(i)?;
                map.insert(column.name().to_string(), value);
            }
            result_rows.push(map);
        }

        let result = QueryResult {
            rows: result_rows,
            execution_time: start.elapsed(),
            row_count: result_rows.len(),
        };

        // ç¼“å­˜ç»“æœ
        if let Ok(mut cache) = self.cache.write().await {
            if cache.len() < self.config.max_cache_size {
                cache.insert(cache_key, result.clone());
            }
        }

        Ok(result)
    }

    // å…¨æ–‡æœç´¢
    pub async fn full_text_search(&self, query: &str, limit: i64) -> Result<QueryResult, sqlx::Error> {
        let sql = r#"
            SELECT id, title, content, metadata, 
                   ts_rank(search_vector, plainto_tsquery('chinese_zh', $1)) as rank
            FROM documents 
            WHERE search_vector @@ plainto_tsquery('chinese_zh', $1)
            ORDER BY rank DESC
            LIMIT $2
        "#;

        self.execute_oltp_query(sql, &[&query, &limit]).await
    }
}
```

#### 1.3 ç¼“å­˜å±‚å®ç°

```rust
// src/cache/redis.rs
use redis::{Client, Connection, Commands};
use serde::{Deserialize, Serialize};
use std::time::Duration;
use tokio::sync::RwLock;

pub struct RedisCache {
    client: Client,
    connection: Arc<RwLock<Option<Connection>>>,
    config: CacheConfig,
}

#[derive(Clone)]
pub struct CacheConfig {
    pub host: String,
    pub port: u16,
    pub password: Option<String>,
    pub database: u8,
    pub default_ttl: Duration,
    pub max_connections: u32,
}

impl RedisCache {
    pub fn new(config: CacheConfig) -> Result<Self, redis::RedisError> {
        let client = Client::open(format!("redis://{}:{}", config.host, config.port))?;
        
        Ok(Self {
            client,
            connection: Arc::new(RwLock::new(None)),
            config,
        })
    }

    pub async fn get<T>(&self, key: &str) -> Result<Option<T>, redis::RedisError>
    where
        T: for<'de> Deserialize<'de>,
    {
        let mut conn = self.get_connection().await?;
        let value: Option<String> = conn.get(key)?;
        
        match value {
            Some(v) => {
                let deserialized: T = serde_json::from_str(&v)?;
                Ok(Some(deserialized))
            }
            None => Ok(None),
        }
    }

    pub async fn set<T>(&self, key: &str, value: &T, ttl: Option<Duration>) -> Result<(), redis::RedisError>
    where
        T: Serialize,
    {
        let mut conn = self.get_connection().await?;
        let serialized = serde_json::to_string(value)?;
        
        match ttl {
            Some(duration) => {
                conn.set_ex(key, serialized, duration.as_secs() as usize)?;
            }
            None => {
                conn.set(key, serialized)?;
            }
        }
        
        Ok(())
    }

    async fn get_connection(&self) -> Result<Connection, redis::RedisError> {
        let mut conn_guard = self.connection.write().await;
        
        if conn_guard.is_none() {
            let conn = self.client.get_connection()?;
            *conn_guard = Some(conn);
        }
        
        Ok(conn_guard.as_ref().unwrap().clone())
    }
}
```

### 2. ç›‘æ§å’Œå¯è§‚æµ‹æ€§

#### 2.1 æ€§èƒ½ç›‘æ§

```rust
// src/monitoring/metrics.rs
use prometheus::{Counter, Histogram, Gauge, Registry};
use std::time::Instant;

pub struct DatabaseMetrics {
    pub query_counter: Counter,
    pub query_duration: Histogram,
    pub connection_pool_size: Gauge,
    pub active_connections: Gauge,
    pub cache_hits: Counter,
    pub cache_misses: Counter,
}

impl DatabaseMetrics {
    pub fn new(registry: &Registry) -> Result<Self, prometheus::Error> {
        let query_counter = Counter::new(
            "database_queries_total",
            "Total number of database queries"
        )?;
        
        let query_duration = Histogram::new(
            "database_query_duration_seconds",
            "Database query duration in seconds"
        )?;
        
        let connection_pool_size = Gauge::new(
            "database_connection_pool_size",
            "Database connection pool size"
        )?;
        
        let active_connections = Gauge::new(
            "database_active_connections",
            "Number of active database connections"
        )?;
        
        let cache_hits = Counter::new(
            "database_cache_hits_total",
            "Total number of cache hits"
        )?;
        
        let cache_misses = Counter::new(
            "database_cache_misses_total",
            "Total number of cache misses"
        )?;

        registry.register(Box::new(query_counter.clone()))?;
        registry.register(Box::new(query_duration.clone()))?;
        registry.register(Box::new(connection_pool_size.clone()))?;
        registry.register(Box::new(active_connections.clone()))?;
        registry.register(Box::new(cache_hits.clone()))?;
        registry.register(Box::new(cache_misses.clone()))?;

        Ok(Self {
            query_counter,
            query_duration,
            connection_pool_size,
            active_connections,
            cache_hits,
            cache_misses,
        })
    }

    pub fn record_query(&self, duration: Duration) {
        self.query_counter.inc();
        self.query_duration.observe(duration.as_secs_f64());
    }

    pub fn update_connection_pool_size(&self, size: f64) {
        self.connection_pool_size.set(size);
    }

    pub fn update_active_connections(&self, count: f64) {
        self.active_connections.set(count);
    }

    pub fn record_cache_hit(&self) {
        self.cache_hits.inc();
    }

    pub fn record_cache_miss(&self) {
        self.cache_misses.inc();
    }
}
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. æŸ¥è¯¢ä¼˜åŒ–

#### 1.1 ç´¢å¼•ä¼˜åŒ–ç­–ç•¥

```sql
-- å¤åˆç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_events_user_timestamp 
ON events (user_id, timestamp DESC) 
WHERE event_type IN ('click', 'view', 'purchase');

-- éƒ¨åˆ†ç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_events_recent 
ON events (timestamp DESC) 
WHERE timestamp > NOW() - INTERVAL '30 days';

-- è¡¨è¾¾å¼ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_users_email_domain 
ON users ((split_part(email, '@', 2)));

-- JSONBç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_events_data_gin 
ON events USING GIN (event_data);

-- å…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_documents_search_gin 
ON documents USING GIN (search_vector);
```

#### 1.2 æŸ¥è¯¢é‡å†™ä¼˜åŒ–

```sql
-- æŸ¥è¯¢é‡å†™ç¤ºä¾‹
-- åŸå§‹æŸ¥è¯¢
SELECT * FROM events 
WHERE user_id = 123 
AND timestamp > '2025-01-01' 
ORDER BY timestamp DESC 
LIMIT 100;

-- ä¼˜åŒ–åæŸ¥è¯¢ï¼ˆä½¿ç”¨ç´¢å¼•ï¼‰
SELECT * FROM events 
WHERE user_id = 123 
AND timestamp > '2025-01-01' 
ORDER BY timestamp DESC 
LIMIT 100;

-- å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–
SET max_parallel_workers_per_gather = 4;
SET parallel_tuple_cost = 0.1;
SET parallel_setup_cost = 1000.0;

-- åˆ†ææŸ¥è¯¢
EXPLAIN (ANALYZE, BUFFERS, VERBOSE) 
SELECT COUNT(*) FROM events 
WHERE timestamp > '2025-01-01';
```

### 2. å­˜å‚¨ä¼˜åŒ–

#### 2.1 åˆ†åŒºç­–ç•¥

```sql
-- æ—¶é—´åˆ†åŒºç­–ç•¥
CREATE TABLE events (
    id BIGSERIAL,
    user_id BIGINT,
    event_type VARCHAR(50),
    event_data JSONB,
    timestamp TIMESTAMPTZ NOT NULL
) PARTITION BY RANGE (timestamp);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE events_2025_01 PARTITION OF events
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE events_2025_02 PARTITION OF events
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');

-- è‡ªåŠ¨åˆ†åŒºç®¡ç†
CREATE OR REPLACE FUNCTION create_monthly_partition(table_name text, start_date date)
RETURNS void AS $$
DECLARE
    partition_name text;
    end_date date;
BEGIN
    partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM');
    end_date := start_date + INTERVAL '1 month';
    
    EXECUTE format('CREATE TABLE IF NOT EXISTS %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',
                   partition_name, table_name, start_date, end_date);
END;
$$ LANGUAGE plpgsql;

-- åˆ›å»ºåˆ†åŒºè§¦å‘å™¨
CREATE OR REPLACE FUNCTION events_partition_trigger()
RETURNS trigger AS $$
BEGIN
    PERFORM create_monthly_partition('events', date_trunc('month', NEW.timestamp)::date);
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER events_partition_trigger
    BEFORE INSERT ON events
    FOR EACH ROW EXECUTE FUNCTION events_partition_trigger();
```

#### 2.2 æ•°æ®å‹ç¼©å’Œå½’æ¡£

```sql
-- æ•°æ®å‹ç¼©é…ç½®
ALTER TABLE events SET (toast_tuple_target = 128);
ALTER TABLE events SET (fillfactor = 90);

-- æ•°æ®å½’æ¡£ç­–ç•¥
CREATE OR REPLACE FUNCTION archive_old_events()
RETURNS void AS $$
BEGIN
    -- å½’æ¡£6ä¸ªæœˆå‰çš„æ•°æ®
    INSERT INTO events_archive 
    SELECT * FROM events 
    WHERE timestamp < NOW() - INTERVAL '6 months';
    
    -- åˆ é™¤å·²å½’æ¡£çš„æ•°æ®
    DELETE FROM events 
    WHERE timestamp < NOW() - INTERVAL '6 months';
    
    -- æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
    ANALYZE events;
END;
$$ LANGUAGE plpgsql;

-- å®šæœŸæ‰§è¡Œå½’æ¡£
SELECT cron.schedule('archive-events', '0 2 * * *', 'SELECT archive_old_events();');
```

## ğŸ”’ å®‰å…¨æ€§å’Œå¯é æ€§

### 1. æ•°æ®å®‰å…¨

#### 1.1 è®¿é—®æ§åˆ¶

```sql
-- åˆ›å»ºåº”ç”¨ç”¨æˆ·
CREATE USER app_user WITH PASSWORD 'secure_password';
CREATE USER readonly_user WITH PASSWORD 'readonly_password';

-- åˆ›å»ºè§’è‰²
CREATE ROLE app_role;
CREATE ROLE readonly_role;

-- åˆ†é…æƒé™
GRANT CONNECT ON DATABASE myapp TO app_role;
GRANT USAGE ON SCHEMA public TO app_role;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_role;
GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_role;

GRANT CONNECT ON DATABASE myapp TO readonly_role;
GRANT USAGE ON SCHEMA public TO readonly_role;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly_role;

-- åˆ†é…è§’è‰²
GRANT app_role TO app_user;
GRANT readonly_role TO readonly_user;

-- è¡Œçº§å®‰å…¨ç­–ç•¥
ALTER TABLE events ENABLE ROW LEVEL SECURITY;

CREATE POLICY events_user_policy ON events
    FOR ALL TO app_user
    USING (user_id = current_setting('app.current_user_id')::bigint);
```

#### 1.2 æ•°æ®åŠ å¯†

```sql
-- åˆ›å»ºåŠ å¯†æ‰©å±•
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- åŠ å¯†æ•æ„Ÿæ•°æ®
CREATE TABLE users_secure (
    id BIGSERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    password_hash TEXT NOT NULL,
    encrypted_ssn TEXT,  -- åŠ å¯†çš„ç¤¾ä¼šå®‰å…¨å·ç 
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- åŠ å¯†å‡½æ•°
CREATE OR REPLACE FUNCTION encrypt_ssn(ssn text)
RETURNS text AS $$
BEGIN
    RETURN encode(encrypt(ssn::bytea, 'encryption_key', 'aes'), 'base64');
END;
$$ LANGUAGE plpgsql;

-- è§£å¯†å‡½æ•°
CREATE OR REPLACE FUNCTION decrypt_ssn(encrypted_ssn text)
RETURNS text AS $$
BEGIN
    RETURN convert_from(decrypt(decode(encrypted_ssn, 'base64'), 'encryption_key', 'aes'), 'UTF8');
END;
$$ LANGUAGE plpgsql;
```

### 2. é«˜å¯ç”¨é…ç½®

#### 2.1 æµå¤åˆ¶é…ç½®

```bash
# postgresql.conf (ä¸»åº“)
wal_level = replica
max_wal_senders = 3
max_replication_slots = 3
hot_standby = on
hot_standby_feedback = on

# pg_hba.conf (ä¸»åº“)
host replication replicator 192.168.1.0/24 md5

# åˆ›å»ºå¤åˆ¶ç”¨æˆ·
CREATE USER replicator WITH REPLICATION ENCRYPTED PASSWORD 'replication_password';

# ä»åº“é…ç½®
# recovery.conf
standby_mode = 'on'
primary_conninfo = 'host=192.168.1.10 port=5432 user=replicator password=replication_password'
trigger_file = '/tmp/postgresql.trigger'
```

#### 2.2 è‡ªåŠ¨æ•…éšœè½¬ç§»

```bash
# Patronié…ç½® (patroni.yml)
scope: postgres
name: postgres-node1

restapi:
  listen: 0.0.0.0:8008
  connect_address: 192.168.1.10:8008

etcd3:
  host: 192.168.1.20:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true
      parameters:
        wal_level: replica
        hot_standby: "on"
        wal_keep_segments: 8
        max_wal_senders: 3
        max_replication_slots: 3
        checkpoint_completion_target: 0.9
        wal_buffers: 16MB
        default_statistics_target: 100

postgresql:
  listen: 0.0.0.0:5432
  connect_address: 192.168.1.10:5432
  data_dir: /var/lib/postgresql/data
  bin_dir: /usr/bin
  pgpass: /tmp/pgpass
  authentication:
    replication:
      username: replicator
      password: replication_password
    superuser:
      username: postgres
      password: postgres_password
  parameters:
    unix_socket_directories: '/var/run/postgresql'

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. æµ‹è¯•ç¯å¢ƒé…ç½®

```yaml
# docker-compose.test.yml
version: '3.8'
services:
  postgres:
    image: timescale/timescaledb:latest-pg15
    environment:
      POSTGRES_DB: testdb
      POSTGRES_USER: testuser
      POSTGRES_PASSWORD: testpass
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./test_data:/docker-entrypoint-initdb.d
    command: >
      postgres
      -c shared_buffers=2GB
      -c effective_cache_size=6GB
      -c work_mem=256MB
      -c maintenance_work_mem=1GB
      -c max_connections=200
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c max_parallel_workers_per_gather=4
      -c max_parallel_workers=8

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru

volumes:
  postgres_data:
```

### 2. æ€§èƒ½æµ‹è¯•è„šæœ¬

```rust
// tests/performance_tests.rs
use sqlx::{PgPool, Row};
use std::time::Instant;
use tokio::time::Duration;

#[tokio::test]
async fn test_oltp_performance() {
    let pool = PgPool::connect("postgresql://testuser:testpass@localhost:5432/testdb").await.unwrap();
    
    // æµ‹è¯•æ’å…¥æ€§èƒ½
    let start = Instant::now();
    for i in 0..10000 {
        sqlx::query("INSERT INTO events (user_id, event_type, event_data, timestamp) VALUES ($1, $2, $3, $4)")
            .bind(i % 1000)
            .bind("test_event")
            .bind(serde_json::json!({"value": i}))
            .bind(chrono::Utc::now())
            .execute(&pool)
            .await
            .unwrap();
    }
    let insert_duration = start.elapsed();
    println!("Inserted 10000 records in {:?}", insert_duration);
    
    // æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
    let start = Instant::now();
    let rows = sqlx::query("SELECT * FROM events WHERE user_id = $1 ORDER BY timestamp DESC LIMIT 100")
        .bind(1)
        .fetch_all(&pool)
        .await
        .unwrap();
    let query_duration = start.elapsed();
    println!("Queried {} records in {:?}", rows.len(), query_duration);
}

#[tokio::test]
async fn test_olap_performance() {
    let pool = PgPool::connect("postgresql://testuser:testpass@localhost:5432/testdb").await.unwrap();
    
    // æµ‹è¯•èšåˆæŸ¥è¯¢æ€§èƒ½
    let start = Instant::now();
    let rows = sqlx::query("
        SELECT 
            user_id,
            COUNT(*) as event_count,
            AVG((event_data->>'value')::numeric) as avg_value,
            MAX(timestamp) as last_event
        FROM events 
        WHERE timestamp > NOW() - INTERVAL '1 day'
        GROUP BY user_id
        ORDER BY event_count DESC
        LIMIT 100
    ")
    .fetch_all(&pool)
    .await
    .unwrap();
    let olap_duration = start.elapsed();
    println!("OLAP query returned {} rows in {:?}", rows.len(), olap_duration);
}

#[tokio::test]
async fn test_full_text_search_performance() {
    let pool = PgPool::connect("postgresql://testuser:testpass@localhost:5432/testdb").await.unwrap();
    
    // æµ‹è¯•å…¨æ–‡æœç´¢æ€§èƒ½
    let start = Instant::now();
    let rows = sqlx::query("
        SELECT id, title, content, 
               ts_rank(search_vector, plainto_tsquery('chinese_zh', $1)) as rank
        FROM documents 
        WHERE search_vector @@ plainto_tsquery('chinese_zh', $1)
        ORDER BY rank DESC
        LIMIT 100
    ")
    .bind("æµ‹è¯•æŸ¥è¯¢")
    .fetch_all(&pool)
    .await
    .unwrap();
    let search_duration = start.elapsed();
    println!("Full text search returned {} results in {:?}", rows.len(), search_duration);
}
```

## ğŸš€ éƒ¨ç½²å’Œè¿ç»´

### 1. Dockeréƒ¨ç½²

```dockerfile
# Dockerfile
FROM timescale/timescaledb:latest-pg15

# å®‰è£…æ‰©å±•
RUN apt-get update && apt-get install -y \
    postgresql-15-zhparser \
    postgresql-15-pg-stat-statements \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶é…ç½®æ–‡ä»¶
COPY postgresql.conf /etc/postgresql/postgresql.conf
COPY pg_hba.conf /etc/postgresql/pg_hba.conf

# å¤åˆ¶åˆå§‹åŒ–è„šæœ¬
COPY init.sql /docker-entrypoint-initdb.d/

# è®¾ç½®æƒé™
RUN chown -R postgres:postgres /etc/postgresql/
RUN chmod 600 /etc/postgresql/postgresql.conf
RUN chmod 600 /etc/postgresql/pg_hba.conf

# æš´éœ²ç«¯å£
EXPOSE 5432

# å¯åŠ¨å‘½ä»¤
CMD ["postgres", "-c", "config_file=/etc/postgresql/postgresql.conf"]
```

### 2. Kuberneteséƒ¨ç½²

```yaml
# k8s/postgres-all-in-one.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
data:
  postgresql.conf: |
    shared_buffers = 2GB
    effective_cache_size = 6GB
    work_mem = 256MB
    maintenance_work_mem = 1GB
    max_connections = 200
    random_page_cost = 1.1
    effective_io_concurrency = 200
    max_parallel_workers_per_gather = 4
    max_parallel_workers = 8
    wal_level = replica
    max_wal_senders = 3
    max_replication_slots = 3
    hot_standby = on
    hot_standby_feedback = on

---
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
type: Opaque
data:
  postgres-password: cG9zdGdyZXM=  # base64 encoded
  replication-password: cmVwbGljYXRpb24=  # base64 encoded

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-all-in-one
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: timescale/timescaledb:latest-pg15
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          value: "myapp"
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: postgres-password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: postgres-data
          mountPath: /var/lib/postgresql/data
        - name: postgres-config
          mountPath: /etc/postgresql/postgresql.conf
          subPath: postgresql.conf
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - exec pg_isready -U postgres -h 127.0.0.1 -p 5432
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - exec pg_isready -U postgres -h 127.0.0.1 -p 5432
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: postgres-config
        configMap:
          name: postgres-config
  volumeClaimTemplates:
  - metadata:
      name: postgres-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: postgres
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
  type: ClusterIP
```

### 3. ç›‘æ§é…ç½®

```yaml
# monitoring/grafana-dashboard.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboard-postgres
  labels:
    grafana_dashboard: "1"
data:
  postgres-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "PostgreSQL All-in-One Dashboard",
        "tags": ["postgresql", "database"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "Query Performance",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(postgresql_stat_database_tup_returned[5m])",
                "legendFormat": "Tuples Returned/sec"
              },
              {
                "expr": "rate(postgresql_stat_database_tup_fetched[5m])",
                "legendFormat": "Tuples Fetched/sec"
              }
            ]
          },
          {
            "id": 2,
            "title": "Connection Pool",
            "type": "graph",
            "targets": [
              {
                "expr": "postgresql_stat_database_numbackends",
                "legendFormat": "Active Connections"
              }
            ]
          },
          {
            "id": 3,
            "title": "Cache Hit Ratio",
            "type": "singlestat",
            "targets": [
              {
                "expr": "rate(postgresql_stat_database_blks_hit[5m]) / (rate(postgresql_stat_database_blks_hit[5m]) + rate(postgresql_stat_database_blks_read[5m])) * 100",
                "legendFormat": "Cache Hit Ratio %"
              }
            ]
          }
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "refresh": "5s"
      }
    }
```

## ğŸ“‹ æ€»ç»“

PostgreSQL All-in-Oneæ¶æ„ä¸ºä¸­å°å‹å›¢é˜Ÿæä¾›äº†ä¸€ä¸ªç»æµé«˜æ•ˆã€æ˜“äºç»´æŠ¤çš„æ•°æ®å¤„ç†è§£å†³æ–¹æ¡ˆã€‚
é€šè¿‡åˆç†çš„è®¾è®¡å’Œä¼˜åŒ–ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿæ»¡è¶³OLTPã€OLAPå’Œå…¨æ–‡æ£€ç´¢çš„ç»¼åˆéœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è¿ç»´å¤æ‚åº¦å’Œæˆæœ¬ã€‚

### ä¸»è¦ä¼˜åŠ¿

1. **æˆæœ¬æ•ˆç›Š**: ç›¸æ¯”åˆ†å¸ƒå¼æ¶æ„èŠ‚çœ60-80%æˆæœ¬
2. **ç®€åŒ–è¿ç»´**: å•ä¸€æ•°æ®åº“å®ä¾‹ï¼Œé™ä½è¿ç»´å¤æ‚åº¦
3. **æ€§èƒ½ä¼˜å¼‚**: æ»¡è¶³ç§’çº§æŸ¥è¯¢å“åº”éœ€æ±‚
4. **åŠŸèƒ½å®Œæ•´**: æ”¯æŒäº‹åŠ¡ã€åˆ†æã€æ£€ç´¢çš„å®Œæ•´åŠŸèƒ½
5. **æ‰©å±•çµæ´»**: æ”¯æŒå‚ç›´å’Œæ°´å¹³æ‰©å±•

### é€‚ç”¨åœºæ™¯1

- å›¢é˜Ÿè§„æ¨¡: 5-50äºº
- æ•°æ®è§„æ¨¡: < 10TB
- æŸ¥è¯¢å»¶è¿Ÿ: å¯æ¥å—ç§’çº§
- å¹¶å‘ç”¨æˆ·: < 10,000 QPS
- é¢„ç®—é™åˆ¶: è¿½æ±‚æˆæœ¬æ•ˆç›Š

### æŠ€æœ¯æ ˆ

- **æ•°æ®åº“**: PostgreSQL 15+ with TimescaleDB
- **ç¼–ç¨‹è¯­è¨€**: Rust 1.90
- **ç¼“å­˜**: Redis 7+
- **ç›‘æ§**: Prometheus + Grafana
- **å®¹å™¨åŒ–**: Docker + Kubernetes
- **CI/CD**: GitHub Actions

é€šè¿‡æœ¬æ–¹æ¡ˆçš„å®æ–½ï¼Œå›¢é˜Ÿå¯ä»¥è·å¾—ä¸€ä¸ªé«˜æ€§èƒ½ã€é«˜å¯ç”¨ã€æ˜“ç»´æŠ¤çš„æ•°æ®å¤„ç†å¹³å°ï¼Œä¸ºä¸šåŠ¡å‘å±•æä¾›å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚
