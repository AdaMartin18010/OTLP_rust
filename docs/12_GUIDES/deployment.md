# ğŸš€ éƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®å½•

- [ğŸš€ éƒ¨ç½²æŒ‡å—](#-éƒ¨ç½²æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ éƒ¨ç½²æ¦‚è§ˆ](#-éƒ¨ç½²æ¦‚è§ˆ)
    - [éƒ¨ç½²ç¯å¢ƒç±»å‹](#éƒ¨ç½²ç¯å¢ƒç±»å‹)
    - [æ¶æ„é€‰æ‹©](#æ¶æ„é€‰æ‹©)
  - [ğŸ’» æœ¬åœ°å¼€å‘éƒ¨ç½²](#-æœ¬åœ°å¼€å‘éƒ¨ç½²)
    - [å¿«é€Ÿå¯åŠ¨](#å¿«é€Ÿå¯åŠ¨)
    - [é…ç½®å¼€å‘ç¯å¢ƒ](#é…ç½®å¼€å‘ç¯å¢ƒ)
    - [æœ¬åœ° Collector é…ç½®](#æœ¬åœ°-collector-é…ç½®)
  - [ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²](#-å®¹å™¨åŒ–éƒ¨ç½²)
    - [Docker é•œåƒæ„å»º](#docker-é•œåƒæ„å»º)
    - [Docker Compose éƒ¨ç½²](#docker-compose-éƒ¨ç½²)
    - [å¯åŠ¨å’Œç®¡ç†](#å¯åŠ¨å’Œç®¡ç†)
  - [â˜¸ï¸ Kubernetes éƒ¨ç½²](#ï¸-kubernetes-éƒ¨ç½²)
    - [Deployment é…ç½®](#deployment-é…ç½®)
    - [Service é…ç½®](#service-é…ç½®)
    - [ConfigMap é…ç½®](#configmap-é…ç½®)
    - [HPA è‡ªåŠ¨æ‰©ç¼©å®¹](#hpa-è‡ªåŠ¨æ‰©ç¼©å®¹)
    - [éƒ¨ç½²å‘½ä»¤](#éƒ¨ç½²å‘½ä»¤)
  - [ğŸ­ ç”Ÿäº§ç¯å¢ƒé…ç½®](#-ç”Ÿäº§ç¯å¢ƒé…ç½®)
    - [ç”Ÿäº§çº§é…ç½®æ–‡ä»¶](#ç”Ÿäº§çº§é…ç½®æ–‡ä»¶)
    - [ç¯å¢ƒå˜é‡é…ç½®](#ç¯å¢ƒå˜é‡é…ç½®)
  - [ğŸ”„ CI/CD é›†æˆ](#-cicd-é›†æˆ)
    - [GitHub Actions é…ç½®](#github-actions-é…ç½®)
  - [ğŸ“Š ç›‘æ§å’Œå‘Šè­¦](#-ç›‘æ§å’Œå‘Šè­¦)
    - [Prometheus é…ç½®](#prometheus-é…ç½®)
    - [å‘Šè­¦è§„åˆ™](#å‘Šè­¦è§„åˆ™)
  - [ğŸ”§ æ•…éšœæ¢å¤](#-æ•…éšœæ¢å¤)
    - [å¤‡ä»½ç­–ç•¥](#å¤‡ä»½ç­–ç•¥)
    - [å›æ»šéƒ¨ç½²](#å›æ»šéƒ¨ç½²)
    - [ç´§æ€¥ä¿®å¤æµç¨‹](#ç´§æ€¥ä¿®å¤æµç¨‹)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## ğŸ¯ éƒ¨ç½²æ¦‚è§ˆ

### éƒ¨ç½²ç¯å¢ƒç±»å‹

| ç¯å¢ƒ | ç”¨é€” | é…ç½®è¦æ±‚ | è¯´æ˜ |
|------|------|----------|------|
| **å¼€å‘ç¯å¢ƒ** | æœ¬åœ°å¼€å‘æµ‹è¯• | ä½é…ç½® | å•æœºéƒ¨ç½² |
| **æµ‹è¯•ç¯å¢ƒ** | é›†æˆæµ‹è¯• | ä¸­ç­‰é…ç½® | å®¹å™¨åŒ–éƒ¨ç½² |
| **é¢„ç”Ÿäº§ç¯å¢ƒ** | ç”Ÿäº§éªŒè¯ | ç”Ÿäº§çº§é…ç½® | K8s é›†ç¾¤ |
| **ç”Ÿäº§ç¯å¢ƒ** | çº¿ä¸ŠæœåŠ¡ | é«˜å¯ç”¨é…ç½® | å¤šåŒºåŸŸéƒ¨ç½² |

### æ¶æ„é€‰æ‹©

```mermaid
graph TB
    A[åº”ç”¨] --> B[OTLP å®¢æˆ·ç«¯]
    B --> C{éƒ¨ç½²æ–¹å¼}
    C --> D[ç›´æ¥éƒ¨ç½²]
    C --> E[å®¹å™¨éƒ¨ç½²]
    C --> F[K8s éƒ¨ç½²]
    D --> G[Collector]
    E --> G
    F --> G
    G --> H[ç›‘æ§åç«¯]
```

---

## ğŸ’» æœ¬åœ°å¼€å‘éƒ¨ç½²

### å¿«é€Ÿå¯åŠ¨

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/OTLP_rust.git
cd OTLP_rust

# 2. æ„å»ºé¡¹ç›®
cargo build --release

# 3. è¿è¡Œç¤ºä¾‹
cargo run --example quick_start

# 4. è¿è¡Œæµ‹è¯•
cargo test
```

### é…ç½®å¼€å‘ç¯å¢ƒ

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export OTLP_ENDPOINT="http://localhost:4317"
export OTLP_SERVICE_NAME="dev-app"
export RUST_LOG=info

# å¯åŠ¨æœ¬åœ° Collector
docker run -d \
  --name otel-collector \
  -p 4317:4317 \
  -p 4318:4318 \
  -p 8888:8888 \
  -v $(pwd)/config/collector-config.yaml:/etc/otel-collector-config.yaml \
  otel/opentelemetry-collector-contrib:latest \
  --config=/etc/otel-collector-config.yaml
```

### æœ¬åœ° Collector é…ç½®

```yaml
# config/collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 1s
    send_batch_size: 1024
  
  memory_limiter:
    check_interval: 1s
    limit_mib: 512

exporters:
  logging:
    loglevel: debug
  
  prometheus:
    endpoint: "0.0.0.0:8889"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [logging]
    
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [logging, prometheus]
    
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [logging]
```

---

## ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²

### Docker é•œåƒæ„å»º

```dockerfile
# Dockerfile
FROM rust:1.90-slim as builder

WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY Cargo.toml Cargo.lock ./
COPY crates/otlp/Cargo.toml ./crates/otlp/
COPY crates/reliability/Cargo.toml ./crates/reliability/

# æ„å»ºä¾èµ–ï¼ˆç¼“å­˜å±‚ï¼‰
RUN mkdir -p crates/otlp/src && \
    mkdir -p crates/reliability/src && \
    echo "fn main() {}" > crates/otlp/src/main.rs && \
    echo "fn main() {}" > crates/reliability/src/main.rs && \
    cargo build --release && \
    rm -rf crates/*/src

# å¤åˆ¶æºä»£ç 
COPY . .

# æ„å»ºåº”ç”¨
RUN cargo build --release

# è¿è¡Œæ—¶é•œåƒ
FROM debian:bookworm-slim

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && \
    apt-get install -y ca-certificates libssl3 && \
    rm -rf /var/lib/apt/lists/*

# å¤åˆ¶å¯æ‰§è¡Œæ–‡ä»¶
COPY --from=builder /app/target/release/otlp-app /usr/local/bin/

# åˆ›å»ºç”¨æˆ·
RUN useradd -r -s /bin/false otlp && \
    chown -R otlp:otlp /usr/local/bin/otlp-app

USER otlp

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8080/health || exit 1

EXPOSE 8080

CMD ["otlp-app"]
```

### Docker Compose éƒ¨ç½²

```yaml
# docker-compose.yml
version: '3.8'

services:
  otlp-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: otlp-app
    ports:
      - "8080:8080"
    environment:
      - OTLP_ENDPOINT=http://otel-collector:4317
      - OTLP_SERVICE_NAME=otlp-app
      - RUST_LOG=info
    depends_on:
      - otel-collector
    networks:
      - otlp-network
    restart: unless-stopped

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter
    networks:
      - otlp-network
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - otlp-network
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - otlp-network
    restart: unless-stopped

networks:
  otlp-network:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
```

### å¯åŠ¨å’Œç®¡ç†

```bash
# æ„å»ºå¹¶å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f otlp-app

# åœæ­¢æœåŠ¡
docker-compose down

# é‡å¯æœåŠ¡
docker-compose restart otlp-app

# æŸ¥çœ‹çŠ¶æ€
docker-compose ps
```

---

## â˜¸ï¸ Kubernetes éƒ¨ç½²

### Deployment é…ç½®

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-app
  namespace: observability
  labels:
    app: otlp-app
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: otlp-app
  template:
    metadata:
      labels:
        app: otlp-app
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: otlp-app
      containers:
      - name: otlp-app
        image: your-registry/otlp-app:v1.0.0
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        env:
        - name: OTLP_ENDPOINT
          value: "http://otel-collector.observability.svc.cluster.local:4317"
        - name: OTLP_SERVICE_NAME
          value: "otlp-app"
        - name: RUST_LOG
          value: "info"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: config
          mountPath: /etc/otlp
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: otlp-app-config
```

### Service é…ç½®

```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: otlp-app
  namespace: observability
  labels:
    app: otlp-app
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: otlp-app
---
apiVersion: v1
kind: Service
metadata:
  name: otlp-app-headless
  namespace: observability
  labels:
    app: otlp-app
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: otlp-app
```

### ConfigMap é…ç½®

```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otlp-app-config
  namespace: observability
data:
  app.yaml: |
    server:
      host: "0.0.0.0"
      port: 8080
    
    otlp:
      endpoint: "http://otel-collector.observability.svc.cluster.local:4317"
      service_name: "otlp-app"
      compression: "gzip"
      timeout: 30s
      
      batch:
        max_size: 1000
        timeout: 5s
        max_queue_size: 10000
      
      retry:
        max_attempts: 3
        initial_interval: 100ms
        max_interval: 5s
    
    logging:
      level: "info"
      format: "json"
```

### HPA è‡ªåŠ¨æ‰©ç¼©å®¹

```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: otlp-app-hpa
  namespace: observability
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otlp-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 15
      selectPolicy: Max
```

### éƒ¨ç½²å‘½ä»¤

```bash
# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace observability

# éƒ¨ç½² ConfigMap
kubectl apply -f k8s/configmap.yaml

# éƒ¨ç½²åº”ç”¨
kubectl apply -f k8s/deployment.yaml

# éƒ¨ç½²æœåŠ¡
kubectl apply -f k8s/service.yaml

# éƒ¨ç½² HPA
kubectl apply -f k8s/hpa.yaml

# æŸ¥çœ‹çŠ¶æ€
kubectl get all -n observability

# æŸ¥çœ‹æ—¥å¿—
kubectl logs -f deployment/otlp-app -n observability

# æ‰©å®¹
kubectl scale deployment otlp-app --replicas=5 -n observability
```

---

## ğŸ­ ç”Ÿäº§ç¯å¢ƒé…ç½®

### ç”Ÿäº§çº§é…ç½®æ–‡ä»¶

```rust
// config/production.rs
use otlp::config::*;
use std::time::Duration;

pub fn create_production_config() -> OtlpConfig {
    OtlpConfig {
        // åŸºç¡€é…ç½®
        endpoint: std::env::var("OTLP_ENDPOINT")
            .unwrap_or_else(|_| "http://otel-collector:4317".to_string()),
        service_name: std::env::var("OTLP_SERVICE_NAME")
            .unwrap_or_else(|_| "production-app".to_string()),
        service_version: env!("CARGO_PKG_VERSION").to_string(),
        environment: "production".to_string(),
        
        // ä¼ è¾“é…ç½®
        transport: TransportConfig {
            protocol: TransportProtocol::Grpc,
            compression: Compression::Gzip,
            tls: Some(TlsConfig {
                enabled: true,
                cert_file: "/etc/ssl/certs/client.crt".to_string(),
                key_file: "/etc/ssl/private/client.key".to_string(),
                ca_file: "/etc/ssl/certs/ca.crt".to_string(),
                verify_server: true,
            }),
        },
        
        // è¿æ¥é…ç½®
        connection: ConnectionConfig {
            connect_timeout: Duration::from_secs(10),
            request_timeout: Duration::from_secs(60),
            keep_alive: Some(Duration::from_secs(60)),
            max_connections: 200,
            max_idle_connections: 100,
        },
        
        // æ‰¹å¤„ç†é…ç½®
        batch: BatchConfig {
            max_batch_size: 2000,
            batch_timeout: Duration::from_millis(200),
            max_queue_size: 50000,
            strategy: BatchStrategy::Hybrid,
        },
        
        // é‡è¯•é…ç½®
        retry: RetryConfig {
            max_attempts: 3,
            initial_interval: Duration::from_millis(100),
            max_interval: Duration::from_secs(5),
            multiplier: 2.0,
            randomization_factor: 0.1,
            retryable_errors: vec![
                ErrorType::Network,
                ErrorType::Timeout,
                ErrorType::Unavailable,
            ],
        },
        
        // ç›‘æ§é…ç½®
        monitoring: MonitoringConfig {
            enable_metrics: true,
            metrics_interval: Duration::from_secs(10),
            enable_health_check: true,
            health_check_interval: Duration::from_secs(30),
        },
        
        // å®‰å…¨é…ç½®
        security: SecurityConfig {
            enable_authentication: true,
            auth_token: std::env::var("OTLP_AUTH_TOKEN").ok(),
            enable_encryption: true,
        },
    }
}
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env.production
# æœåŠ¡é…ç½®
OTLP_ENDPOINT=https://otel-collector.prod.example.com:4317
OTLP_SERVICE_NAME=production-app
OTLP_SERVICE_VERSION=1.0.0
OTLP_ENVIRONMENT=production

# å®‰å…¨é…ç½®
OTLP_AUTH_TOKEN=your-production-token
OTLP_TLS_CERT=/etc/ssl/certs/client.crt
OTLP_TLS_KEY=/etc/ssl/private/client.key
OTLP_TLS_CA=/etc/ssl/certs/ca.crt

# æ€§èƒ½é…ç½®
OTLP_MAX_BATCH_SIZE=2000
OTLP_BATCH_TIMEOUT=200ms
OTLP_MAX_CONNECTIONS=200

# æ—¥å¿—é…ç½®
RUST_LOG=info
RUST_BACKTRACE=1

# ç›‘æ§é…ç½®
METRICS_PORT=8080
HEALTH_CHECK_PORT=8081
```

---

## ğŸ”„ CI/CD é›†æˆ

### GitHub Actions é…ç½®

```yaml
# .github/workflows/deploy.yml
name: Build and Deploy

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: 1.90
          override: true
      
      - name: Run tests
        run: cargo test --all-features
      
      - name: Run clippy
        run: cargo clippy -- -D warnings

  build:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v3
      
      - name: Log in to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up kubectl
        uses: azure/setup-kubectl@v3
      
      - name: Configure kubectl
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=./kubeconfig
      
      - name: Deploy to Kubernetes
        run: |
          kubectl apply -f k8s/
          kubectl rollout status deployment/otlp-app -n observability
```

---

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦

### Prometheus é…ç½®

```yaml
# config/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'otlp-app'
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
            - observability
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__

alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

rule_files:
  - 'alerts.yml'
```

### å‘Šè­¦è§„åˆ™

```yaml
# config/alerts.yml
groups:
  - name: otlp-app
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} for {{ $labels.instance }}"
      
      - alert: HighResponseTime
        expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "P99 response time is {{ $value }}s for {{ $labels.instance }}"
      
      - alert: PodNotReady
        expr: kube_pod_status_phase{namespace="observability",pod=~"otlp-app-.*",phase!="Running"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod not ready"
          description: "Pod {{ $labels.pod }} is not ready"
```

---

## ğŸ”§ æ•…éšœæ¢å¤

### å¤‡ä»½ç­–ç•¥

```bash
# å¤‡ä»½é…ç½®
kubectl get configmap otlp-app-config -n observability -o yaml > backup/configmap-$(date +%Y%m%d).yaml
kubectl get deployment otlp-app -n observability -o yaml > backup/deployment-$(date +%Y%m%d).yaml

# æ¢å¤é…ç½®
kubectl apply -f backup/configmap-20251020.yaml
kubectl apply -f backup/deployment-20251020.yaml
```

### å›æ»šéƒ¨ç½²

```bash
# æŸ¥çœ‹éƒ¨ç½²å†å²
kubectl rollout history deployment/otlp-app -n observability

# å›æ»šåˆ°ä¸Šä¸€ç‰ˆæœ¬
kubectl rollout undo deployment/otlp-app -n observability

# å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬
kubectl rollout undo deployment/otlp-app --to-revision=2 -n observability
```

### ç´§æ€¥ä¿®å¤æµç¨‹

1. **è¯†åˆ«é—®é¢˜**: é€šè¿‡ç›‘æ§å‘Šè­¦è¯†åˆ«é—®é¢˜
2. **å¿«é€Ÿå›æ»š**: å¦‚æœæ˜¯æ–°ç‰ˆæœ¬é—®é¢˜ï¼Œç«‹å³å›æ»š
3. **ç´§æ€¥ä¿®å¤**: ä¿®å¤ä»£ç å¹¶å¿«é€Ÿéƒ¨ç½²
4. **éªŒè¯ä¿®å¤**: ç¡®è®¤é—®é¢˜å·²è§£å†³
5. **æ€»ç»“å¤ç›˜**: è®°å½•é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

---

## ğŸ“š å‚è€ƒèµ„æº

- [å®‰è£…æŒ‡å—](installation.md)
- [å¿«é€Ÿå…¥é—¨](quick-start.md)
- [ç›‘æ§é…ç½®](monitoring.md)
- [æ•…éšœæ’é™¤](troubleshooting.md)

---

*æœ€åæ›´æ–°: 2025å¹´10æœˆ20æ—¥*  
*ç‰ˆæœ¬: 1.0.0*
