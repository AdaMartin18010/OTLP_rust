# ğŸš€ éƒ¨ç½²è¿ç»´æŒ‡å—

**ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025å¹´10æœˆ26æ—¥
**çŠ¶æ€**: ğŸŸ¢ æ´»è·ƒç»´æŠ¤

> **ç®€ä»‹**: OTLP Rust éƒ¨ç½²è¿ç»´æŒ‡å— - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ã€ç›‘æ§è®¾ç½®ã€æ•…éšœæ’é™¤å’Œæ€§èƒ½è°ƒä¼˜ã€‚

---

## ğŸ“‹ ç›®å½•

- [ğŸš€ éƒ¨ç½²è¿ç»´æŒ‡å—](#-éƒ¨ç½²è¿ç»´æŒ‡å—)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ­ ç”Ÿäº§éƒ¨ç½²](#-ç”Ÿäº§éƒ¨ç½²)
    - [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
      - [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
      - [ä¾èµ–å®‰è£…](#ä¾èµ–å®‰è£…)
    - [å®¹å™¨åŒ–éƒ¨ç½²](#å®¹å™¨åŒ–éƒ¨ç½²)
      - [Dockerfile ä¼˜åŒ–](#dockerfile-ä¼˜åŒ–)
      - [Docker Compose é…ç½®](#docker-compose-é…ç½®)
    - [Kubernetes éƒ¨ç½²](#kubernetes-éƒ¨ç½²)
      - [Deployment é…ç½®](#deployment-é…ç½®)
      - [Service é…ç½®](#service-é…ç½®)
      - [ConfigMap é…ç½®](#configmap-é…ç½®)
    - [äº‘åŸç”Ÿéƒ¨ç½²](#äº‘åŸç”Ÿéƒ¨ç½²)
      - [Helm Chart](#helm-chart)
  - [ğŸ“Š ç›‘æ§è®¾ç½®](#-ç›‘æ§è®¾ç½®)
    - [æŒ‡æ ‡æ”¶é›†](#æŒ‡æ ‡æ”¶é›†)
      - [Prometheus é…ç½®](#prometheus-é…ç½®)
      - [è‡ªå®šä¹‰æŒ‡æ ‡](#è‡ªå®šä¹‰æŒ‡æ ‡)
    - [æ—¥å¿—èšåˆ](#æ—¥å¿—èšåˆ)
      - [ç»“æ„åŒ–æ—¥å¿—](#ç»“æ„åŒ–æ—¥å¿—)
    - [åˆ†å¸ƒå¼è¿½è¸ª](#åˆ†å¸ƒå¼è¿½è¸ª)
      - [Jaeger é›†æˆ](#jaeger-é›†æˆ)
    - [å‘Šè­¦é…ç½®](#å‘Šè­¦é…ç½®)
      - [AlertManager è§„åˆ™](#alertmanager-è§„åˆ™)
  - [ğŸ”§ æ•…éšœæ’é™¤](#-æ•…éšœæ’é™¤)
    - [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
      - [è¿æ¥é—®é¢˜](#è¿æ¥é—®é¢˜)
      - [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜)
    - [è¯Šæ–­å·¥å…·](#è¯Šæ–­å·¥å…·)
      - [å¥åº·æ£€æŸ¥ç«¯ç‚¹](#å¥åº·æ£€æŸ¥ç«¯ç‚¹)
      - [è°ƒè¯•æ¨¡å¼](#è°ƒè¯•æ¨¡å¼)
    - [æ•…éšœæ¢å¤](#æ•…éšœæ¢å¤)
      - [è‡ªåŠ¨æ¢å¤æœºåˆ¶](#è‡ªåŠ¨æ¢å¤æœºåˆ¶)
  - [âš¡ æ€§èƒ½è°ƒä¼˜](#-æ€§èƒ½è°ƒä¼˜)
    - [ç³»ç»Ÿè°ƒä¼˜](#ç³»ç»Ÿè°ƒä¼˜)
      - [å†…æ ¸å‚æ•°ä¼˜åŒ–](#å†…æ ¸å‚æ•°ä¼˜åŒ–)
    - [åº”ç”¨è°ƒä¼˜](#åº”ç”¨è°ƒä¼˜)
      - [è¿è¡Œæ—¶ä¼˜åŒ–](#è¿è¡Œæ—¶ä¼˜åŒ–)
    - [ç½‘ç»œè°ƒä¼˜](#ç½‘ç»œè°ƒä¼˜)
      - [è¿æ¥æ± ä¼˜åŒ–](#è¿æ¥æ± ä¼˜åŒ–)
  - [ğŸ”’ å®‰å…¨é…ç½®](#-å®‰å…¨é…ç½®)
    - [è®¤è¯æˆæƒ](#è®¤è¯æˆæƒ)
      - [RBAC é…ç½®](#rbac-é…ç½®)
    - [ç½‘ç»œå®‰å…¨](#ç½‘ç»œå®‰å…¨)
      - [NetworkPolicy é…ç½®](#networkpolicy-é…ç½®)
  - [ğŸ”„ è¿ç»´è‡ªåŠ¨åŒ–](#-è¿ç»´è‡ªåŠ¨åŒ–)
    - [CI/CD æµæ°´çº¿](#cicd-æµæ°´çº¿)
      - [GitHub Actions](#github-actions)
    - [è‡ªåŠ¨åŒ–éƒ¨ç½²](#è‡ªåŠ¨åŒ–éƒ¨ç½²)
      - [ArgoCD é…ç½®](#argocd-é…ç½®)
  - [ğŸ”— ç›¸å…³æ–‡æ¡£](#-ç›¸å…³æ–‡æ¡£)

## ğŸ­ ç”Ÿäº§éƒ¨ç½²

### ç¯å¢ƒå‡†å¤‡

#### ç³»ç»Ÿè¦æ±‚

```bash
# æ£€æŸ¥ç³»ç»Ÿè¦æ±‚
echo "=== ç³»ç»Ÿä¿¡æ¯ ==="
uname -a
cat /etc/os-release

echo "=== å†…å­˜ä¿¡æ¯ ==="
free -h

echo "=== CPU ä¿¡æ¯ ==="
nproc
lscpu | grep "Model name"

echo "=== ç£ç›˜ç©ºé—´ ==="
df -h

echo "=== ç½‘ç»œé…ç½® ==="
ip addr show
```

#### ä¾èµ–å®‰è£…

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install -y curl wget git build-essential pkg-config libssl-dev

# CentOS/RHEL
sudo yum update -y
sudo yum install -y curl wget git gcc openssl-devel

# å®‰è£… Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source ~/.cargo/env

# éªŒè¯å®‰è£…
rustc --version
cargo --version
```

### å®¹å™¨åŒ–éƒ¨ç½²

#### Dockerfile ä¼˜åŒ–

```dockerfile
# å¤šé˜¶æ®µæ„å»ºä¼˜åŒ–
FROM rust:1.90-alpine AS builder

# å®‰è£…æ„å»ºä¾èµ–
RUN apk add --no-cache musl-dev openssl-dev pkgconfig

WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY Cargo.toml Cargo.lock ./

# æ„å»ºä¾èµ–ç¼“å­˜
RUN cargo fetch

# å¤åˆ¶æºä»£ç 
COPY src ./src
COPY otlp ./otlp

# æ„å»ºåº”ç”¨
RUN cargo build --release --target x86_64-unknown-linux-musl

# è¿è¡Œæ—¶é•œåƒ
FROM alpine:latest

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apk --no-cache add ca-certificates tzdata

# åˆ›å»ºé root ç”¨æˆ·
RUN addgroup -g 1001 -S otlp && \
    adduser -u 1001 -S otlp -G otlp

WORKDIR /app

# å¤åˆ¶äºŒè¿›åˆ¶æ–‡ä»¶
COPY --from=builder /app/target/x86_64-unknown-linux-musl/release/otlp-client ./

# è®¾ç½®æƒé™
RUN chown -R otlp:otlp /app
USER otlp

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¯åŠ¨åº”ç”¨
CMD ["./otlp-client"]
```

#### Docker Compose é…ç½®

```yaml
version: '3.8'

services:
  otlp-client:
    build: .
    ports:
      - "8080:8080"
    environment:
      - RUST_LOG=info
      - OTLP_ENDPOINT=http://collector:4317
      - OTLP_PROTOCOL=grpc
    volumes:
      - ./config:/app/config:ro
      - ./logs:/app/logs
    depends_on:
      - collector
      - redis
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  collector:
    image: otel/opentelemetry-collector:latest
    ports:
      - "4317:4317"
      - "4318:4318"
    volumes:
      - ./collector-config.yaml:/etc/collector-config.yaml:ro
    command: ["--config=/etc/collector-config.yaml"]
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
    restart: unless-stopped

volumes:
  redis-data:
  prometheus-data:
  grafana-data:
```

### Kubernetes éƒ¨ç½²

#### Deployment é…ç½®

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-client
  namespace: otlp
  labels:
    app: otlp-client
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: otlp-client
  template:
    metadata:
      labels:
        app: otlp-client
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: otlp-client
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
      containers:
      - name: otlp-client
        image: otlp-client:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        env:
        - name: RUST_LOG
          value: "info"
        - name: OTLP_ENDPOINT
          value: "http://collector:4317"
        - name: OTLP_PROTOCOL
          value: "grpc"
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: config
          mountPath: /app/config
          readOnly: true
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: config
        configMap:
          name: otlp-client-config
      - name: logs
        emptyDir: {}
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 300
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 300
```

#### Service é…ç½®

```yaml
apiVersion: v1
kind: Service
metadata:
  name: otlp-client
  namespace: otlp
  labels:
    app: otlp-client
spec:
  type: ClusterIP
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: otlp-client
---
apiVersion: v1
kind: Service
metadata:
  name: otlp-client-headless
  namespace: otlp
  labels:
    app: otlp-client
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: otlp-client
```

#### ConfigMap é…ç½®

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otlp-client-config
  namespace: otlp
data:
  config.yaml: |
    server:
      host: "0.0.0.0"
      port: 8080
      workers: 4

    otlp:
      endpoint: "http://collector:4317"
      protocol: "grpc"
      timeout: "10s"
      retry:
        max_retries: 3
        initial_delay: "100ms"
        max_delay: "30s"
        multiplier: 2.0

    batch:
      max_size: 512
      timeout: "5s"
      max_queue_size: 2048

    metrics:
      enabled: true
      port: 9090
      path: "/metrics"

    logging:
      level: "info"
      format: "json"
      output: "stdout"
```

### äº‘åŸç”Ÿéƒ¨ç½²

#### Helm Chart

```yaml
# values.yaml
replicaCount: 3

image:
  repository: otlp-client
  tag: "latest"
  pullPolicy: IfNotPresent
  pullSecrets: []

nameOverride: ""
fullnameOverride: ""

service:
  type: ClusterIP
  port: 8080
  targetPort: 8080

ingress:
  enabled: false
  className: ""
  annotations: {}
  hosts:
    - host: otlp-client.local
      paths:
        - path: /
          pathType: Prefix
  tls: []

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}

podAnnotations: {}

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1001
  runAsGroup: 1001
  fsGroup: 1001

securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1001
  capabilities:
    drop:
    - ALL

config:
  otlp:
    endpoint: "http://collector:4317"
    protocol: "grpc"
    timeout: "10s"

  batch:
    max_size: 512
    timeout: "5s"

  metrics:
    enabled: true
    port: 9090
```

## ğŸ“Š ç›‘æ§è®¾ç½®

### æŒ‡æ ‡æ”¶é›†

#### Prometheus é…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'otlp-client'
    static_configs:
      - targets: ['otlp-client:8080']
    metrics_path: '/metrics'
    scrape_interval: 10s
    scrape_timeout: 5s

  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

#### è‡ªå®šä¹‰æŒ‡æ ‡

```rust
use prometheus::{Counter, Histogram, Gauge, register_counter, register_histogram, register_gauge};

// è‡ªå®šä¹‰æŒ‡æ ‡å®šä¹‰
pub struct Metrics {
    pub requests_total: Counter,
    pub request_duration: Histogram,
    pub active_connections: Gauge,
    pub batch_size: Histogram,
    pub errors_total: Counter,
}

impl Metrics {
    pub fn new() -> Self {
        Self {
            requests_total: register_counter!(
                "otlp_requests_total",
                "Total number of requests"
            ).unwrap(),
            request_duration: register_histogram!(
                "otlp_request_duration_seconds",
                "Request duration in seconds"
            ).unwrap(),
            active_connections: register_gauge!(
                "otlp_active_connections",
                "Number of active connections"
            ).unwrap(),
            batch_size: register_histogram!(
                "otlp_batch_size",
                "Batch size distribution"
            ).unwrap(),
            errors_total: register_counter!(
                "otlp_errors_total",
                "Total number of errors"
            ).unwrap(),
        }
    }
}
```

### æ—¥å¿—èšåˆ

#### ç»“æ„åŒ–æ—¥å¿—

```rust
use tracing::{info, warn, error, debug};
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};

// æ—¥å¿—é…ç½®
pub fn init_logging() {
    tracing_subscriber::registry()
        .with(
            tracing_subscriber::EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| "otlp_client=info".into()),
        )
        .with(tracing_subscriber::fmt::layer()
            .with_target(false)
            .with_thread_ids(true)
            .with_file(true)
            .with_line_number(true)
            .json()
        )
        .init();
}

// ç»“æ„åŒ–æ—¥å¿—ä½¿ç”¨
pub async fn process_request(request: &Request) -> Result<Response, Error> {
    let span = tracing::info_span!("process_request",
        request_id = %request.id,
        method = %request.method,
        path = %request.path
    );

    let _enter = span.enter();

    info!(
        request_id = %request.id,
        method = %request.method,
        path = %request.path,
        "Processing request"
    );

    // å¤„ç†é€»è¾‘
    let result = handle_request(request).await;

    match &result {
        Ok(response) => {
            info!(
                request_id = %request.id,
                status_code = response.status_code,
                duration_ms = response.duration.as_millis(),
                "Request completed successfully"
            );
        }
        Err(error) => {
            error!(
                request_id = %request.id,
                error = %error,
                "Request failed"
            );
        }
    }

    result
}
```

### åˆ†å¸ƒå¼è¿½è¸ª

#### Jaeger é›†æˆ

```rust
use opentelemetry::trace::{TraceContextExt, Tracer};
use opentelemetry_jaeger::new_agent_pipeline;
use tracing_opentelemetry::OpenTelemetrySpanExt;

// Jaeger é…ç½®
pub fn init_tracing() -> Result<(), Box<dyn std::error::Error>> {
    let tracer = new_agent_pipeline()
        .with_service_name("otlp-client")
        .with_endpoint("http://jaeger:14268/api/traces")
        .install_simple()?;

    tracing_subscriber::registry()
        .with(tracing_opentelemetry::layer().with_tracer(tracer))
        .with(tracing_subscriber::EnvFilter::from_default_env())
        .init();

    Ok(())
}

// è¿½è¸ªä½¿ç”¨
pub async fn send_trace_data(&self, data: TelemetryData) -> Result<(), OtlpError> {
    let span = tracing::info_span!("send_trace_data");
    let _enter = span.enter();

    // åˆ›å»ºå­ span
    let child_span = tracing::info_span!("serialize_data");
    let _child_enter = child_span.enter();

    let serialized = serde_json::to_string(&data)?;
    drop(_child_enter);

    // å‘é€æ•°æ®
    let send_span = tracing::info_span!("send_to_collector");
    let _send_enter = send_span.enter();

    self.transport.send(&serialized).await?;

    Ok(())
}
```

### å‘Šè­¦é…ç½®

#### AlertManager è§„åˆ™

```yaml
# alert_rules.yml
groups:
- name: otlp_client_alerts
  rules:
  - alert: HighErrorRate
    expr: rate(otlp_errors_total[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value }} errors per second"

  - alert: HighLatency
    expr: histogram_quantile(0.95, rate(otlp_request_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High latency detected"
      description: "95th percentile latency is {{ $value }} seconds"

  - alert: ServiceDown
    expr: up{job="otlp-client"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "OTLP Client service is down"
      description: "Service has been down for more than 1 minute"

  - alert: HighMemoryUsage
    expr: (container_memory_usage_bytes{name="otlp-client"} / container_spec_memory_limit_bytes) > 0.8
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage"
      description: "Memory usage is {{ $value | humanizePercentage }}"
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### è¿æ¥é—®é¢˜

```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
curl -v http://collector:4317/health

# æ£€æŸ¥ DNS è§£æ
nslookup collector

# æ£€æŸ¥ç«¯å£è¿é€šæ€§
telnet collector 4317

# æ£€æŸ¥é˜²ç«å¢™è§„åˆ™
iptables -L -n
```

#### æ€§èƒ½é—®é¢˜

```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
top
htop
iostat -x 1
vmstat 1

# æ£€æŸ¥ç½‘ç»œç»Ÿè®¡
ss -tuln
netstat -i
iftop

# æ£€æŸ¥åº”ç”¨æŒ‡æ ‡
curl http://localhost:8080/metrics
```

### è¯Šæ–­å·¥å…·

#### å¥åº·æ£€æŸ¥ç«¯ç‚¹

```rust
use axum::{Router, routing::get, Json};
use serde_json::{json, Value};

// å¥åº·æ£€æŸ¥å®ç°
pub fn create_health_router() -> Router {
    Router::new()
        .route("/health", get(health_check))
        .route("/ready", get(readiness_check))
        .route("/metrics", get(metrics_endpoint))
}

async fn health_check() -> Json<Value> {
    Json(json!({
        "status": "healthy",
        "timestamp": chrono::Utc::now(),
        "version": env!("CARGO_PKG_VERSION"),
        "uptime": get_uptime(),
    }))
}

async fn readiness_check() -> Json<Value> {
    let checks = vec![
        ("database", check_database().await),
        ("collector", check_collector().await),
        ("redis", check_redis().await),
    ];

    let all_ready = checks.iter().all(|(_, ready)| *ready);

    Json(json!({
        "status": if all_ready { "ready" } else { "not_ready" },
        "checks": checks.into_iter().collect::<std::collections::HashMap<_, _>>(),
    }))
}
```

#### è°ƒè¯•æ¨¡å¼

```rust
// è°ƒè¯•é…ç½®
pub struct DebugConfig {
    pub enable_profiling: bool,
    pub enable_tracing: bool,
    pub log_level: tracing::Level,
    pub dump_config: bool,
}

impl DebugConfig {
    pub fn from_env() -> Self {
        Self {
            enable_profiling: std::env::var("ENABLE_PROFILING")
                .unwrap_or_default()
                .parse()
                .unwrap_or(false),
            enable_tracing: std::env::var("ENABLE_TRACING")
                .unwrap_or_default()
                .parse()
                .unwrap_or(false),
            log_level: std::env::var("LOG_LEVEL")
                .unwrap_or_else(|_| "info".to_string())
                .parse()
                .unwrap_or(tracing::Level::INFO),
            dump_config: std::env::var("DUMP_CONFIG")
                .unwrap_or_default()
                .parse()
                .unwrap_or(false),
        }
    }
}
```

### æ•…éšœæ¢å¤

#### è‡ªåŠ¨æ¢å¤æœºåˆ¶

```rust
use tokio::time::{sleep, Duration};

// è‡ªåŠ¨æ¢å¤å®ç°
pub struct AutoRecovery {
    max_retries: u32,
    base_delay: Duration,
    max_delay: Duration,
}

impl AutoRecovery {
    pub async fn recover<F, T>(&self, mut operation: F) -> Result<T, OtlpError>
    where
        F: FnMut() -> Pin<Box<dyn Future<Output = Result<T, OtlpError>> + Send>>,
    {
        let mut delay = self.base_delay;

        for attempt in 0..self.max_retries {
            match operation().await {
                Ok(result) => return Ok(result),
                Err(e) => {
                    if attempt < self.max_retries - 1 {
                        warn!("Attempt {} failed: {}, retrying in {:?}", attempt + 1, e, delay);
                        sleep(delay).await;
                        delay = std::cmp::min(delay * 2, self.max_delay);
                    } else {
                        error!("All {} attempts failed, giving up", self.max_retries);
                        return Err(e);
                    }
                }
            }
        }

        Err(OtlpError::Custom("Max retries exceeded".to_string()))
    }
}
```

## âš¡ æ€§èƒ½è°ƒä¼˜

### ç³»ç»Ÿè°ƒä¼˜

#### å†…æ ¸å‚æ•°ä¼˜åŒ–

```bash
# /etc/sysctl.conf
# ç½‘ç»œä¼˜åŒ–
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 65536 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728
net.core.netdev_max_backlog = 5000
net.ipv4.tcp_congestion_control = bbr

# æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
fs.file-max = 2097152
fs.nr_open = 2097152

# å†…å­˜ç®¡ç†
vm.swappiness = 10
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5

# åº”ç”¨é…ç½®
echo "* soft nofile 65536" >> /etc/security/limits.conf
echo "* hard nofile 65536" >> /etc/security/limits.conf
```

### åº”ç”¨è°ƒä¼˜

#### è¿è¡Œæ—¶ä¼˜åŒ–

```rust
// è¿è¡Œæ—¶é…ç½®ä¼˜åŒ–
pub fn optimize_runtime() -> Result<Runtime, std::io::Error> {
    Builder::new_multi_thread()
        .worker_threads(num_cpus::get())
        .max_blocking_threads(512)
        .thread_name("otlp-worker")
        .thread_stack_size(3 * 1024 * 1024) // 3MB stack
        .enable_all()
        .build()
}

// å†…å­˜æ± é…ç½®
pub struct MemoryConfig {
    pub pool_size: usize,
    pub buffer_size: usize,
    pub max_memory: usize,
}

impl Default for MemoryConfig {
    fn default() -> Self {
        Self {
            pool_size: 1000,
            buffer_size: 64 * 1024, // 64KB
            max_memory: 512 * 1024 * 1024, // 512MB
        }
    }
}
```

### ç½‘ç»œè°ƒä¼˜

#### è¿æ¥æ± ä¼˜åŒ–

```rust
// è¿æ¥æ± é…ç½®
pub struct ConnectionPoolConfig {
    pub max_connections: usize,
    pub max_connections_per_host: usize,
    pub connection_timeout: Duration,
    pub idle_timeout: Duration,
    pub keep_alive: bool,
}

impl Default for ConnectionPoolConfig {
    fn default() -> Self {
        Self {
            max_connections: 100,
            max_connections_per_host: 10,
            connection_timeout: Duration::from_secs(30),
            idle_timeout: Duration::from_secs(90),
            keep_alive: true,
        }
    }
}
```

## ğŸ”’ å®‰å…¨é…ç½®

### è®¤è¯æˆæƒ

#### RBAC é…ç½®

```yaml
# rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otlp-client
  namespace: otlp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: otlp
  name: otlp-client-role
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: otlp-client-rolebinding
  namespace: otlp
subjects:
- kind: ServiceAccount
  name: otlp-client
  namespace: otlp
roleRef:
  kind: Role
  name: otlp-client-role
  apiGroup: rbac.authorization.k8s.io
```

### ç½‘ç»œå®‰å…¨

#### NetworkPolicy é…ç½®

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: otlp-client-netpol
  namespace: otlp
spec:
  podSelector:
    matchLabels:
      app: otlp-client
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: otlp
    ports:
    - protocol: TCP
      port: 4317
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
```

## ğŸ”„ è¿ç»´è‡ªåŠ¨åŒ–

### CI/CD æµæ°´çº¿

#### GitHub Actions

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: 1.90.0
        override: true

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    - name: Run tests
      run: |
        cargo test --all-features
        cargo clippy --all-features -- -D warnings
        cargo fmt -- --check

    - name: Run benchmarks
      run: cargo bench

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Login to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Build and push
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4

    - name: Deploy to Kubernetes
      run: |
        kubectl set image deployment/otlp-client \
          otlp-client=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
        kubectl rollout status deployment/otlp-client
```

### è‡ªåŠ¨åŒ–éƒ¨ç½²

#### ArgoCD é…ç½®

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: otlp-client
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/example/otlp-client
    targetRevision: HEAD
    path: k8s
    helm:
      valueFiles:
      - values.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: otlp
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [å¿«é€Ÿå¼€å§‹æŒ‡å—](../01_GETTING_STARTED/README.md)
- [API å‚è€ƒæ–‡æ¡£](../03_API_REFERENCE/README.md)
- [æ¶æ„è®¾è®¡æ–‡æ¡£](../04_ARCHITECTURE/README.md)
- [å®ç°æŒ‡å—](../05_IMPLEMENTATION/README.md)
- [é›†æˆæŒ‡å—](../07_INTEGRATION/README.md)

---

**éƒ¨ç½²è¿ç»´æŒ‡å—ç‰ˆæœ¬**: 1.0.0
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ
**ç»´æŠ¤è€…**: OTLP Rust è¿ç»´å›¢é˜Ÿ
