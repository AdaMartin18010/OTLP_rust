# OTLPé™çº§ä¸å‡çº§ç­–ç•¥ - Rustå®Œæ•´å®ç°

> **Rustç‰ˆæœ¬**: 1.90+  
> **æ ¸å¿ƒä¾èµ–**: tokio 1.47.1, governor 0.7  
> **OpenTelemetry**: 0.31.0  
> **æœ€åæ›´æ–°**: 2025å¹´10æœˆ9æ—¥

---

## ğŸ“‹ ç›®å½•

- [OTLPé™çº§ä¸å‡çº§ç­–ç•¥ - Rustå®Œæ•´å®ç°](#otlpé™çº§ä¸å‡çº§ç­–ç•¥---rustå®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. é™çº§ç­–ç•¥æ¦‚è¿°](#1-é™çº§ç­–ç•¥æ¦‚è¿°)
    - [1.1 é™çº§åœºæ™¯](#11-é™çº§åœºæ™¯)
    - [1.2 é™çº§ç­–ç•¥](#12-é™çº§ç­–ç•¥)
  - [2. è‡ªé€‚åº”é™çº§](#2-è‡ªé€‚åº”é™çº§)
    - [2.1 CPUè§¦å‘å™¨](#21-cpuè§¦å‘å™¨)
    - [2.2 å†…å­˜è§¦å‘å™¨](#22-å†…å­˜è§¦å‘å™¨)
    - [2.3 å»¶è¿Ÿè§¦å‘å™¨](#23-å»¶è¿Ÿè§¦å‘å™¨)
  - [3. é™æµä¸ç†”æ–­](#3-é™æµä¸ç†”æ–­)
    - [3.1 ä»¤ç‰Œæ¡¶é™æµå™¨](#31-ä»¤ç‰Œæ¡¶é™æµå™¨)
    - [3.2 ç†”æ–­å™¨](#32-ç†”æ–­å™¨)
  - [4. å¹³æ»‘å‡çº§ç­–ç•¥](#4-å¹³æ»‘å‡çº§ç­–ç•¥)
    - [4.1 æ»šåŠ¨å‡çº§](#41-æ»šåŠ¨å‡çº§)
  - [5. ç°åº¦å‘å¸ƒ](#5-ç°åº¦å‘å¸ƒ)
    - [5.1 é‡‘ä¸é›€å‘å¸ƒ](#51-é‡‘ä¸é›€å‘å¸ƒ)
  - [6. å®Œæ•´å®ç°](#6-å®Œæ•´å®ç°)
    - [6.1 ç»¼åˆç¤ºä¾‹](#61-ç»¼åˆç¤ºä¾‹)
  - [æ€»ç»“](#æ€»ç»“)

---

## 1. é™çº§ç­–ç•¥æ¦‚è¿°

### 1.1 é™çº§åœºæ™¯

```text
é™çº§è§¦å‘æ¡ä»¶:

1. è¿‡è½½ä¿æŠ¤
   - CPUä½¿ç”¨ç‡ > 80%
   - å†…å­˜ä½¿ç”¨ç‡ > 85%
   - é˜Ÿåˆ—æ·±åº¦ > é˜ˆå€¼

2. æœåŠ¡è´¨é‡ä¸‹é™
   - å»¶è¿Ÿ > SLA
   - é”™è¯¯ç‡ > é˜ˆå€¼
   - è¶…æ—¶ç‡ > é˜ˆå€¼

3. æˆæœ¬æ§åˆ¶
   - è¶…å‡ºé¢„ç®—
   - é…é¢ä¸è¶³

4. æ•…éšœæ¢å¤
   - éƒ¨åˆ†èŠ‚ç‚¹æ•…éšœ
   - ç½‘ç»œåˆ†åŒº
```

### 1.2 é™çº§ç­–ç•¥

```rust
use std::sync::Arc;
use tokio::sync::RwLock;
use std::time::{Duration, Instant};

/// é™çº§æ§åˆ¶å™¨
pub struct DegradationController {
    /// å½“å‰é™çº§çº§åˆ«
    current_level: Arc<RwLock<DegradationLevel>>,
    
    /// é™çº§é…ç½®
    config: Arc<RwLock<DegradationConfig>>,
    
    /// æŒ‡æ ‡æ”¶é›†å™¨
    metrics: Arc<MetricsCollector>,
    
    /// è§¦å‘å™¨
    triggers: Arc<RwLock<Vec<Box<dyn DegradationTrigger>>>>,
    
    /// çŠ¶æ€å†å²
    history: Arc<RwLock<CircularBuffer<DegradationEvent>>>,
}

/// é™çº§çº§åˆ«
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum DegradationLevel {
    /// æ­£å¸¸
    Normal = 0,
    
    /// è½»åº¦é™çº§ (é‡‡æ ·ç‡ 50%)
    Light = 1,
    
    /// ä¸­åº¦é™çº§ (é‡‡æ ·ç‡ 10%)
    Moderate = 2,
    
    /// é‡åº¦é™çº§ (é‡‡æ ·ç‡ 1%)
    Heavy = 3,
    
    /// ç´§æ€¥é™çº§ (ä»…é”™è¯¯è¿½è¸ª)
    Emergency = 4,
}

/// é™çº§é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DegradationConfig {
    /// å¯ç”¨é™çº§
    pub enabled: bool,
    
    /// CPUé˜ˆå€¼
    pub cpu_threshold: f64,
    
    /// å†…å­˜é˜ˆå€¼
    pub memory_threshold: f64,
    
    /// å»¶è¿Ÿé˜ˆå€¼(æ¯«ç§’)
    pub latency_threshold_ms: f64,
    
    /// é”™è¯¯ç‡é˜ˆå€¼
    pub error_rate_threshold: f64,
    
    /// é˜Ÿåˆ—æ·±åº¦é˜ˆå€¼
    pub queue_depth_threshold: usize,
    
    /// æ¢å¤é˜ˆå€¼
    pub recovery_threshold: f64,
    
    /// è¯„ä¼°é—´éš”
    pub evaluation_interval: Duration,
    
    /// é™çº§ç­–ç•¥
    pub strategies: Vec<DegradationStrategy>,
}

/// é™çº§ç­–ç•¥
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DegradationStrategy {
    /// é™ä½é‡‡æ ·ç‡
    ReduceSamplingRate {
        target_rate: f64,
    },
    
    /// ç¦ç”¨éå…³é”®è¿½è¸ª
    DisableNonCritical {
        critical_services: Vec<String>,
    },
    
    /// å¢åŠ æ‰¹å¤„ç†å¤§å°
    IncreaseBatchSize {
        target_size: usize,
    },
    
    /// å»¶é•¿æ‰¹å¤„ç†ç­‰å¾…æ—¶é—´
    IncreaseBatchWait {
        target_wait: Duration,
    },
    
    /// ç¦ç”¨è¯¦ç»†å±æ€§
    DisableVerboseAttributes,
    
    /// ä»…è¿½è¸ªé”™è¯¯
    ErrorsOnly,
}

impl DegradationController {
    pub fn new(
        config: DegradationConfig,
        metrics: Arc<MetricsCollector>,
    ) -> Self {
        Self {
            current_level: Arc::new(RwLock::new(DegradationLevel::Normal)),
            config: Arc::new(RwLock::new(config)),
            metrics,
            triggers: Arc::new(RwLock::new(Vec::new())),
            history: Arc::new(RwLock::new(CircularBuffer::new(1000))),
        }
    }
    
    /// å¯åŠ¨é™çº§æ§åˆ¶å™¨
    pub async fn start(&self) {
        self.start_evaluator().await;
        self.start_recovery_monitor().await;
    }
    
    /// å¯åŠ¨è¯„ä¼°å™¨
    async fn start_evaluator(&self) {
        let current_level = Arc::clone(&self.current_level);
        let config = Arc::clone(&self.config);
        let metrics = Arc::clone(&self.metrics);
        let triggers = Arc::clone(&self.triggers);
        let history = Arc::clone(&self.history);
        
        tokio::spawn(async move {
            loop {
                let cfg = config.read().await;
                let interval = cfg.evaluation_interval;
                drop(cfg);
                
                tokio::time::sleep(interval).await;
                
                // è¯„ä¼°å½“å‰çŠ¶æ€
                let triggers_list = triggers.read().await;
                let mut max_level = DegradationLevel::Normal;
                
                for trigger in triggers_list.iter() {
                    if let Some(level) = trigger.evaluate().await {
                        max_level = max_level.max(level);
                    }
                }
                
                // æ›´æ–°é™çº§çº§åˆ«
                let mut current = current_level.write().await;
                if *current != max_level {
                    let event = DegradationEvent {
                        timestamp: Instant::now(),
                        from_level: *current,
                        to_level: max_level,
                        reason: "Automatic evaluation".to_string(),
                    };
                    
                    *current = max_level;
                    
                    let mut hist = history.write().await;
                    hist.push(event);
                    
                    tracing::warn!(
                        "Degradation level changed: {:?} -> {:?}",
                        event.from_level,
                        event.to_level
                    );
                    
                    // è®°å½•æŒ‡æ ‡
                    metrics.record_degradation_change(event.to_level as i64);
                }
            }
        });
    }
    
    /// å¯åŠ¨æ¢å¤ç›‘æ§
    async fn start_recovery_monitor(&self) {
        let current_level = Arc::clone(&self.current_level);
        let config = Arc::clone(&self.config);
        let metrics = Arc::clone(&self.metrics);
        let history = Arc::clone(&self.history);
        
        tokio::spawn(async move {
            let mut ticker = tokio::time::interval(Duration::from_secs(30));
            
            loop {
                ticker.tick().await;
                
                let level = *current_level.read().await;
                
                if level == DegradationLevel::Normal {
                    continue;
                }
                
                // æ£€æŸ¥æ˜¯å¦å¯ä»¥æ¢å¤
                let cfg = config.read().await;
                let can_recover = Self::check_recovery_conditions(&metrics, cfg.recovery_threshold).await;
                
                if can_recover {
                    let new_level = match level {
                        DegradationLevel::Emergency => DegradationLevel::Heavy,
                        DegradationLevel::Heavy => DegradationLevel::Moderate,
                        DegradationLevel::Moderate => DegradationLevel::Light,
                        DegradationLevel::Light => DegradationLevel::Normal,
                        DegradationLevel::Normal => DegradationLevel::Normal,
                    };
                    
                    let mut current = current_level.write().await;
                    
                    let event = DegradationEvent {
                        timestamp: Instant::now(),
                        from_level: *current,
                        to_level: new_level,
                        reason: "Automatic recovery".to_string(),
                    };
                    
                    *current = new_level;
                    
                    let mut hist = history.write().await;
                    hist.push(event);
                    
                    tracing::info!(
                        "Degradation level recovered: {:?} -> {:?}",
                        event.from_level,
                        event.to_level
                    );
                    
                    metrics.record_degradation_change(new_level as i64);
                }
            }
        });
    }
    
    /// æ£€æŸ¥æ¢å¤æ¡ä»¶
    async fn check_recovery_conditions(
        metrics: &Arc<MetricsCollector>,
        threshold: f64,
    ) -> bool {
        let load = metrics.get_current_load().await;
        
        load.cpu_usage < threshold &&
        load.memory_usage < threshold &&
        load.error_rate < 0.01 &&
        load.avg_latency_ms < 100.0
    }
    
    /// è·å–å½“å‰é™çº§çº§åˆ«
    pub async fn get_current_level(&self) -> DegradationLevel {
        *self.current_level.read().await
    }
    
    /// è·å–æ¨èé‡‡æ ·ç‡
    pub async fn get_recommended_sampling_rate(&self) -> f64 {
        match *self.current_level.read().await {
            DegradationLevel::Normal => 1.0,
            DegradationLevel::Light => 0.5,
            DegradationLevel::Moderate => 0.1,
            DegradationLevel::Heavy => 0.01,
            DegradationLevel::Emergency => 0.001,
        }
    }
    
    /// æ˜¯å¦åº”è¯¥è¿½è¸ª
    pub async fn should_trace(&self, service_name: &str, is_error: bool) -> bool {
        let level = *self.current_level.read().await;
        
        match level {
            DegradationLevel::Normal => true,
            DegradationLevel::Light | DegradationLevel::Moderate => {
                is_error || self.is_critical_service(service_name).await
            }
            DegradationLevel::Heavy | DegradationLevel::Emergency => is_error,
        }
    }
    
    async fn is_critical_service(&self, service_name: &str) -> bool {
        let config = self.config.read().await;
        
        for strategy in &config.strategies {
            if let DegradationStrategy::DisableNonCritical { critical_services } = strategy {
                return critical_services.contains(&service_name.to_string());
            }
        }
        
        false
    }
}

/// é™çº§äº‹ä»¶
#[derive(Debug, Clone)]
pub struct DegradationEvent {
    pub timestamp: Instant,
    pub from_level: DegradationLevel,
    pub to_level: DegradationLevel,
    pub reason: String,
}

/// é™çº§è§¦å‘å™¨trait
#[async_trait::async_trait]
pub trait DegradationTrigger: Send + Sync {
    async fn evaluate(&self) -> Option<DegradationLevel>;
    fn name(&self) -> &str;
}
```

---

## 2. è‡ªé€‚åº”é™çº§

### 2.1 CPUè§¦å‘å™¨

```rust
use sysinfo::{System, SystemExt};

/// CPUè§¦å‘å™¨
pub struct CpuTrigger {
    system: Arc<RwLock<System>>,
    thresholds: CpuThresholds,
}

#[derive(Debug, Clone)]
pub struct CpuThresholds {
    pub light: f64,
    pub moderate: f64,
    pub heavy: f64,
    pub emergency: f64,
}

impl CpuTrigger {
    pub fn new(thresholds: CpuThresholds) -> Self {
        Self {
            system: Arc::new(RwLock::new(System::new_all())),
            thresholds,
        }
    }
}

#[async_trait::async_trait]
impl DegradationTrigger for CpuTrigger {
    async fn evaluate(&self) -> Option<DegradationLevel> {
        let mut sys = self.system.write().await;
        sys.refresh_cpu();
        
        // ç­‰å¾…ä¸€å°æ®µæ—¶é—´ä»¥è·å–å‡†ç¡®çš„CPUä½¿ç”¨ç‡
        tokio::time::sleep(Duration::from_millis(200)).await;
        sys.refresh_cpu();
        
        let cpu_usage = sys.global_cpu_info().cpu_usage() as f64;
        
        let level = if cpu_usage >= self.thresholds.emergency {
            DegradationLevel::Emergency
        } else if cpu_usage >= self.thresholds.heavy {
            DegradationLevel::Heavy
        } else if cpu_usage >= self.thresholds.moderate {
            DegradationLevel::Moderate
        } else if cpu_usage >= self.thresholds.light {
            DegradationLevel::Light
        } else {
            DegradationLevel::Normal
        };
        
        if level > DegradationLevel::Normal {
            tracing::warn!("CPU trigger activated: usage={:.2}%, level={:?}", cpu_usage, level);
            Some(level)
        } else {
            None
        }
    }
    
    fn name(&self) -> &str {
        "cpu"
    }
}
```

### 2.2 å†…å­˜è§¦å‘å™¨

```rust
/// å†…å­˜è§¦å‘å™¨
pub struct MemoryTrigger {
    system: Arc<RwLock<System>>,
    thresholds: MemoryThresholds,
}

#[derive(Debug, Clone)]
pub struct MemoryThresholds {
    pub light: f64,
    pub moderate: f64,
    pub heavy: f64,
    pub emergency: f64,
}

impl MemoryTrigger {
    pub fn new(thresholds: MemoryThresholds) -> Self {
        Self {
            system: Arc::new(RwLock::new(System::new_all())),
            thresholds,
        }
    }
}

#[async_trait::async_trait]
impl DegradationTrigger for MemoryTrigger {
    async fn evaluate(&self) -> Option<DegradationLevel> {
        let mut sys = self.system.write().await;
        sys.refresh_memory();
        
        let total_memory = sys.total_memory();
        let used_memory = sys.used_memory();
        let memory_usage = (used_memory as f64 / total_memory as f64) * 100.0;
        
        let level = if memory_usage >= self.thresholds.emergency {
            DegradationLevel::Emergency
        } else if memory_usage >= self.thresholds.heavy {
            DegradationLevel::Heavy
        } else if memory_usage >= self.thresholds.moderate {
            DegradationLevel::Moderate
        } else if memory_usage >= self.thresholds.light {
            DegradationLevel::Light
        } else {
            DegradationLevel::Normal
        };
        
        if level > DegradationLevel::Normal {
            tracing::warn!("Memory trigger activated: usage={:.2}%, level={:?}", memory_usage, level);
            Some(level)
        } else {
            None
        }
    }
    
    fn name(&self) -> &str {
        "memory"
    }
}
```

### 2.3 å»¶è¿Ÿè§¦å‘å™¨

```rust
/// å»¶è¿Ÿè§¦å‘å™¨
pub struct LatencyTrigger {
    metrics: Arc<MetricsCollector>,
    thresholds: LatencyThresholds,
}

#[derive(Debug, Clone)]
pub struct LatencyThresholds {
    pub light_ms: f64,
    pub moderate_ms: f64,
    pub heavy_ms: f64,
    pub emergency_ms: f64,
}

impl LatencyTrigger {
    pub fn new(metrics: Arc<MetricsCollector>, thresholds: LatencyThresholds) -> Self {
        Self {
            metrics,
            thresholds,
        }
    }
}

#[async_trait::async_trait]
impl DegradationTrigger for LatencyTrigger {
    async fn evaluate(&self) -> Option<DegradationLevel> {
        let load = self.metrics.get_current_load().await;
        let latency_ms = load.avg_latency_ms;
        
        let level = if latency_ms >= self.thresholds.emergency_ms {
            DegradationLevel::Emergency
        } else if latency_ms >= self.thresholds.heavy_ms {
            DegradationLevel::Heavy
        } else if latency_ms >= self.thresholds.moderate_ms {
            DegradationLevel::Moderate
        } else if latency_ms >= self.thresholds.light_ms {
            DegradationLevel::Light
        } else {
            DegradationLevel::Normal
        };
        
        if level > DegradationLevel::Normal {
            tracing::warn!("Latency trigger activated: latency={:.2}ms, level={:?}", latency_ms, level);
            Some(level)
        } else {
            None
        }
    }
    
    fn name(&self) -> &str {
        "latency"
    }
}
```

---

## 3. é™æµä¸ç†”æ–­

### 3.1 ä»¤ç‰Œæ¡¶é™æµå™¨

```rust
use governor::{Quota, RateLimiter as GovernorRateLimiter};
use governor::state::{InMemoryState, NotKeyed};
use governor::clock::DefaultClock;
use nonzero_ext::*;

/// é™æµå™¨
pub struct RateLimiter {
    limiter: Arc<GovernorRateLimiter<NotKeyed, InMemoryState, DefaultClock>>,
    max_qps: u64,
    burst_size: u32,
}

impl RateLimiter {
    pub fn new(max_qps: u64, burst_size: u32) -> Self {
        let quota = Quota::per_second(nonzero!(max_qps as u32))
            .allow_burst(nonzero!(burst_size));
        
        let limiter = Arc::new(GovernorRateLimiter::direct(quota));
        
        Self {
            limiter,
            max_qps,
            burst_size,
        }
    }
    
    /// æ£€æŸ¥æ˜¯å¦å…è®¸é€šè¿‡
    pub fn check(&self) -> bool {
        self.limiter.check().is_ok()
    }
    
    /// å¼‚æ­¥ç­‰å¾…ç›´åˆ°å…è®¸é€šè¿‡
    pub async fn wait(&self) {
        self.limiter.until_ready().await;
    }
    
    /// æ›´æ–°é™æµé…ç½®
    pub fn update(&mut self, max_qps: u64, burst_size: u32) {
        let quota = Quota::per_second(nonzero!(max_qps as u32))
            .allow_burst(nonzero!(burst_size));
        
        self.limiter = Arc::new(GovernorRateLimiter::direct(quota));
        self.max_qps = max_qps;
        self.burst_size = burst_size;
        
        tracing::info!("Rate limiter updated: max_qps={}, burst={}", max_qps, burst_size);
    }
}
```

### 3.2 ç†”æ–­å™¨

```rust
use std::sync::atomic::{AtomicU64, AtomicBool, Ordering};

/// ç†”æ–­å™¨
pub struct CircuitBreaker {
    /// ç†”æ–­å™¨çŠ¶æ€
    state: Arc<RwLock<CircuitState>>,
    
    /// é…ç½®
    config: CircuitBreakerConfig,
    
    /// ç»Ÿè®¡
    stats: Arc<CircuitStats>,
}

/// ç†”æ–­å™¨çŠ¶æ€
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum CircuitState {
    /// å…³é—­(æ­£å¸¸)
    Closed,
    
    /// æ‰“å¼€(ç†”æ–­)
    Open,
    
    /// åŠå¼€(è¯•æ¢)
    HalfOpen,
}

/// ç†”æ–­å™¨é…ç½®
#[derive(Debug, Clone)]
pub struct CircuitBreakerConfig {
    /// å¤±è´¥é˜ˆå€¼
    pub failure_threshold: u64,
    
    /// æˆåŠŸé˜ˆå€¼(åŠå¼€çŠ¶æ€)
    pub success_threshold: u64,
    
    /// è¶…æ—¶æ—¶é—´
    pub timeout: Duration,
    
    /// åŠå¼€è¶…æ—¶
    pub half_open_timeout: Duration,
}

/// ç†”æ–­å™¨ç»Ÿè®¡
#[derive(Debug)]
struct CircuitStats {
    total_requests: AtomicU64,
    failed_requests: AtomicU64,
    successful_requests: AtomicU64,
    last_failure_time: RwLock<Option<Instant>>,
}

impl CircuitBreaker {
    pub fn new(config: CircuitBreakerConfig) -> Self {
        Self {
            state: Arc::new(RwLock::new(CircuitState::Closed)),
            config,
            stats: Arc::new(CircuitStats {
                total_requests: AtomicU64::new(0),
                failed_requests: AtomicU64::new(0),
                successful_requests: AtomicU64::new(0),
                last_failure_time: RwLock::new(None),
            }),
        }
    }
    
    /// æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚
    pub async fn allow_request(&self) -> bool {
        let state = *self.state.read().await;
        
        match state {
            CircuitState::Closed => true,
            
            CircuitState::Open => {
                // æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›å…¥åŠå¼€çŠ¶æ€
                let last_failure = self.stats.last_failure_time.read().await;
                if let Some(last_time) = *last_failure {
                    if last_time.elapsed() >= self.config.timeout {
                        drop(last_failure);
                        let mut state_mut = self.state.write().await;
                        *state_mut = CircuitState::HalfOpen;
                        
                        tracing::info!("Circuit breaker transitioning to HalfOpen");
                        return true;
                    }
                }
                false
            }
            
            CircuitState::HalfOpen => {
                // åŠå¼€çŠ¶æ€å…è®¸å°‘é‡è¯·æ±‚
                true
            }
        }
    }
    
    /// è®°å½•æˆåŠŸ
    pub async fn record_success(&self) {
        self.stats.total_requests.fetch_add(1, Ordering::Relaxed);
        self.stats.successful_requests.fetch_add(1, Ordering::Relaxed);
        
        let state = *self.state.read().await;
        
        if state == CircuitState::HalfOpen {
            let success_count = self.stats.successful_requests.load(Ordering::Relaxed);
            
            if success_count >= self.config.success_threshold {
                drop(state);
                let mut state_mut = self.state.write().await;
                *state_mut = CircuitState::Closed;
                
                // é‡ç½®ç»Ÿè®¡
                self.stats.failed_requests.store(0, Ordering::Relaxed);
                self.stats.successful_requests.store(0, Ordering::Relaxed);
                
                tracing::info!("Circuit breaker closed");
            }
        }
    }
    
    /// è®°å½•å¤±è´¥
    pub async fn record_failure(&self) {
        self.stats.total_requests.fetch_add(1, Ordering::Relaxed);
        self.stats.failed_requests.fetch_add(1, Ordering::Relaxed);
        
        let mut last_failure = self.stats.last_failure_time.write().await;
        *last_failure = Some(Instant::now());
        drop(last_failure);
        
        let state = *self.state.read().await;
        
        match state {
            CircuitState::Closed => {
                let failed_count = self.stats.failed_requests.load(Ordering::Relaxed);
                
                if failed_count >= self.config.failure_threshold {
                    drop(state);
                    let mut state_mut = self.state.write().await;
                    *state_mut = CircuitState::Open;
                    
                    tracing::warn!("Circuit breaker opened due to failures: {}", failed_count);
                }
            }
            
            CircuitState::HalfOpen => {
                // åŠå¼€çŠ¶æ€ä¸‹å¤±è´¥ï¼Œç›´æ¥æ‰“å¼€
                drop(state);
                let mut state_mut = self.state.write().await;
                *state_mut = CircuitState::Open;
                
                tracing::warn!("Circuit breaker reopened from HalfOpen");
            }
            
            CircuitState::Open => {
                // å·²ç»æ‰“å¼€ï¼Œæ— éœ€æ“ä½œ
            }
        }
    }
    
    /// è·å–å½“å‰çŠ¶æ€
    pub async fn get_state(&self) -> CircuitState {
        *self.state.read().await
    }
}
```

---

## 4. å¹³æ»‘å‡çº§ç­–ç•¥

### 4.1 æ»šåŠ¨å‡çº§

```rust
/// æ»šåŠ¨å‡çº§æ§åˆ¶å™¨
pub struct RollingUpgradeController {
    /// å®ä¾‹åˆ—è¡¨
    instances: Arc<RwLock<Vec<Instance>>>,
    
    /// å‡çº§é…ç½®
    config: RollingUpgradeConfig,
    
    /// å¥åº·æ£€æŸ¥å™¨
    health_checker: Arc<HealthChecker>,
}

/// å‡çº§é…ç½®
#[derive(Debug, Clone)]
pub struct RollingUpgradeConfig {
    /// æ‰¹æ¬¡å¤§å°
    pub batch_size: usize,
    
    /// æ‰¹æ¬¡é—´ç­‰å¾…æ—¶é—´
    pub batch_wait: Duration,
    
    /// å¥åº·æ£€æŸ¥é—´éš”
    pub health_check_interval: Duration,
    
    /// æœ€å¤§å¹¶å‘å‡çº§æ•°
    pub max_concurrent: usize,
    
    /// å›æ»šé˜ˆå€¼
    pub rollback_threshold: f64,
}

/// å®ä¾‹
#[derive(Debug, Clone)]
pub struct Instance {
    pub id: String,
    pub address: String,
    pub version: String,
    pub status: InstanceStatus,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum InstanceStatus {
    Running,
    Upgrading,
    Draining,
    Stopped,
    Failed,
}

impl RollingUpgradeController {
    pub fn new(
        instances: Vec<Instance>,
        config: RollingUpgradeConfig,
        health_checker: Arc<HealthChecker>,
    ) -> Self {
        Self {
            instances: Arc::new(RwLock::new(instances)),
            config,
            health_checker,
        }
    }
    
    /// æ‰§è¡Œæ»šåŠ¨å‡çº§
    pub async fn execute_upgrade(&self, target_version: String) -> Result<(), Box<dyn std::error::Error>> {
        let instances = self.instances.read().await;
        let total = instances.len();
        let batches = (total + self.config.batch_size - 1) / self.config.batch_size;
        
        tracing::info!(
            "Starting rolling upgrade to version {}: {} instances in {} batches",
            target_version,
            total,
            batches
        );
        
        for batch_idx in 0..batches {
            let start_idx = batch_idx * self.config.batch_size;
            let end_idx = ((batch_idx + 1) * self.config.batch_size).min(total);
            
            tracing::info!("Upgrading batch {}/{}: instances {}-{}", batch_idx + 1, batches, start_idx, end_idx);
            
            // å‡çº§å½“å‰æ‰¹æ¬¡
            self.upgrade_batch(start_idx, end_idx, &target_version).await?;
            
            // å¥åº·æ£€æŸ¥
            if !self.check_batch_health(start_idx, end_idx).await? {
                tracing::error!("Health check failed for batch {}, initiating rollback", batch_idx + 1);
                self.rollback(batch_idx, &target_version).await?;
                return Err("Upgrade failed, rolled back".into());
            }
            
            // ç­‰å¾…ä¸‹ä¸€æ‰¹æ¬¡
            if batch_idx < batches - 1 {
                tokio::time::sleep(self.config.batch_wait).await;
            }
        }
        
        tracing::info!("Rolling upgrade completed successfully");
        Ok(())
    }
    
    /// å‡çº§æ‰¹æ¬¡
    async fn upgrade_batch(
        &self,
        start: usize,
        end: usize,
        target_version: &str,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let mut instances = self.instances.write().await;
        let batch: Vec<_> = instances[start..end].iter_mut().collect();
        
        // å¹¶å‘å‡çº§
        let mut tasks = Vec::new();
        
        for instance in batch {
            instance.status = InstanceStatus::Draining;
            
            let id = instance.id.clone();
            let version = target_version.to_string();
            
            let task = tokio::spawn(async move {
                // 1. æ’ç©ºæµé‡
                Self::drain_traffic(&id).await?;
                
                // 2. åœæ­¢å®ä¾‹
                Self::stop_instance(&id).await?;
                
                // 3. å‡çº§
                Self::upgrade_instance(&id, &version).await?;
                
                // 4. å¯åŠ¨å®ä¾‹
                Self::start_instance(&id).await?;
                
                Ok::<_, Box<dyn std::error::Error + Send + Sync>>(())
            });
            
            tasks.push(task);
            
            // æ§åˆ¶å¹¶å‘æ•°
            if tasks.len() >= self.config.max_concurrent {
                futures::future::try_join_all(tasks).await?;
                tasks = Vec::new();
            }
        }
        
        // ç­‰å¾…å‰©ä½™ä»»åŠ¡
        if !tasks.is_empty() {
            futures::future::try_join_all(tasks).await?;
        }
        
        // æ›´æ–°çŠ¶æ€
        for instance in &mut instances[start..end] {
            instance.version = target_version.to_string();
            instance.status = InstanceStatus::Running;
        }
        
        Ok(())
    }
    
    /// æ£€æŸ¥æ‰¹æ¬¡å¥åº·çŠ¶æ€
    async fn check_batch_health(
        &self,
        start: usize,
        end: usize,
    ) -> Result<bool, Box<dyn std::error::Error>> {
        let instances = self.instances.read().await;
        let batch = &instances[start..end];
        
        // å¤šæ¬¡å¥åº·æ£€æŸ¥
        for _ in 0..3 {
            tokio::time::sleep(self.config.health_check_interval).await;
            
            let mut healthy_count = 0;
            
            for instance in batch {
                if self.health_checker.check_instance(&instance.id).await {
                    healthy_count += 1;
                }
            }
            
            let health_ratio = healthy_count as f64 / batch.len() as f64;
            
            if health_ratio < self.config.rollback_threshold {
                return Ok(false);
            }
        }
        
        Ok(true)
    }
    
    /// å›æ»š
    async fn rollback(
        &self,
        failed_batch: usize,
        _target_version: &str,
    ) -> Result<(), Box<dyn std::error::Error>> {
        tracing::warn!("Initiating rollback from batch {}", failed_batch);
        
        // ç®€åŒ–å®ç°ï¼šå›æ»šåˆ°ä¹‹å‰çš„ç‰ˆæœ¬
        let mut instances = self.instances.write().await;
        
        for (idx, instance) in instances.iter_mut().enumerate() {
            if idx / self.config.batch_size <= failed_batch {
                instance.status = InstanceStatus::Failed;
                // å®é™…åº”è¯¥å›æ»šåˆ°previous_version
            }
        }
        
        Ok(())
    }
    
    // è¾…åŠ©æ–¹æ³•
    async fn drain_traffic(_instance_id: &str) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        tokio::time::sleep(Duration::from_secs(5)).await;
        Ok(())
    }
    
    async fn stop_instance(_instance_id: &str) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        tokio::time::sleep(Duration::from_secs(2)).await;
        Ok(())
    }
    
    async fn upgrade_instance(_instance_id: &str, _version: &str) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        tokio::time::sleep(Duration::from_secs(10)).await;
        Ok(())
    }
    
    async fn start_instance(_instance_id: &str) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
        tokio::time::sleep(Duration::from_secs(3)).await;
        Ok(())
    }
}
```

---

## 5. ç°åº¦å‘å¸ƒ

### 5.1 é‡‘ä¸é›€å‘å¸ƒ

```rust
/// é‡‘ä¸é›€å‘å¸ƒæ§åˆ¶å™¨
pub struct CanaryController {
    /// æµé‡åˆ†é…
    traffic_split: Arc<RwLock<TrafficSplit>>,
    
    /// æŒ‡æ ‡æ¯”è¾ƒå™¨
    metrics_comparator: Arc<MetricsComparator>,
    
    /// é…ç½®
    config: CanaryConfig,
}

/// æµé‡åˆ†é…
#[derive(Debug, Clone)]
pub struct TrafficSplit {
    /// ç¨³å®šç‰ˆæœ¬æƒé‡
    pub stable_weight: u32,
    
    /// é‡‘ä¸é›€ç‰ˆæœ¬æƒé‡
    pub canary_weight: u32,
}

/// é‡‘ä¸é›€é…ç½®
#[derive(Debug, Clone)]
pub struct CanaryConfig {
    /// åˆå§‹é‡‘ä¸é›€æµé‡ç™¾åˆ†æ¯”
    pub initial_traffic: f64,
    
    /// æµé‡é€’å¢æ­¥é•¿
    pub traffic_increment: f64,
    
    /// é€’å¢é—´éš”
    pub increment_interval: Duration,
    
    /// æŒ‡æ ‡é˜ˆå€¼
    pub metrics_threshold: MetricsThreshold,
}

#[derive(Debug, Clone)]
pub struct MetricsThreshold {
    pub max_error_rate_increase: f64,
    pub max_latency_increase: f64,
}

impl CanaryController {
    pub fn new(
        config: CanaryConfig,
        metrics_comparator: Arc<MetricsComparator>,
    ) -> Self {
        let initial_canary_weight = (config.initial_traffic * 100.0) as u32;
        let stable_weight = 100 - initial_canary_weight;
        
        Self {
            traffic_split: Arc::new(RwLock::new(TrafficSplit {
                stable_weight,
                canary_weight: initial_canary_weight,
            })),
            metrics_comparator,
            config,
        }
    }
    
    /// å¼€å§‹é‡‘ä¸é›€å‘å¸ƒ
    pub async fn start_canary(&self) -> Result<(), Box<dyn std::error::Error>> {
        tracing::info!("Starting canary deployment with {}% traffic", self.config.initial_traffic * 100.0);
        
        loop {
            // ç­‰å¾…è§‚å¯Ÿé—´éš”
            tokio::time::sleep(self.config.increment_interval).await;
            
            // æ¯”è¾ƒæŒ‡æ ‡
            let comparison = self.metrics_comparator.compare().await?;
            
            if !comparison.is_healthy(&self.config.metrics_threshold) {
                tracing::error!("Canary metrics unhealthy, rolling back");
                self.rollback_canary().await?;
                return Err("Canary deployment failed".into());
            }
            
            // å¢åŠ é‡‘ä¸é›€æµé‡
            let current_split = self.traffic_split.read().await.clone();
            let new_canary_weight = current_split.canary_weight + (self.config.traffic_increment * 100.0) as u32;
            
            if new_canary_weight >= 100 {
                // é‡‘ä¸é›€æˆä¸ºç¨³å®šç‰ˆæœ¬
                self.promote_canary().await?;
                tracing::info!("Canary promoted to stable");
                break;
            }
            
            let mut split = self.traffic_split.write().await;
            split.canary_weight = new_canary_weight;
            split.stable_weight = 100 - new_canary_weight;
            
            tracing::info!("Canary traffic increased to {}%", new_canary_weight);
        }
        
        Ok(())
    }
    
    /// å›æ»šé‡‘ä¸é›€
    async fn rollback_canary(&self) -> Result<(), Box<dyn std::error::Error>> {
        let mut split = self.traffic_split.write().await;
        split.canary_weight = 0;
        split.stable_weight = 100;
        
        tracing::info!("Canary rolled back, 100% traffic to stable");
        Ok(())
    }
    
    /// æå‡é‡‘ä¸é›€ä¸ºç¨³å®šç‰ˆæœ¬
    async fn promote_canary(&self) -> Result<(), Box<dyn std::error::Error>> {
        let mut split = self.traffic_split.write().await;
        split.stable_weight = 0;
        split.canary_weight = 100;
        
        // å®é™…åº”è¯¥äº¤æ¢ç‰ˆæœ¬æ ‡ç­¾
        Ok(())
    }
    
    /// è·å–å½“å‰æµé‡åˆ†é…
    pub async fn get_traffic_split(&self) -> TrafficSplit {
        self.traffic_split.read().await.clone()
    }
}

/// æŒ‡æ ‡æ¯”è¾ƒå™¨
pub struct MetricsComparator {
    stable_metrics: Arc<RwLock<VersionMetrics>>,
    canary_metrics: Arc<RwLock<VersionMetrics>>,
}

#[derive(Debug, Clone, Default)]
pub struct VersionMetrics {
    pub error_rate: f64,
    pub avg_latency_ms: f64,
    pub p99_latency_ms: f64,
    pub request_count: u64,
}

#[derive(Debug)]
pub struct MetricsComparison {
    pub error_rate_diff: f64,
    pub latency_diff: f64,
}

impl MetricsComparison {
    fn is_healthy(&self, threshold: &MetricsThreshold) -> bool {
        self.error_rate_diff <= threshold.max_error_rate_increase &&
        self.latency_diff <= threshold.max_latency_increase
    }
}

impl MetricsComparator {
    pub fn new() -> Self {
        Self {
            stable_metrics: Arc::new(RwLock::new(VersionMetrics::default())),
            canary_metrics: Arc::new(RwLock::new(VersionMetrics::default())),
        }
    }
    
    /// æ¯”è¾ƒæŒ‡æ ‡
    pub async fn compare(&self) -> Result<MetricsComparison, Box<dyn std::error::Error>> {
        let stable = self.stable_metrics.read().await;
        let canary = self.canary_metrics.read().await;
        
        Ok(MetricsComparison {
            error_rate_diff: canary.error_rate - stable.error_rate,
            latency_diff: canary.avg_latency_ms - stable.avg_latency_ms,
        })
    }
}
```

---

## 6. å®Œæ•´å®ç°

### 6.1 ç»¼åˆç¤ºä¾‹

```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt::init();
    
    // åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨
    let metrics = Arc::new(MetricsCollector::new());
    
    // åˆ›å»ºé™çº§æ§åˆ¶å™¨
    let degradation_config = DegradationConfig {
        enabled: true,
        cpu_threshold: 80.0,
        memory_threshold: 85.0,
        latency_threshold_ms: 100.0,
        error_rate_threshold: 0.05,
        queue_depth_threshold: 10000,
        recovery_threshold: 0.6,
        evaluation_interval: Duration::from_secs(30),
        strategies: vec![
            DegradationStrategy::ReduceSamplingRate { target_rate: 0.1 },
            DegradationStrategy::DisableNonCritical {
                critical_services: vec!["payment".to_string(), "checkout".to_string()],
            },
        ],
    };
    
    let degradation_controller = DegradationController::new(
        degradation_config,
        Arc::clone(&metrics),
    );
    
    // æ·»åŠ è§¦å‘å™¨
    {
        let mut triggers = degradation_controller.triggers.write().await;
        triggers.push(Box::new(CpuTrigger::new(CpuThresholds {
            light: 70.0,
            moderate: 80.0,
            heavy: 90.0,
            emergency: 95.0,
        })));
        
        triggers.push(Box::new(MemoryTrigger::new(MemoryThresholds {
            light: 75.0,
            moderate: 85.0,
            heavy: 92.0,
            emergency: 97.0,
        })));
    }
    
    degradation_controller.start().await;
    
    // åˆ›å»ºé™æµå™¨
    let mut rate_limiter = RateLimiter::new(10000, 1000);
    
    // åˆ›å»ºç†”æ–­å™¨
    let circuit_breaker = CircuitBreaker::new(CircuitBreakerConfig {
        failure_threshold: 10,
        success_threshold: 5,
        timeout: Duration::from_secs(30),
        half_open_timeout: Duration::from_secs(10),
    });
    
    tracing::info!("Degradation and protection systems started");
    
    // æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†
    for i in 0..100 {
        // æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚
        if !rate_limiter.check() {
            tracing::warn!("Request {} rate limited", i);
            continue;
        }
        
        if !circuit_breaker.allow_request().await {
            tracing::warn!("Request {} blocked by circuit breaker", i);
            continue;
        }
        
        // æ£€æŸ¥é™çº§çº§åˆ«
        let level = degradation_controller.get_current_level().await;
        let sampling_rate = degradation_controller.get_recommended_sampling_rate().await;
        
        tracing::info!(
            "Request {}: degradation_level={:?}, sampling_rate={:.4}",
            i,
            level,
            sampling_rate
        );
        
        // æ¨¡æ‹Ÿå¤„ç†
        let success = (i % 10) != 0; // 10% å¤±è´¥ç‡
        
        if success {
            circuit_breaker.record_success().await;
        } else {
            circuit_breaker.record_failure().await;
        }
        
        tokio::time::sleep(Duration::from_millis(100)).await;
    }
    
    Ok(())
}
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†OTLPé™çº§ä¸å‡çº§ç­–ç•¥çš„å®Œæ•´Rustå®ç°ï¼ŒåŒ…æ‹¬:

âœ… **é™çº§ç­–ç•¥**:

- è‡ªé€‚åº”é™çº§(CPU/å†…å­˜/å»¶è¿Ÿè§¦å‘)
- åˆ†çº§é™çº§(Normal/Light/Moderate/Heavy/Emergency)
- è‡ªåŠ¨æ¢å¤æœºåˆ¶

âœ… **ä¿æŠ¤æœºåˆ¶**:

- ä»¤ç‰Œæ¡¶é™æµ
- ç†”æ–­å™¨(Circuit Breaker)
- è¿‡è½½ä¿æŠ¤

âœ… **å‡çº§ç­–ç•¥**:

- æ»šåŠ¨å‡çº§(æ‰¹æ¬¡å‡çº§)
- é‡‘ä¸é›€å‘å¸ƒ(ç°åº¦å‘å¸ƒ)
- è‡ªåŠ¨å›æ»š

âœ… **ç”Ÿäº§å°±ç»ª**:

- å®Œæ•´çš„ç›‘æ§æŒ‡æ ‡
- å¥åº·æ£€æŸ¥æœºåˆ¶
- è¯¦ç»†çš„æ—¥å¿—è®°å½•

---

**å‚è€ƒèµ„æº**:

- [Site Reliability Engineering](https://sre.google/books/)
- [Release Engineering](https://en.wikipedia.org/wiki/Release_engineering)
- [Chaos Engineering](https://principlesofchaos.org/)
