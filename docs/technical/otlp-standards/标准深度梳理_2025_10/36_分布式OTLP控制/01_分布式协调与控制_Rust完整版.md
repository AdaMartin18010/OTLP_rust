# åˆ†å¸ƒå¼OTLPåè°ƒä¸æ§åˆ¶ - Rustå®Œæ•´å®ç°

> **Rustç‰ˆæœ¬**: 1.90+  
> **æ ¸å¿ƒä¾èµ–**: tokio 1.47.1, etcd-client 0.15, consul 0.6  
> **OpenTelemetry**: 0.31.0  
> **æœ€åæ›´æ–°**: 2025å¹´10æœˆ9æ—¥

---

## ğŸ“‹ ç›®å½•

- [åˆ†å¸ƒå¼OTLPåè°ƒä¸æ§åˆ¶ - Rustå®Œæ•´å®ç°](#åˆ†å¸ƒå¼otlpåè°ƒä¸æ§åˆ¶---rustå®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. åˆ†å¸ƒå¼OTLPæ¶æ„æ¦‚è¿°](#1-åˆ†å¸ƒå¼otlpæ¶æ„æ¦‚è¿°)
    - [1.1 æ¶æ„è®¾è®¡](#11-æ¶æ„è®¾è®¡)
    - [1.2 æ ¸å¿ƒç»„ä»¶](#12-æ ¸å¿ƒç»„ä»¶)
  - [2. å…¨å±€æ„ŸçŸ¥æœºåˆ¶](#2-å…¨å±€æ„ŸçŸ¥æœºåˆ¶)
    - [2.1 å…¨å±€æ‹“æ‰‘æ„ŸçŸ¥](#21-å…¨å±€æ‹“æ‰‘æ„ŸçŸ¥)
    - [2.2 å…¨å±€é‡‡æ ·åè°ƒ](#22-å…¨å±€é‡‡æ ·åè°ƒ)
  - [3. æœ¬åœ°æ„ŸçŸ¥ä¸ä¼˜åŒ–](#3-æœ¬åœ°æ„ŸçŸ¥ä¸ä¼˜åŒ–)
    - [3.1 æœ¬åœ°è´Ÿè½½æ„ŸçŸ¥](#31-æœ¬åœ°è´Ÿè½½æ„ŸçŸ¥)
    - [3.2 æœ¬åœ°æ™ºèƒ½è·¯ç”±](#32-æœ¬åœ°æ™ºèƒ½è·¯ç”±)
  - [4. åˆ†å¸ƒå¼åè°ƒæœåŠ¡](#4-åˆ†å¸ƒå¼åè°ƒæœåŠ¡)
    - [4.1 åŸºäºetcdçš„åè°ƒå®ç°](#41-åŸºäºetcdçš„åè°ƒå®ç°)
  - [5. é…ç½®ç®¡ç†ä¸çƒ­æ›´æ–°](#5-é…ç½®ç®¡ç†ä¸çƒ­æ›´æ–°)
    - [5.1 é…ç½®çƒ­æ›´æ–°å®ç°](#51-é…ç½®çƒ­æ›´æ–°å®ç°)
  - [6. æœåŠ¡å‘ç°ä¸æ³¨å†Œ](#6-æœåŠ¡å‘ç°ä¸æ³¨å†Œ)
    - [6.1 æœåŠ¡æ³¨å†Œå®ç°](#61-æœåŠ¡æ³¨å†Œå®ç°)
  - [7. åˆ†å¸ƒå¼é”ä¸é€‰ä¸¾](#7-åˆ†å¸ƒå¼é”ä¸é€‰ä¸¾)
    - [7.1 åˆ†å¸ƒå¼é”å®ç°](#71-åˆ†å¸ƒå¼é”å®ç°)
    - [7.2 Leaderé€‰ä¸¾å®ç°](#72-leaderé€‰ä¸¾å®ç°)
  - [8. å®Œæ•´å®ç°ç¤ºä¾‹](#8-å®Œæ•´å®ç°ç¤ºä¾‹)
    - [8.1 ç»¼åˆç¤ºä¾‹](#81-ç»¼åˆç¤ºä¾‹)
  - [9. é…ç½®ç¤ºä¾‹](#9-é…ç½®ç¤ºä¾‹)
    - [9.1 å…¨å±€é…ç½®ç¤ºä¾‹](#91-å…¨å±€é…ç½®ç¤ºä¾‹)
  - [10. æ€§èƒ½ä¼˜åŒ–](#10-æ€§èƒ½ä¼˜åŒ–)
    - [10.1 æ‰¹å¤„ç†ä¼˜åŒ–](#101-æ‰¹å¤„ç†ä¼˜åŒ–)
  - [11. ç›‘æ§ä¸å‘Šè­¦](#11-ç›‘æ§ä¸å‘Šè­¦)
    - [11.1 æŒ‡æ ‡æ”¶é›†](#111-æŒ‡æ ‡æ”¶é›†)
  - [12. æ€»ç»“](#12-æ€»ç»“)

---

## 1. åˆ†å¸ƒå¼OTLPæ¶æ„æ¦‚è¿°

### 1.1 æ¶æ„è®¾è®¡

```text
åˆ†å¸ƒå¼OTLPæ¶æ„å±‚æ¬¡:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å…¨å±€æ§åˆ¶å¹³é¢ (Global Control Plane)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  é…ç½®ä¸­å¿ƒ (etcd/Consul)                           â”‚   â”‚
â”‚  â”‚  - å…¨å±€é…ç½®ç®¡ç†                                    â”‚  â”‚
â”‚  â”‚  - é‡‡æ ·ç­–ç•¥åŒæ­¥                                    â”‚  â”‚
â”‚  â”‚  - æ‹“æ‰‘æ„ŸçŸ¥                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              åŒºåŸŸæ§åˆ¶å™¨ (Regional Controllers)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Region-1 â”‚  â”‚ Region-2 â”‚  â”‚ Region-N â”‚               â”‚
â”‚  â”‚ è´Ÿè½½å‡è¡¡  â”‚  â”‚ è´Ÿè½½å‡è¡¡  â”‚  â”‚ è´Ÿè½½å‡è¡¡  â”‚              â”‚
â”‚  â”‚ æœ¬åœ°æ„ŸçŸ¥  â”‚  â”‚ æœ¬åœ°æ„ŸçŸ¥  â”‚  â”‚ æœ¬åœ°æ„ŸçŸ¥  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ•°æ®å¹³é¢ (Data Plane)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚Collector1â”‚  â”‚Collector2â”‚  â”‚CollectorNâ”‚               â”‚
â”‚  â”‚ Receiver â”‚  â”‚ Receiver â”‚  â”‚ Receiver â”‚               â”‚
â”‚  â”‚Processor â”‚  â”‚Processor â”‚  â”‚Processor â”‚               â”‚
â”‚  â”‚ Exporter â”‚  â”‚ Exporter â”‚  â”‚ Exporter â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒç»„ä»¶

```rust
use tokio::sync::{RwLock, broadcast, mpsc};
use std::sync::Arc;
use std::collections::HashMap;
use serde::{Serialize, Deserialize};

/// åˆ†å¸ƒå¼OTLPæ§åˆ¶å™¨
pub struct DistributedOtlpController {
    /// å…¨å±€é…ç½®
    global_config: Arc<RwLock<GlobalConfig>>,
    
    /// åŒºåŸŸæ§åˆ¶å™¨
    regional_controllers: Arc<RwLock<HashMap<String, RegionalController>>>,
    
    /// é…ç½®å˜æ›´é€šçŸ¥
    config_changes: broadcast::Sender<ConfigChange>,
    
    /// å¥åº·çŠ¶æ€
    health: Arc<RwLock<HealthStatus>>,
    
    /// åè°ƒæœåŠ¡å®¢æˆ·ç«¯
    coordinator: Arc<dyn CoordinatorClient>,
}

/// å…¨å±€é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GlobalConfig {
    /// å…¨å±€é‡‡æ ·ç‡
    pub global_sampling_rate: f64,
    
    /// é‡‡æ ·ç­–ç•¥
    pub sampling_strategies: Vec<SamplingStrategy>,
    
    /// é™çº§é…ç½®
    pub degradation: DegradationConfig,
    
    /// é™æµé…ç½®
    pub rate_limiting: RateLimitingConfig,
    
    /// è·¯ç”±è§„åˆ™
    pub routing_rules: Vec<RoutingRule>,
    
    /// å…ƒæ•°æ®
    pub metadata: HashMap<String, String>,
    
    /// ç‰ˆæœ¬å·
    pub version: u64,
}

/// åŒºåŸŸæ§åˆ¶å™¨
pub struct RegionalController {
    /// åŒºåŸŸID
    region_id: String,
    
    /// æœ¬åœ°é…ç½®
    local_config: Arc<RwLock<LocalConfig>>,
    
    /// Collectorå®ä¾‹
    collectors: Arc<RwLock<HashMap<String, CollectorInfo>>>,
    
    /// è´Ÿè½½å‡è¡¡å™¨
    load_balancer: Arc<dyn LoadBalancer>,
    
    /// å¥åº·æ£€æŸ¥å™¨
    health_checker: Arc<HealthChecker>,
    
    /// æŒ‡æ ‡æ”¶é›†å™¨
    metrics: Arc<MetricsCollector>,
}

/// æœ¬åœ°é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocalConfig {
    /// åŒºåŸŸç‰¹å®šé‡‡æ ·ç‡
    pub regional_sampling_rate: f64,
    
    /// æœ¬åœ°è·¯ç”±åå¥½
    pub local_routing_preference: Vec<String>,
    
    /// ç¼“å†²åŒºå¤§å°
    pub buffer_size: usize,
    
    /// æ‰¹å¤„ç†é…ç½®
    pub batch_config: BatchConfig,
    
    /// é‡è¯•ç­–ç•¥
    pub retry_policy: RetryPolicy,
}

impl DistributedOtlpController {
    /// åˆ›å»ºæ–°çš„åˆ†å¸ƒå¼æ§åˆ¶å™¨
    pub async fn new(
        coordinator_url: &str,
        region_id: String,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        let coordinator = create_coordinator_client(coordinator_url).await?;
        
        let (config_tx, _) = broadcast::channel(1000);
        
        let global_config = Arc::new(RwLock::new(
            coordinator.get_global_config().await?
        ));
        
        let health = Arc::new(RwLock::new(HealthStatus::healthy()));
        
        Ok(Self {
            global_config,
            regional_controllers: Arc::new(RwLock::new(HashMap::new())),
            config_changes: config_tx,
            health,
            coordinator,
        })
    }
    
    /// å¯åŠ¨æ§åˆ¶å™¨
    pub async fn start(&self) -> Result<(), Box<dyn std::error::Error>> {
        // å¯åŠ¨é…ç½®ç›‘å¬
        self.start_config_watcher().await?;
        
        // å¯åŠ¨å¥åº·æ£€æŸ¥
        self.start_health_checks().await?;
        
        // å¯åŠ¨æŒ‡æ ‡æ”¶é›†
        self.start_metrics_collection().await?;
        
        Ok(())
    }
    
    /// ç›‘å¬é…ç½®å˜æ›´
    async fn start_config_watcher(&self) -> Result<(), Box<dyn std::error::Error>> {
        let coordinator = Arc::clone(&self.coordinator);
        let global_config = Arc::clone(&self.global_config);
        let config_changes = self.config_changes.clone();
        
        tokio::spawn(async move {
            let mut watch_stream = coordinator.watch_config().await.unwrap();
            
            while let Some(change) = watch_stream.recv().await {
                // æ›´æ–°å…¨å±€é…ç½®
                let mut config = global_config.write().await;
                config.apply_change(&change);
                
                // å¹¿æ’­é…ç½®å˜æ›´
                let _ = config_changes.send(change);
                
                tracing::info!("Config updated: {:?}", config.version);
            }
        });
        
        Ok(())
    }
    
    /// æ³¨å†ŒåŒºåŸŸæ§åˆ¶å™¨
    pub async fn register_region(
        &self,
        region_id: String,
        local_config: LocalConfig,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let regional_controller = RegionalController::new(
            region_id.clone(),
            local_config,
            self.coordinator.clone(),
        ).await?;
        
        let mut controllers = self.regional_controllers.write().await;
        controllers.insert(region_id.clone(), regional_controller);
        
        // æ³¨å†Œåˆ°åè°ƒæœåŠ¡
        self.coordinator.register_region(&region_id).await?;
        
        Ok(())
    }
}
```

---

## 2. å…¨å±€æ„ŸçŸ¥æœºåˆ¶

### 2.1 å…¨å±€æ‹“æ‰‘æ„ŸçŸ¥

```rust
use std::time::{Duration, Instant};
use tokio::time::interval;

/// å…¨å±€æ‹“æ‰‘ç®¡ç†å™¨
pub struct GlobalTopologyManager {
    /// æ‹“æ‰‘çŠ¶æ€
    topology: Arc<RwLock<Topology>>,
    
    /// æ›´æ–°é—´éš”
    update_interval: Duration,
    
    /// åè°ƒå™¨
    coordinator: Arc<dyn CoordinatorClient>,
}

/// æ‹“æ‰‘ç»“æ„
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Topology {
    /// åŒºåŸŸåˆ—è¡¨
    pub regions: HashMap<String, RegionTopology>,
    
    /// å…¨å±€ç»Ÿè®¡
    pub global_stats: GlobalStats,
    
    /// æ›´æ–°æ—¶é—´
    pub last_updated: Instant,
}

/// åŒºåŸŸæ‹“æ‰‘
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RegionTopology {
    /// åŒºåŸŸID
    pub region_id: String,
    
    /// Collectoråˆ—è¡¨
    pub collectors: Vec<CollectorTopology>,
    
    /// åŒºåŸŸç»Ÿè®¡
    pub stats: RegionStats,
    
    /// å¥åº·çŠ¶æ€
    pub health: RegionHealth,
}

/// Collectoræ‹“æ‰‘
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CollectorTopology {
    /// Collector ID
    pub id: String,
    
    /// åœ°å€
    pub address: String,
    
    /// å®¹é‡
    pub capacity: Capacity,
    
    /// å½“å‰è´Ÿè½½
    pub current_load: Load,
    
    /// å¥åº·çŠ¶æ€
    pub health: HealthStatus,
}

/// å…¨å±€ç»Ÿè®¡
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GlobalStats {
    /// æ€»spanæ•°
    pub total_spans: u64,
    
    /// æ€»å­—èŠ‚æ•°
    pub total_bytes: u64,
    
    /// å¹³å‡å»¶è¿Ÿ
    pub avg_latency_ms: f64,
    
    /// é”™è¯¯ç‡
    pub error_rate: f64,
    
    /// é‡‡æ ·ç‡
    pub effective_sampling_rate: f64,
}

impl GlobalTopologyManager {
    pub fn new(
        coordinator: Arc<dyn CoordinatorClient>,
        update_interval: Duration,
    ) -> Self {
        Self {
            topology: Arc::new(RwLock::new(Topology::default())),
            update_interval,
            coordinator,
        }
    }
    
    /// å¯åŠ¨æ‹“æ‰‘æ›´æ–°
    pub async fn start(&self) {
        let topology = Arc::clone(&self.topology);
        let coordinator = Arc::clone(&self.coordinator);
        let update_interval = self.update_interval;
        
        tokio::spawn(async move {
            let mut ticker = interval(update_interval);
            
            loop {
                ticker.tick().await;
                
                match Self::update_topology(&coordinator).await {
                    Ok(new_topology) => {
                        let mut topo = topology.write().await;
                        *topo = new_topology;
                        
                        tracing::debug!("Topology updated: {} regions", topo.regions.len());
                    }
                    Err(e) => {
                        tracing::error!("Failed to update topology: {}", e);
                    }
                }
            }
        });
    }
    
    /// æ›´æ–°æ‹“æ‰‘ä¿¡æ¯
    async fn update_topology(
        coordinator: &Arc<dyn CoordinatorClient>,
    ) -> Result<Topology, Box<dyn std::error::Error>> {
        // è·å–æ‰€æœ‰åŒºåŸŸ
        let regions = coordinator.list_regions().await?;
        
        let mut region_topologies = HashMap::new();
        let mut global_stats = GlobalStats::default();
        
        for region_id in regions {
            // è·å–åŒºåŸŸè¯¦æƒ…
            let region = coordinator.get_region(&region_id).await?;
            
            // è·å–Collectoråˆ—è¡¨
            let collectors = coordinator.list_collectors(&region_id).await?;
            
            let mut collector_topologies = Vec::new();
            
            for collector_id in collectors {
                let collector = coordinator.get_collector(&collector_id).await?;
                
                collector_topologies.push(CollectorTopology {
                    id: collector.id,
                    address: collector.address,
                    capacity: collector.capacity,
                    current_load: collector.current_load,
                    health: collector.health,
                });
                
                // ç´¯åŠ å…¨å±€ç»Ÿè®¡
                global_stats.total_spans += collector.stats.spans;
                global_stats.total_bytes += collector.stats.bytes;
            }
            
            region_topologies.insert(
                region_id.clone(),
                RegionTopology {
                    region_id: region_id.clone(),
                    collectors: collector_topologies,
                    stats: region.stats,
                    health: region.health,
                },
            );
        }
        
        // è®¡ç®—å¹³å‡æŒ‡æ ‡
        if !region_topologies.is_empty() {
            let total_collectors: usize = region_topologies.values()
                .map(|r| r.collectors.len())
                .sum();
            
            if total_collectors > 0 {
                global_stats.avg_latency_ms /= total_collectors as f64;
            }
        }
        
        Ok(Topology {
            regions: region_topologies,
            global_stats,
            last_updated: Instant::now(),
        })
    }
    
    /// è·å–æœ€ä¼˜Collector
    pub async fn get_best_collector(
        &self,
        region_id: Option<&str>,
        criteria: &SelectionCriteria,
    ) -> Option<CollectorTopology> {
        let topology = self.topology.read().await;
        
        let collectors: Vec<&CollectorTopology> = if let Some(rid) = region_id {
            // ä¼˜å…ˆé€‰æ‹©æŒ‡å®šåŒºåŸŸ
            topology.regions.get(rid)
                .map(|r| r.collectors.iter().collect())
                .unwrap_or_default()
        } else {
            // å…¨å±€é€‰æ‹©
            topology.regions.values()
                .flat_map(|r| r.collectors.iter())
                .collect()
        };
        
        // æ ¹æ®æ ‡å‡†é€‰æ‹©æœ€ä¼˜Collector
        collectors.into_iter()
            .filter(|c| c.health.is_healthy() && c.has_capacity())
            .max_by_key(|c| criteria.score(c))
            .cloned()
    }
}

/// é€‰æ‹©æ ‡å‡†
#[derive(Debug, Clone)]
pub struct SelectionCriteria {
    /// æƒé‡: è´Ÿè½½
    pub load_weight: f64,
    
    /// æƒé‡: å»¶è¿Ÿ
    pub latency_weight: f64,
    
    /// æƒé‡: å®¹é‡
    pub capacity_weight: f64,
}

impl SelectionCriteria {
    fn score(&self, collector: &CollectorTopology) -> i64 {
        let load_score = (1.0 - collector.current_load.utilization()) * self.load_weight;
        let latency_score = (1.0 / (1.0 + collector.current_load.avg_latency_ms)) * self.latency_weight;
        let capacity_score = collector.capacity.available_ratio() * self.capacity_weight;
        
        ((load_score + latency_score + capacity_score) * 1000.0) as i64
    }
}
```

### 2.2 å…¨å±€é‡‡æ ·åè°ƒ

```rust
/// å…¨å±€é‡‡æ ·åè°ƒå™¨
pub struct GlobalSamplingCoordinator {
    /// å½“å‰ç­–ç•¥
    current_strategy: Arc<RwLock<SamplingStrategy>>,
    
    /// ç›®æ ‡é‡‡æ ·ç‡
    target_rate: Arc<RwLock<f64>>,
    
    /// å®é™…é‡‡æ ·ç‡ç»Ÿè®¡
    actual_rates: Arc<RwLock<HashMap<String, f64>>>,
    
    /// åè°ƒå™¨
    coordinator: Arc<dyn CoordinatorClient>,
}

/// é‡‡æ ·ç­–ç•¥
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SamplingStrategy {
    /// å›ºå®šæ¯”ä¾‹
    Fixed { rate: f64 },
    
    /// è‡ªé€‚åº”
    Adaptive {
        min_rate: f64,
        max_rate: f64,
        target_qps: u64,
    },
    
    /// åŸºäºä¼˜å…ˆçº§
    PriorityBased {
        high_priority_rate: f64,
        normal_rate: f64,
        low_priority_rate: f64,
    },
    
    /// åŸºäºæˆæœ¬
    CostBased {
        budget_per_hour: f64,
        cost_per_span: f64,
    },
}

impl GlobalSamplingCoordinator {
    pub fn new(coordinator: Arc<dyn CoordinatorClient>) -> Self {
        Self {
            current_strategy: Arc::new(RwLock::new(SamplingStrategy::Fixed { rate: 0.1 })),
            target_rate: Arc::new(RwLock::new(0.1)),
            actual_rates: Arc::new(RwLock::new(HashMap::new())),
            coordinator,
        }
    }
    
    /// å¯åŠ¨é‡‡æ ·åè°ƒ
    pub async fn start(&self) {
        self.start_strategy_adjuster().await;
        self.start_rate_aggregator().await;
    }
    
    /// å¯åŠ¨ç­–ç•¥è°ƒæ•´å™¨
    async fn start_strategy_adjuster(&self) {
        let current_strategy = Arc::clone(&self.current_strategy);
        let target_rate = Arc::clone(&self.target_rate);
        let actual_rates = Arc::clone(&self.actual_rates);
        let coordinator = Arc::clone(&self.coordinator);
        
        tokio::spawn(async move {
            let mut ticker = interval(Duration::from_secs(60));
            
            loop {
                ticker.tick().await;
                
                // è·å–å…¨å±€ç»Ÿè®¡
                let global_stats = match coordinator.get_global_stats().await {
                    Ok(stats) => stats,
                    Err(e) => {
                        tracing::error!("Failed to get global stats: {}", e);
                        continue;
                    }
                };
                
                // è®¡ç®—å¹³å‡å®é™…é‡‡æ ·ç‡
                let rates = actual_rates.read().await;
                let avg_actual_rate = if rates.is_empty() {
                    0.0
                } else {
                    rates.values().sum::<f64>() / rates.len() as f64
                };
                
                // æ ¹æ®ç­–ç•¥è°ƒæ•´ç›®æ ‡é‡‡æ ·ç‡
                let strategy = current_strategy.read().await;
                let new_target = match &*strategy {
                    SamplingStrategy::Fixed { rate } => *rate,
                    
                    SamplingStrategy::Adaptive { min_rate, max_rate, target_qps } => {
                        Self::calculate_adaptive_rate(
                            *min_rate,
                            *max_rate,
                            *target_qps,
                            global_stats.current_qps,
                        )
                    }
                    
                    SamplingStrategy::PriorityBased { normal_rate, .. } => {
                        *normal_rate // ç®€åŒ–å®ç°
                    }
                    
                    SamplingStrategy::CostBased { budget_per_hour, cost_per_span } => {
                        Self::calculate_cost_based_rate(
                            *budget_per_hour,
                            *cost_per_span,
                            global_stats.current_qps,
                        )
                    }
                };
                
                let mut target = target_rate.write().await;
                *target = new_target;
                
                tracing::info!(
                    "Sampling rate adjusted: target={:.4}, actual={:.4}, qps={}",
                    new_target,
                    avg_actual_rate,
                    global_stats.current_qps
                );
            }
        });
    }
    
    /// è®¡ç®—è‡ªé€‚åº”é‡‡æ ·ç‡
    fn calculate_adaptive_rate(
        min_rate: f64,
        max_rate: f64,
        target_qps: u64,
        current_qps: u64,
    ) -> f64 {
        if current_qps == 0 {
            return max_rate;
        }
        
        let ratio = target_qps as f64 / current_qps as f64;
        ratio.clamp(min_rate, max_rate)
    }
    
    /// è®¡ç®—åŸºäºæˆæœ¬çš„é‡‡æ ·ç‡
    fn calculate_cost_based_rate(
        budget_per_hour: f64,
        cost_per_span: f64,
        current_qps: u64,
    ) -> f64 {
        if current_qps == 0 || cost_per_span == 0.0 {
            return 1.0;
        }
        
        let max_spans_per_hour = budget_per_hour / cost_per_span;
        let current_spans_per_hour = current_qps as f64 * 3600.0;
        
        (max_spans_per_hour / current_spans_per_hour).min(1.0)
    }
}
```

---

## 3. æœ¬åœ°æ„ŸçŸ¥ä¸ä¼˜åŒ–

### 3.1 æœ¬åœ°è´Ÿè½½æ„ŸçŸ¥

```rust
use std::sync::atomic::{AtomicU64, Ordering};

/// æœ¬åœ°è´Ÿè½½ç›‘æ§å™¨
pub struct LocalLoadMonitor {
    /// å½“å‰QPS
    current_qps: Arc<AtomicU64>,
    
    /// å½“å‰å­—èŠ‚/ç§’
    current_bps: Arc<AtomicU64>,
    
    /// é˜Ÿåˆ—æ·±åº¦
    queue_depth: Arc<AtomicU64>,
    
    /// CPUä½¿ç”¨ç‡
    cpu_usage: Arc<RwLock<f64>>,
    
    /// å†…å­˜ä½¿ç”¨ç‡
    memory_usage: Arc<RwLock<f64>>,
    
    /// å†å²ç»Ÿè®¡
    history: Arc<RwLock<CircularBuffer<LoadSnapshot>>>,
}

/// è´Ÿè½½å¿«ç…§
#[derive(Debug, Clone)]
pub struct LoadSnapshot {
    pub timestamp: Instant,
    pub qps: u64,
    pub bps: u64,
    pub queue_depth: u64,
    pub cpu_usage: f64,
    pub memory_usage: f64,
}

impl LocalLoadMonitor {
    pub fn new(history_size: usize) -> Self {
        Self {
            current_qps: Arc::new(AtomicU64::new(0)),
            current_bps: Arc::new(AtomicU64::new(0)),
            queue_depth: Arc::new(AtomicU64::new(0)),
            cpu_usage: Arc::new(RwLock::new(0.0)),
            memory_usage: Arc::new(RwLock::new(0.0)),
            history: Arc::new(RwLock::new(CircularBuffer::new(history_size))),
        }
    }
    
    /// å¯åŠ¨ç›‘æ§
    pub async fn start(&self) {
        self.start_qps_counter().await;
        self.start_resource_monitor().await;
        self.start_history_recorder().await;
    }
    
    /// å¯åŠ¨QPSè®¡æ•°å™¨
    async fn start_qps_counter(&self) {
        let current_qps = Arc::clone(&self.current_qps);
        let current_bps = Arc::clone(&self.current_bps);
        
        tokio::spawn(async move {
            let mut ticker = interval(Duration::from_secs(1));
            let mut last_spans = 0u64;
            let mut last_bytes = 0u64;
            
            loop {
                ticker.tick().await;
                
                let current_spans = current_qps.load(Ordering::Relaxed);
                let current_bytes = current_bps.load(Ordering::Relaxed);
                
                let qps = current_spans.saturating_sub(last_spans);
                let bps = current_bytes.saturating_sub(last_bytes);
                
                last_spans = current_spans;
                last_bytes = current_bytes;
                
                tracing::debug!("Current load: {} spans/s, {} bytes/s", qps, bps);
            }
        });
    }
    
    /// è®°å½•span
    pub fn record_span(&self, size_bytes: u64) {
        self.current_qps.fetch_add(1, Ordering::Relaxed);
        self.current_bps.fetch_add(size_bytes, Ordering::Relaxed);
    }
    
    /// è·å–å½“å‰è´Ÿè½½
    pub async fn get_current_load(&self) -> Load {
        Load {
            qps: self.current_qps.load(Ordering::Relaxed),
            bps: self.current_bps.load(Ordering::Relaxed),
            queue_depth: self.queue_depth.load(Ordering::Relaxed),
            cpu_usage: *self.cpu_usage.read().await,
            memory_usage: *self.memory_usage.read().await,
            avg_latency_ms: self.calculate_avg_latency().await,
        }
    }
    
    /// è®¡ç®—å¹³å‡å»¶è¿Ÿ
    async fn calculate_avg_latency(&self) -> f64 {
        let history = self.history.read().await;
        
        if history.is_empty() {
            return 0.0;
        }
        
        // ç®€åŒ–å®ç°ï¼šåŸºäºé˜Ÿåˆ—æ·±åº¦ä¼°ç®—
        let avg_queue_depth: f64 = history.iter()
            .map(|s| s.queue_depth as f64)
            .sum::<f64>() / history.len() as f64;
        
        // Little's Law: L = Î» * W
        // W = L / Î»
        let avg_qps: f64 = history.iter()
            .map(|s| s.qps as f64)
            .sum::<f64>() / history.len() as f64;
        
        if avg_qps > 0.0 {
            (avg_queue_depth / avg_qps) * 1000.0 // è½¬æ¢ä¸ºæ¯«ç§’
        } else {
            0.0
        }
    }
    
    /// é¢„æµ‹æœªæ¥è´Ÿè½½
    pub async fn predict_load(&self, horizon_secs: u64) -> PredictedLoad {
        let history = self.history.read().await;
        
        if history.len() < 10 {
            // æ•°æ®ä¸è¶³ï¼Œè¿”å›å½“å‰è´Ÿè½½
            return PredictedLoad {
                qps: self.current_qps.load(Ordering::Relaxed),
                confidence: 0.5,
            };
        }
        
        // ç®€å•çš„çº¿æ€§å›å½’é¢„æµ‹
        let recent: Vec<&LoadSnapshot> = history.iter().rev().take(60).collect();
        
        let sum_qps: u64 = recent.iter().map(|s| s.qps).sum();
        let avg_qps = sum_qps / recent.len() as u64;
        
        // è®¡ç®—è¶‹åŠ¿
        let first_half_avg = recent.iter().take(30).map(|s| s.qps).sum::<u64>() / 30;
        let second_half_avg = recent.iter().skip(30).map(|s| s.qps).sum::<u64>() / 30;
        let trend = second_half_avg as i64 - first_half_avg as i64;
        
        let predicted_qps = (avg_qps as i64 + trend * horizon_secs as i64 / 60).max(0) as u64;
        
        PredictedLoad {
            qps: predicted_qps,
            confidence: 0.8, // ç®€åŒ–ï¼šå›ºå®šç½®ä¿¡åº¦
        }
    }
}

/// é¢„æµ‹è´Ÿè½½
#[derive(Debug, Clone)]
pub struct PredictedLoad {
    pub qps: u64,
    pub confidence: f64,
}

/// è´Ÿè½½
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Load {
    pub qps: u64,
    pub bps: u64,
    pub queue_depth: u64,
    pub cpu_usage: f64,
    pub memory_usage: f64,
    pub avg_latency_ms: f64,
}

impl Load {
    pub fn utilization(&self) -> f64 {
        // ç»¼åˆåˆ©ç”¨ç‡
        (self.cpu_usage + self.memory_usage) / 2.0
    }
}
```

### 3.2 æœ¬åœ°æ™ºèƒ½è·¯ç”±

```rust
/// æœ¬åœ°æ™ºèƒ½è·¯ç”±å™¨
pub struct LocalIntelligentRouter {
    /// åç«¯åˆ—è¡¨
    backends: Arc<RwLock<Vec<Backend>>>,
    
    /// è´Ÿè½½ç›‘æ§å™¨
    load_monitor: Arc<LocalLoadMonitor>,
    
    /// è·¯ç”±ç­–ç•¥
    strategy: Arc<RwLock<RoutingStrategy>>,
    
    /// ç»Ÿè®¡ä¿¡æ¯
    stats: Arc<RwLock<RoutingStats>>,
}

/// åç«¯
#[derive(Debug, Clone)]
pub struct Backend {
    pub id: String,
    pub address: String,
    pub weight: u32,
    pub health: HealthStatus,
    pub load: Load,
}

/// è·¯ç”±ç­–ç•¥
#[derive(Debug, Clone)]
pub enum RoutingStrategy {
    /// è½®è¯¢
    RoundRobin,
    
    /// åŠ æƒè½®è¯¢
    WeightedRoundRobin,
    
    /// æœ€å°‘è¿æ¥
    LeastConnections,
    
    /// ä¸€è‡´æ€§å“ˆå¸Œ
    ConsistentHash,
    
    /// æ™ºèƒ½è·¯ç”±ï¼ˆåŸºäºè´Ÿè½½å’Œå»¶è¿Ÿï¼‰
    Intelligent {
        load_weight: f64,
        latency_weight: f64,
    },
}

impl LocalIntelligentRouter {
    pub fn new(load_monitor: Arc<LocalLoadMonitor>) -> Self {
        Self {
            backends: Arc::new(RwLock::new(Vec::new())),
            load_monitor,
            strategy: Arc::new(RwLock::new(RoutingStrategy::Intelligent {
                load_weight: 0.7,
                latency_weight: 0.3,
            })),
            stats: Arc::new(RwLock::new(RoutingStats::default())),
        }
    }
    
    /// é€‰æ‹©åç«¯
    pub async fn select_backend(&self, request: &RoutingRequest) -> Option<Backend> {
        let backends = self.backends.read().await;
        let strategy = self.strategy.read().await;
        
        match &*strategy {
            RoutingStrategy::RoundRobin => {
                self.select_round_robin(&backends).await
            }
            
            RoutingStrategy::WeightedRoundRobin => {
                self.select_weighted_round_robin(&backends).await
            }
            
            RoutingStrategy::LeastConnections => {
                self.select_least_connections(&backends).await
            }
            
            RoutingStrategy::ConsistentHash => {
                self.select_consistent_hash(&backends, request).await
            }
            
            RoutingStrategy::Intelligent { load_weight, latency_weight } => {
                self.select_intelligent(&backends, *load_weight, *latency_weight).await
            }
        }
    }
    
    /// æ™ºèƒ½é€‰æ‹©åç«¯
    async fn select_intelligent(
        &self,
        backends: &[Backend],
        load_weight: f64,
        latency_weight: f64,
    ) -> Option<Backend> {
        let healthy_backends: Vec<&Backend> = backends.iter()
            .filter(|b| b.health.is_healthy())
            .collect();
        
        if healthy_backends.is_empty() {
            return None;
        }
        
        // è®¡ç®—æ¯ä¸ªåç«¯çš„è¯„åˆ†
        let scores: Vec<(usize, f64)> = healthy_backends.iter()
            .enumerate()
            .map(|(idx, backend)| {
                let load_score = 1.0 - backend.load.utilization();
                let latency_score = 1.0 / (1.0 + backend.load.avg_latency_ms);
                let total_score = load_score * load_weight + latency_score * latency_weight;
                (idx, total_score)
            })
            .collect();
        
        // é€‰æ‹©å¾—åˆ†æœ€é«˜çš„åç«¯
        scores.iter()
            .max_by(|a, b| a.1.partial_cmp(&b.1).unwrap())
            .and_then(|(idx, _)| healthy_backends.get(*idx))
            .map(|&b| b.clone())
    }
    
    /// è½®è¯¢é€‰æ‹©
    async fn select_round_robin(&self, backends: &[Backend]) -> Option<Backend> {
        let healthy: Vec<&Backend> = backends.iter()
            .filter(|b| b.health.is_healthy())
            .collect();
        
        if healthy.is_empty() {
            return None;
        }
        
        let mut stats = self.stats.write().await;
        let idx = stats.round_robin_counter % healthy.len();
        stats.round_robin_counter += 1;
        
        healthy.get(idx).map(|&b| b.clone())
    }
    
    /// æœ€å°‘è¿æ¥é€‰æ‹©
    async fn select_least_connections(&self, backends: &[Backend]) -> Option<Backend> {
        backends.iter()
            .filter(|b| b.health.is_healthy())
            .min_by_key(|b| b.load.queue_depth)
            .cloned()
    }
}

/// è·¯ç”±è¯·æ±‚
#[derive(Debug)]
pub struct RoutingRequest {
    pub trace_id: TraceId,
    pub service_name: String,
    pub span_count: usize,
}

/// è·¯ç”±ç»Ÿè®¡
#[derive(Debug, Default)]
struct RoutingStats {
    round_robin_counter: usize,
    total_requests: u64,
    successful_routes: u64,
    failed_routes: u64,
}
```

---

## 4. åˆ†å¸ƒå¼åè°ƒæœåŠ¡

### 4.1 åŸºäºetcdçš„åè°ƒå®ç°

```rust
use etcd_client::{Client as EtcdClient, GetOptions, WatchOptions, PutOptions};

/// etcdåè°ƒå®¢æˆ·ç«¯
pub struct EtcdCoordinator {
    client: EtcdClient,
    prefix: String,
}

impl EtcdCoordinator {
    pub async fn new(endpoints: Vec<String>, prefix: String) -> Result<Self, Box<dyn std::error::Error>> {
        let client = EtcdClient::connect(endpoints, None).await?;
        
        Ok(Self {
            client,
            prefix,
        })
    }
    
    /// è·å–å…¨å±€é…ç½®
    pub async fn get_global_config(&mut self) -> Result<GlobalConfig, Box<dyn std::error::Error>> {
        let key = format!("{}/global/config", self.prefix);
        
        let resp = self.client.get(key, None).await?;
        
        if let Some(kv) = resp.kvs().first() {
            let config: GlobalConfig = serde_json::from_slice(kv.value())?;
            Ok(config)
        } else {
            Ok(GlobalConfig::default())
        }
    }
    
    /// ç›‘å¬é…ç½®å˜æ›´
    pub async fn watch_config(&mut self) -> Result<mpsc::Receiver<ConfigChange>, Box<dyn std::error::Error>> {
        let key = format!("{}/global/config", self.prefix);
        
        let (tx, rx) = mpsc::channel(100);
        
        let (mut watcher, mut stream) = self.client.watch(key, None).await?;
        
        tokio::spawn(async move {
            while let Some(resp) = stream.message().await.transpose() {
                match resp {
                    Ok(watch_resp) => {
                        for event in watch_resp.events() {
                            if let Some(kv) = event.kv() {
                                if let Ok(config) = serde_json::from_slice::<GlobalConfig>(kv.value()) {
                                    let change = ConfigChange {
                                        version: config.version,
                                        config,
                                    };
                                    
                                    if tx.send(change).await.is_err() {
                                        break;
                                    }
                                }
                            }
                        }
                    }
                    Err(e) => {
                        tracing::error!("Watch error: {}", e);
                        break;
                    }
                }
            }
        });
        
        Ok(rx)
    }
    
    /// æ³¨å†ŒCollector
    pub async fn register_collector(
        &mut self,
        collector_id: &str,
        info: &CollectorInfo,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let key = format!("{}/collectors/{}", self.prefix, collector_id);
        let value = serde_json::to_string(info)?;
        
        // å¸¦TTLçš„æ³¨å†Œï¼ˆå¿ƒè·³æœºåˆ¶ï¼‰
        let lease = self.client.lease_grant(30, None).await?;
        
        self.client.put(key, value, Some(PutOptions::new().with_lease(lease.id()))).await?;
        
        // å¯åŠ¨å¿ƒè·³ä¿æŒ
        self.keep_alive_lease(lease.id()).await?;
        
        Ok(())
    }
    
    /// ä¿æŒç§Ÿçº¦æ´»è·ƒ
    async fn keep_alive_lease(&mut self, lease_id: i64) -> Result<(), Box<dyn std::error::Error>> {
        let mut client = self.client.clone();
        
        tokio::spawn(async move {
            let (mut keeper, mut stream) = match client.lease_keep_alive(lease_id).await {
                Ok(k) => k,
                Err(e) => {
                    tracing::error!("Failed to create lease keeper: {}", e);
                    return;
                }
            };
            
            let mut ticker = interval(Duration::from_secs(10));
            
            loop {
                ticker.tick().await;
                
                if keeper.keep_alive().await.is_err() {
                    tracing::error!("Failed to keep lease alive");
                    break;
                }
                
                // æ¶ˆè´¹å“åº”
                if stream.message().await.is_none() {
                    break;
                }
            }
        });
        
        Ok(())
    }
}

#[async_trait::async_trait]
impl CoordinatorClient for EtcdCoordinator {
    async fn get_global_config(&self) -> Result<GlobalConfig, Box<dyn std::error::Error>> {
        let mut client = self.clone();
        client.get_global_config().await
    }
    
    async fn watch_config(&self) -> Result<mpsc::Receiver<ConfigChange>, Box<dyn std::error::Error>> {
        let mut client = self.clone();
        client.watch_config().await
    }
    
    async fn register_region(&self, region_id: &str) -> Result<(), Box<dyn std::error::Error>> {
        let mut client = self.clone();
        let key = format!("{}/regions/{}", client.prefix, region_id);
        client.client.put(key, "", None).await?;
        Ok(())
    }
    
    async fn list_regions(&self) -> Result<Vec<String>, Box<dyn std::error::Error>> {
        let mut client = self.clone();
        let key = format!("{}/regions/", client.prefix);
        
        let resp = client.client.get(key, Some(GetOptions::new().with_prefix())).await?;
        
        let regions: Vec<String> = resp.kvs().iter()
            .filter_map(|kv| {
                std::str::from_utf8(kv.key()).ok()
                    .and_then(|k| k.rsplit('/').next())
                    .map(|s| s.to_string())
            })
            .collect();
        
        Ok(regions)
    }
    
    // ... å…¶ä»–æ–¹æ³•å®ç°
}
```

---

## 5. é…ç½®ç®¡ç†ä¸çƒ­æ›´æ–°

### 5.1 é…ç½®çƒ­æ›´æ–°å®ç°

```rust
/// é…ç½®ç®¡ç†å™¨
pub struct ConfigManager {
    /// å½“å‰é…ç½®
    current: Arc<RwLock<GlobalConfig>>,
    
    /// é…ç½®ç›‘å¬å™¨
    listeners: Arc<RwLock<Vec<broadcast::Sender<GlobalConfig>>>>,
    
    /// éªŒè¯å™¨
    validator: Arc<dyn ConfigValidator>,
}

impl ConfigManager {
    pub fn new(initial_config: GlobalConfig, validator: Arc<dyn ConfigValidator>) -> Self {
        Self {
            current: Arc::new(RwLock::new(initial_config)),
            listeners: Arc::new(RwLock::new(Vec::new())),
            validator,
        }
    }
    
    /// æ›´æ–°é…ç½®
    pub async fn update_config(&self, new_config: GlobalConfig) -> Result<(), ConfigError> {
        // éªŒè¯æ–°é…ç½®
        self.validator.validate(&new_config)?;
        
        // åº”ç”¨é…ç½®
        let mut current = self.current.write().await;
        let old_config = current.clone();
        *current = new_config.clone();
        
        // é€šçŸ¥æ‰€æœ‰ç›‘å¬å™¨
        let listeners = self.listeners.read().await;
        for listener in listeners.iter() {
            let _ = listener.send(new_config.clone());
        }
        
        tracing::info!(
            "Config updated: version {} -> {}",
            old_config.version,
            new_config.version
        );
        
        Ok(())
    }
    
    /// è®¢é˜…é…ç½®å˜æ›´
    pub async fn subscribe(&self) -> broadcast::Receiver<GlobalConfig> {
        let (tx, rx) = broadcast::channel(100);
        
        let mut listeners = self.listeners.write().await;
        listeners.push(tx);
        
        rx
    }
    
    /// è·å–å½“å‰é…ç½®
    pub async fn get_current(&self) -> GlobalConfig {
        self.current.read().await.clone()
    }
}

/// é…ç½®éªŒè¯å™¨
#[async_trait::async_trait]
pub trait ConfigValidator: Send + Sync {
    fn validate(&self, config: &GlobalConfig) -> Result<(), ConfigError>;
}

/// é»˜è®¤é…ç½®éªŒè¯å™¨
pub struct DefaultConfigValidator;

impl DefaultConfigValidator {
    pub fn new() -> Self {
        Self
    }
}

impl ConfigValidator for DefaultConfigValidator {
    fn validate(&self, config: &GlobalConfig) -> Result<(), ConfigError> {
        // é‡‡æ ·ç‡èŒƒå›´æ£€æŸ¥
        if config.global_sampling_rate < 0.0 || config.global_sampling_rate > 1.0 {
            return Err(ConfigError::InvalidValue(
                "global_sampling_rate must be between 0.0 and 1.0".to_string()
            ));
        }
        
        // é™çº§é…ç½®æ£€æŸ¥
        if config.degradation.threshold < 0.0 || config.degradation.threshold > 1.0 {
            return Err(ConfigError::InvalidValue(
                "degradation threshold must be between 0.0 and 1.0".to_string()
            ));
        }
        
        // é™æµé…ç½®æ£€æŸ¥
        if config.rate_limiting.max_qps == 0 {
            return Err(ConfigError::InvalidValue(
                "max_qps must be greater than 0".to_string()
            ));
        }
        
        Ok(())
    }
}

/// é…ç½®é”™è¯¯
#[derive(Debug, thiserror::Error)]
pub enum ConfigError {
    #[error("Invalid config value: {0}")]
    InvalidValue(String),
    
    #[error("Config validation failed: {0}")]
    ValidationFailed(String),
}
```

---

## 6. æœåŠ¡å‘ç°ä¸æ³¨å†Œ

### 6.1 æœåŠ¡æ³¨å†Œå®ç°

```rust
use std::net::SocketAddr;

/// æœåŠ¡æ³¨å†Œå™¨
pub struct ServiceRegistry {
    /// åè°ƒå®¢æˆ·ç«¯
    coordinator: Arc<dyn CoordinatorClient>,
    
    /// æœåŠ¡ä¿¡æ¯
    service_info: Arc<RwLock<ServiceInfo>>,
    
    /// å¥åº·æ£€æŸ¥å™¨
    health_checker: Arc<HealthChecker>,
}

/// æœåŠ¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceInfo {
    pub id: String,
    pub name: String,
    pub address: SocketAddr,
    pub region: String,
    pub metadata: HashMap<String, String>,
    pub health: HealthStatus,
    pub registered_at: u64,
}

impl ServiceRegistry {
    pub fn new(
        coordinator: Arc<dyn CoordinatorClient>,
        service_info: ServiceInfo,
    ) -> Self {
        Self {
            coordinator,
            service_info: Arc::new(RwLock::new(service_info)),
            health_checker: Arc::new(HealthChecker::new()),
        }
    }
    
    /// æ³¨å†ŒæœåŠ¡
    pub async fn register(&self) -> Result<(), Box<dyn std::error::Error>> {
        let service_info = self.service_info.read().await;
        
        self.coordinator.register_service(&service_info).await?;
        
        tracing::info!(
            "Service registered: {} at {}",
            service_info.name,
            service_info.address
        );
        
        // å¯åŠ¨å¿ƒè·³
        self.start_heartbeat().await;
        
        Ok(())
    }
    
    /// å¯åŠ¨å¿ƒè·³
    async fn start_heartbeat(&self) {
        let coordinator = Arc::clone(&self.coordinator);
        let service_info = Arc::clone(&self.service_info);
        let health_checker = Arc::clone(&self.health_checker);
        
        tokio::spawn(async move {
            let mut ticker = interval(Duration::from_secs(10));
            
            loop {
                ticker.tick().await;
                
                // æ£€æŸ¥å¥åº·çŠ¶æ€
                let health = health_checker.check().await;
                
                // æ›´æ–°æœåŠ¡ä¿¡æ¯
                let mut info = service_info.write().await;
                info.health = health.clone();
                
                // å‘é€å¿ƒè·³
                if let Err(e) = coordinator.heartbeat(&info.id, &health).await {
                    tracing::error!("Heartbeat failed: {}", e);
                }
            }
        });
    }
    
    /// æ³¨é”€æœåŠ¡
    pub async fn deregister(&self) -> Result<(), Box<dyn std::error::Error>> {
        let service_info = self.service_info.read().await;
        
        self.coordinator.deregister_service(&service_info.id).await?;
        
        tracing::info!("Service deregistered: {}", service_info.name);
        
        Ok(())
    }
}

/// å¥åº·æ£€æŸ¥å™¨
pub struct HealthChecker {
    checks: Arc<RwLock<Vec<Box<dyn HealthCheck>>>>,
}

impl HealthChecker {
    pub fn new() -> Self {
        Self {
            checks: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    /// æ·»åŠ å¥åº·æ£€æŸ¥
    pub async fn add_check(&self, check: Box<dyn HealthCheck>) {
        let mut checks = self.checks.write().await;
        checks.push(check);
    }
    
    /// æ‰§è¡Œå¥åº·æ£€æŸ¥
    pub async fn check(&self) -> HealthStatus {
        let checks = self.checks.read().await;
        
        for check in checks.iter() {
            if !check.check().await {
                return HealthStatus::Unhealthy {
                    reason: check.name().to_string(),
                };
            }
        }
        
        HealthStatus::Healthy
    }
}

/// å¥åº·æ£€æŸ¥trait
#[async_trait::async_trait]
pub trait HealthCheck: Send + Sync {
    async fn check(&self) -> bool;
    fn name(&self) -> &str;
}

/// CPUå¥åº·æ£€æŸ¥
pub struct CpuHealthCheck {
    threshold: f64,
}

#[async_trait::async_trait]
impl HealthCheck for CpuHealthCheck {
    async fn check(&self) -> bool {
        // å®é™…å®ç°éœ€è¦è·å–çœŸå®CPUä½¿ç”¨ç‡
        let cpu_usage = get_cpu_usage().await;
        cpu_usage < self.threshold
    }
    
    fn name(&self) -> &str {
        "cpu"
    }
}

/// å¥åº·çŠ¶æ€
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum HealthStatus {
    Healthy,
    Unhealthy { reason: String },
    Unknown,
}

impl HealthStatus {
    pub fn healthy() -> Self {
        Self::Healthy
    }
    
    pub fn is_healthy(&self) -> bool {
        matches!(self, Self::Healthy)
    }
}

// è¾…åŠ©å‡½æ•°
async fn get_cpu_usage() -> f64 {
    // ç®€åŒ–å®ç°
    0.5
}
```

---

## 7. åˆ†å¸ƒå¼é”ä¸é€‰ä¸¾

### 7.1 åˆ†å¸ƒå¼é”å®ç°

```rust
use tokio::sync::Mutex as TokioMutex;
use std::time::SystemTime;

/// åˆ†å¸ƒå¼é”
pub struct DistributedLock {
    coordinator: Arc<dyn CoordinatorClient>,
    lock_name: String,
    lease_duration: Duration,
    holder: Arc<TokioMutex<Option<String>>>,
}

impl DistributedLock {
    pub fn new(
        coordinator: Arc<dyn CoordinatorClient>,
        lock_name: String,
        lease_duration: Duration,
    ) -> Self {
        Self {
            coordinator,
            lock_name,
            lease_duration,
            holder: Arc::new(TokioMutex::new(None)),
        }
    }
    
    /// è·å–é”
    pub async fn acquire(&self) -> Result<LockGuard, Box<dyn std::error::Error>> {
        let lock_id = uuid::Uuid::new_v4().to_string();
        
        loop {
            match self.try_acquire(&lock_id).await {
                Ok(true) => {
                    let mut holder = self.holder.lock().await;
                    *holder = Some(lock_id.clone());
                    
                    return Ok(LockGuard {
                        lock: self.clone(),
                        lock_id,
                    });
                }
                Ok(false) => {
                    // é”è¢«å ç”¨ï¼Œç­‰å¾…åé‡è¯•
                    tokio::time::sleep(Duration::from_millis(100)).await;
                }
                Err(e) => {
                    return Err(e);
                }
            }
        }
    }
    
    /// å°è¯•è·å–é”
    async fn try_acquire(&self, lock_id: &str) -> Result<bool, Box<dyn std::error::Error>> {
        self.coordinator.try_lock(&self.lock_name, lock_id, self.lease_duration).await
    }
    
    /// é‡Šæ”¾é”
    async fn release(&self, lock_id: &str) -> Result<(), Box<dyn std::error::Error>> {
        self.coordinator.unlock(&self.lock_name, lock_id).await?;
        
        let mut holder = self.holder.lock().await;
        *holder = None;
        
        Ok(())
    }
}

impl Clone for DistributedLock {
    fn clone(&self) -> Self {
        Self {
            coordinator: Arc::clone(&self.coordinator),
            lock_name: self.lock_name.clone(),
            lease_duration: self.lease_duration,
            holder: Arc::clone(&self.holder),
        }
    }
}

/// é”å®ˆå«
pub struct LockGuard {
    lock: DistributedLock,
    lock_id: String,
}

impl Drop for LockGuard {
    fn drop(&mut self) {
        let lock = self.lock.clone();
        let lock_id = self.lock_id.clone();
        
        tokio::spawn(async move {
            if let Err(e) = lock.release(&lock_id).await {
                tracing::error!("Failed to release lock: {}", e);
            }
        });
    }
}
```

### 7.2 Leaderé€‰ä¸¾å®ç°

```rust
/// Leaderé€‰ä¸¾å™¨
pub struct LeaderElection {
    coordinator: Arc<dyn CoordinatorClient>,
    election_name: String,
    node_id: String,
    is_leader: Arc<RwLock<bool>>,
    leader_id: Arc<RwLock<Option<String>>>,
}

impl LeaderElection {
    pub fn new(
        coordinator: Arc<dyn CoordinatorClient>,
        election_name: String,
        node_id: String,
    ) -> Self {
        Self {
            coordinator,
            election_name,
            node_id,
            is_leader: Arc::new(RwLock::new(false)),
            leader_id: Arc::new(RwLock::new(None)),
        }
    }
    
    /// å¼€å§‹é€‰ä¸¾
    pub async fn start(&self) {
        let coordinator = Arc::clone(&self.coordinator);
        let election_name = self.election_name.clone();
        let node_id = self.node_id.clone();
        let is_leader = Arc::clone(&self.is_leader);
        let leader_id = Arc::clone(&self.leader_id);
        
        tokio::spawn(async move {
            loop {
                match coordinator.campaign(&election_name, &node_id).await {
                    Ok(true) => {
                        // æˆä¸ºLeader
                        *is_leader.write().await = true;
                        *leader_id.write().await = Some(node_id.clone());
                        
                        tracing::info!("Became leader: {}", node_id);
                        
                        // ä¿æŒLeaderçŠ¶æ€
                        Self::maintain_leadership(
                            &coordinator,
                            &election_name,
                            &node_id,
                            &is_leader,
                        ).await;
                    }
                    Ok(false) => {
                        // è§‚å¯Ÿå½“å‰Leader
                        if let Ok(current_leader) = coordinator.observe(&election_name).await {
                            *leader_id.write().await = Some(current_leader);
                        }
                    }
                    Err(e) => {
                        tracing::error!("Election error: {}", e);
                    }
                }
                
                tokio::time::sleep(Duration::from_secs(5)).await;
            }
        });
    }
    
    /// ç»´æŒLeaderçŠ¶æ€
    async fn maintain_leadership(
        coordinator: &Arc<dyn CoordinatorClient>,
        election_name: &str,
        node_id: &str,
        is_leader: &Arc<RwLock<bool>>,
    ) {
        let mut ticker = interval(Duration::from_secs(10));
        
        loop {
            ticker.tick().await;
            
            match coordinator.proclaim(election_name, node_id).await {
                Ok(()) => {
                    // LeaderçŠ¶æ€ç»´æŒæˆåŠŸ
                }
                Err(e) => {
                    tracing::error!("Failed to maintain leadership: {}", e);
                    *is_leader.write().await = false;
                    break;
                }
            }
        }
    }
    
    /// æ˜¯å¦æ˜¯Leader
    pub async fn is_leader(&self) -> bool {
        *self.is_leader.read().await
    }
    
    /// è·å–å½“å‰Leader
    pub async fn get_leader(&self) -> Option<String> {
        self.leader_id.read().await.clone()
    }
    
    /// è¾èŒ
    pub async fn resign(&self) -> Result<(), Box<dyn std::error::Error>> {
        if *self.is_leader.read().await {
            self.coordinator.resign(&self.election_name, &self.node_id).await?;
            *self.is_leader.write().await = false;
            tracing::info!("Resigned from leadership");
        }
        
        Ok(())
    }
}
```

---

## 8. å®Œæ•´å®ç°ç¤ºä¾‹

### 8.1 ç»¼åˆç¤ºä¾‹

```rust
use tracing_subscriber;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆå§‹åŒ–æ—¥å¿—
    tracing_subscriber::fmt::init();
    
    // åˆ›å»ºetcdåè°ƒå®¢æˆ·ç«¯
    let etcd_endpoints = vec!["http://localhost:2379".to_string()];
    let coordinator = Arc::new(
        EtcdCoordinator::new(etcd_endpoints, "/otlp".to_string()).await?
    );
    
    // åˆ›å»ºåˆ†å¸ƒå¼æ§åˆ¶å™¨
    let controller = DistributedOtlpController::new(
        "http://localhost:2379",
        "region-1".to_string(),
    ).await?;
    
    // å¯åŠ¨æ§åˆ¶å™¨
    controller.start().await?;
    
    // æ³¨å†ŒåŒºåŸŸæ§åˆ¶å™¨
    let local_config = LocalConfig {
        regional_sampling_rate: 0.1,
        local_routing_preference: vec!["collector-1".to_string()],
        buffer_size: 10000,
        batch_config: BatchConfig {
            max_batch_size: 100,
            max_wait_time: Duration::from_millis(100),
        },
        retry_policy: RetryPolicy {
            max_retries: 3,
            initial_backoff: Duration::from_millis(100),
            max_backoff: Duration::from_secs(10),
        },
    };
    
    controller.register_region("region-1".to_string(), local_config).await?;
    
    // åˆ›å»ºå…¨å±€æ‹“æ‰‘ç®¡ç†å™¨
    let topology_manager = GlobalTopologyManager::new(
        Arc::clone(&coordinator),
        Duration::from_secs(30),
    );
    topology_manager.start().await;
    
    // åˆ›å»ºå…¨å±€é‡‡æ ·åè°ƒå™¨
    let sampling_coordinator = GlobalSamplingCoordinator::new(Arc::clone(&coordinator));
    sampling_coordinator.start().await;
    
    // åˆ›å»ºæœ¬åœ°è´Ÿè½½ç›‘æ§å™¨
    let load_monitor = Arc::new(LocalLoadMonitor::new(600));
    load_monitor.start().await;
    
    // åˆ›å»ºæœ¬åœ°æ™ºèƒ½è·¯ç”±å™¨
    let router = LocalIntelligentRouter::new(Arc::clone(&load_monitor));
    
    // åˆ›å»ºæœåŠ¡æ³¨å†Œå™¨
    let service_info = ServiceInfo {
        id: "collector-1".to_string(),
        name: "otlp-collector".to_string(),
        address: "0.0.0.0:4317".parse()?,
        region: "region-1".to_string(),
        metadata: HashMap::new(),
        health: HealthStatus::Healthy,
        registered_at: SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)?
            .as_secs(),
    };
    
    let registry = ServiceRegistry::new(Arc::clone(&coordinator), service_info);
    registry.register().await?;
    
    // åˆ›å»ºLeaderé€‰ä¸¾å™¨
    let election = LeaderElection::new(
        Arc::clone(&coordinator),
        "otlp-controller".to_string(),
        "node-1".to_string(),
    );
    election.start().await;
    
    // ç­‰å¾…ä¿¡å·
    tokio::signal::ctrl_c().await?;
    
    // æ¸…ç†
    registry.deregister().await?;
    election.resign().await?;
    
    Ok(())
}
```

---

## 9. é…ç½®ç¤ºä¾‹

### 9.1 å…¨å±€é…ç½®ç¤ºä¾‹

```yaml
# global-config.yaml
global:
  sampling_rate: 0.1
  
  sampling_strategies:
    - type: adaptive
      min_rate: 0.01
      max_rate: 0.5
      target_qps: 100000
    
    - type: priority_based
      rules:
        - priority: high
          rate: 1.0
          services: ["payment", "checkout"]
        - priority: normal
          rate: 0.1
        - priority: low
          rate: 0.01
          services: ["debug", "test"]
  
  degradation:
    enabled: true
    threshold: 0.8
    target_rate: 0.01
    recovery_threshold: 0.6
  
  rate_limiting:
    max_qps: 1000000
    burst_size: 10000
    per_service_limit: 10000
  
  routing_rules:
    - service: "critical-*"
      target_region: "us-east-1"
      priority: high
    
    - service: "batch-*"
      target_region: "us-west-2"
      priority: low
```

---

## 10. æ€§èƒ½ä¼˜åŒ–

### 10.1 æ‰¹å¤„ç†ä¼˜åŒ–

```rust
/// æ‰¹å¤„ç†é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchConfig {
    pub max_batch_size: usize,
    pub max_wait_time: Duration,
}

/// æ‰¹å¤„ç†å™¨
pub struct Batcher<T> {
    config: BatchConfig,
    buffer: Arc<TokioMutex<Vec<T>>>,
    tx: mpsc::Sender<Vec<T>>,
}

impl<T: Send + 'static> Batcher<T> {
    pub fn new(config: BatchConfig) -> (Self, mpsc::Receiver<Vec<T>>) {
        let (tx, rx) = mpsc::channel(100);
        
        let batcher = Self {
            config,
            buffer: Arc::new(TokioMutex::new(Vec::with_capacity(config.max_batch_size))),
            tx,
        };
        
        batcher.start_flush_timer();
        
        (batcher, rx)
    }
    
    /// æ·»åŠ é¡¹ç›®
    pub async fn add(&self, item: T) -> Result<(), Box<dyn std::error::Error>> {
        let mut buffer = self.buffer.lock().await;
        buffer.push(item);
        
        if buffer.len() >= self.config.max_batch_size {
            let batch = std::mem::replace(&mut *buffer, Vec::with_capacity(self.config.max_batch_size));
            self.tx.send(batch).await?;
        }
        
        Ok(())
    }
    
    /// å¯åŠ¨å®šæ—¶åˆ·æ–°
    fn start_flush_timer(&self) {
        let buffer = Arc::clone(&self.buffer);
        let tx = self.tx.clone();
        let max_wait = self.config.max_wait_time;
        
        tokio::spawn(async move {
            let mut ticker = interval(max_wait);
            
            loop {
                ticker.tick().await;
                
                let mut buf = buffer.lock().await;
                if !buf.is_empty() {
                    let batch = std::mem::take(&mut *buf);
                    let _ = tx.send(batch).await;
                }
            }
        });
    }
}
```

---

## 11. ç›‘æ§ä¸å‘Šè­¦

### 11.1 æŒ‡æ ‡æ”¶é›†

```rust
use prometheus::{IntCounterVec, HistogramVec, Registry};

/// æŒ‡æ ‡æ”¶é›†å™¨
pub struct MetricsCollector {
    registry: Registry,
    
    // è®¡æ•°å™¨
    requests_total: IntCounterVec,
    errors_total: IntCounterVec,
    
    // ç›´æ–¹å›¾
    request_duration: HistogramVec,
    batch_size: HistogramVec,
}

impl MetricsCollector {
    pub fn new() -> Self {
        let registry = Registry::new();
        
        let requests_total = IntCounterVec::new(
            opts!("otlp_requests_total", "Total number of OTLP requests"),
            &["region", "service"]
        ).unwrap();
        
        let errors_total = IntCounterVec::new(
            opts!("otlp_errors_total", "Total number of errors"),
            &["region", "error_type"]
        ).unwrap();
        
        let request_duration = HistogramVec::new(
            histogram_opts!("otlp_request_duration_seconds", "Request duration"),
            &["region", "method"]
        ).unwrap();
        
        let batch_size = HistogramVec::new(
            histogram_opts!("otlp_batch_size", "Batch size"),
            &["region"]
        ).unwrap();
        
        registry.register(Box::new(requests_total.clone())).unwrap();
        registry.register(Box::new(errors_total.clone())).unwrap();
        registry.register(Box::new(request_duration.clone())).unwrap();
        registry.register(Box::new(batch_size.clone())).unwrap();
        
        Self {
            registry,
            requests_total,
            errors_total,
            request_duration,
            batch_size,
        }
    }
    
    pub fn record_request(&self, region: &str, service: &str) {
        self.requests_total
            .with_label_values(&[region, service])
            .inc();
    }
    
    pub fn record_error(&self, region: &str, error_type: &str) {
        self.errors_total
            .with_label_values(&[region, error_type])
            .inc();
    }
    
    pub fn record_duration(&self, region: &str, method: &str, duration: Duration) {
        self.request_duration
            .with_label_values(&[region, method])
            .observe(duration.as_secs_f64());
    }
}
```

---

## 12. æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†åˆ†å¸ƒå¼OTLPåè°ƒä¸æ§åˆ¶çš„å®Œæ•´Rustå®ç°ï¼ŒåŒ…æ‹¬:

âœ… **æ ¸å¿ƒç‰¹æ€§**:

- å…¨å±€æ„ŸçŸ¥ä¸æœ¬åœ°æ„ŸçŸ¥æœºåˆ¶
- åˆ†å¸ƒå¼åè°ƒæœåŠ¡(etcd/Consul)
- é…ç½®ç®¡ç†ä¸çƒ­æ›´æ–°
- æœåŠ¡å‘ç°ä¸æ³¨å†Œ
- åˆ†å¸ƒå¼é”ä¸Leaderé€‰ä¸¾
- æ™ºèƒ½è·¯ç”±ä¸è´Ÿè½½å‡è¡¡

âœ… **ç”Ÿäº§å°±ç»ª**:

- å¥åº·æ£€æŸ¥ä¸è‡ªåŠ¨æ¢å¤
- æ€§èƒ½ç›‘æ§ä¸å‘Šè­¦
- æ‰¹å¤„ç†ä¼˜åŒ–
- å®Œæ•´çš„é”™è¯¯å¤„ç†

âœ… **å¯æ‰©å±•æ€§**:

- æ”¯æŒå¤šåŒºåŸŸéƒ¨ç½²
- æ°´å¹³æ‰©å±•èƒ½åŠ›
- æ’ä»¶åŒ–æ¶æ„

---

**å‚è€ƒèµ„æº**:

- [etcd Documentation](https://etcd.io/docs/)
- [Consul Documentation](https://www.consul.io/docs)
- [OpenTelemetry Collector](https://opentelemetry.io/docs/collector/)
- [Distributed Systems Patterns](https://martinfowler.com/articles/patterns-of-distributed-systems/)
