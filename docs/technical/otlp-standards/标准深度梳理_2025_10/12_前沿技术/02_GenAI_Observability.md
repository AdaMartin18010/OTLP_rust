# GenAI Observability: ç”Ÿæˆå¼AIå¯è§‚æµ‹æ€§

> **è§„èŒƒç‰ˆæœ¬**: v1.30.0 (Experimental)  
> **æœ€åæ›´æ–°**: 2025å¹´10æœˆ8æ—¥

---

## ç›®å½•

- [GenAI Observability: ç”Ÿæˆå¼AIå¯è§‚æµ‹æ€§](#genai-observability-ç”Ÿæˆå¼aiå¯è§‚æµ‹æ€§)
  - [ç›®å½•](#ç›®å½•)
  - [1. æ¦‚è¿°](#1-æ¦‚è¿°)
    - [1.1 ä¸ºä»€ä¹ˆéœ€è¦GenAIå¯è§‚æµ‹æ€§](#11-ä¸ºä»€ä¹ˆéœ€è¦genaiå¯è§‚æµ‹æ€§)
    - [1.2 ç‰¹æ®ŠæŒ‘æˆ˜](#12-ç‰¹æ®ŠæŒ‘æˆ˜)
  - [2. GenAIè¯­ä¹‰çº¦å®š](#2-genaiè¯­ä¹‰çº¦å®š)
    - [2.1 LLMè°ƒç”¨å±æ€§](#21-llmè°ƒç”¨å±æ€§)
    - [2.2 Tokenä½¿ç”¨è¿½è¸ª](#22-tokenä½¿ç”¨è¿½è¸ª)
  - [3. Promptè¿½è¸ª](#3-promptè¿½è¸ª)
  - [4. æˆæœ¬è¿½è¸ª](#4-æˆæœ¬è¿½è¸ª)
  - [5. æ€§èƒ½ç›‘æ§](#5-æ€§èƒ½ç›‘æ§)
  - [6. è´¨é‡ç›‘æ§](#6-è´¨é‡ç›‘æ§)
  - [7. å®ç°ç¤ºä¾‹](#7-å®ç°ç¤ºä¾‹)
    - [7.1 OpenAIé›†æˆ](#71-openaié›†æˆ)
    - [7.2 LangChainé›†æˆ](#72-langchainé›†æˆ)
  - [8. Dashboardä¸å‘Šè­¦](#8-dashboardä¸å‘Šè­¦)
  - [9. å®‰å…¨ä¸åˆè§„](#9-å®‰å…¨ä¸åˆè§„)
  - [10. æœ€ä½³å®è·µ](#10-æœ€ä½³å®è·µ)
  - [11. å‚è€ƒèµ„æº](#11-å‚è€ƒèµ„æº)

---

## 1. æ¦‚è¿°

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦GenAIå¯è§‚æµ‹æ€§

```text
GenAIåº”ç”¨æŒ‘æˆ˜:
1. ä¸ç¡®å®šæ€§
   - è¾“å‡ºéç¡®å®šæ€§
   - éš¾ä»¥é¢„æµ‹è¡Œä¸º
   - éœ€è¦ç›‘æ§è´¨é‡

2. æˆæœ¬é«˜
   - Tokenè®¡è´¹
   - APIè°ƒç”¨æ˜‚è´µ
   - éœ€è¦ç²¾ç¡®è¿½è¸ªæˆæœ¬

3. å»¶è¿Ÿé«˜
   - LLMå“åº”æ…¢ (1-10s)
   - ç”¨æˆ·ä½“éªŒå…³é”®
   - éœ€è¦ä¼˜åŒ–æ€§èƒ½

4. è´¨é‡éš¾è¯„ä¼°
   - ä¸»è§‚æ€§å¼º
   - éœ€è¦å¤šç»´åº¦è¯„ä¼°
   - éœ€è¦æŒç»­ç›‘æ§

5. å®‰å…¨é£é™©
   - Promptæ³¨å…¥
   - æ•°æ®æ³„éœ²
   - éœ€è¦å®¡è®¡

å¯è§‚æµ‹æ€§è§£å†³:
âœ… è¿½è¸ªæ¯æ¬¡LLMè°ƒç”¨
âœ… ç›‘æ§Tokenä½¿ç”¨ä¸æˆæœ¬
âœ… åˆ†æå»¶è¿Ÿä¸æ€§èƒ½
âœ… è¯„ä¼°è¾“å‡ºè´¨é‡
âœ… æ£€æµ‹å¼‚å¸¸è¡Œä¸º
âœ… å®¡è®¡æ•æ„Ÿæ“ä½œ
```

### 1.2 ç‰¹æ®ŠæŒ‘æˆ˜

```text
ä¸ä¼ ç»Ÿåº”ç”¨çš„åŒºåˆ«:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç»´åº¦           â”‚ ä¼ ç»Ÿåº”ç”¨      â”‚ GenAIåº”ç”¨    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å»¶è¿Ÿ           â”‚ 10-100ms     â”‚ 1-10s        â”‚
â”‚ æˆæœ¬å¯é¢„æµ‹æ€§    â”‚ é«˜           â”‚ ä½           â”‚
â”‚ è¾“å‡ºç¡®å®šæ€§      â”‚ é«˜           â”‚ ä½           â”‚
â”‚ ç›‘æ§å¤æ‚åº¦      â”‚ ä¸­           â”‚ é«˜           â”‚
â”‚ æˆæœ¬è¿½è¸ª        â”‚ ç®€å•         â”‚ å¤æ‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

éœ€è¦è¿½è¸ª:
- æ¯æ¬¡APIè°ƒç”¨
- Tokenä½¿ç”¨ (input + output)
- æ¨¡å‹ç‰ˆæœ¬
- å‚æ•°é…ç½® (temperature, max_tokens)
- Promptæ¨¡æ¿
- è¾“å‡ºè´¨é‡æŒ‡æ ‡
- æˆæœ¬å½’å› 
```

---

## 2. GenAIè¯­ä¹‰çº¦å®š

### 2.1 LLMè°ƒç”¨å±æ€§

```text
æ ¸å¿ƒå±æ€§:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å±æ€§å                    â”‚ ç±»å‹    â”‚ ç¤ºä¾‹               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gen_ai.system            â”‚ string  â”‚ "openai"           â”‚
â”‚ gen_ai.request.model     â”‚ string  â”‚ "gpt-4"            â”‚
â”‚ gen_ai.request.max_tokensâ”‚ int     â”‚ 1000               â”‚
â”‚ gen_ai.request.temperatureâ”‚float   â”‚ 0.7                â”‚
â”‚ gen_ai.request.top_p     â”‚ float   â”‚ 1.0                â”‚
â”‚ gen_ai.usage.input_tokensâ”‚ int     â”‚ 150                â”‚
â”‚ gen_ai.usage.output_tokensâ”‚int     â”‚ 50                 â”‚
â”‚ gen_ai.response.id       â”‚ string  â”‚ "chatcmpl-abc123"  â”‚
â”‚ gen_ai.response.model    â”‚ string  â”‚ "gpt-4-0613"       â”‚
â”‚ gen_ai.response.finish_reasonâ”‚stringâ”‚"stop"             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

gen_ai.systemæ ‡å‡†å€¼:
- "openai" (OpenAI)
- "anthropic" (Claude)
- "cohere"
- "huggingface"
- "vertex_ai" (Google)
- "azure_openai"

gen_ai.response.finish_reason:
- "stop" (æ­£å¸¸ç»“æŸ)
- "length" (è¾¾åˆ°max_tokens)
- "content_filter" (å†…å®¹è¿‡æ»¤)
- "tool_calls" (å‡½æ•°è°ƒç”¨)

å®Œæ•´ç¤ºä¾‹:
{
  "gen_ai.system": "openai",
  "gen_ai.request.model": "gpt-4-turbo-preview",
  "gen_ai.request.max_tokens": 1000,
  "gen_ai.request.temperature": 0.7,
  "gen_ai.request.top_p": 1.0,
  "gen_ai.request.frequency_penalty": 0.0,
  "gen_ai.request.presence_penalty": 0.0,
  "gen_ai.usage.input_tokens": 150,
  "gen_ai.usage.output_tokens": 50,
  "gen_ai.usage.total_tokens": 200,
  "gen_ai.response.id": "chatcmpl-8abc123def",
  "gen_ai.response.model": "gpt-4-0613",
  "gen_ai.response.finish_reason": "stop"
}
```

### 2.2 Tokenä½¿ç”¨è¿½è¸ª

```text
Tokenå±æ€§:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å±æ€§å                   â”‚ ç±»å‹    â”‚ ç¤ºä¾‹   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gen_ai.usage.input_tokensâ”‚ int     â”‚ 150    â”‚
â”‚ gen_ai.usage.output_tokensâ”‚int    â”‚ 50     â”‚
â”‚ gen_ai.usage.total_tokensâ”‚ int     â”‚ 200    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è®¡ç®—å…¬å¼:
total_tokens = input_tokens + output_tokens

Tokenæˆæœ¬:
ä¸åŒæ¨¡å‹ä¸åŒä»·æ ¼

GPT-4 Turbo:
- Input: $0.01 / 1K tokens
- Output: $0.03 / 1K tokens

GPT-3.5 Turbo:
- Input: $0.0005 / 1K tokens
- Output: $0.0015 / 1K tokens

ç¤ºä¾‹è®¡ç®—:
è¯·æ±‚:
- Input: 150 tokens
- Output: 50 tokens
- æ¨¡å‹: GPT-4 Turbo

æˆæœ¬:
- Input cost: 150 * $0.01 / 1000 = $0.0015
- Output cost: 50 * $0.03 / 1000 = $0.0015
- Total: $0.003

Spanå±æ€§:
span.SetAttributes(
    attribute.Int("gen_ai.usage.input_tokens", 150),
    attribute.Int("gen_ai.usage.output_tokens", 50),
    attribute.Float64("gen_ai.cost.usd", 0.003),
)
```

---

## 3. Promptè¿½è¸ª

```text
Promptå±æ€§:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å±æ€§å                    â”‚ ç±»å‹    â”‚ ç¤ºä¾‹               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gen_ai.prompt.0.role     â”‚ string  â”‚ "system"           â”‚
â”‚ gen_ai.prompt.0.content  â”‚ string  â”‚ "You are..."       â”‚
â”‚ gen_ai.prompt.1.role     â”‚ string  â”‚ "user"             â”‚
â”‚ gen_ai.prompt.1.content  â”‚ string  â”‚ "Summarize..."     â”‚
â”‚ gen_ai.completion.0.role â”‚ string  â”‚ "assistant"        â”‚
â”‚ gen_ai.completion.0.contentâ”‚string â”‚ "Here is..."       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ³¨æ„: 
âš ï¸ Promptå’ŒCompletionå¯èƒ½åŒ…å«æ•æ„Ÿæ•°æ®
âš ï¸ ç”Ÿäº§ç¯å¢ƒå¯èƒ½éœ€è¦è„±æ•æˆ–ä¸è®°å½•

è„±æ•ç­–ç•¥:
1. å®Œå…¨ä¸è®°å½•
   - åˆè§„è¦æ±‚é«˜

2. è®°å½•å…ƒæ•°æ®
   - é•¿åº¦ã€Tokenæ•°
   - ä¸è®°å½•å†…å®¹

3. å“ˆå¸Œ
   - è®°å½•å†…å®¹å“ˆå¸Œ
   - ç”¨äºå»é‡

4. éƒ¨åˆ†è®°å½•
   - åªè®°å½•å‰100å­—ç¬¦
   - æ•æ„Ÿéƒ¨åˆ†åˆ é™¤

5. é‡‡æ ·è®°å½•
   - åªè®°å½•1%è¯·æ±‚
   - ç”¨äºè°ƒè¯•

ç¤ºä¾‹ (è®°å½•å…ƒæ•°æ®):
span.SetAttributes(
    attribute.Int("gen_ai.prompt.length", len(prompt)),
    attribute.Int("gen_ai.prompt.messages", 2),
    // ä¸è®°å½•å®é™…å†…å®¹
)

ç¤ºä¾‹ (å®Œæ•´è®°å½•, å¼€å‘ç¯å¢ƒ):
span.AddEvent("gen_ai.prompt", trace.WithAttributes(
    attribute.String("role", "user"),
    attribute.String("content", promptText),
))

span.AddEvent("gen_ai.completion", trace.WithAttributes(
    attribute.String("role", "assistant"),
    attribute.String("content", completionText),
))
```

---

## 4. æˆæœ¬è¿½è¸ª

```go
// æˆæœ¬è®¡ç®—å™¨
type CostCalculator struct {
    // ä»·æ ¼è¡¨ (USD per 1K tokens)
    prices map[string]ModelPrice
}

type ModelPrice struct {
    InputPer1K  float64
    OutputPer1K float64
}

func NewCostCalculator() *CostCalculator {
    return &CostCalculator{
        prices: map[string]ModelPrice{
            "gpt-4-turbo-preview": {
                InputPer1K:  0.01,
                OutputPer1K: 0.03,
            },
            "gpt-4": {
                InputPer1K:  0.03,
                OutputPer1K: 0.06,
            },
            "gpt-3.5-turbo": {
                InputPer1K:  0.0005,
                OutputPer1K: 0.0015,
            },
        },
    }
}

func (c *CostCalculator) Calculate(model string, inputTokens, outputTokens int) float64 {
    price, ok := c.prices[model]
    if !ok {
        return 0
    }
    
    inputCost := float64(inputTokens) * price.InputPer1K / 1000
    outputCost := float64(outputTokens) * price.OutputPer1K / 1000
    
    return inputCost + outputCost
}

// è¿½è¸ªæˆæœ¬
func trackLLMCall(ctx context.Context, model string, inputTokens, outputTokens int) {
    _, span := tracer.Start(ctx, "llm.call")
    defer span.End()
    
    calc := NewCostCalculator()
    cost := calc.Calculate(model, inputTokens, outputTokens)
    
    span.SetAttributes(
        attribute.String("gen_ai.system", "openai"),
        attribute.String("gen_ai.request.model", model),
        attribute.Int("gen_ai.usage.input_tokens", inputTokens),
        attribute.Int("gen_ai.usage.output_tokens", outputTokens),
        attribute.Int("gen_ai.usage.total_tokens", inputTokens+outputTokens),
        attribute.Float64("gen_ai.cost.usd", cost),
    )
    
    // è®°å½•æŒ‡æ ‡
    costCounter.Add(ctx, cost, metric.WithAttributes(
        attribute.String("model", model),
    ))
    
    tokenCounter.Add(ctx, int64(inputTokens+outputTokens), metric.WithAttributes(
        attribute.String("model", model),
        attribute.String("type", "total"),
    ))
}

// æˆæœ¬å½’å› 
func attributeCost(ctx context.Context, userID, feature string, cost float64) {
    costCounter.Add(ctx, cost, metric.WithAttributes(
        attribute.String("user_id", userID),
        attribute.String("feature", feature),
    ))
}

// æˆæœ¬æŠ¥è¡¨æŸ¥è¯¢ (Prometheus)
// æ¯ä¸ªç”¨æˆ·çš„æˆæœ¬
sum by (user_id) (rate(gen_ai_cost_usd_total[24h]))

// æ¯ä¸ªåŠŸèƒ½çš„æˆæœ¬
sum by (feature) (rate(gen_ai_cost_usd_total[24h]))

// æ¯ä¸ªæ¨¡å‹çš„æˆæœ¬
sum by (model) (rate(gen_ai_cost_usd_total[24h]))

// æ€»æˆæœ¬
sum(rate(gen_ai_cost_usd_total[24h])) * 24 * 30  # æœˆæˆæœ¬é¢„ä¼°
```

---

## 5. æ€§èƒ½ç›‘æ§

```go
// å»¶è¿Ÿè¿½è¸ª
func monitorLLMLatency(ctx context.Context) {
    meter := otel.Meter("genai")
    
    // Histogram: LLMå“åº”å»¶è¿Ÿ
    latencyHistogram, _ := meter.Float64Histogram(
        "gen_ai.latency",
        metric.WithDescription("LLM response latency"),
        metric.WithUnit("ms"),
    )
    
    // Time to First Token (TTFT)
    ttftHistogram, _ := meter.Float64Histogram(
        "gen_ai.ttft",
        metric.WithDescription("Time to first token"),
        metric.WithUnit("ms"),
    )
    
    // Tokens per second
    tpsGauge, _ := meter.Float64Histogram(
        "gen_ai.tokens_per_second",
        metric.WithDescription("Token generation speed"),
        metric.WithUnit("tokens/s"),
    )
}

// ä½¿ç”¨ç¤ºä¾‹
func callLLM(ctx context.Context, prompt string) (string, error) {
    ctx, span := tracer.Start(ctx, "llm.call")
    defer span.End()
    
    start := time.Now()
    firstTokenTime := time.Time{}
    
    // æµå¼å“åº”
    stream, err := client.CreateChatCompletionStream(ctx, request)
    if err != nil {
        span.RecordError(err)
        return "", err
    }
    defer stream.Close()
    
    var response strings.Builder
    tokenCount := 0
    
    for {
        chunk, err := stream.Recv()
        if err == io.EOF {
            break
        }
        if err != nil {
            span.RecordError(err)
            return "", err
        }
        
        // è®°å½•ç¬¬ä¸€ä¸ªtokenæ—¶é—´
        if firstTokenTime.IsZero() {
            firstTokenTime = time.Now()
            ttft := firstTokenTime.Sub(start).Milliseconds()
            
            span.SetAttributes(attribute.Int64("gen_ai.ttft_ms", ttft))
            ttftHistogram.Record(ctx, float64(ttft))
        }
        
        response.WriteString(chunk.Choices[0].Delta.Content)
        tokenCount++
    }
    
    duration := time.Since(start)
    
    // è®°å½•å»¶è¿Ÿ
    latencyHistogram.Record(ctx, float64(duration.Milliseconds()),
        metric.WithAttributes(
            attribute.String("model", "gpt-4"),
        ),
    )
    
    // è®°å½•Tokens/s
    if duration.Seconds() > 0 {
        tps := float64(tokenCount) / duration.Seconds()
        tpsGauge.Record(ctx, tps)
    }
    
    span.SetAttributes(
        attribute.Int64("gen_ai.latency_ms", duration.Milliseconds()),
        attribute.Int("gen_ai.output_tokens", tokenCount),
    )
    
    return response.String(), nil
}
```

---

## 6. è´¨é‡ç›‘æ§

```go
// è´¨é‡æŒ‡æ ‡
type QualityMetrics struct {
    relevance   float64  // ç›¸å…³æ€§ (0-1)
    coherence   float64  // è¿è´¯æ€§ (0-1)
    groundedness float64 // äº‹å®æ€§ (0-1)
    safety      float64  // å®‰å…¨æ€§ (0-1)
}

// è¯„ä¼°è¾“å‡ºè´¨é‡
func evaluateQuality(ctx context.Context, prompt, completion string) QualityMetrics {
    // ä½¿ç”¨å¦ä¸€ä¸ªLLMè¯„ä¼°è´¨é‡
    evaluator := NewQualityEvaluator()
    
    metrics := evaluator.Evaluate(ctx, prompt, completion)
    
    // è®°å½•æŒ‡æ ‡
    meter := otel.Meter("genai.quality")
    
    relevanceGauge, _ := meter.Float64Histogram("gen_ai.quality.relevance")
    relevanceGauge.Record(ctx, metrics.relevance)
    
    coherenceGauge, _ := meter.Float64Histogram("gen_ai.quality.coherence")
    coherenceGauge.Record(ctx, metrics.coherence)
    
    // æ£€æµ‹ä½è´¨é‡è¾“å‡º
    if metrics.relevance < 0.5 {
        logger.Warn("Low relevance score", 
            "prompt", prompt[:100],
            "score", metrics.relevance)
    }
    
    return metrics
}

// æ£€æµ‹Hallucination (å¹»è§‰)
func detectHallucination(ctx context.Context, completion string, sources []string) bool {
    // æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŸºäºå¯é æ¥æº
    checker := NewFactChecker()
    
    isGrounded := checker.Check(ctx, completion, sources)
    
    if !isGrounded {
        // è®°å½•å¹»è§‰äº‹ä»¶
        span := trace.SpanFromContext(ctx)
        span.AddEvent("hallucination_detected", trace.WithAttributes(
            attribute.String("completion", completion[:100]),
        ))
        
        hallucinationCounter.Add(ctx, 1)
    }
    
    return !isGrounded
}

// å®‰å…¨æ£€æµ‹
func detectUnsafeContent(ctx context.Context, text string) []string {
    detector := NewSafetyDetector()
    
    violations := detector.Detect(ctx, text)
    
    if len(violations) > 0 {
        span := trace.SpanFromContext(ctx)
        span.SetAttributes(
            attribute.Bool("gen_ai.unsafe_content", true),
            attribute.StringSlice("gen_ai.violations", violations),
        )
        
        safetyViolationCounter.Add(ctx, int64(len(violations)))
    }
    
    return violations
}
```

---

## 7. å®ç°ç¤ºä¾‹

### 7.1 OpenAIé›†æˆ

```go
package genai

import (
    "context"
    
    "github.com/sashabaranov/go-openai"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/attribute"
    "go.opentelemetry.io/otel/trace"
)

type InstrumentedClient struct {
    client *openai.Client
    tracer trace.Tracer
    calculator *CostCalculator
}

func NewInstrumentedClient(apiKey string) *InstrumentedClient {
    return &InstrumentedClient{
        client: openai.NewClient(apiKey),
        tracer: otel.Tracer("genai.openai"),
        calculator: NewCostCalculator(),
    }
}

func (c *InstrumentedClient) CreateChatCompletion(
    ctx context.Context,
    req openai.ChatCompletionRequest,
) (openai.ChatCompletionResponse, error) {
    
    ctx, span := c.tracer.Start(ctx, "openai.chat.completion",
        trace.WithSpanKind(trace.SpanKindClient),
    )
    defer span.End()
    
    // è®°å½•è¯·æ±‚å±æ€§
    span.SetAttributes(
        attribute.String("gen_ai.system", "openai"),
        attribute.String("gen_ai.request.model", req.Model),
        attribute.Int("gen_ai.request.max_tokens", req.MaxTokens),
        attribute.Float64("gen_ai.request.temperature", float64(req.Temperature)),
        attribute.Float64("gen_ai.request.top_p", float64(req.TopP)),
        attribute.Int("gen_ai.request.n", req.N),
    )
    
    // è®°å½•prompt (å¯é€‰ï¼Œæ³¨æ„æ•æ„Ÿæ•°æ®)
    if shouldRecordPrompt() {
        for i, msg := range req.Messages {
            span.AddEvent("gen_ai.prompt", trace.WithAttributes(
                attribute.Int("index", i),
                attribute.String("role", msg.Role),
                attribute.Int("content_length", len(msg.Content)),
                // attribute.String("content", msg.Content), // ç”Ÿäº§ç¯å¢ƒæ…ç”¨
            ))
        }
    }
    
    // è°ƒç”¨API
    resp, err := c.client.CreateChatCompletion(ctx, req)
    if err != nil {
        span.RecordError(err)
        span.SetStatus(codes.Error, err.Error())
        return resp, err
    }
    
    // è®°å½•å“åº”å±æ€§
    span.SetAttributes(
        attribute.String("gen_ai.response.id", resp.ID),
        attribute.String("gen_ai.response.model", resp.Model),
        attribute.Int("gen_ai.usage.input_tokens", resp.Usage.PromptTokens),
        attribute.Int("gen_ai.usage.output_tokens", resp.Usage.CompletionTokens),
        attribute.Int("gen_ai.usage.total_tokens", resp.Usage.TotalTokens),
    )
    
    if len(resp.Choices) > 0 {
        span.SetAttributes(
            attribute.String("gen_ai.response.finish_reason", 
                string(resp.Choices[0].FinishReason)),
        )
    }
    
    // è®¡ç®—æˆæœ¬
    cost := c.calculator.Calculate(
        req.Model,
        resp.Usage.PromptTokens,
        resp.Usage.CompletionTokens,
    )
    span.SetAttributes(attribute.Float64("gen_ai.cost.usd", cost))
    
    // è®°å½•æŒ‡æ ‡
    recordMetrics(ctx, req.Model, resp.Usage, cost)
    
    span.SetStatus(codes.Ok, "")
    return resp, nil
}

func recordMetrics(ctx context.Context, model string, usage openai.Usage, cost float64) {
    meter := otel.Meter("genai.openai")
    
    // Tokenè®¡æ•°
    tokenCounter, _ := meter.Int64Counter("gen_ai.tokens")
    tokenCounter.Add(ctx, int64(usage.TotalTokens),
        metric.WithAttributes(
            attribute.String("model", model),
        ),
    )
    
    // æˆæœ¬
    costCounter, _ := meter.Float64Counter("gen_ai.cost")
    costCounter.Add(ctx, cost,
        metric.WithAttributes(
            attribute.String("model", model),
        ),
    )
}
```

### 7.2 LangChainé›†æˆ

```python
from langchain.callbacks.base import BaseCallbackHandler
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode

class OpenTelemetryCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.tracer = trace.get_tracer(__name__)
        self.span_stack = []
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLMè°ƒç”¨å¼€å§‹"""
        span = self.tracer.start_span("llm.call")
        self.span_stack.append(span)
        
        # è®°å½•è¯·æ±‚å±æ€§
        span.set_attributes({
            "gen_ai.system": "openai",
            "gen_ai.request.model": kwargs.get("invocation_params", {}).get("model_name"),
            "gen_ai.request.temperature": kwargs.get("invocation_params", {}).get("temperature"),
        })
    
    def on_llm_end(self, response, **kwargs):
        """LLMè°ƒç”¨ç»“æŸ"""
        if not self.span_stack:
            return
        
        span = self.span_stack.pop()
        
        # è®°å½•Tokenä½¿ç”¨
        if hasattr(response, "llm_output") and response.llm_output:
            token_usage = response.llm_output.get("token_usage", {})
            span.set_attributes({
                "gen_ai.usage.input_tokens": token_usage.get("prompt_tokens", 0),
                "gen_ai.usage.output_tokens": token_usage.get("completion_tokens", 0),
                "gen_ai.usage.total_tokens": token_usage.get("total_tokens", 0),
            })
        
        span.set_status(Status(StatusCode.OK))
        span.end()
    
    def on_llm_error(self, error, **kwargs):
        """LLMè°ƒç”¨é”™è¯¯"""
        if not self.span_stack:
            return
        
        span = self.span_stack.pop()
        span.record_exception(error)
        span.set_status(Status(StatusCode.ERROR, str(error)))
        span.end()

# ä½¿ç”¨
from langchain.llms import OpenAI

llm = OpenAI(
    temperature=0.7,
    callbacks=[OpenTelemetryCallbackHandler()]
)

response = llm("Summarize this text...")
```

---

## 8. Dashboardä¸å‘Šè­¦

**Grafana Dashboard**:

```text
Panel 1: LLMè°ƒç”¨é‡
Query: sum(rate(gen_ai_tokens_total[5m])) by (model)
Visualization: Time series

Panel 2: Tokenä½¿ç”¨
Query: sum by (model, type) (rate(gen_ai_tokens_total[5m]))
Visualization: Stacked area chart

Panel 3: æˆæœ¬
Query: sum(rate(gen_ai_cost_usd_total[1h])) * 24 * 30
Visualization: Stat (æœˆæˆæœ¬é¢„ä¼°)

Panel 4: å»¶è¿Ÿåˆ†å¸ƒ
Query: histogram_quantile(0.99, rate(gen_ai_latency_bucket[5m]))
Visualization: Heatmap

Panel 5: é”™è¯¯ç‡
Query: rate(gen_ai_errors_total[5m]) / rate(gen_ai_requests_total[5m])
Visualization: Time series

Panel 6: æˆæœ¬åˆ†è§£ (æŒ‰åŠŸèƒ½)
Query: sum by (feature) (rate(gen_ai_cost_usd_total[1h]))
Visualization: Pie chart
```

**å‘Šè­¦è§„åˆ™**:

```yaml
# Prometheuså‘Šè­¦
groups:
  - name: genai_alerts
    rules:
      # é«˜æˆæœ¬å‘Šè­¦
      - alert: HighGenAICost
        expr: sum(rate(gen_ai_cost_usd_total[1h])) * 24 * 30 > 1000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High GenAI monthly cost: {{ $value | humanize }}USD"
      
      # é«˜å»¶è¿Ÿå‘Šè­¦
      - alert: HighGenAILatency
        expr: histogram_quantile(0.99, rate(gen_ai_latency_bucket[5m])) > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High GenAI p99 latency: {{ $value | humanize }}ms"
      
      # é«˜é”™è¯¯ç‡å‘Šè­¦
      - alert: HighGenAIErrorRate
        expr: rate(gen_ai_errors_total[5m]) / rate(gen_ai_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High GenAI error rate: {{ $value | humanizePercentage }}"
      
      # å¼‚å¸¸Tokenä½¿ç”¨
      - alert: AnomalousTokenUsage
        expr: rate(gen_ai_tokens_total[5m]) > avg_over_time(rate(gen_ai_tokens_total[5m])[1h:5m]) * 3
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Anomalous token usage spike"
```

---

## 9. å®‰å…¨ä¸åˆè§„

```text
å®‰å…¨è€ƒè™‘:
1. Promptæ³¨å…¥æ£€æµ‹
   - æ£€æµ‹æ¶æ„Prompt
   - è®°å½•å¯ç–‘è¯·æ±‚
   - è‡ªåŠ¨æ‹¦æˆª

2. æ•°æ®æ³„éœ²é˜²æŠ¤
   - ä¸è®°å½•æ•æ„ŸPrompt/Completion
   - ä½¿ç”¨è„±æ•
   - é™åˆ¶è®¿é—®

3. å®¡è®¡æ—¥å¿—
   - è®°å½•æ‰€æœ‰LLMè°ƒç”¨
   - ç”¨æˆ·å½’å› 
   - åˆè§„è¦æ±‚

4. æˆæœ¬æ§åˆ¶
   - ç”¨æˆ·é™é¢
   - åŠŸèƒ½é™é¢
   - è‡ªåŠ¨ç†”æ–­

ç¤ºä¾‹å®ç°:
func secureTrace(ctx context.Context, prompt, completion string) {
    _, span := tracer.Start(ctx, "llm.call")
    defer span.End()
    
    // 1. æ£€æµ‹Promptæ³¨å…¥
    if detectInjection(prompt) {
        span.SetAttributes(attribute.Bool("security.injection_detected", true))
        injectionCounter.Add(ctx, 1)
        return // æ‹’ç»è¯·æ±‚
    }
    
    // 2. è„±æ•
    sanitizedPrompt := sanitize(prompt)
    
    // 3. åªè®°å½•å…ƒæ•°æ®
    span.SetAttributes(
        attribute.Int("prompt.length", len(prompt)),
        attribute.String("prompt.hash", hash(prompt)),
        // ä¸è®°å½•å®é™…å†…å®¹
    )
    
    // 4. å®¡è®¡æ—¥å¿—
    auditLog(ctx, "llm.call", userID, sanitizedPrompt)
}

åˆè§„å®è·µ:
âœ… æ•°æ®æœ€å°åŒ–
   - åªè®°å½•å¿…è¦ä¿¡æ¯
   - å®šæœŸåˆ é™¤æ—§æ•°æ®

âœ… è®¿é—®æ§åˆ¶
   - RBAC
   - å®¡è®¡è¿½è¸ª

âœ… æ•°æ®ä¿ç•™
   - æ˜ç¡®ä¿ç•™æœŸé™
   - è‡ªåŠ¨æ¸…ç†

âœ… é€æ˜åº¦
   - ç”¨æˆ·çŸ¥æƒ…
   - å¯æŸ¥è¯¢è‡ªå·±çš„æ•°æ®
```

---

## 10. æœ€ä½³å®è·µ

```text
âœ… DO (æ¨è)
1. å§‹ç»ˆè¿½è¸ªTokenä½¿ç”¨
   - æ¯æ¬¡è°ƒç”¨è®°å½•
   - æˆæœ¬å½’å› åˆ°ç”¨æˆ·/åŠŸèƒ½

2. ç›‘æ§å…³é”®æŒ‡æ ‡
   - å»¶è¿Ÿ (p50, p99)
   - é”™è¯¯ç‡
   - æˆæœ¬
   - Token/s

3. é‡‡æ ·Prompt/Completion
   - ç”Ÿäº§ç¯å¢ƒå°‘è®°å½• (< 1%)
   - å¼€å‘ç¯å¢ƒå…¨è®°å½•
   - ä½¿ç”¨è„±æ•

4. è®¾ç½®å‘Šè­¦
   - æˆæœ¬è¶…é¢„ç®—
   - å»¶è¿Ÿè¿‡é«˜
   - é”™è¯¯ç‡é«˜
   - Tokenä½¿ç”¨å¼‚å¸¸

5. æˆæœ¬ä¼˜åŒ–
   - ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
   - ç¼“å­˜é‡å¤è¯·æ±‚
   - ä¼˜åŒ–Prompté•¿åº¦
   - æ‰¹å¤„ç†

âŒ DON'T (é¿å…)
1. ä¸è¦è®°å½•æ•æ„ŸPrompt
2. ä¸è¦å¿½ç•¥æˆæœ¬è¿½è¸ª
3. ä¸è¦è·³è¿‡é”™è¯¯å¤„ç†
4. ä¸è¦é˜»å¡åº”ç”¨ (åŒæ­¥è¿½è¸ª)
5. ä¸è¦å¿½ç•¥è´¨é‡ç›‘æ§

ç¤ºä¾‹æœ€ä½³é…ç½®:
span.SetAttributes(
    // âœ… å¿…éœ€
    attribute.String("gen_ai.system", "openai"),
    attribute.String("gen_ai.request.model", "gpt-4"),
    attribute.Int("gen_ai.usage.input_tokens", 150),
    attribute.Int("gen_ai.usage.output_tokens", 50),
    attribute.Float64("gen_ai.cost.usd", 0.003),
    
    // âœ… æ¨è
    attribute.Int64("gen_ai.latency_ms", 1234),
    attribute.String("gen_ai.response.finish_reason", "stop"),
    
    // âš ï¸ å¯é€‰ (å°å¿ƒæ•æ„Ÿæ•°æ®)
    attribute.Int("gen_ai.prompt.length", len(prompt)),
    
    // âŒ é¿å… (ç”Ÿäº§ç¯å¢ƒ)
    // attribute.String("gen_ai.prompt.content", prompt),
)
```

---

## 11. å‚è€ƒèµ„æº

- **GenAIè¯­ä¹‰çº¦å®š**: <https://opentelemetry.io/docs/specs/semconv/gen-ai/>
- **LangChain Observability**: <https://python.langchain.com/docs/integrations/callbacks/opentelemetry>
- **OpenLIT**: <https://github.com/openlit/openlit> (GenAIå¯è§‚æµ‹æ€§å·¥å…·)

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ  
**è§„èŒƒçŠ¶æ€**: ğŸš§ å®éªŒæ€§ (Experimental)  
**ç”Ÿäº§å°±ç»ª**: éƒ¨åˆ†åŠŸèƒ½å·²æˆç†Ÿ
