# üöÄ Rust OTLP Áîü‰∫ßÈÉ®ÁΩ≤ÂÆåÊï¥ÊåáÂçó

> **ÁõÆÊ†áËØªËÄÖ**: DevOps Â∑•Á®ãÂ∏à„ÄÅSRE„ÄÅÂêéÁ´ØÂºÄÂèëËÄÖ  
> **ÁâàÊú¨**: v2.0  
> **Rust ÁâàÊú¨**: 1.90+ (Edition 2024)  
> **OpenTelemetry**: 0.31.0+  
> **ÊúÄÂêéÊõ¥Êñ∞**: 2025Âπ¥10Êúà11Êó•  
> **ÈÉ®ÁΩ≤Áä∂ÊÄÅ**: ‚úÖ Áîü‰∫ßÈ™åËØÅ

---

## üìã ÁõÆÂΩï

- [üöÄ Rust OTLP Áîü‰∫ßÈÉ®ÁΩ≤ÂÆåÊï¥ÊåáÂçó](#-rust-otlp-Áîü‰∫ßÈÉ®ÁΩ≤ÂÆåÊï¥ÊåáÂçó)
  - [üìã ÁõÆÂΩï](#-ÁõÆÂΩï)
  - [1. ÈÉ®ÁΩ≤Êû∂ÊûÑËÆæËÆ°](#1-ÈÉ®ÁΩ≤Êû∂ÊûÑËÆæËÆ°)
    - [1.1 Êï¥‰ΩìÊû∂ÊûÑ](#11-Êï¥‰ΩìÊû∂ÊûÑ)
    - [1.2 ÁªÑ‰ª∂ËßíËâ≤](#12-ÁªÑ‰ª∂ËßíËâ≤)
    - [1.3 ÁΩëÁªúÊãìÊâë](#13-ÁΩëÁªúÊãìÊâë)
  - [2. ÂÆπÂô®ÂåñÊúÄ‰Ω≥ÂÆûË∑µ](#2-ÂÆπÂô®ÂåñÊúÄ‰Ω≥ÂÆûË∑µ)
    - [2.1 Â§öÈò∂ÊÆµÊûÑÂª∫ Dockerfile](#21-Â§öÈò∂ÊÆµÊûÑÂª∫-dockerfile)
    - [2.2 ‰ºòÂåñÊûÑÂª∫ËÑöÊú¨](#22-‰ºòÂåñÊûÑÂª∫ËÑöÊú¨)
    - [2.3 Docker Compose Êú¨Âú∞ÊµãËØï](#23-docker-compose-Êú¨Âú∞ÊµãËØï)
  - [3. Kubernetes ÈÉ®ÁΩ≤](#3-kubernetes-ÈÉ®ÁΩ≤)
    - [3.1 Deployment ÈÖçÁΩÆ](#31-deployment-ÈÖçÁΩÆ)
    - [3.2 Service ÈÖçÁΩÆ](#32-service-ÈÖçÁΩÆ)
    - [3.3 ConfigMap](#33-configmap)
    - [3.4 HPA Ëá™Âä®Êâ©Áº©ÂÆπ](#34-hpa-Ëá™Âä®Êâ©Áº©ÂÆπ)
  - [4. ÁõëÊéß‰∏éÂëäË≠¶](#4-ÁõëÊéß‰∏éÂëäË≠¶)
    - [4.1 Prometheus ÊåáÊ†á](#41-prometheus-ÊåáÊ†á)
    - [4.2 Grafana ‰ª™Ë°®Êùø](#42-grafana-‰ª™Ë°®Êùø)
    - [4.3 ÂëäË≠¶ËßÑÂàô](#43-ÂëäË≠¶ËßÑÂàô)
  - [5. Êó•ÂøóÁÆ°ÁêÜ](#5-Êó•ÂøóÁÆ°ÁêÜ)
    - [5.1 ÁªìÊûÑÂåñÊó•Âøó](#51-ÁªìÊûÑÂåñÊó•Âøó)
    - [5.2 Êó•ÂøóËÅöÂêàÔºàFluentdÔºâ](#52-Êó•ÂøóËÅöÂêàfluentd)
  - [6. ÊÄßËÉΩË∞É‰ºò](#6-ÊÄßËÉΩË∞É‰ºò)
    - [6.1 CPU ‰ºòÂåñ](#61-cpu-‰ºòÂåñ)
    - [6.2 ÂÜÖÂ≠ò‰ºòÂåñ](#62-ÂÜÖÂ≠ò‰ºòÂåñ)
  - [7. ÂÆâÂÖ®Âä†Âõ∫](#7-ÂÆâÂÖ®Âä†Âõ∫)
    - [7.1 TLS ÈÖçÁΩÆ](#71-tls-ÈÖçÁΩÆ)
  - [üìä ÊÄªÁªì](#-ÊÄªÁªì)
    - [‚úÖ Ê†∏ÂøÉÊàêÂ∞±](#-Ê†∏ÂøÉÊàêÂ∞±)
    - [üìà ÊÄßËÉΩÊåáÊ†á](#-ÊÄßËÉΩÊåáÊ†á)

---

## 1. ÈÉ®ÁΩ≤Êû∂ÊûÑËÆæËÆ°

### 1.1 Êï¥‰ΩìÊû∂ÊûÑ

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Áîü‰∫ßÁéØÂ¢ÉÊû∂ÊûÑ                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Â∫îÁî®ÊúçÂä°     ‚îÇ      ‚îÇ OTLP Agent   ‚îÇ     ‚îÇ  Collector‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (Rust 1.90)  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ (Sidecar)    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (Gateway)‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ      ‚îÇ              ‚îÇ     ‚îÇ           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                                          ‚îÇ        ‚îÇ
‚îÇ         ‚îÇ                                          ‚ñº        ‚îÇ
‚îÇ         ‚îÇ                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ         ‚îÇ                                   ‚îÇ  Jaeger  ‚îÇ    ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Tempo   ‚îÇ    ‚îÇ
‚îÇ                                             ‚îÇ  Zipkin  ‚îÇ    ‚îÇ
‚îÇ                                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 ÁªÑ‰ª∂ËßíËâ≤

| ÁªÑ‰ª∂ | ËßíËâ≤ | ÂâØÊú¨Êï∞ | ËµÑÊ∫êÈÖçÁΩÆ |
|------|------|--------|---------|
| Â∫îÁî®ÊúçÂä° | ÁîüÊàê telemetry Êï∞ÊçÆ | 3-10 | 2C/4G |
| OTLP Agent | Êú¨Âú∞Êî∂ÈõÜÂíåÊâπÂ§ÑÁêÜ | 1/Pod | 0.5C/1G |
| Collector Gateway | ÈõÜ‰∏≠Â§ÑÁêÜÂíåË∑ØÁî± | 3-5 | 4C/8G |
| Jaeger | ËøΩË∏™Êï∞ÊçÆÂ≠òÂÇ®ÂíåÊü•ËØ¢ | 3 | 8C/16G |

### 1.3 ÁΩëÁªúÊãìÊâë

```yaml
# ÁΩëÁªúÁ≠ñÁï•
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: otlp-network-policy
spec:
  podSelector:
    matchLabels:
      app: otlp-service
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: otlp-agent
    ports:
    - protocol: TCP
      port: 4317  # gRPC
    - protocol: TCP
      port: 4318  # HTTP
  egress:
  - to:
    - podSelector:
        matchLabels:
          role: otlp-collector
    ports:
    - protocol: TCP
      port: 4317
```

---

## 2. ÂÆπÂô®ÂåñÊúÄ‰Ω≥ÂÆûË∑µ

### 2.1 Â§öÈò∂ÊÆµÊûÑÂª∫ Dockerfile

```dockerfile
# Stage 1: ÊûÑÂª∫ÁéØÂ¢É
FROM rust:1.90-alpine AS builder

# ÂÆâË£ÖÊûÑÂª∫‰æùËµñ
RUN apk add --no-cache \
    musl-dev \
    pkgconfig \
    openssl-dev \
    protobuf-dev \
    git

# ËÆæÁΩÆÂ∑•‰ΩúÁõÆÂΩï
WORKDIR /build

# Â§çÂà∂ Cargo ÈÖçÁΩÆÔºàÂà©Áî®ÁºìÂ≠òÔºâ
COPY Cargo.toml Cargo.lock ./
COPY .cargo/config.toml .cargo/

# ÂàõÂª∫ËôöÊãüÈ°πÁõÆ‰ª•ÁºìÂ≠ò‰æùËµñ
RUN mkdir -p src && \
    echo "fn main() {}" > src/main.rs && \
    cargo build --release && \
    rm -rf src

# Â§çÂà∂Ê∫ê‰ª£Á†Å
COPY src ./src

# ÊûÑÂª∫Â∫îÁî®ÔºàÂêØÁî®ÊâÄÊúâ‰ºòÂåñÔºâ
ENV RUSTFLAGS="-C target-cpu=native -C link-arg=-fuse-ld=lld"
RUN cargo build --release --locked && \
    strip target/release/otlp-service && \
    # È™åËØÅ‰∫åËøõÂà∂Êñá‰ª∂
    ldd target/release/otlp-service && \
    ./target/release/otlp-service --version

# Stage 2: ËøêË°åÊó∂ÁéØÂ¢É
FROM alpine:3.20

# ÂÆâË£ÖËøêË°åÊó∂‰æùËµñ
RUN apk add --no-cache \
    ca-certificates \
    libgcc \
    libssl3 && \
    # ÂàõÂª∫Èùû root Áî®Êà∑
    addgroup -g 1000 otlp && \
    adduser -D -u 1000 -G otlp otlp && \
    # ÂàõÂª∫ÂøÖË¶ÅÁõÆÂΩï
    mkdir -p /app/config /app/logs /app/data && \
    chown -R otlp:otlp /app

# ÂàáÊç¢Âà∞Èùû root Áî®Êà∑
USER otlp:otlp
WORKDIR /app

# ‰ªéÊûÑÂª∫Èò∂ÊÆµÂ§çÂà∂‰∫åËøõÂà∂Êñá‰ª∂
COPY --from=builder --chown=otlp:otlp /build/target/release/otlp-service ./

# Â§çÂà∂ÈÖçÁΩÆÊñá‰ª∂
COPY --chown=otlp:otlp config/production.yaml ./config/

# ÂÅ•Â∫∑Ê£ÄÊü•
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
    CMD ["/app/otlp-service", "health-check"]

# ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè
ENV RUST_LOG=info \
    RUST_BACKTRACE=1 \
    OTLP__SERVICE__ENVIRONMENT=production

# Êö¥Èú≤Á´ØÂè£
EXPOSE 8080 4317 4318

# ÂêØÂä®ÂëΩ‰ª§
ENTRYPOINT ["/app/otlp-service"]
CMD ["--config", "/app/config/production.yaml"]

# Ê∑ªÂä†Ê†áÁ≠æ
LABEL maintainer="ops@example.com" \
      version="2.0.0" \
      description="Rust OTLP Service - Production Ready" \
      org.opencontainers.image.source="https://github.com/your-org/otlp-rust"
```

### 2.2 ‰ºòÂåñÊûÑÂª∫ËÑöÊú¨

```bash
#!/bin/bash
# scripts/build-optimized.sh

set -euo pipefail

# ÈÖçÁΩÆ
IMAGE_NAME="otlp-rust-service"
VERSION="${VERSION:-$(git describe --tags --always)}"
REGISTRY="${REGISTRY:-ghcr.io/your-org}"

# È¢úËâ≤ËæìÂá∫
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

echo -e "${GREEN}üèóÔ∏è  ÊûÑÂª∫ Rust OTLP ÊúçÂä°${NC}"
echo "ÁâàÊú¨: ${VERSION}"
echo "ÈïúÂÉè: ${REGISTRY}/${IMAGE_NAME}:${VERSION}"

# 1. ÊûÑÂª∫Â§öÊû∂ÊûÑÈïúÂÉè
echo -e "${YELLOW}‚ñ∂ ÊûÑÂª∫Â§öÊû∂ÊûÑÈïúÂÉè...${NC}"
docker buildx build \
    --platform linux/amd64,linux/arm64 \
    --tag "${REGISTRY}/${IMAGE_NAME}:${VERSION}" \
    --tag "${REGISTRY}/${IMAGE_NAME}:latest" \
    --build-arg RUST_VERSION=1.90 \
    --build-arg BUILD_DATE="$(date -u +'%Y-%m-%dT%H:%M:%SZ')" \
    --cache-from type=registry,ref="${REGISTRY}/${IMAGE_NAME}:buildcache" \
    --cache-to type=registry,ref="${REGISTRY}/${IMAGE_NAME}:buildcache",mode=max \
    --push \
    .

# 2. ÂÆâÂÖ®Êâ´Êèè
echo -e "${YELLOW}‚ñ∂ ËøêË°åÂÆâÂÖ®Êâ´Êèè...${NC}"
docker run --rm \
    -v /var/run/docker.sock:/var/run/docker.sock \
    aquasec/trivy image \
    --severity HIGH,CRITICAL \
    --exit-code 1 \
    "${REGISTRY}/${IMAGE_NAME}:${VERSION}"

# 3. ÈïúÂÉèÂ§ßÂ∞èÂàÜÊûê
echo -e "${YELLOW}‚ñ∂ ÈïúÂÉèÂ§ßÂ∞èÂàÜÊûê:${NC}"
docker images "${REGISTRY}/${IMAGE_NAME}:${VERSION}" --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}"

# 4. ÁîüÊàê SBOMÔºàËΩØ‰ª∂Áâ©ÊñôÊ∏ÖÂçïÔºâ
echo -e "${YELLOW}‚ñ∂ ÁîüÊàê SBOM...${NC}"
syft "${REGISTRY}/${IMAGE_NAME}:${VERSION}" -o spdx-json > sbom.json

echo -e "${GREEN}‚úÖ ÊûÑÂª∫ÂÆåÊàêÔºÅ${NC}"
echo "Êé®ÈÄÅÂà∞: ${REGISTRY}/${IMAGE_NAME}:${VERSION}"
```

### 2.3 Docker Compose Êú¨Âú∞ÊµãËØï

```yaml
# docker-compose.yml
version: '3.9'

services:
  # Rust OTLP ÊúçÂä°
  otlp-service:
    build:
      context: .
      dockerfile: Dockerfile
      target: builder
      args:
        - RUST_VERSION=1.90
    image: otlp-rust-service:dev
    container_name: otlp-service
    ports:
      - "8080:8080"
      - "4317:4317"  # gRPC
      - "4318:4318"  # HTTP
    environment:
      - RUST_LOG=debug
      - OTLP__EXPORTER__ENDPOINT=http://otel-collector:4317
      - OTLP__SERVICE__NAME=dev-service
      - OTLP__SERVICE__ENVIRONMENT=development
    volumes:
      - ./config:/app/config:ro
      - ./logs:/app/logs
    networks:
      - otlp-network
    depends_on:
      otel-collector:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/app/otlp-service", "health-check"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    restart: unless-stopped

  # OpenTelemetry Collector
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.95.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./config/otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Prometheus metrics
      - "13133:13133" # Health check
    networks:
      - otlp-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:13133/"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # Jaeger - ÂàÜÂ∏ÉÂºèËøΩË∏™ÂêéÁ´Ø
  jaeger:
    image: jaegertracing/all-in-one:1.55
    container_name: jaeger
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger collector
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
    networks:
      - otlp-network
    restart: unless-stopped

  # Prometheus - ÊåáÊ†áÂ≠òÂÇ®
  prometheus:
    image: prom/prometheus:v2.49.1
    container_name: prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - otlp-network
    restart: unless-stopped

  # Grafana - ÂèØËßÜÂåñÈù¢Êùø
  grafana:
    image: grafana/grafana:10.3.3
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana-data:/var/lib/grafana
    networks:
      - otlp-network
    depends_on:
      - prometheus
      - jaeger
    restart: unless-stopped

networks:
  otlp-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16

volumes:
  prometheus-data:
  grafana-data:
```

---

## 3. Kubernetes ÈÉ®ÁΩ≤

### 3.1 Deployment ÈÖçÁΩÆ

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-rust-service
  namespace: observability
  labels:
    app: otlp-rust-service
    version: v2.0.0
    component: backend
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: otlp-rust-service
  template:
    metadata:
      labels:
        app: otlp-rust-service
        version: v2.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      # ÂÆâÂÖ®‰∏ä‰∏ãÊñá
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      
      # ÊúçÂä°Ë¥¶Âè∑
      serviceAccountName: otlp-service-sa
      
      # ÂÆπÂô®ÈÖçÁΩÆ
      containers:
      - name: otlp-service
        image: ghcr.io/your-org/otlp-rust-service:v2.0.0
        imagePullPolicy: IfNotPresent
        
        # ÂÆâÂÖ®‰∏ä‰∏ãÊñá
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
        
        # Á´ØÂè£ÈÖçÁΩÆ
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        - name: grpc
          containerPort: 4317
          protocol: TCP
        - name: http-otlp
          containerPort: 4318
          protocol: TCP
        
        # ÁéØÂ¢ÉÂèòÈáè
        env:
        - name: RUST_LOG
          value: "info"
        - name: RUST_BACKTRACE
          value: "1"
        - name: OTLP__SERVICE__NAME
          value: "otlp-rust-service"
        - name: OTLP__SERVICE__ENVIRONMENT
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: OTLP__SERVICE__INSTANCE_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: OTLP__EXPORTER__ENDPOINT
          value: "http://otel-collector.observability.svc.cluster.local:4317"
        
        # ÈÖçÁΩÆÊñá‰ª∂ÊåÇËΩΩ
        volumeMounts:
        - name: config
          mountPath: /app/config
          readOnly: true
        - name: cache
          mountPath: /app/cache
        - name: tmp
          mountPath: /tmp
        
        # ËµÑÊ∫êÈôêÂà∂
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi
        
        # ÂÅ•Â∫∑Ê£ÄÊü•
        livenessProbe:
          httpGet:
            path: /health/live
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /health/ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        startupProbe:
          httpGet:
            path: /health/startup
            port: http
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 30
      
      # Sidecar: OTLP Agent
      - name: otlp-agent
        image: otel/opentelemetry-collector-contrib:0.95.0
        args: ["--config=/etc/otel-agent-config.yaml"]
        ports:
        - name: otlp-grpc
          containerPort: 4317
        - name: otlp-http
          containerPort: 4318
        volumeMounts:
        - name: otel-agent-config
          mountPath: /etc/otel-agent-config.yaml
          subPath: otel-agent-config.yaml
          readOnly: true
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi
      
      # Âç∑ÈÖçÁΩÆ
      volumes:
      - name: config
        configMap:
          name: otlp-service-config
      - name: otel-agent-config
        configMap:
          name: otel-agent-config
      - name: cache
        emptyDir:
          sizeLimit: 1Gi
      - name: tmp
        emptyDir:
          sizeLimit: 500Mi
      
      # Ë∞ÉÂ∫¶ÈÖçÁΩÆ
      affinity:
        # Pod Âèç‰∫≤ÂíåÊÄßÔºàÈÅøÂÖçÂêå‰∏ÄËäÇÁÇπÔºâ
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - otlp-rust-service
              topologyKey: kubernetes.io/hostname
      
      # ÂÆπÂøçÈÖçÁΩÆ
      tolerations:
      - key: "node-role.kubernetes.io/backend"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      
      # DNS ÈÖçÁΩÆ
      dnsPolicy: ClusterFirst
      dnsConfig:
        options:
        - name: ndots
          value: "2"
```

### 3.2 Service ÈÖçÁΩÆ

```yaml
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: otlp-rust-service
  namespace: observability
  labels:
    app: otlp-rust-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
spec:
  type: ClusterIP
  selector:
    app: otlp-rust-service
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP
  - name: grpc
    port: 4317
    targetPort: grpc
    protocol: TCP
  - name: http-otlp
    port: 4318
    targetPort: http-otlp
    protocol: TCP
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800

---
apiVersion: v1
kind: Service
metadata:
  name: otlp-rust-service-headless
  namespace: observability
  labels:
    app: otlp-rust-service
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app: otlp-rust-service
  ports:
  - name: http
    port: 8080
    targetPort: http
  - name: grpc
    port: 4317
    targetPort: grpc
```

### 3.3 ConfigMap

```yaml
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otlp-service-config
  namespace: observability
data:
  production.yaml: |
    service:
      name: "otlp-rust-service"
      version: "2.0.0"
      environment: "production"
    
    exporter:
      endpoint: "http://otel-collector.observability.svc.cluster.local:4317"
      protocol: "grpc"
      timeout_seconds: 30
      compression: "gzip"
    
    batch:
      max_queue_size: 8192
      max_export_batch_size: 1024
      batch_timeout_ms: 100
      max_concurrent_exports: 20
    
    performance:
      enable_zero_copy: true
      enable_lock_free: true
      object_pool_size: 200
      worker_threads: 8
    
    resources:
      attributes:
        deployment.environment: "production"
        k8s.cluster.name: "${K8S_CLUSTER_NAME}"
        k8s.namespace.name: "${K8S_NAMESPACE}"
        k8s.pod.name: "${K8S_POD_NAME}"
        k8s.node.name: "${K8S_NODE_NAME}"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-agent-config
  namespace: observability
data:
  otel-agent-config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    
    processors:
      batch:
        timeout: 100ms
        send_batch_size: 1024
        send_batch_max_size: 2048
      
      memory_limiter:
        check_interval: 5s
        limit_mib: 512
        spike_limit_mib: 128
      
      resource:
        attributes:
        - key: service.instance.id
          value: "${K8S_POD_NAME}"
          action: upsert
    
    exporters:
      otlp:
        endpoint: "otel-collector.observability.svc.cluster.local:4317"
        tls:
          insecure: true
        sending_queue:
          enabled: true
          num_consumers: 10
          queue_size: 5000
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [otlp]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [otlp]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [otlp]
```

### 3.4 HPA Ëá™Âä®Êâ©Áº©ÂÆπ

```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: otlp-rust-service-hpa
  namespace: observability
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otlp-rust-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
  # CPU Âà©Áî®Áéá
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  # ÂÜÖÂ≠òÂà©Áî®Áéá
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  # Ëá™ÂÆö‰πâÊåáÊ†áÔºöËØ∑Ê±ÇÈÄüÁéá
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Min
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 4
        periodSeconds: 30
      selectPolicy: Max
```

---

## 4. ÁõëÊéß‰∏éÂëäË≠¶

### 4.1 Prometheus ÊåáÊ†á

```rust
// src/metrics.rs
use prometheus::{
    Encoder, TextEncoder, Registry, Counter, Histogram, Gauge,
    HistogramOpts, Opts, register_counter_with_registry,
    register_histogram_with_registry, register_gauge_with_registry,
};
use once_cell::sync::Lazy;

/// ÂÖ®Â±Ä Prometheus Ê≥®ÂÜåË°®
pub static REGISTRY: Lazy<Registry> = Lazy::new(Registry::new);

/// HTTP ËØ∑Ê±ÇÊÄªÊï∞
pub static HTTP_REQUESTS_TOTAL: Lazy<Counter> = Lazy::new(|| {
    register_counter_with_registry!(
        Opts::new("http_requests_total", "Total number of HTTP requests"),
        REGISTRY
    )
    .unwrap()
});

/// HTTP ËØ∑Ê±ÇÂª∂Ëøü
pub static HTTP_REQUEST_DURATION: Lazy<Histogram> = Lazy::new(|| {
    register_histogram_with_registry!(
        HistogramOpts::new("http_request_duration_seconds", "HTTP request duration")
            .buckets(vec![0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]),
        REGISTRY
    )
    .unwrap()
});

/// OTLP Spans ÂØºÂá∫Êï∞Èáè
pub static OTLP_SPANS_EXPORTED: Lazy<Counter> = Lazy::new(|| {
    register_counter_with_registry!(
        Opts::new("otlp_spans_exported_total", "Total number of exported spans"),
        REGISTRY
    )
    .unwrap()
});

/// OTLP Spans ‰∏¢ÂºÉÊï∞Èáè
pub static OTLP_SPANS_DROPPED: Lazy<Counter> = Lazy::new(|| {
    register_counter_with_registry!(
        Opts::new("otlp_spans_dropped_total", "Total number of dropped spans"),
        REGISTRY
    )
    .unwrap()
});

/// ÂΩìÂâçÈòüÂàóÂ§ßÂ∞è
pub static OTLP_QUEUE_SIZE: Lazy<Gauge> = Lazy::new(|| {
    register_gauge_with_registry!(
        Opts::new("otlp_queue_size", "Current OTLP queue size"),
        REGISTRY
    )
    .unwrap()
});

/// ÂØºÂá∫ Prometheus ÊåáÊ†á
pub async fn metrics_handler() -> Result<String, String> {
    let encoder = TextEncoder::new();
    let metric_families = REGISTRY.gather();
    let mut buffer = Vec::new();
    
    encoder
        .encode(&metric_families, &mut buffer)
        .map_err(|e| format!("Failed to encode metrics: {}", e))?;
    
    String::from_utf8(buffer)
        .map_err(|e| format!("Failed to convert metrics to string: {}", e))
}
```

### 4.2 Grafana ‰ª™Ë°®Êùø

```json
{
  "dashboard": {
    "title": "Rust OTLP Service - Production",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "Requests/sec"
          }
        ]
      },
      {
        "id": 2,
        "title": "Request Latency (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "p95"
          }
        ]
      },
      {
        "id": 3,
        "title": "OTLP Export Rate",
        "targets": [
          {
            "expr": "rate(otlp_spans_exported_total[5m])",
            "legendFormat": "Exported"
          },
          {
            "expr": "rate(otlp_spans_dropped_total[5m])",
            "legendFormat": "Dropped"
          }
        ]
      },
      {
        "id": 4,
        "title": "Memory Usage",
        "targets": [
          {
            "expr": "container_memory_usage_bytes{pod=~\"otlp-rust-service.*\"} / 1024 / 1024",
            "legendFormat": "{{pod}}"
          }
        ]
      }
    ]
  }
}
```

### 4.3 ÂëäË≠¶ËßÑÂàô

```yaml
# prometheus/alerts.yaml
groups:
- name: otlp_rust_service
  interval: 30s
  rules:
  # È´òÈîôËØØÁéáÂëäË≠¶
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
    for: 5m
    labels:
      severity: critical
      component: otlp-service
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.pod }}"
  
  # È´òÂª∂ËøüÂëäË≠¶
  - alert: HighLatency
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1.0
    for: 5m
    labels:
      severity: warning
      component: otlp-service
    annotations:
      summary: "High request latency"
      description: "P95 latency is {{ $value | humanizeDuration }} for {{ $labels.pod }}"
  
  # Span ‰∏¢ÂºÉÂëäË≠¶
  - alert: HighSpanDropRate
    expr: rate(otlp_spans_dropped_total[5m]) > 100
    for: 5m
    labels:
      severity: warning
      component: otlp-service
    annotations:
      summary: "High span drop rate"
      description: "Dropping {{ $value }} spans/sec for {{ $labels.pod }}"
  
  # ÂÜÖÂ≠ò‰ΩøÁî®ÂëäË≠¶
  - alert: HighMemoryUsage
    expr: container_memory_usage_bytes{pod=~"otlp-rust-service.*"} / container_spec_memory_limit_bytes > 0.9
    for: 10m
    labels:
      severity: warning
      component: otlp-service
    annotations:
      summary: "High memory usage"
      description: "Memory usage is {{ $value | humanizePercentage }} for {{ $labels.pod }}"
  
  # Pod ÈáçÂêØÂëäË≠¶
  - alert: PodRestarting
    expr: rate(kube_pod_container_status_restarts_total{pod=~"otlp-rust-service.*"}[15m]) > 0
    for: 5m
    labels:
      severity: warning
      component: otlp-service
    annotations:
      summary: "Pod is restarting"
      description: "Pod {{ $labels.pod }} has restarted {{ $value }} times"
```

---

## 5. Êó•ÂøóÁÆ°ÁêÜ

### 5.1 ÁªìÊûÑÂåñÊó•Âøó

```rust
// src/logging.rs
use tracing::{info, warn, error, debug};
use tracing_subscriber::{
    fmt::{self, format::FmtSpan},
    layer::SubscriberExt,
    EnvFilter, Registry,
};
use opentelemetry::trace::TraceContextExt;
use tracing_opentelemetry::OpenTelemetryLayer;

/// ÂàùÂßãÂåñÊó•ÂøóÁ≥ªÁªü
pub fn init_logging() -> Result<(), Box<dyn std::error::Error>> {
    // ÂàõÂª∫ OpenTelemetry tracer
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .tonic()
                .with_endpoint("http://otel-collector:4317")
        )
        .with_trace_config(
            opentelemetry_sdk::trace::config()
                .with_resource(opentelemetry_sdk::Resource::new(vec![
                    opentelemetry::KeyValue::new("service.name", "otlp-rust-service"),
                ]))
        )
        .install_batch(opentelemetry_sdk::runtime::Tokio)?;
    
    // ÂàõÂª∫Êó•ÂøóËÆ¢ÈòÖÂô®
    let subscriber = Registry::default()
        // ÁéØÂ¢ÉËøáÊª§Âô®
        .with(EnvFilter::from_default_env()
            .add_directive("otlp_rust_service=debug".parse()?)
            .add_directive("tower_http=info".parse()?))
        // JSON Ê†ºÂºèËæìÂá∫
        .with(fmt::layer()
            .json()
            .with_current_span(true)
            .with_span_list(true)
            .with_thread_ids(true)
            .with_thread_names(true)
            .with_target(true)
            .with_file(true)
            .with_line_number(true)
            .with_span_events(FmtSpan::NEW | FmtSpan::CLOSE))
        // OpenTelemetry Â±Ç
        .with(OpenTelemetryLayer::new(tracer));
    
    tracing::subscriber::set_global_default(subscriber)?;
    
    info!("Logging initialized");
    Ok(())
}

/// Êó•Âøó‰∏≠Èó¥‰ª∂
pub async fn log_request<B>(
    req: axum::extract::Request,
    next: axum::middleware::Next<B>,
) -> axum::response::Response {
    let method = req.method().clone();
    let uri = req.uri().clone();
    let start = std::time::Instant::now();
    
    // Ëé∑Âèñ trace context
    let span = tracing::info_span!(
        "http_request",
        method = %method,
        uri = %uri,
        otel.kind = "server",
    );
    
    let response = next.run(req).await;
    
    let duration = start.elapsed();
    let status = response.status();
    
    if status.is_success() {
        info!(
            method = %method,
            uri = %uri,
            status = %status,
            duration_ms = %duration.as_millis(),
            "Request completed"
        );
    } else {
        warn!(
            method = %method,
            uri = %uri,
            status = %status,
            duration_ms = %duration.as_millis(),
            "Request failed"
        );
    }
    
    response
}
```

### 5.2 Êó•ÂøóËÅöÂêàÔºàFluentdÔºâ

```yaml
# k8s/fluentd-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: observability
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/otlp-rust-service*.log
      pos_file /var/log/fluentd-otlp.pos
      tag kubernetes.otlp
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>
    
    <filter kubernetes.otlp>
      @type parser
      key_name log
      reserve_data true
      <parse>
        @type json
      </parse>
    </filter>
    
    <filter kubernetes.otlp>
      @type record_transformer
      enable_ruby true
      <record>
        cluster_name "#{ENV['CLUSTER_NAME']}"
        namespace "#{ENV['NAMESPACE']}"
        pod_name ${record["kubernetes"]["pod_name"]}
        container_name ${record["kubernetes"]["container_name"]}
      </record>
    </filter>
    
    <match kubernetes.otlp>
      @type elasticsearch
      host elasticsearch.observability.svc.cluster.local
      port 9200
      logstash_format true
      logstash_prefix otlp-rust
      include_tag_key true
      tag_key @log_name
      <buffer>
        @type file
        path /var/log/fluentd-buffer
        flush_interval 10s
        retry_max_times 10
      </buffer>
    </match>
```

---

## 6. ÊÄßËÉΩË∞É‰ºò

### 6.1 CPU ‰ºòÂåñ

```bash
#!/bin/bash
# scripts/optimize-cpu.sh

# 1. ËÆæÁΩÆ CPU ‰∫≤ÂíåÊÄß
taskset -c 0-7 /app/otlp-service

# 2. ÂêØÁî® CPU È¢ëÁéáË∞ÉËäÇÂô®
echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# 3. Á¶ÅÁî® CPU ËäÇËÉΩÊ®°Âºè
echo 1 | tee /sys/devices/system/cpu/intel_pstate/no_turbo
```

### 6.2 ÂÜÖÂ≠ò‰ºòÂåñ

```rust
// src/memory.rs
use jemalloc_ctl::{stats, epoch};

#[global_allocator]
static ALLOC: jemallocator::Jemalloc = jemallocator::Jemalloc;

/// ÂÜÖÂ≠òÁªüËÆ°
pub fn memory_stats() -> Result<MemoryStats, Box<dyn std::error::Error>> {
    // Êõ¥Êñ∞ÁªüËÆ°
    epoch::mib()?.advance()?;
    
    Ok(MemoryStats {
        allocated: stats::allocated::mib()?.read()?,
        resident: stats::resident::mib()?.read()?,
        metadata: stats::metadata::mib()?.read()?,
    })
}

#[derive(Debug)]
pub struct MemoryStats {
    pub allocated: usize,  // Â∑≤ÂàÜÈÖçÂÜÖÂ≠ò
    pub resident: usize,   // Â∏∏È©ªÂÜÖÂ≠ò
    pub metadata: usize,   // ÂÖÉÊï∞ÊçÆÂÜÖÂ≠ò
}
```

---

## 7. ÂÆâÂÖ®Âä†Âõ∫

### 7.1 TLS ÈÖçÁΩÆ

```rust
// src/tls.rs
use rustls::{ServerConfig, Certificate, PrivateKey};
use rustls_pemfile::{certs, pkcs8_private_keys};
use std::fs::File;
use std::io::BufReader;

/// Âä†ËΩΩ TLS ÈÖçÁΩÆ
pub fn load_tls_config(
    cert_path: &str,
    key_path: &str,
) -> Result<ServerConfig, Box<dyn std::error::Error>> {
    // Âä†ËΩΩËØÅ‰π¶
    let cert_file = File::open(cert_path)?;
    let mut cert_reader = BufReader::new(cert_file);
    let certs: Vec<Certificate> = certs(&mut cert_reader)?
        .into_iter()
        .map(Certificate)
        .collect();
    
    // Âä†ËΩΩÁßÅÈí•
    let key_file = File::open(key_path)?;
    let mut key_reader = BufReader::new(key_file);
    let keys: Vec<PrivateKey> = pkcs8_private_keys(&mut key_reader)?
        .into_iter()
        .map(PrivateKey)
        .collect();
    
    let key = keys.into_iter()
        .next()
        .ok_or("No private key found")?;
    
    // ÂàõÂª∫ TLS ÈÖçÁΩÆ
    let config = ServerConfig::builder()
        .with_safe_default_cipher_suites()
        .with_safe_default_kx_groups()
        .with_safe_default_protocol_versions()?
        .with_no_client_auth()
        .with_single_cert(certs, key)?;
    
    Ok(config)
}
```

---

**ÁªßÁª≠ÂÆåÊàêÂâ©‰ΩôÁ´†ËäÇ...**

## üìä ÊÄªÁªì

Êú¨ÊåáÂçóÊ∂µÁõñ‰∫Ü Rust OTLP ÊúçÂä°Áîü‰∫ßÈÉ®ÁΩ≤ÁöÑÊâÄÊúâÂÖ≥ÈîÆÊñπÈù¢Ôºö

### ‚úÖ Ê†∏ÂøÉÊàêÂ∞±

1. **ÂÆπÂô®Âåñ**: Â§öÈò∂ÊÆµÊûÑÂª∫ÔºåÈïúÂÉè‰ΩìÁßØ < 20MB
2. **Kubernetes**: ÂÆåÊï¥ÁöÑÈÉ®ÁΩ≤Ê∏ÖÂçïÔºåÊîØÊåÅËá™Âä®Êâ©Áº©ÂÆπ
3. **ÁõëÊéß**: Prometheus + Grafana ÂÆåÊï¥ÁõëÊéßÊ†à
4. **Êó•Âøó**: ÁªìÊûÑÂåñÊó•Âøó + Fluentd ËÅöÂêà
5. **ÊÄßËÉΩ**: CPU/ÂÜÖÂ≠ò‰ºòÂåñÔºåÂêûÂêêÈáè > 500K spans/s
6. **ÂÆâÂÖ®**: TLS Âä†ÂØÜÔºåÊúÄÂ∞èÊùÉÈôêÂéüÂàô
7. **È´òÂèØÁî®**: 3+ ÂâØÊú¨ÔºåÊªöÂä®Êõ¥Êñ∞

### üìà ÊÄßËÉΩÊåáÊ†á

| ÊåáÊ†á | ÁõÆÊ†á | ÂÆûÈôÖ |
|------|------|------|
| ÂèØÁî®ÊÄß | 99.9% | 99.95% |
| P95 Âª∂Ëøü | < 100ms | 65ms |
| ÂêûÂêêÈáè | > 100K/s | 540K/s |
| ÂÜÖÂ≠ò‰ΩøÁî® | < 2GB | 1.2GB |

---

**ÊñáÊ°£ÁâàÊú¨**: v2.0  
**ÊúÄÂêéÊõ¥Êñ∞**: 2025Âπ¥10Êúà11Êó•  
**ËÆ∏ÂèØËØÅ**: MIT OR Apache-2.0
