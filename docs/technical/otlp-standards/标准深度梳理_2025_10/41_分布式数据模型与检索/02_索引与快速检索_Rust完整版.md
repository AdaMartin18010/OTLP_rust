# åˆ†å¸ƒå¼ç´¢å¼•ä¸å¿«é€Ÿæ£€ç´¢ - Rust å®Œæ•´å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **Rust ç‰ˆæœ¬**: 1.90+  
> **æœ€åæ›´æ–°**: 2025-10-10

---

## ğŸ“‹ ç›®å½•

- [åˆ†å¸ƒå¼ç´¢å¼•ä¸å¿«é€Ÿæ£€ç´¢ - Rust å®Œæ•´å®ç°](#åˆ†å¸ƒå¼ç´¢å¼•ä¸å¿«é€Ÿæ£€ç´¢---rust-å®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
    - [ç´¢å¼•ç±»å‹](#ç´¢å¼•ç±»å‹)
    - [æ ¸å¿ƒæŒ‘æˆ˜](#æ ¸å¿ƒæŒ‘æˆ˜)
  - [ğŸ” TraceID ç´¢å¼•](#-traceid-ç´¢å¼•)
    - [1. Bloom Filter é¢„è¿‡æ»¤](#1-bloom-filter-é¢„è¿‡æ»¤)
    - [2. LSM-Tree ç´¢å¼•](#2-lsm-tree-ç´¢å¼•)
    - [3. å€’æ’ç´¢å¼•ï¼ˆæ ‡ç­¾æ£€ç´¢ï¼‰](#3-å€’æ’ç´¢å¼•æ ‡ç­¾æ£€ç´¢)
  - [ğŸ“Š Metric åç§°ç´¢å¼•](#-metric-åç§°ç´¢å¼•)
    - [1. Trie æ ‘ç´¢å¼•](#1-trie-æ ‘ç´¢å¼•)
    - [2. æ—¶é—´åºåˆ—ç´¢å¼•](#2-æ—¶é—´åºåˆ—ç´¢å¼•)
  - [ğŸ“ æ—¥å¿—å…¨æ–‡ç´¢å¼•](#-æ—¥å¿—å…¨æ–‡ç´¢å¼•)
    - [1. å…¨æ–‡å€’æ’ç´¢å¼•](#1-å…¨æ–‡å€’æ’ç´¢å¼•)
    - [2. N-Gram ç´¢å¼•](#2-n-gram-ç´¢å¼•)
  - [âš¡ åˆ†å¸ƒå¼ç´¢å¼•åè°ƒ](#-åˆ†å¸ƒå¼ç´¢å¼•åè°ƒ)
    - [1. åˆ†å¸ƒå¼å€’æ’ç´¢å¼•](#1-åˆ†å¸ƒå¼å€’æ’ç´¢å¼•)
    - [2. åˆ†å¸ƒå¼æŸ¥è¯¢æ‰§è¡Œå™¨](#2-åˆ†å¸ƒå¼æŸ¥è¯¢æ‰§è¡Œå™¨)
  - [ğŸ’¾ ç´¢å¼•æŒä¹…åŒ–ä¸æ¢å¤](#-ç´¢å¼•æŒä¹…åŒ–ä¸æ¢å¤)
    - [1. ç´¢å¼•å¿«ç…§ç®¡ç†å™¨](#1-ç´¢å¼•å¿«ç…§ç®¡ç†å™¨)
  - [ğŸ“ˆ å®Œæ•´ç¤ºä¾‹ï¼šç»Ÿä¸€ç´¢å¼•ç³»ç»Ÿ](#-å®Œæ•´ç¤ºä¾‹ç»Ÿä¸€ç´¢å¼•ç³»ç»Ÿ)
  - [ğŸ¯ æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
    - [ç¼“å­˜ç­–ç•¥](#ç¼“å­˜ç­–ç•¥)
    - [å‹ç¼©æŠ€æœ¯](#å‹ç¼©æŠ€æœ¯)
  - [ğŸ“Š æ€§èƒ½æŒ‡æ ‡](#-æ€§èƒ½æŒ‡æ ‡)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## ğŸ¯ æ¦‚è¿°

åˆ†å¸ƒå¼ OTLP ç³»ç»Ÿéœ€è¦é«˜æ•ˆçš„ç´¢å¼•æœºåˆ¶æ¥æ”¯æŒå¿«é€Ÿæ£€ç´¢ã€‚
æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä½¿ç”¨ Rust å®ç°ç”Ÿäº§çº§åˆ«çš„ç´¢å¼•ç³»ç»Ÿã€‚

### ç´¢å¼•ç±»å‹

- âœ… **TraceID ç´¢å¼•**: ç²¾ç¡®æŸ¥æ‰¾
- âœ… **å±æ€§æ ‡ç­¾ç´¢å¼•**: å¤šç»´è¿‡æ»¤
- âœ… **æ—¶é—´ç´¢å¼•**: èŒƒå›´æŸ¥è¯¢
- âœ… **å…¨æ–‡ç´¢å¼•**: æ—¥å¿—æœç´¢
- âœ… **å€’æ’ç´¢å¼•**: æ ‡ç­¾æŸ¥æ‰¾

### æ ¸å¿ƒæŒ‘æˆ˜

- ğŸ”´ **é«˜åŸºæ•°é—®é¢˜**: Trace/Span ID æ•°é‡å·¨å¤§
- ğŸ”´ **å®æ—¶æ€§è¦æ±‚**: å†™å…¥åç«‹å³å¯æŸ¥
- ğŸ”´ **åˆ†å¸ƒå¼ä¸€è‡´æ€§**: å¤šèŠ‚ç‚¹ç´¢å¼•åŒæ­¥
- ğŸ”´ **å­˜å‚¨å¼€é”€**: ç´¢å¼•å¤§å°æ§åˆ¶

---

## ğŸ” TraceID ç´¢å¼•

### 1. Bloom Filter é¢„è¿‡æ»¤

**ç”¨é€”**: å¿«é€Ÿåˆ¤æ–­ TraceID æ˜¯å¦å­˜åœ¨ï¼Œé¿å…æ— æ•ˆæŸ¥è¯¢

```rust
use std::collections::hash_map::DefaultHasher;
use std::hash::{Hash, Hasher};
use bitvec::prelude::*;

/// Bloom Filter å®ç°
pub struct BloomFilter {
    /// ä½æ•°ç»„
    bits: BitVec,
    
    /// å“ˆå¸Œå‡½æ•°æ•°é‡
    num_hashes: usize,
    
    /// é¢„æœŸå…ƒç´ æ•°é‡
    expected_elements: usize,
    
    /// å®é™…å…ƒç´ æ•°é‡
    actual_elements: std::sync::atomic::AtomicUsize,
}

impl BloomFilter {
    /// åˆ›å»º Bloom Filter
    /// 
    /// # å‚æ•°
    /// - `expected_elements`: é¢„æœŸå…ƒç´ æ•°é‡
    /// - `false_positive_rate`: æœŸæœ›çš„å‡é˜³æ€§ç‡ï¼ˆå¦‚ 0.01 è¡¨ç¤º 1%ï¼‰
    pub fn new(expected_elements: usize, false_positive_rate: f64) -> Self {
        // è®¡ç®—æœ€ä¼˜ä½æ•°ç»„å¤§å°
        let num_bits = Self::optimal_num_bits(expected_elements, false_positive_rate);
        
        // è®¡ç®—æœ€ä¼˜å“ˆå¸Œå‡½æ•°æ•°é‡
        let num_hashes = Self::optimal_num_hashes(num_bits, expected_elements);
        
        Self {
            bits: bitvec![0; num_bits],
            num_hashes,
            expected_elements,
            actual_elements: std::sync::atomic::AtomicUsize::new(0),
        }
    }
    
    fn optimal_num_bits(n: usize, p: f64) -> usize {
        let n = n as f64;
        let numerator = -n * p.ln();
        let denominator = (2.0_f64.ln()).powi(2);
        (numerator / denominator).ceil() as usize
    }
    
    fn optimal_num_hashes(m: usize, n: usize) -> usize {
        let m = m as f64;
        let n = n as f64;
        ((m / n) * 2.0_f64.ln()).ceil() as usize
    }
    
    /// æ’å…¥å…ƒç´ 
    pub fn insert<T: Hash>(&mut self, item: &T) {
        for i in 0..self.num_hashes {
            let hash = self.hash_with_seed(item, i);
            let index = (hash as usize) % self.bits.len();
            self.bits.set(index, true);
        }
        
        self.actual_elements.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
    }
    
    /// æ£€æŸ¥å…ƒç´ æ˜¯å¦å¯èƒ½å­˜åœ¨
    pub fn contains<T: Hash>(&self, item: &T) -> bool {
        for i in 0..self.num_hashes {
            let hash = self.hash_with_seed(item, i);
            let index = (hash as usize) % self.bits.len();
            
            if !self.bits[index] {
                return false; // ç¡®å®šä¸å­˜åœ¨
            }
        }
        
        true // å¯èƒ½å­˜åœ¨ï¼ˆæœ‰å‡é˜³æ€§ï¼‰
    }
    
    fn hash_with_seed<T: Hash>(&self, item: &T, seed: usize) -> u64 {
        let mut hasher = DefaultHasher::new();
        item.hash(&mut hasher);
        seed.hash(&mut hasher);
        hasher.finish()
    }
    
    /// è·å–å‡é˜³æ€§ç‡ä¼°è®¡
    pub fn false_positive_rate(&self) -> f64 {
        let m = self.bits.len() as f64;
        let n = self.actual_elements.load(std::sync::atomic::Ordering::Relaxed) as f64;
        let k = self.num_hashes as f64;
        
        (1.0 - (-k * n / m).exp()).powf(k)
    }
}

/// Trace ID Bloom Filter ç´¢å¼•
pub struct TraceIdBloomIndex {
    /// åˆ†åŒºçš„ Bloom Filters
    filters: std::sync::Arc<tokio::sync::RwLock<HashMap<String, BloomFilter>>>,
}

impl TraceIdBloomIndex {
    pub fn new() -> Self {
        Self {
            filters: std::sync::Arc::new(tokio::sync::RwLock::new(HashMap::new())),
        }
    }
    
    /// ä¸ºåˆ†åŒºæ·»åŠ  TraceID
    pub async fn insert(&self, partition_id: &str, trace_id: &[u8; 16]) {
        let mut filters = self.filters.write().await;
        
        let filter = filters
            .entry(partition_id.to_string())
            .or_insert_with(|| BloomFilter::new(100_000, 0.01)); // 10ä¸‡å…ƒç´ ï¼Œ1%å‡é˜³æ€§
        
        filter.insert(trace_id);
    }
    
    /// æ£€æŸ¥ TraceID æ˜¯å¦å¯èƒ½åœ¨åˆ†åŒºä¸­
    pub async fn might_contain(&self, partition_id: &str, trace_id: &[u8; 16]) -> bool {
        let filters = self.filters.read().await;
        
        if let Some(filter) = filters.get(partition_id) {
            filter.contains(trace_id)
        } else {
            false // åˆ†åŒºä¸å­˜åœ¨
        }
    }
    
    /// æŸ¥æ‰¾ TraceID å¯èƒ½å­˜åœ¨çš„åˆ†åŒº
    pub async fn find_candidate_partitions(&self, trace_id: &[u8; 16]) -> Vec<String> {
        let filters = self.filters.read().await;
        
        let mut candidates = Vec::new();
        
        for (partition_id, filter) in filters.iter() {
            if filter.contains(trace_id) {
                candidates.push(partition_id.clone());
            }
        }
        
        candidates
    }
}
```

---

### 2. LSM-Tree ç´¢å¼•

**ç”¨é€”**: é«˜æ•ˆçš„å†™å…¥å’ŒèŒƒå›´æŸ¥è¯¢

```rust
use serde::{Serialize, Deserialize};
use std::sync::Arc;
use tokio::sync::RwLock;

/// LSM-Tree å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
pub struct LsmTree<K, V>
where
    K: Ord + Clone + Serialize + for<'de> Deserialize<'de>,
    V: Clone + Serialize + for<'de> Deserialize<'de>,
{
    /// MemTableï¼ˆå†…å­˜è¡¨ï¼‰
    memtable: Arc<RwLock<BTreeMap<K, V>>>,
    
    /// SSTablesï¼ˆç£ç›˜è¡¨ï¼‰
    sstables: Arc<RwLock<Vec<SSTable<K, V>>>>,
    
    /// MemTable æœ€å¤§å¤§å°
    memtable_size_limit: usize,
    
    /// æ•°æ®ç›®å½•
    data_dir: std::path::PathBuf,
}

use std::collections::BTreeMap;

impl<K, V> LsmTree<K, V>
where
    K: Ord + Clone + Serialize + for<'de> Deserialize<'de> + std::fmt::Debug,
    V: Clone + Serialize + for<'de> Deserialize<'de> + std::fmt::Debug,
{
    pub fn new(data_dir: std::path::PathBuf, memtable_size_limit: usize) -> Self {
        std::fs::create_dir_all(&data_dir).ok();
        
        Self {
            memtable: Arc::new(RwLock::new(BTreeMap::new())),
            sstables: Arc::new(RwLock::new(Vec::new())),
            memtable_size_limit,
            data_dir,
        }
    }
    
    /// æ’å…¥é”®å€¼å¯¹
    pub async fn put(&self, key: K, value: V) -> Result<(), Box<dyn std::error::Error>> {
        let mut memtable = self.memtable.write().await;
        memtable.insert(key, value);
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦ flush
        if memtable.len() >= self.memtable_size_limit {
            drop(memtable); // é‡Šæ”¾é”
            self.flush().await?;
        }
        
        Ok(())
    }
    
    /// æŸ¥è¯¢é”®
    pub async fn get(&self, key: &K) -> Option<V> {
        // 1. å…ˆæŸ¥ MemTable
        {
            let memtable = self.memtable.read().await;
            if let Some(value) = memtable.get(key) {
                return Some(value.clone());
            }
        }
        
        // 2. æŒ‰ä»æ–°åˆ°æ—§çš„é¡ºåºæŸ¥è¯¢ SSTables
        let sstables = self.sstables.read().await;
        
        for sstable in sstables.iter().rev() {
            if let Some(value) = sstable.get(key) {
                return Some(value);
            }
        }
        
        None
    }
    
    /// èŒƒå›´æŸ¥è¯¢
    pub async fn range(&self, start: &K, end: &K) -> Vec<(K, V)> {
        let mut results = BTreeMap::new();
        
        // 1. ä» MemTable æŸ¥è¯¢
        {
            let memtable = self.memtable.read().await;
            for (k, v) in memtable.range(start.clone()..=end.clone()) {
                results.insert(k.clone(), v.clone());
            }
        }
        
        // 2. ä» SSTables æŸ¥è¯¢ï¼ˆæ–°æ•°æ®è¦†ç›–æ—§æ•°æ®ï¼‰
        let sstables = self.sstables.read().await;
        
        for sstable in sstables.iter().rev() {
            for (k, v) in sstable.range(start, end) {
                results.entry(k).or_insert(v);
            }
        }
        
        results.into_iter().collect()
    }
    
    /// å°† MemTable flush åˆ°ç£ç›˜
    async fn flush(&self) -> Result<(), Box<dyn std::error::Error>> {
        // 1. åˆ›å»ºæ–°çš„ç©º MemTable
        let old_memtable = {
            let mut memtable = self.memtable.write().await;
            std::mem::replace(&mut *memtable, BTreeMap::new())
        };
        
        if old_memtable.is_empty() {
            return Ok(());
        }
        
        // 2. ç”Ÿæˆ SSTable æ–‡ä»¶å
        let timestamp = chrono::Utc::now().timestamp_millis();
        let sstable_path = self.data_dir.join(format!("sstable_{}.sst", timestamp));
        
        // 3. å†™å…¥ SSTable
        let sstable = SSTable::create(sstable_path, old_memtable).await?;
        
        // 4. æ·»åŠ åˆ° SSTables åˆ—è¡¨
        {
            let mut sstables = self.sstables.write().await;
            sstables.push(sstable);
        }
        
        // 5. è§¦å‘ Compactionï¼ˆå¦‚æœéœ€è¦ï¼‰
        self.maybe_compact().await?;
        
        Ok(())
    }
    
    /// å¯èƒ½è§¦å‘ Compaction
    async fn maybe_compact(&self) -> Result<(), Box<dyn std::error::Error>> {
        let sstables = self.sstables.read().await;
        
        // ç®€å•ç­–ç•¥ï¼šå½“ SSTable æ•°é‡è¶…è¿‡ 10 æ—¶è¿›è¡Œ Compaction
        if sstables.len() > 10 {
            drop(sstables);
            self.compact().await?;
        }
        
        Ok(())
    }
    
    /// Compactionï¼ˆåˆå¹¶ SSTablesï¼‰
    async fn compact(&self) -> Result<(), Box<dyn std::error::Error>> {
        // ç®€åŒ–å®ç°ï¼šåˆå¹¶æ‰€æœ‰ SSTables
        let old_sstables = {
            let mut sstables = self.sstables.write().await;
            std::mem::replace(&mut *sstables, Vec::new())
        };
        
        if old_sstables.is_empty() {
            return Ok(());
        }
        
        // åˆå¹¶æ‰€æœ‰æ•°æ®
        let mut merged_data = BTreeMap::new();
        
        for sstable in &old_sstables {
            for (k, v) in sstable.iter() {
                merged_data.insert(k, v);
            }
        }
        
        // åˆ›å»ºæ–°çš„ SSTable
        let timestamp = chrono::Utc::now().timestamp_millis();
        let sstable_path = self.data_dir.join(format!("sstable_compacted_{}.sst", timestamp));
        
        let new_sstable = SSTable::create(sstable_path, merged_data).await?;
        
        // æ›´æ–° SSTables åˆ—è¡¨
        {
            let mut sstables = self.sstables.write().await;
            sstables.push(new_sstable);
        }
        
        // åˆ é™¤æ—§çš„ SSTables
        for sstable in old_sstables {
            sstable.delete().await?;
        }
        
        Ok(())
    }
}

/// SSTableï¼ˆSorted String Tableï¼‰
#[derive(Clone)]
pub struct SSTable<K, V>
where
    K: Ord + Clone + Serialize + for<'de> Deserialize<'de>,
    V: Clone + Serialize + for<'de> Deserialize<'de>,
{
    /// æ–‡ä»¶è·¯å¾„
    path: std::path::PathBuf,
    
    /// ç´¢å¼•ï¼ˆå†…å­˜ä¸­çš„é”®åˆ°æ–‡ä»¶åç§»çš„æ˜ å°„ï¼‰
    index: Arc<BTreeMap<K, u64>>,
    
    /// Bloom Filterï¼ˆå¿«é€Ÿåˆ¤æ–­é”®æ˜¯å¦å­˜åœ¨ï¼‰
    bloom: Arc<BloomFilter>,
}

impl<K, V> SSTable<K, V>
where
    K: Ord + Clone + Serialize + for<'de> Deserialize<'de> + Hash + std::fmt::Debug,
    V: Clone + Serialize + for<'de> Deserialize<'de> + std::fmt::Debug,
{
    /// åˆ›å»º SSTable
    pub async fn create(
        path: std::path::PathBuf,
        data: BTreeMap<K, V>,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        use tokio::io::AsyncWriteExt;
        
        let mut file = tokio::fs::File::create(&path).await?;
        let mut index = BTreeMap::new();
        let mut bloom = BloomFilter::new(data.len(), 0.01);
        
        let mut offset = 0u64;
        
        for (key, value) in &data {
            // åºåˆ—åŒ–é”®å€¼å¯¹
            let serialized = bincode::serialize(&(key, value))?;
            let len = serialized.len() as u64;
            
            // å†™å…¥æ•°æ®
            file.write_all(&serialized).await?;
            
            // æ›´æ–°ç´¢å¼•
            index.insert(key.clone(), offset);
            
            // æ›´æ–° Bloom Filter
            bloom.insert(key);
            
            offset += len;
        }
        
        file.flush().await?;
        
        Ok(Self {
            path,
            index: Arc::new(index),
            bloom: Arc::new(bloom),
        })
    }
    
    /// æŸ¥è¯¢é”®
    pub fn get(&self, key: &K) -> Option<V> {
        // 1. Bloom Filter é¢„è¿‡æ»¤
        if !self.bloom.contains(key) {
            return None;
        }
        
        // 2. æŸ¥è¯¢ç´¢å¼•
        let offset = self.index.get(key)?;
        
        // 3. ä»æ–‡ä»¶è¯»å–ï¼ˆåŒæ­¥ I/Oï¼Œå®é™…åº”è¯¥ç”¨å¼‚æ­¥ï¼‰
        let mut file = std::fs::File::open(&self.path).ok()?;
        use std::io::{Seek, SeekFrom, Read};
        
        file.seek(SeekFrom::Start(*offset)).ok()?;
        
        // è¯»å–æ•°æ®ï¼ˆç®€åŒ–ï¼šè¿™é‡Œå‡è®¾çŸ¥é“é•¿åº¦ï¼‰
        let mut buffer = Vec::new();
        file.read_to_end(&mut buffer).ok()?;
        
        let (stored_key, value): (K, V) = bincode::deserialize(&buffer).ok()?;
        
        if &stored_key == key {
            Some(value)
        } else {
            None
        }
    }
    
    /// èŒƒå›´æŸ¥è¯¢
    pub fn range(&self, start: &K, end: &K) -> Vec<(K, V)> {
        let mut results = Vec::new();
        
        // ä½¿ç”¨ç´¢å¼•å®šä½èŒƒå›´
        for (key, offset) in self.index.range(start.clone()..=end.clone()) {
            if let Some(value) = self.get(key) {
                results.push((key.clone(), value));
            }
        }
        
        results
    }
    
    /// è¿­ä»£æ‰€æœ‰é”®å€¼å¯¹
    pub fn iter(&self) -> Vec<(K, V)> {
        let mut results = Vec::new();
        
        for key in self.index.keys() {
            if let Some(value) = self.get(key) {
                results.push((key.clone(), value));
            }
        }
        
        results
    }
    
    /// åˆ é™¤ SSTable æ–‡ä»¶
    pub async fn delete(&self) -> Result<(), Box<dyn std::error::Error>> {
        tokio::fs::remove_file(&self.path).await?;
        Ok(())
    }
}

/// TraceID LSM ç´¢å¼•
pub type TraceIdLsmIndex = LsmTree<[u8; 16], TraceMetadata>;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TraceMetadata {
    pub partition_id: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub service_name: String,
    pub span_count: usize,
}
```

---

### 3. å€’æ’ç´¢å¼•ï¼ˆæ ‡ç­¾æ£€ç´¢ï¼‰

**ç”¨é€”**: åŸºäºå±æ€§æ ‡ç­¾å¿«é€Ÿè¿‡æ»¤

```rust
use std::collections::{HashMap, HashSet};

/// å€’æ’ç´¢å¼•
pub struct InvertedIndex {
    /// æ ‡ç­¾é”®å€¼ -> Trace IDs
    index: Arc<RwLock<HashMap<(String, String), HashSet<[u8; 16]>>>>,
}

impl InvertedIndex {
    pub fn new() -> Self {
        Self {
            index: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// æ·»åŠ  Trace çš„æ ‡ç­¾
    pub async fn insert(&self, trace_id: [u8; 16], attributes: &HashMap<String, String>) {
        let mut index = self.index.write().await;
        
        for (key, value) in attributes {
            let entry = index
                .entry((key.clone(), value.clone()))
                .or_insert_with(HashSet::new);
            
            entry.insert(trace_id);
        }
    }
    
    /// æŸ¥è¯¢åŒ…å«æŒ‡å®šæ ‡ç­¾çš„æ‰€æœ‰ TraceIDs
    pub async fn query(&self, attribute_key: &str, attribute_value: &str) -> Vec<[u8; 16]> {
        let index = self.index.read().await;
        
        if let Some(trace_ids) = index.get(&(attribute_key.to_string(), attribute_value.to_string())) {
            trace_ids.iter().copied().collect()
        } else {
            Vec::new()
        }
    }
    
    /// å¤šæ¡ä»¶ AND æŸ¥è¯¢
    pub async fn query_and(&self, conditions: Vec<(String, String)>) -> Vec<[u8; 16]> {
        if conditions.is_empty() {
            return Vec::new();
        }
        
        let index = self.index.read().await;
        
        // è·å–ç¬¬ä¸€ä¸ªæ¡ä»¶çš„ç»“æœé›†
        let mut result_set: HashSet<[u8; 16]> = if let Some(first_cond) = conditions.first() {
            index
                .get(&first_cond.clone())
                .cloned()
                .unwrap_or_default()
        } else {
            return Vec::new();
        };
        
        // å¯¹å‰©ä½™æ¡ä»¶æ±‚äº¤é›†
        for condition in conditions.iter().skip(1) {
            if let Some(trace_ids) = index.get(condition) {
                result_set.retain(|id| trace_ids.contains(id));
            } else {
                return Vec::new(); // ä»»ä¸€æ¡ä»¶æ— ç»“æœï¼Œæ•´ä½“ä¸ºç©º
            }
        }
        
        result_set.into_iter().collect()
    }
    
    /// å¤šæ¡ä»¶ OR æŸ¥è¯¢
    pub async fn query_or(&self, conditions: Vec<(String, String)>) -> Vec<[u8; 16]> {
        let index = self.index.read().await;
        
        let mut result_set = HashSet::new();
        
        for condition in conditions {
            if let Some(trace_ids) = index.get(&condition) {
                result_set.extend(trace_ids.iter().copied());
            }
        }
        
        result_set.into_iter().collect()
    }
    
    /// ç»Ÿè®¡æ ‡ç­¾åŸºæ•°
    pub async fn cardinality(&self, attribute_key: &str) -> usize {
        let index = self.index.read().await;
        
        index
            .keys()
            .filter(|(key, _)| key == attribute_key)
            .count()
    }
}
```

---

## ğŸ“Š Metric åç§°ç´¢å¼•

### 1. Trie æ ‘ç´¢å¼•

**ç”¨é€”**: å‰ç¼€åŒ¹é…å’Œè‡ªåŠ¨è¡¥å…¨

```rust
/// Trie æ ‘èŠ‚ç‚¹
#[derive(Debug, Clone)]
struct TrieNode {
    children: HashMap<char, Box<TrieNode>>,
    is_end_of_word: bool,
    metric_metadata: Option<MetricMetadata>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricMetadata {
    pub name: String,
    pub metric_type: String,
    pub unit: Option<String>,
    pub description: Option<String>,
}

/// Trie æ ‘ç´¢å¼•
pub struct TrieIndex {
    root: Arc<RwLock<TrieNode>>,
}

impl TrieIndex {
    pub fn new() -> Self {
        Self {
            root: Arc::new(RwLock::new(TrieNode {
                children: HashMap::new(),
                is_end_of_word: false,
                metric_metadata: None,
            })),
        }
    }
    
    /// æ’å…¥ Metric åç§°
    pub async fn insert(&self, name: &str, metadata: MetricMetadata) {
        let mut current = self.root.write().await;
        
        for ch in name.chars() {
            current = current
                .children
                .entry(ch)
                .or_insert_with(|| {
                    Box::new(TrieNode {
                        children: HashMap::new(),
                        is_end_of_word: false,
                        metric_metadata: None,
                    })
                });
        }
        
        current.is_end_of_word = true;
        current.metric_metadata = Some(metadata);
    }
    
    /// ç²¾ç¡®æŸ¥æ‰¾
    pub async fn search(&self, name: &str) -> Option<MetricMetadata> {
        let current = self.root.read().await;
        let mut node = &*current;
        
        for ch in name.chars() {
            if let Some(child) = node.children.get(&ch) {
                node = child.as_ref();
            } else {
                return None;
            }
        }
        
        if node.is_end_of_word {
            node.metric_metadata.clone()
        } else {
            None
        }
    }
    
    /// å‰ç¼€åŒ¹é…
    pub async fn starts_with(&self, prefix: &str) -> Vec<String> {
        let current = self.root.read().await;
        let mut node = &*current;
        
        // å…ˆæ‰¾åˆ°å‰ç¼€å¯¹åº”çš„èŠ‚ç‚¹
        for ch in prefix.chars() {
            if let Some(child) = node.children.get(&ch) {
                node = child.as_ref();
            } else {
                return Vec::new(); // å‰ç¼€ä¸å­˜åœ¨
            }
        }
        
        // ä»è¯¥èŠ‚ç‚¹å¼€å§‹ DFS æ”¶é›†æ‰€æœ‰å®Œæ•´å•è¯
        let mut results = Vec::new();
        Self::collect_words(node, prefix.to_string(), &mut results);
        
        results
    }
    
    fn collect_words(node: &TrieNode, current_word: String, results: &mut Vec<String>) {
        if node.is_end_of_word {
            results.push(current_word.clone());
        }
        
        for (ch, child) in &node.children {
            let mut new_word = current_word.clone();
            new_word.push(*ch);
            Self::collect_words(child, new_word, results);
        }
    }
    
    /// æ¨¡ç³ŠåŒ¹é…ï¼ˆLevenshtein è·ç¦»ï¼‰
    pub async fn fuzzy_search(&self, query: &str, max_distance: usize) -> Vec<(String, usize)> {
        let current = self.root.read().await;
        let mut results = Vec::new();
        
        Self::fuzzy_search_recursive(
            &*current,
            String::new(),
            query,
            max_distance,
            &mut results,
        );
        
        results.sort_by_key(|(_, dist)| *dist);
        results
    }
    
    fn fuzzy_search_recursive(
        node: &TrieNode,
        current_word: String,
        query: &str,
        max_distance: usize,
        results: &mut Vec<(String, usize)>,
    ) {
        if node.is_end_of_word {
            let distance = Self::levenshtein_distance(&current_word, query);
            if distance <= max_distance {
                results.push((current_word.clone(), distance));
            }
        }
        
        for (ch, child) in &node.children {
            let mut new_word = current_word.clone();
            new_word.push(*ch);
            Self::fuzzy_search_recursive(child, new_word, query, max_distance, results);
        }
    }
    
    fn levenshtein_distance(s1: &str, s2: &str) -> usize {
        let len1 = s1.len();
        let len2 = s2.len();
        let mut matrix = vec![vec![0; len2 + 1]; len1 + 1];
        
        for i in 0..=len1 {
            matrix[i][0] = i;
        }
        
        for j in 0..=len2 {
            matrix[0][j] = j;
        }
        
        for (i, c1) in s1.chars().enumerate() {
            for (j, c2) in s2.chars().enumerate() {
                let cost = if c1 == c2 { 0 } else { 1 };
                
                matrix[i + 1][j + 1] = (matrix[i][j + 1] + 1) // deletion
                    .min(matrix[i + 1][j] + 1) // insertion
                    .min(matrix[i][j] + cost); // substitution
            }
        }
        
        matrix[len1][len2]
    }
}
```

---

### 2. æ—¶é—´åºåˆ—ç´¢å¼•

**ç”¨é€”**: é«˜æ•ˆçš„æ—¶é—´èŒƒå›´æŸ¥è¯¢

```rust
/// æ—¶é—´åºåˆ—ç´¢å¼•
pub struct TimeSeriesIndex {
    /// æ—¶é—´æ¡¶ -> Metric åç§°åˆ—è¡¨
    time_buckets: Arc<RwLock<BTreeMap<i64, HashSet<String>>>>,
    
    /// æ—¶é—´æ¡¶å¤§å°ï¼ˆç§’ï¼‰
    bucket_size: i64,
}

impl TimeSeriesIndex {
    pub fn new(bucket_size_seconds: i64) -> Self {
        Self {
            time_buckets: Arc::new(RwLock::new(BTreeMap::new())),
            bucket_size: bucket_size_seconds,
        }
    }
    
    /// æ’å…¥ Metric
    pub async fn insert(&self, metric_name: String, timestamp: chrono::DateTime<chrono::Utc>) {
        let bucket = timestamp.timestamp() / self.bucket_size;
        
        let mut buckets = self.time_buckets.write().await;
        buckets
            .entry(bucket)
            .or_insert_with(HashSet::new)
            .insert(metric_name);
    }
    
    /// æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰ Metrics
    pub async fn query_range(
        &self,
        start: chrono::DateTime<chrono::Utc>,
        end: chrono::DateTime<chrono::Utc>,
    ) -> Vec<String> {
        let start_bucket = start.timestamp() / self.bucket_size;
        let end_bucket = end.timestamp() / self.bucket_size;
        
        let buckets = self.time_buckets.read().await;
        
        let mut result = HashSet::new();
        
        for (_, metrics) in buckets.range(start_bucket..=end_bucket) {
            result.extend(metrics.iter().cloned());
        }
        
        result.into_iter().collect()
    }
}
```

---

## ğŸ“ æ—¥å¿—å…¨æ–‡ç´¢å¼•

### 1. å…¨æ–‡å€’æ’ç´¢å¼•

```rust
/// å…¨æ–‡å€’æ’ç´¢å¼•
pub struct FullTextIndex {
    /// è¯é¡¹ -> (æ–‡æ¡£ID, ä½ç½®åˆ—è¡¨)
    index: Arc<RwLock<HashMap<String, Vec<(String, Vec<usize>)>>>>,
    
    /// åœç”¨è¯é›†åˆ
    stop_words: HashSet<String>,
}

impl FullTextIndex {
    pub fn new() -> Self {
        let stop_words = ["the", "is", "at", "which", "on", "a", "an", "and", "or", "but"]
            .iter()
            .map(|s| s.to_string())
            .collect();
        
        Self {
            index: Arc::new(RwLock::new(HashMap::new())),
            stop_words,
        }
    }
    
    /// åˆ†è¯
    fn tokenize(&self, text: &str) -> Vec<String> {
        text.to_lowercase()
            .split(|c: char| !c.is_alphanumeric())
            .filter(|s| !s.is_empty() && !self.stop_words.contains(*s))
            .map(|s| s.to_string())
            .collect()
    }
    
    /// ç´¢å¼•æ–‡æ¡£
    pub async fn index_document(&self, doc_id: String, text: &str) {
        let tokens = self.tokenize(text);
        let mut index = self.index.write().await;
        
        for (position, token) in tokens.iter().enumerate() {
            let postings = index.entry(token.clone()).or_insert_with(Vec::new);
            
            // æŸ¥æ‰¾æ˜¯å¦å·²æœ‰è¯¥æ–‡æ¡£çš„è®°å½•
            if let Some((_, positions)) = postings.iter_mut().find(|(id, _)| id == &doc_id) {
                positions.push(position);
            } else {
                postings.push((doc_id.clone(), vec![position]));
            }
        }
    }
    
    /// æœç´¢å•ä¸ªè¯
    pub async fn search(&self, query: &str) -> Vec<String> {
        let tokens = self.tokenize(query);
        
        if tokens.is_empty() {
            return Vec::new();
        }
        
        let index = self.index.read().await;
        
        if let Some(postings) = index.get(&tokens[0]) {
            postings.iter().map(|(doc_id, _)| doc_id.clone()).collect()
        } else {
            Vec::new()
        }
    }
    
    /// çŸ­è¯­æœç´¢ï¼ˆè¦æ±‚è¯æŒ‰é¡ºåºå‡ºç°ï¼‰
    pub async fn phrase_search(&self, query: &str) -> Vec<String> {
        let tokens = self.tokenize(query);
        
        if tokens.is_empty() {
            return Vec::new();
        }
        
        let index = self.index.read().await;
        
        // è·å–ç¬¬ä¸€ä¸ªè¯çš„å€’æ’åˆ—è¡¨
        let first_postings = match index.get(&tokens[0]) {
            Some(p) => p,
            None => return Vec::new(),
        };
        
        let mut result = Vec::new();
        
        // å¯¹æ¯ä¸ªæ–‡æ¡£æ£€æŸ¥çŸ­è¯­æ˜¯å¦åŒ¹é…
        'doc_loop: for (doc_id, first_positions) in first_postings {
            for &first_pos in first_positions {
                let mut all_match = true;
                
                // æ£€æŸ¥åç»­è¯æ˜¯å¦åœ¨è¿ç»­ä½ç½®å‡ºç°
                for (i, token) in tokens.iter().enumerate().skip(1) {
                    let expected_pos = first_pos + i;
                    
                    if let Some(postings) = index.get(token) {
                        if let Some((_, positions)) = postings.iter().find(|(id, _)| id == doc_id) {
                            if !positions.contains(&expected_pos) {
                                all_match = false;
                                break;
                            }
                        } else {
                            all_match = false;
                            break;
                        }
                    } else {
                        all_match = false;
                        break;
                    }
                }
                
                if all_match {
                    result.push(doc_id.clone());
                    continue 'doc_loop;
                }
            }
        }
        
        result
    }
}
```

---

### 2. N-Gram ç´¢å¼•

**ç”¨é€”**: å­ä¸²åŒ¹é…

```rust
/// N-Gram ç´¢å¼•
pub struct NGramIndex {
    /// N-Gram -> æ–‡æ¡£IDåˆ—è¡¨
    index: Arc<RwLock<HashMap<String, HashSet<String>>>>,
    
    /// N å€¼
    n: usize,
}

impl NGramIndex {
    pub fn new(n: usize) -> Self {
        Self {
            index: Arc::new(RwLock::new(HashMap::new())),
            n,
        }
    }
    
    /// ç”Ÿæˆ N-Grams
    fn generate_ngrams(&self, text: &str) -> Vec<String> {
        let text = text.to_lowercase();
        let chars: Vec<char> = text.chars().collect();
        
        if chars.len() < self.n {
            return vec![text];
        }
        
        chars
            .windows(self.n)
            .map(|window| window.iter().collect())
            .collect()
    }
    
    /// ç´¢å¼•æ–‡æ¡£
    pub async fn index_document(&self, doc_id: String, text: &str) {
        let ngrams = self.generate_ngrams(text);
        let mut index = self.index.write().await;
        
        for ngram in ngrams {
            index
                .entry(ngram)
                .or_insert_with(HashSet::new)
                .insert(doc_id.clone());
        }
    }
    
    /// æœç´¢å­ä¸²
    pub async fn search(&self, query: &str) -> Vec<String> {
        let ngrams = self.generate_ngrams(query);
        
        if ngrams.is_empty() {
            return Vec::new();
        }
        
        let index = self.index.read().await;
        
        // è·å–ç¬¬ä¸€ä¸ª N-Gram çš„æ–‡æ¡£é›†åˆ
        let mut result_set = match index.get(&ngrams[0]) {
            Some(docs) => docs.clone(),
            None => return Vec::new(),
        };
        
        // æ±‚äº¤é›†
        for ngram in ngrams.iter().skip(1) {
            if let Some(docs) = index.get(ngram) {
                result_set.retain(|doc| docs.contains(doc));
            } else {
                return Vec::new();
            }
        }
        
        result_set.into_iter().collect()
    }
}
```

---

## âš¡ åˆ†å¸ƒå¼ç´¢å¼•åè°ƒ

### 1. åˆ†å¸ƒå¼å€’æ’ç´¢å¼•

```rust
/// åˆ†å¸ƒå¼å€’æ’ç´¢å¼•åè°ƒå™¨
pub struct DistributedInvertedIndex {
    /// æœ¬åœ°ç´¢å¼•
    local_index: Arc<InvertedIndex>,
    
    /// è¿œç¨‹èŠ‚ç‚¹å®¢æˆ·ç«¯
    remote_clients: Arc<RwLock<HashMap<String, RemoteIndexClient>>>,
    
    /// åˆ†ç‰‡ç­–ç•¥
    shard_strategy: ShardStrategy,
}

#[derive(Debug, Clone)]
pub enum ShardStrategy {
    /// åŸºäºé”®çš„å“ˆå¸Œåˆ†ç‰‡
    HashBased { num_shards: usize },
    
    /// åŸºäºèŒƒå›´åˆ†ç‰‡
    RangeBased,
}

impl DistributedInvertedIndex {
    pub fn new(local_index: Arc<InvertedIndex>, shard_strategy: ShardStrategy) -> Self {
        Self {
            local_index,
            remote_clients: Arc::new(RwLock::new(HashMap::new())),
            shard_strategy,
        }
    }
    
    /// æ³¨å†Œè¿œç¨‹èŠ‚ç‚¹
    pub async fn register_remote_node(&self, node_id: String, client: RemoteIndexClient) {
        let mut clients = self.remote_clients.write().await;
        clients.insert(node_id, client);
    }
    
    /// ç¡®å®šé”®åº”è¯¥è·¯ç”±åˆ°å“ªä¸ªèŠ‚ç‚¹
    fn get_target_node(&self, key: &str) -> Option<String> {
        match &self.shard_strategy {
            ShardStrategy::HashBased { num_shards } => {
                use std::collections::hash_map::DefaultHasher;
                use std::hash::{Hash, Hasher};
                
                let mut hasher = DefaultHasher::new();
                key.hash(&mut hasher);
                let shard_id = (hasher.finish() as usize) % num_shards;
                
                Some(format!("node-{}", shard_id))
            }
            ShardStrategy::RangeBased => {
                // ç®€åŒ–å®ç°
                None
            }
        }
    }
    
    /// åˆ†å¸ƒå¼æŸ¥è¯¢
    pub async fn distributed_query(&self, conditions: Vec<(String, String)>) -> Vec<[u8; 16]> {
        // 1. ç¡®å®šéœ€è¦æŸ¥è¯¢çš„èŠ‚ç‚¹
        let mut node_queries: HashMap<String, Vec<(String, String)>> = HashMap::new();
        
        for condition in conditions {
            if let Some(node_id) = self.get_target_node(&condition.0) {
                node_queries
                    .entry(node_id)
                    .or_insert_with(Vec::new)
                    .push(condition);
            }
        }
        
        // 2. å¹¶å‘æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹
        let mut handles = Vec::new();
        let clients = self.remote_clients.read().await;
        
        for (node_id, queries) in node_queries {
            if let Some(client) = clients.get(&node_id) {
                let client = client.clone();
                let handle = tokio::spawn(async move {
                    client.query_and(queries).await
                });
                handles.push(handle);
            }
        }
        
        // 3. åˆå¹¶ç»“æœï¼ˆæ±‚äº¤é›†ï¼‰
        let mut result_sets = Vec::new();
        
        for handle in handles {
            if let Ok(Ok(trace_ids)) = handle.await {
                result_sets.push(trace_ids.into_iter().collect::<HashSet<_>>());
            }
        }
        
        if result_sets.is_empty() {
            return Vec::new();
        }
        
        // æ±‚äº¤é›†
        let mut result = result_sets[0].clone();
        
        for set in result_sets.iter().skip(1) {
            result.retain(|id| set.contains(id));
        }
        
        result.into_iter().collect()
    }
}

/// è¿œç¨‹ç´¢å¼•å®¢æˆ·ç«¯ï¼ˆç®€åŒ–å®ç°ï¼‰
#[derive(Clone)]
pub struct RemoteIndexClient {
    node_url: String,
    http_client: reqwest::Client,
}

impl RemoteIndexClient {
    pub fn new(node_url: String) -> Self {
        Self {
            node_url,
            http_client: reqwest::Client::new(),
        }
    }
    
    pub async fn query_and(&self, conditions: Vec<(String, String)>) -> Result<Vec<[u8; 16]>, Box<dyn std::error::Error>> {
        let url = format!("{}/index/query_and", self.node_url);
        
        let response = self.http_client
            .post(&url)
            .json(&conditions)
            .send()
            .await?;
        
        let trace_ids = response.json::<Vec<[u8; 16]>>().await?;
        
        Ok(trace_ids)
    }
}
```

---

### 2. åˆ†å¸ƒå¼æŸ¥è¯¢æ‰§è¡Œå™¨

```rust
/// åˆ†å¸ƒå¼æŸ¥è¯¢æ‰§è¡Œå™¨
pub struct DistributedQueryExecutor {
    distributed_index: Arc<DistributedInvertedIndex>,
    shard_manager: Arc<crate::ShardManager>,
}

impl DistributedQueryExecutor {
    pub fn new(
        distributed_index: Arc<DistributedInvertedIndex>,
        shard_manager: Arc<crate::ShardManager>,
    ) -> Self {
        Self {
            distributed_index,
            shard_manager,
        }
    }
    
    /// æ‰§è¡Œå¤æ‚æŸ¥è¯¢
    pub async fn execute_query(&self, query: Query) -> Result<Vec<SearchResult>, String> {
        match query {
            Query::Attribute { key, value } => {
                let trace_ids = self.distributed_index
                    .distributed_query(vec![(key, value)])
                    .await;
                
                self.fetch_traces(trace_ids).await
            }
            
            Query::And(queries) => {
                // å¯¹æ‰€æœ‰å­æŸ¥è¯¢æ±‚äº¤é›†
                let mut result_sets = Vec::new();
                
                for query in queries {
                    let results = self.execute_query(*query).await?;
                    result_sets.push(results.into_iter().collect::<HashSet<_>>());
                }
                
                if result_sets.is_empty() {
                    return Ok(Vec::new());
                }
                
                let mut result = result_sets[0].clone();
                
                for set in result_sets.iter().skip(1) {
                    result.retain(|r| set.contains(r));
                }
                
                Ok(result.into_iter().collect())
            }
            
            Query::Or(queries) => {
                // å¯¹æ‰€æœ‰å­æŸ¥è¯¢æ±‚å¹¶é›†
                let mut result_set = HashSet::new();
                
                for query in queries {
                    let results = self.execute_query(*query).await?;
                    result_set.extend(results);
                }
                
                Ok(result_set.into_iter().collect())
            }
            
            Query::TimeRange { start, end } => {
                self.query_by_time_range(start, end).await
            }
        }
    }
    
    async fn fetch_traces(&self, trace_ids: Vec<[u8; 16]>) -> Result<Vec<SearchResult>, String> {
        // ç®€åŒ–å®ç°ï¼šå®é™…éœ€è¦ä»å­˜å‚¨å±‚è·å–å®Œæ•´çš„ Trace æ•°æ®
        let results = trace_ids
            .into_iter()
            .map(|id| SearchResult {
                trace_id: id,
                score: 1.0,
            })
            .collect();
        
        Ok(results)
    }
    
    async fn query_by_time_range(
        &self,
        start: chrono::DateTime<chrono::Utc>,
        end: chrono::DateTime<chrono::Utc>,
    ) -> Result<Vec<SearchResult>, String> {
        // TODO: å®ç°æ—¶é—´èŒƒå›´æŸ¥è¯¢
        Ok(Vec::new())
    }
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct SearchResult {
    pub trace_id: [u8; 16],
    pub score: f64,
}

#[derive(Debug, Clone)]
pub enum Query {
    Attribute { key: String, value: String },
    And(Vec<Box<Query>>),
    Or(Vec<Box<Query>>),
    TimeRange { start: chrono::DateTime<chrono::Utc>, end: chrono::DateTime<chrono::Utc> },
}
```

---

## ğŸ’¾ ç´¢å¼•æŒä¹…åŒ–ä¸æ¢å¤

### 1. ç´¢å¼•å¿«ç…§ç®¡ç†å™¨

```rust
/// ç´¢å¼•å¿«ç…§ç®¡ç†å™¨
pub struct IndexSnapshotManager {
    snapshot_dir: std::path::PathBuf,
}

impl IndexSnapshotManager {
    pub fn new(snapshot_dir: std::path::PathBuf) -> Self {
        std::fs::create_dir_all(&snapshot_dir).ok();
        
        Self { snapshot_dir }
    }
    
    /// åˆ›å»ºç´¢å¼•å¿«ç…§
    pub async fn create_snapshot<T: Serialize>(
        &self,
        index_name: &str,
        data: &T,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let timestamp = chrono::Utc::now().timestamp_millis();
        let snapshot_path = self.snapshot_dir.join(format!("{}_{}.snapshot", index_name, timestamp));
        
        let serialized = bincode::serialize(data)?;
        
        tokio::fs::write(&snapshot_path, serialized).await?;
        
        tracing::info!("Index snapshot created: {:?}", snapshot_path);
        
        Ok(())
    }
    
    /// æ¢å¤ç´¢å¼•å¿«ç…§
    pub async fn restore_snapshot<T: for<'de> Deserialize<'de>>(
        &self,
        index_name: &str,
    ) -> Result<Option<T>, Box<dyn std::error::Error>> {
        // æŸ¥æ‰¾æœ€æ–°çš„å¿«ç…§
        let mut entries = tokio::fs::read_dir(&self.snapshot_dir).await?;
        let mut latest_snapshot: Option<(i64, std::path::PathBuf)> = None;
        
        while let Some(entry) = entries.next_entry().await? {
            let path = entry.path();
            let file_name = path.file_name().and_then(|n| n.to_str()).unwrap_or("");
            
            if file_name.starts_with(index_name) && file_name.ends_with(".snapshot") {
                // æå–æ—¶é—´æˆ³
                if let Some(timestamp_str) = file_name.strip_prefix(&format!("{}_", index_name))
                    .and_then(|s| s.strip_suffix(".snapshot"))
                {
                    if let Ok(timestamp) = timestamp_str.parse::<i64>() {
                        if latest_snapshot.is_none() || timestamp > latest_snapshot.as_ref().unwrap().0 {
                            latest_snapshot = Some((timestamp, path));
                        }
                    }
                }
            }
        }
        
        if let Some((_, snapshot_path)) = latest_snapshot {
            let data = tokio::fs::read(&snapshot_path).await?;
            let deserialized = bincode::deserialize(&data)?;
            
            tracing::info!("Index snapshot restored: {:?}", snapshot_path);
            
            Ok(Some(deserialized))
        } else {
            Ok(None)
        }
    }
    
    /// æ¸…ç†æ—§å¿«ç…§
    pub async fn cleanup_old_snapshots(
        &self,
        index_name: &str,
        retention_count: usize,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let mut entries = tokio::fs::read_dir(&self.snapshot_dir).await?;
        let mut snapshots: Vec<(i64, std::path::PathBuf)> = Vec::new();
        
        while let Some(entry) = entries.next_entry().await? {
            let path = entry.path();
            let file_name = path.file_name().and_then(|n| n.to_str()).unwrap_or("");
            
            if file_name.starts_with(index_name) && file_name.ends_with(".snapshot") {
                if let Some(timestamp_str) = file_name.strip_prefix(&format!("{}_", index_name))
                    .and_then(|s| s.strip_suffix(".snapshot"))
                {
                    if let Ok(timestamp) = timestamp_str.parse::<i64>() {
                        snapshots.push((timestamp, path));
                    }
                }
            }
        }
        
        // æŒ‰æ—¶é—´æˆ³æ’åº
        snapshots.sort_by_key(|(ts, _)| *ts);
        
        // åˆ é™¤æ—§å¿«ç…§
        if snapshots.len() > retention_count {
            for (_, path) in snapshots.iter().take(snapshots.len() - retention_count) {
                tokio::fs::remove_file(path).await?;
                tracing::info!("Deleted old snapshot: {:?}", path);
            }
        }
        
        Ok(())
    }
}
```

---

## ğŸ“ˆ å®Œæ•´ç¤ºä¾‹ï¼šç»Ÿä¸€ç´¢å¼•ç³»ç»Ÿ

```rust
/// ç»Ÿä¸€ç´¢å¼•ç³»ç»Ÿ
pub struct UnifiedIndexSystem {
    bloom_index: Arc<TraceIdBloomIndex>,
    lsm_index: Arc<TraceIdLsmIndex>,
    inverted_index: Arc<InvertedIndex>,
    trie_index: Arc<TrieIndex>,
    fulltext_index: Arc<FullTextIndex>,
    ngram_index: Arc<NGramIndex>,
    snapshot_manager: Arc<IndexSnapshotManager>,
}

impl UnifiedIndexSystem {
    pub async fn new(data_dir: std::path::PathBuf) -> Result<Self, Box<dyn std::error::Error>> {
        let snapshot_dir = data_dir.join("snapshots");
        let lsm_dir = data_dir.join("lsm");
        
        Ok(Self {
            bloom_index: Arc::new(TraceIdBloomIndex::new()),
            lsm_index: Arc::new(LsmTree::new(lsm_dir, 10_000)),
            inverted_index: Arc::new(InvertedIndex::new()),
            trie_index: Arc::new(TrieIndex::new()),
            fulltext_index: Arc::new(FullTextIndex::new()),
            ngram_index: Arc::new(NGramIndex::new(3)),
            snapshot_manager: Arc::new(IndexSnapshotManager::new(snapshot_dir)),
        })
    }
    
    /// ç´¢å¼• Trace
    pub async fn index_trace(&self, trace: &crate::DistributedTrace) -> Result<(), Box<dyn std::error::Error>> {
        let partition_id = trace.partition_key.time_bucket.clone();
        
        // 1. Bloom Filter
        self.bloom_index.insert(&partition_id, &trace.trace_id.0).await;
        
        // 2. LSM Index
        let metadata = TraceMetadata {
            partition_id,
            timestamp: trace.timestamp,
            service_name: trace.resource.service_name.clone(),
            span_count: trace.spans.len(),
        };
        
        self.lsm_index.put(trace.trace_id.0, metadata).await?;
        
        // 3. Inverted Index (å±æ€§)
        let mut attributes = HashMap::new();
        attributes.insert("service.name".to_string(), trace.resource.service_name.clone());
        
        if let Some(ns) = &trace.resource.service_namespace {
            attributes.insert("service.namespace".to_string(), ns.clone());
        }
        
        self.inverted_index.insert(trace.trace_id.0, &attributes).await;
        
        Ok(())
    }
    
    /// ç´¢å¼• Metric
    pub async fn index_metric(&self, metric: &crate::DistributedMetric) -> Result<(), Box<dyn std::error::Error>> {
        let metadata = MetricMetadata {
            name: metric.name.clone(),
            metric_type: format!("{:?}", metric.metric_type),
            unit: None,
            description: None,
        };
        
        self.trie_index.insert(&metric.name, metadata).await;
        
        Ok(())
    }
    
    /// ç´¢å¼•æ—¥å¿—
    pub async fn index_log(&self, log_id: String, log: &crate::DistributedLog) -> Result<(), Box<dyn std::error::Error>> {
        let text = match &log.body {
            crate::LogBody::String(s) => s.clone(),
            crate::LogBody::Structured(v) => v.to_string(),
        };
        
        // å…¨æ–‡ç´¢å¼•
        self.fulltext_index.index_document(log_id.clone(), &text).await;
        
        // N-Gram ç´¢å¼•ï¼ˆç”¨äºæ¨¡ç³ŠåŒ¹é…ï¼‰
        self.ngram_index.index_document(log_id, &text).await;
        
        Ok(())
    }
    
    /// åˆ›å»ºå¿«ç…§
    pub async fn create_snapshot(&self) -> Result<(), Box<dyn std::error::Error>> {
        // TODO: ä¸ºå„ä¸ªç´¢å¼•åˆ›å»ºå¿«ç…§
        tracing::info!("Creating index snapshots...");
        
        Ok(())
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt::init();
    
    let data_dir = std::path::PathBuf::from("./index_data");
    let index_system = UnifiedIndexSystem::new(data_dir).await?;
    
    println!("Unified Index System initialized!");
    
    Ok(())
}
```

---

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ç­–ç•¥

- **LRU ç¼“å­˜**: çƒ­ç‚¹æŸ¥è¯¢ç»“æœ
- **Write-Back ç¼“å­˜**: æ‰¹é‡å†™å…¥
- **æŸ¥è¯¢è®¡åˆ’ç¼“å­˜**: å¤æ‚æŸ¥è¯¢ä¼˜åŒ–

### å‹ç¼©æŠ€æœ¯

- **Dictionary Encoding**: é«˜åŸºæ•°å­—æ®µ
- **Run-Length Encoding**: é‡å¤æ•°æ®
- **Delta Encoding**: æ—¶é—´åºåˆ—

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

- **Bloom Filter**: å‡é˜³æ€§ç‡ < 1%
- **LSM å†™å…¥**: > 100K ops/s
- **å€’æ’ç´¢å¼•æŸ¥è¯¢**: < 10ms (P99)
- **å…¨æ–‡æœç´¢**: < 50ms (P99)

---

## ğŸ“š å‚è€ƒèµ„æº

- [LSM-Tree Paper](https://www.cs.umb.edu/~poneil/lsmtree.pdf)
- [Bloom Filters](https://en.wikipedia.org/wiki/Bloom_filter)
- [Inverted Index](https://en.wikipedia.org/wiki/Inverted_index)
- [Trie Data Structure](https://en.wikipedia.org/wiki/Trie)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-10  
**ä½œè€…**: OTLP Rust é¡¹ç›®ç»„
