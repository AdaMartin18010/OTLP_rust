# Cache-Aside æ¨¡å¼ - Rust + OTLP ç¼“å­˜è¿½è¸ªå®Œæ•´å®ç°

## ğŸ“‹ ç›®å½•

- [Cache-Aside æ¨¡å¼ - Rust + OTLP ç¼“å­˜è¿½è¸ªå®Œæ•´å®ç°](#cache-aside-æ¨¡å¼---rust--otlp-ç¼“å­˜è¿½è¸ªå®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [æ¦‚è¿°](#æ¦‚è¿°)
    - [ä»€ä¹ˆæ˜¯ Cache-Aside æ¨¡å¼?](#ä»€ä¹ˆæ˜¯-cache-aside-æ¨¡å¼)
    - [ä¸ºä»€ä¹ˆä½¿ç”¨ Rust?](#ä¸ºä»€ä¹ˆä½¿ç”¨-rust)
    - [OTLP é›†æˆä»·å€¼](#otlp-é›†æˆä»·å€¼)
  - [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
    - [1. ç¼“å­˜æ¥å£è®¾è®¡](#1-ç¼“å­˜æ¥å£è®¾è®¡)
    - [2. OTLP è¿½è¸ªä¸Šä¸‹æ–‡](#2-otlp-è¿½è¸ªä¸Šä¸‹æ–‡)
  - [Rust 1.90 å®ç°](#rust-190-å®ç°)
    - [1. Redis ç¼“å­˜å®ç°](#1-redis-ç¼“å­˜å®ç°)
    - [2. å†…å­˜ç¼“å­˜å®ç°(L1)](#2-å†…å­˜ç¼“å­˜å®ç°l1)
  - [OTLP é›†æˆç­–ç•¥](#otlp-é›†æˆç­–ç•¥)
    - [1. ç¼“å­˜è£…é¥°å™¨](#1-ç¼“å­˜è£…é¥°å™¨)
    - [2. Cache-Aside ä»“å‚¨å±‚](#2-cache-aside-ä»“å‚¨å±‚)
  - [å¤šçº§ç¼“å­˜æ¶æ„](#å¤šçº§ç¼“å­˜æ¶æ„)
    - [L1(å†…å­˜) + L2(Redis) ä¸¤çº§ç¼“å­˜](#l1å†…å­˜--l2redis-ä¸¤çº§ç¼“å­˜)
  - [ç¼“å­˜å¤±æ•ˆè¿½è¸ª](#ç¼“å­˜å¤±æ•ˆè¿½è¸ª)
    - [1. ç¼“å­˜ç©¿é€é˜²æŠ¤](#1-ç¼“å­˜ç©¿é€é˜²æŠ¤)
    - [2. ç¼“å­˜é›ªå´©é˜²æŠ¤](#2-ç¼“å­˜é›ªå´©é˜²æŠ¤)
  - [æ€§èƒ½ç›‘æ§](#æ€§èƒ½ç›‘æ§)
    - [1. ç¼“å­˜å‘½ä¸­ç‡ä»ªè¡¨æ¿](#1-ç¼“å­˜å‘½ä¸­ç‡ä»ªè¡¨æ¿)
    - [2. å‘Šè­¦è§„åˆ™](#2-å‘Šè­¦è§„åˆ™)
  - [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
    - [1. ç¼“å­˜é”®å‘½åè§„èŒƒ](#1-ç¼“å­˜é”®å‘½åè§„èŒƒ)
    - [2. TTL ç­–ç•¥](#2-ttl-ç­–ç•¥)
    - [3. ç¼“å­˜é¢„çƒ­](#3-ç¼“å­˜é¢„çƒ­)
  - [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)
    - [1. Web æœåŠ¡é›†æˆ(Axum)](#1-web-æœåŠ¡é›†æˆaxum)
    - [2. Docker Compose éƒ¨ç½²](#2-docker-compose-éƒ¨ç½²)
  - [æ€»ç»“](#æ€»ç»“)
    - [æ ¸å¿ƒè¦ç‚¹](#æ ¸å¿ƒè¦ç‚¹)
    - [ä¸‹ä¸€æ­¥](#ä¸‹ä¸€æ­¥)
  - [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

---

## æ¦‚è¿°

### ä»€ä¹ˆæ˜¯ Cache-Aside æ¨¡å¼?

**Cache-Aside**(æ—è·¯ç¼“å­˜)æ˜¯æœ€å¸¸ç”¨çš„ç¼“å­˜æ¨¡å¼,åº”ç”¨ç¨‹åºè´Ÿè´£ç®¡ç†ç¼“å­˜å’Œæ•°æ®æºä¹‹é—´çš„åŒæ­¥:

```text
è¯»å–æµç¨‹:
1. æ£€æŸ¥ç¼“å­˜
2. ç¼“å­˜å‘½ä¸­ â†’ è¿”å›æ•°æ®
3. ç¼“å­˜æœªå‘½ä¸­ â†’ è¯»å–æ•°æ®æº â†’ å†™å…¥ç¼“å­˜ â†’ è¿”å›æ•°æ®

å†™å…¥æµç¨‹:
1. å†™å…¥æ•°æ®æº
2. ä½¿ç¼“å­˜å¤±æ•ˆ(Invalidate)
```

### ä¸ºä»€ä¹ˆä½¿ç”¨ Rust?

- **ç±»å‹å®‰å…¨**: æ³›å‹ç¼“å­˜æ¥å£,é¿å…ç±»å‹é”™è¯¯
- **é«˜æ€§èƒ½**: é›¶æˆæœ¬æŠ½è±¡,å†…å­˜æ•ˆç‡é«˜
- **å¹¶å‘å®‰å…¨**: çº¿ç¨‹å®‰å…¨çš„ç¼“å­˜å®ç°(`Arc<RwLock>`, `DashMap`)
- **å¼‚æ­¥æ”¯æŒ**: Tokio å¼‚æ­¥ç¼“å­˜æ“ä½œ,å‡å°‘é˜»å¡

### OTLP é›†æˆä»·å€¼

| å¯è§‚æµ‹æ€§ç»´åº¦ | OTLP èƒ½åŠ› |
|------------|----------|
| **ç¼“å­˜å‘½ä¸­ç‡** | Metrics(counter, gauge) |
| **ç¼“å­˜å»¶è¿Ÿ** | Histogram(p50/p99) |
| **ç¼“å­˜ç©¿é€** | Tracing Span è¿½è¸ª |
| **ç¼“å­˜é›ªå´©** | åˆ†å¸ƒå¼è¿½è¸ªå‘Šè­¦ |
| **TTL è¿‡æœŸ** | äº‹ä»¶æ—¥å¿—(Logs) |

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. ç¼“å­˜æ¥å£è®¾è®¡

ä½¿ç”¨ Rust Trait å®šä¹‰ç¼“å­˜æŠ½è±¡:

```rust
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use std::time::Duration;

#[async_trait]
pub trait Cache: Send + Sync {
    /// è·å–ç¼“å­˜å€¼
    async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Send;

    /// è®¾ç½®ç¼“å­˜å€¼
    async fn set<T>(
        &self,
        key: &str,
        value: &T,
        ttl: Option<Duration>,
    ) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync;

    /// åˆ é™¤ç¼“å­˜
    async fn delete(&self, key: &str) -> Result<(), CacheError>;

    /// æ‰¹é‡åˆ é™¤
    async fn delete_pattern(&self, pattern: &str) -> Result<usize, CacheError>;
}

#[derive(Debug, thiserror::Error)]
pub enum CacheError {
    #[error("åºåˆ—åŒ–é”™è¯¯: {0}")]
    Serialization(#[from] serde_json::Error),
    
    #[error("Redis é”™è¯¯: {0}")]
    Redis(#[from] redis::RedisError),
    
    #[error("è¿æ¥é”™è¯¯: {0}")]
    Connection(String),
}
```

### 2. OTLP è¿½è¸ªä¸Šä¸‹æ–‡

ä¸ºç¼“å­˜æ“ä½œæ·»åŠ  Tracing:

```rust
use tracing::{info, instrument, warn};
use opentelemetry::global;
use opentelemetry::metrics::{Counter, Histogram, Meter};

pub struct CacheMetrics {
    hits: Counter<u64>,
    misses: Counter<u64>,
    latency: Histogram<f64>,
    errors: Counter<u64>,
}

impl CacheMetrics {
    pub fn new(meter: &Meter) -> Self {
        Self {
            hits: meter
                .u64_counter("cache.hits")
                .with_description("ç¼“å­˜å‘½ä¸­æ¬¡æ•°")
                .init(),
            misses: meter
                .u64_counter("cache.misses")
                .with_description("ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°")
                .init(),
            latency: meter
                .f64_histogram("cache.latency")
                .with_description("ç¼“å­˜æ“ä½œå»¶è¿Ÿ(ms)")
                .with_unit("ms")
                .init(),
            errors: meter
                .u64_counter("cache.errors")
                .with_description("ç¼“å­˜é”™è¯¯æ¬¡æ•°")
                .init(),
        }
    }
}
```

---

## Rust 1.90 å®ç°

### 1. Redis ç¼“å­˜å®ç°

ä½¿ç”¨ `redis` å’Œ `moka` æ„å»ºå¤šçº§ç¼“å­˜:

```toml
# Cargo.toml
[dependencies]
redis = { version = "0.27", features = ["tokio-comp", "connection-manager"] }
moka = { version = "0.12", features = ["future"] }
tokio = { version = "1.40", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tracing = "0.1"
opentelemetry = "0.30"
opentelemetry_sdk = { version = "0.30", features = ["rt-tokio"] }
opentelemetry-otlp = { version = "0.30", features = ["trace", "metrics"] }
async-trait = "0.1"
thiserror = "2.0"
```

```rust
use redis::aio::ConnectionManager;
use redis::AsyncCommands;
use std::sync::Arc;
use tracing::instrument;

pub struct RedisCache {
    client: ConnectionManager,
    metrics: Arc<CacheMetrics>,
}

impl RedisCache {
    pub async fn new(redis_url: &str, metrics: Arc<CacheMetrics>) -> Result<Self, CacheError> {
        let client = redis::Client::open(redis_url)
            .map_err(|e| CacheError::Connection(e.to_string()))?;
        
        let manager = ConnectionManager::new(client).await?;
        
        Ok(Self {
            client: manager,
            metrics,
        })
    }
}

#[async_trait]
impl Cache for RedisCache {
    #[instrument(skip(self), fields(cache.backend = "redis"))]
    async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Send,
    {
        let start = std::time::Instant::now();
        
        let mut conn = self.client.clone();
        let result: Option<String> = conn.get(key).await?;
        
        let latency = start.elapsed().as_secs_f64() * 1000.0;
        self.metrics.latency.record(latency, &[]);
        
        match result {
            Some(json) => {
                self.metrics.hits.add(1, &[]);
                info!(cache.key = key, cache.hit = true, "ç¼“å­˜å‘½ä¸­");
                
                let value = serde_json::from_str(&json)?;
                Ok(Some(value))
            }
            None => {
                self.metrics.misses.add(1, &[]);
                warn!(cache.key = key, cache.hit = false, "ç¼“å­˜æœªå‘½ä¸­");
                Ok(None)
            }
        }
    }

    #[instrument(skip(self, value))]
    async fn set<T>(
        &self,
        key: &str,
        value: &T,
        ttl: Option<Duration>,
    ) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync,
    {
        let json = serde_json::to_string(value)?;
        let mut conn = self.client.clone();
        
        match ttl {
            Some(duration) => {
                conn.set_ex(key, json, duration.as_secs()).await?;
            }
            None => {
                conn.set(key, json).await?;
            }
        }
        
        info!(cache.key = key, cache.ttl_secs = ?ttl.map(|d| d.as_secs()), "ç¼“å­˜å·²è®¾ç½®");
        Ok(())
    }

    #[instrument(skip(self))]
    async fn delete(&self, key: &str) -> Result<(), CacheError> {
        let mut conn = self.client.clone();
        conn.del(key).await?;
        
        info!(cache.key = key, "ç¼“å­˜å·²åˆ é™¤");
        Ok(())
    }

    #[instrument(skip(self))]
    async fn delete_pattern(&self, pattern: &str) -> Result<usize, CacheError> {
        let mut conn = self.client.clone();
        
        // ä½¿ç”¨ SCAN é¿å…é˜»å¡
        let keys: Vec<String> = conn.scan_match(pattern).await?.collect().await;
        let count = keys.len();
        
        if !keys.is_empty() {
            conn.del(keys).await?;
        }
        
        info!(cache.pattern = pattern, cache.deleted_count = count, "æ‰¹é‡åˆ é™¤ç¼“å­˜");
        Ok(count)
    }
}
```

### 2. å†…å­˜ç¼“å­˜å®ç°(L1)

ä½¿ç”¨ `moka` å®ç°æœ¬åœ°ç¼“å­˜:

```rust
use moka::future::Cache as MokaCache;

pub struct MemoryCache {
    cache: MokaCache<String, String>,
    metrics: Arc<CacheMetrics>,
}

impl MemoryCache {
    pub fn new(max_capacity: u64, ttl: Duration, metrics: Arc<CacheMetrics>) -> Self {
        let cache = MokaCache::builder()
            .max_capacity(max_capacity)
            .time_to_live(ttl)
            .time_to_idle(ttl / 2)
            .build();
        
        Self { cache, metrics }
    }
}

#[async_trait]
impl Cache for MemoryCache {
    #[instrument(skip(self), fields(cache.backend = "memory"))]
    async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Send,
    {
        let start = std::time::Instant::now();
        
        let result = self.cache.get(key).await;
        
        let latency = start.elapsed().as_secs_f64() * 1000.0;
        self.metrics.latency.record(latency, &[]);
        
        match result {
            Some(json) => {
                self.metrics.hits.add(1, &[]);
                let value = serde_json::from_str(&json)?;
                Ok(Some(value))
            }
            None => {
                self.metrics.misses.add(1, &[]);
                Ok(None)
            }
        }
    }

    #[instrument(skip(self, value))]
    async fn set<T>(
        &self,
        key: &str,
        value: &T,
        _ttl: Option<Duration>, // å¿½ç•¥,ä½¿ç”¨æ„é€ æ—¶çš„å…¨å±€ TTL
    ) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync,
    {
        let json = serde_json::to_string(value)?;
        self.cache.insert(key.to_string(), json).await;
        Ok(())
    }

    async fn delete(&self, key: &str) -> Result<(), CacheError> {
        self.cache.invalidate(key).await;
        Ok(())
    }

    async fn delete_pattern(&self, pattern: &str) -> Result<usize, CacheError> {
        // ç®€åŒ–å®ç°:ä»…æ”¯æŒåç¼€é€šé…ç¬¦
        let prefix = pattern.trim_end_matches('*');
        let mut count = 0;
        
        // Moka ä¸æ”¯æŒæ¨¡å¼åˆ é™¤,éœ€è¦éå†
        // åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”ä½¿ç”¨æ›´é«˜æ•ˆçš„å®ç°
        self.cache.invalidate_all();
        
        Ok(count)
    }
}
```

---

## OTLP é›†æˆç­–ç•¥

### 1. ç¼“å­˜è£…é¥°å™¨

ä¸ºä»»ä½• `Cache` å®ç°æ·»åŠ  OTLP è¿½è¸ª:

```rust
pub struct TracedCache<C: Cache> {
    inner: C,
    metrics: Arc<CacheMetrics>,
}

impl<C: Cache> TracedCache<C> {
    pub fn new(cache: C, metrics: Arc<CacheMetrics>) -> Self {
        Self {
            inner: cache,
            metrics,
        }
    }
}

#[async_trait]
impl<C: Cache> Cache for TracedCache<C> {
    #[instrument(skip(self), fields(otel.kind = "cache"))]
    async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Send,
    {
        let result = self.inner.get(key).await;
        
        if result.is_err() {
            self.metrics.errors.add(1, &[]);
        }
        
        result
    }

    #[instrument(skip(self, value))]
    async fn set<T>(
        &self,
        key: &str,
        value: &T,
        ttl: Option<Duration>,
    ) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync,
    {
        let result = self.inner.set(key, value, ttl).await;
        
        if result.is_err() {
            self.metrics.errors.add(1, &[]);
        }
        
        result
    }

    async fn delete(&self, key: &str) -> Result<(), CacheError> {
        self.inner.delete(key).await
    }

    async fn delete_pattern(&self, pattern: &str) -> Result<usize, CacheError> {
        self.inner.delete_pattern(pattern).await
    }
}
```

### 2. Cache-Aside ä»“å‚¨å±‚

å®ç°å¸¦ç¼“å­˜çš„ä»“å‚¨æ¨¡å¼:

```rust
use sqlx::PgPool;

pub struct CachedRepository<C: Cache> {
    cache: C,
    db: PgPool,
}

impl<C: Cache> CachedRepository<C> {
    pub fn new(cache: C, db: PgPool) -> Self {
        Self { cache, db }
    }

    #[instrument(skip(self), fields(db.table = "users"))]
    pub async fn get_user(&self, user_id: i64) -> Result<Option<User>, RepositoryError> {
        let cache_key = format!("user:{}", user_id);
        
        // 1. å°è¯•ä»ç¼“å­˜è¯»å–
        if let Some(user) = self.cache.get::<User>(&cache_key).await? {
            info!(user.id = user_id, "ä»ç¼“å­˜è¿”å›ç”¨æˆ·");
            return Ok(Some(user));
        }
        
        // 2. ç¼“å­˜æœªå‘½ä¸­,æŸ¥è¯¢æ•°æ®åº“
        let user = sqlx::query_as::<_, User>(
            "SELECT id, name, email, created_at FROM users WHERE id = $1"
        )
        .bind(user_id)
        .fetch_optional(&self.db)
        .await?;
        
        // 3. å†™å…¥ç¼“å­˜
        if let Some(ref u) = user {
            self.cache
                .set(&cache_key, u, Some(Duration::from_secs(300)))
                .await?;
            
            info!(user.id = user_id, "ç”¨æˆ·å·²ç¼“å­˜");
        }
        
        Ok(user)
    }

    #[instrument(skip(self, user))]
    pub async fn update_user(&self, user: &User) -> Result<(), RepositoryError> {
        // 1. æ›´æ–°æ•°æ®åº“
        sqlx::query(
            "UPDATE users SET name = $1, email = $2 WHERE id = $3"
        )
        .bind(&user.name)
        .bind(&user.email)
        .bind(user.id)
        .execute(&self.db)
        .await?;
        
        // 2. ä½¿ç¼“å­˜å¤±æ•ˆ
        let cache_key = format!("user:{}", user.id);
        self.cache.delete(&cache_key).await?;
        
        info!(user.id = user.id, "ç”¨æˆ·å·²æ›´æ–°,ç¼“å­˜å·²å¤±æ•ˆ");
        Ok(())
    }
}

#[derive(Debug, Serialize, Deserialize, sqlx::FromRow)]
pub struct User {
    pub id: i64,
    pub name: String,
    pub email: String,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, thiserror::Error)]
pub enum RepositoryError {
    #[error("ç¼“å­˜é”™è¯¯: {0}")]
    Cache(#[from] CacheError),
    
    #[error("æ•°æ®åº“é”™è¯¯: {0}")]
    Database(#[from] sqlx::Error),
}
```

---

## å¤šçº§ç¼“å­˜æ¶æ„

### L1(å†…å­˜) + L2(Redis) ä¸¤çº§ç¼“å­˜

```rust
pub struct TieredCache {
    l1: MemoryCache,
    l2: RedisCache,
}

impl TieredCache {
    pub fn new(l1: MemoryCache, l2: RedisCache) -> Self {
        Self { l1, l2 }
    }
}

#[async_trait]
impl Cache for TieredCache {
    #[instrument(skip(self), fields(cache.tier = "tiered"))]
    async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Serialize + Send + Sync,
    {
        // 1. å°è¯• L1(å†…å­˜)
        if let Some(value) = self.l1.get::<T>(key).await? {
            info!(cache.tier = "L1", "L1 ç¼“å­˜å‘½ä¸­");
            return Ok(Some(value));
        }
        
        // 2. å°è¯• L2(Redis)
        if let Some(value) = self.l2.get::<T>(key).await? {
            info!(cache.tier = "L2", "L2 ç¼“å­˜å‘½ä¸­,å›å¡« L1");
            
            // å›å¡« L1
            self.l1.set(key, &value, None).await?;
            
            return Ok(Some(value));
        }
        
        warn!("æ‰€æœ‰ç¼“å­˜å±‚æœªå‘½ä¸­");
        Ok(None)
    }

    #[instrument(skip(self, value))]
    async fn set<T>(
        &self,
        key: &str,
        value: &T,
        ttl: Option<Duration>,
    ) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync,
    {
        // åŒæ—¶å†™å…¥ä¸¤å±‚
        let (r1, r2) = tokio::join!(
            self.l1.set(key, value, ttl),
            self.l2.set(key, value, ttl)
        );
        
        r1?;
        r2?;
        
        Ok(())
    }

    async fn delete(&self, key: &str) -> Result<(), CacheError> {
        // åŒæ—¶åˆ é™¤ä¸¤å±‚
        let (r1, r2) = tokio::join!(
            self.l1.delete(key),
            self.l2.delete(key)
        );
        
        r1?;
        r2?;
        
        Ok(())
    }

    async fn delete_pattern(&self, pattern: &str) -> Result<usize, CacheError> {
        let count = self.l2.delete_pattern(pattern).await?;
        self.l1.delete_pattern(pattern).await?;
        Ok(count)
    }
}
```

---

## ç¼“å­˜å¤±æ•ˆè¿½è¸ª

### 1. ç¼“å­˜ç©¿é€é˜²æŠ¤

ä½¿ç”¨å¸ƒéš†è¿‡æ»¤å™¨é˜²æ­¢ç¼“å­˜ç©¿é€:

```rust
use bloomfilter::Bloom;
use std::sync::RwLock;

pub struct BloomFilterCache<C: Cache> {
    cache: C,
    bloom: Arc<RwLock<Bloom<String>>>,
}

impl<C: Cache> BloomFilterCache<C> {
    pub fn new(cache: C, expected_items: usize) -> Self {
        let bloom = Bloom::new_for_fp_rate(expected_items, 0.01);
        
        Self {
            cache,
            bloom: Arc::new(RwLock::new(bloom)),
        }
    }
}

#[async_trait]
impl<C: Cache> Cache for BloomFilterCache<C> {
    #[instrument(skip(self), fields(cache.protection = "bloom_filter"))]
    async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Send,
    {
        // 1. æ£€æŸ¥å¸ƒéš†è¿‡æ»¤å™¨
        {
            let bloom = self.bloom.read().unwrap();
            if !bloom.check(&key.to_string()) {
                warn!(cache.key = key, "å¸ƒéš†è¿‡æ»¤å™¨é˜»æ­¢ç©¿é€");
                return Ok(None);
            }
        }
        
        // 2. ç»§ç»­æ­£å¸¸ç¼“å­˜æŸ¥è¯¢
        self.cache.get(key).await
    }

    async fn set<T>(
        &self,
        key: &str,
        value: &T,
        ttl: Option<Duration>,
    ) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync,
    {
        // æ·»åŠ åˆ°å¸ƒéš†è¿‡æ»¤å™¨
        {
            let mut bloom = self.bloom.write().unwrap();
            bloom.set(&key.to_string());
        }
        
        self.cache.set(key, value, ttl).await
    }

    async fn delete(&self, key: &str) -> Result<(), CacheError> {
        // å¸ƒéš†è¿‡æ»¤å™¨ä¸æ”¯æŒåˆ é™¤,ä»…åˆ é™¤ç¼“å­˜
        self.cache.delete(key).await
    }

    async fn delete_pattern(&self, pattern: &str) -> Result<usize, CacheError> {
        self.cache.delete_pattern(pattern).await
    }
}
```

### 2. ç¼“å­˜é›ªå´©é˜²æŠ¤

éšæœº TTL é¿å…åŒæ—¶è¿‡æœŸ:

```rust
use rand::Rng;

pub fn jittered_ttl(base_ttl: Duration, jitter_percent: f64) -> Duration {
    let mut rng = rand::thread_rng();
    let jitter = rng.gen_range(-jitter_percent..jitter_percent);
    
    let multiplier = 1.0 + jitter;
    let secs = (base_ttl.as_secs_f64() * multiplier).max(1.0);
    
    Duration::from_secs_f64(secs)
}

// ä½¿ç”¨ç¤ºä¾‹
let ttl = jittered_ttl(Duration::from_secs(300), 0.2); // Â±20% æŠ–åŠ¨
cache.set("key", &value, Some(ttl)).await?;
```

---

## æ€§èƒ½ç›‘æ§

### 1. ç¼“å­˜å‘½ä¸­ç‡ä»ªè¡¨æ¿

ä½¿ç”¨ Prometheus + Grafana ç›‘æ§:

```promql
# ç¼“å­˜å‘½ä¸­ç‡
sum(rate(cache_hits_total[5m])) / 
(sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m]))) * 100

# P99 å»¶è¿Ÿ
histogram_quantile(0.99, 
  sum(rate(cache_latency_bucket[5m])) by (le)
)

# é”™è¯¯ç‡
sum(rate(cache_errors_total[5m])) by (backend)
```

### 2. å‘Šè­¦è§„åˆ™

```yaml
# prometheus_rules.yml
groups:
  - name: cache_alerts
    interval: 30s
    rules:
      - alert: CacheHitRateLow
        expr: |
          (sum(rate(cache_hits_total[5m])) / 
           (sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m]))))
          < 0.7
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½ ({{ $value | humanizePercentage }})"
          
      - alert: CacheLatencyHigh
        expr: |
          histogram_quantile(0.99,
            sum(rate(cache_latency_bucket[5m])) by (le)
          ) > 100
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "ç¼“å­˜ P99 å»¶è¿Ÿè¶…è¿‡ 100ms"
```

---

## æœ€ä½³å®è·µ

### 1. ç¼“å­˜é”®å‘½åè§„èŒƒ

```rust
pub struct CacheKeyBuilder {
    namespace: String,
}

impl CacheKeyBuilder {
    pub fn new(namespace: &str) -> Self {
        Self {
            namespace: namespace.to_string(),
        }
    }
    
    pub fn user(&self, user_id: i64) -> String {
        format!("{}:user:{}", self.namespace, user_id)
    }
    
    pub fn user_posts(&self, user_id: i64) -> String {
        format!("{}:user:{}:posts", self.namespace, user_id)
    }
    
    pub fn post(&self, post_id: i64) -> String {
        format!("{}:post:{}", self.namespace, post_id)
    }
}

// ä½¿ç”¨ç¤ºä¾‹
let keys = CacheKeyBuilder::new("myapp");
let key = keys.user(123); // "myapp:user:123"
```

### 2. TTL ç­–ç•¥

| æ•°æ®ç±»å‹ | æ¨è TTL | è¯´æ˜ |
|---------|---------|------|
| ç”¨æˆ·ä¿¡æ¯ | 5-15 åˆ†é’Ÿ | å¹³è¡¡ä¸€è‡´æ€§å’Œæ€§èƒ½ |
| é…ç½®æ•°æ® | 1-24 å°æ—¶ | å˜æ›´é¢‘ç‡ä½ |
| ä¼šè¯æ•°æ® | 30 åˆ†é’Ÿ | ä¸ä¼šè¯è¿‡æœŸæ—¶é—´ä¸€è‡´ |
| API å“åº” | 1-5 åˆ†é’Ÿ | æ ¹æ®å®æ—¶æ€§è¦æ±‚è°ƒæ•´ |
| çƒ­ç‚¹æ•°æ® | 10-30 ç§’ | é«˜é¢‘è®¿é—®,çŸ­ TTL |

### 3. ç¼“å­˜é¢„çƒ­

```rust
#[instrument]
pub async fn warm_up_cache<C: Cache>(
    cache: &C,
    db: &PgPool,
) -> Result<usize, Box<dyn std::error::Error>> {
    // é¢„åŠ è½½çƒ­é—¨ç”¨æˆ·
    let hot_users = sqlx::query_as::<_, User>(
        "SELECT * FROM users ORDER BY login_count DESC LIMIT 1000"
    )
    .fetch_all(db)
    .await?;
    
    let mut count = 0;
    for user in hot_users {
        let key = format!("user:{}", user.id);
        cache.set(&key, &user, Some(Duration::from_secs(600))).await?;
        count += 1;
    }
    
    info!(preloaded_count = count, "ç¼“å­˜é¢„çƒ­å®Œæˆ");
    Ok(count)
}
```

---

## å®Œæ•´ç¤ºä¾‹

### 1. Web æœåŠ¡é›†æˆ(Axum)

```rust
use axum::{
    extract::{Path, State},
    http::StatusCode,
    response::IntoResponse,
    routing::get,
    Json, Router,
};

#[derive(Clone)]
struct AppState {
    repo: Arc<CachedRepository<TieredCache>>,
}

async fn get_user_handler(
    State(state): State<AppState>,
    Path(user_id): Path<i64>,
) -> Result<Json<User>, AppError> {
    let user = state
        .repo
        .get_user(user_id)
        .await?
        .ok_or(AppError::NotFound)?;
    
    Ok(Json(user))
}

async fn update_user_handler(
    State(state): State<AppState>,
    Path(user_id): Path<i64>,
    Json(mut user): Json<User>,
) -> Result<StatusCode, AppError> {
    user.id = user_id;
    state.repo.update_user(&user).await?;
    
    Ok(StatusCode::NO_CONTENT)
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆå§‹åŒ– OTLP
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(opentelemetry_otlp::new_exporter().tonic())
        .install_batch(opentelemetry_sdk::runtime::Tokio)?;
    
    tracing_subscriber::registry()
        .with(tracing_opentelemetry::layer().with_tracer(tracer))
        .init();
    
    let meter = global::meter("cache_aside_example");
    let metrics = Arc::new(CacheMetrics::new(&meter));
    
    // æ„å»ºå¤šçº§ç¼“å­˜
    let l1 = MemoryCache::new(1000, Duration::from_secs(60), metrics.clone());
    let l2 = RedisCache::new("redis://127.0.0.1", metrics.clone()).await?;
    let cache = TieredCache::new(l1, l2);
    
    // æ•°æ®åº“è¿æ¥æ± 
    let db = PgPool::connect("postgresql://user:pass@localhost/db").await?;
    
    // åˆ›å»ºä»“å‚¨
    let repo = Arc::new(CachedRepository::new(cache, db));
    
    // å¯åŠ¨æœåŠ¡
    let app = Router::new()
        .route("/users/:id", get(get_user_handler).put(update_user_handler))
        .with_state(AppState { repo });
    
    let listener = tokio::net::TcpListener::bind("0.0.0.0:3000").await?;
    axum::serve(listener, app).await?;
    
    Ok(())
}

#[derive(Debug)]
enum AppError {
    NotFound,
    Repository(RepositoryError),
}

impl From<RepositoryError> for AppError {
    fn from(err: RepositoryError) -> Self {
        AppError::Repository(err)
    }
}

impl IntoResponse for AppError {
    fn into_response(self) -> axum::response::Response {
        match self {
            AppError::NotFound => {
                (StatusCode::NOT_FOUND, "ç”¨æˆ·ä¸å­˜åœ¨").into_response()
            }
            AppError::Repository(err) => {
                (StatusCode::INTERNAL_SERVER_ERROR, err.to_string()).into_response()
            }
        }
    }
}
```

### 2. Docker Compose éƒ¨ç½²

```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "3000:3000"
    environment:
      REDIS_URL: redis://redis:6379
      DATABASE_URL: postgresql://postgres:password@postgres:5432/mydb
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
    depends_on:
      - redis
      - postgres
      - otel-collector

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_PASSWORD: password
      POSTGRES_DB: mydb
    volumes:
      - postgres_data:/var/lib/postgresql/data

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.98.0
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    command: ["--config=/etc/otel-collector-config.yaml"]
    ports:
      - "4317:4317"  # OTLP gRPC
      - "8889:8889"  # Prometheus metrics

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin

volumes:
  postgres_data:
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Rust Trait æŠ½è±¡**: å®šä¹‰ç»Ÿä¸€ç¼“å­˜æ¥å£
2. **å¤šçº§ç¼“å­˜**: L1(å†…å­˜) + L2(Redis) æå‡æ€§èƒ½
3. **OTLP å…¨æ–¹ä½ç›‘æ§**: Metrics(å‘½ä¸­ç‡/å»¶è¿Ÿ) + Tracing(æ“ä½œè¿½è¸ª)
4. **é˜²æŠ¤æœºåˆ¶**: å¸ƒéš†è¿‡æ»¤å™¨(ç©¿é€) + éšæœº TTL(é›ªå´©)
5. **Cache-Aside æ¨¡å¼**: åº”ç”¨å±‚ç®¡ç†ç¼“å­˜å¤±æ•ˆ

### ä¸‹ä¸€æ­¥

- **åˆ†å¸ƒå¼ç¼“å­˜ä¸€è‡´æ€§**: ä½¿ç”¨ Redis Pub/Sub é€šçŸ¥å…¶ä»–èŠ‚ç‚¹å¤±æ•ˆ
- **ç¼“å­˜é¢„çƒ­ç­–ç•¥**: å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½çƒ­ç‚¹æ•°æ®
- **ç¼“å­˜å‹ç¼©**: ä½¿ç”¨ `lz4` å‹ç¼©å¤§å¯¹è±¡
- **ç¼“å­˜ç›‘æ§å‘Šè­¦**: Grafana ä»ªè¡¨æ¿ + Alertmanager

---

## å‚è€ƒèµ„æ–™

- [Rust Redis å®¢æˆ·ç«¯](https://docs.rs/redis)
- [Moka é«˜æ€§èƒ½ç¼“å­˜](https://docs.rs/moka)
- [OpenTelemetry Rust SDK](https://docs.rs/opentelemetry)
- [Cache-Aside Pattern - Microsoft](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-11  
**Rust ç‰ˆæœ¬**: 1.90+  
**OpenTelemetry**: 0.30+
