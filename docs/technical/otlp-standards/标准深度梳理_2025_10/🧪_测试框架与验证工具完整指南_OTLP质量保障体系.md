# ğŸ§ª æµ‹è¯•æ¡†æ¶ä¸éªŒè¯å·¥å…·å®Œæ•´æŒ‡å—_OTLPè´¨é‡ä¿éšœä½“ç³»

## ğŸ“‹ ç›®å½•

- [ğŸ§ª æµ‹è¯•æ¡†æ¶ä¸éªŒè¯å·¥å…·å®Œæ•´æŒ‡å—\_OTLPè´¨é‡ä¿éšœä½“ç³»](#-æµ‹è¯•æ¡†æ¶ä¸éªŒè¯å·¥å…·å®Œæ•´æŒ‡å—_otlpè´¨é‡ä¿éšœä½“ç³»)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [1. OTLP æµ‹è¯•ä½“ç³»æ¦‚è¿°](#1-otlp-æµ‹è¯•ä½“ç³»æ¦‚è¿°)
    - [1.1 æµ‹è¯•é‡‘å­—å¡”ä¸æµ‹è¯•ç­–ç•¥](#11-æµ‹è¯•é‡‘å­—å¡”ä¸æµ‹è¯•ç­–ç•¥)
      - [åˆ†å±‚æµ‹è¯•ç­–ç•¥](#åˆ†å±‚æµ‹è¯•ç­–ç•¥)
    - [1.2 OTLP ç‰¹å®šçš„æµ‹è¯•æŒ‘æˆ˜](#12-otlp-ç‰¹å®šçš„æµ‹è¯•æŒ‘æˆ˜)
      - [æŒ‘æˆ˜ 1: åˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸ç¡®å®šæ€§](#æŒ‘æˆ˜-1-åˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸ç¡®å®šæ€§)
      - [æŒ‘æˆ˜ 2: å¤šåè®®å¤šè¯­è¨€å…¼å®¹æ€§](#æŒ‘æˆ˜-2-å¤šåè®®å¤šè¯­è¨€å…¼å®¹æ€§)
      - [æŒ‘æˆ˜ 3: æ€§èƒ½ä¸æ­£ç¡®æ€§çš„å¹³è¡¡](#æŒ‘æˆ˜-3-æ€§èƒ½ä¸æ­£ç¡®æ€§çš„å¹³è¡¡)
    - [1.3 ç«¯åˆ°ç«¯è´¨é‡ä¿éšœæµç¨‹](#13-ç«¯åˆ°ç«¯è´¨é‡ä¿éšœæµç¨‹)
      - [è´¨é‡é—¨ç¦æ ‡å‡†](#è´¨é‡é—¨ç¦æ ‡å‡†)
    - [1.4 æµ‹è¯•ç¯å¢ƒä¸åŸºç¡€è®¾æ–½](#14-æµ‹è¯•ç¯å¢ƒä¸åŸºç¡€è®¾æ–½)
      - [æ¨èçš„æµ‹è¯•ç¯å¢ƒæ¶æ„](#æ¨èçš„æµ‹è¯•ç¯å¢ƒæ¶æ„)
      - [æµ‹è¯•ç¯å¢ƒç®¡ç†è„šæœ¬](#æµ‹è¯•ç¯å¢ƒç®¡ç†è„šæœ¬)
  - [2. å•å…ƒæµ‹è¯•æ¡†æ¶](#2-å•å…ƒæµ‹è¯•æ¡†æ¶)
    - [2.1 å¤šè¯­è¨€å•å…ƒæµ‹è¯•æœ€ä½³å®è·µ](#21-å¤šè¯­è¨€å•å…ƒæµ‹è¯•æœ€ä½³å®è·µ)
      - [Python (pytest)](#python-pytest)
      - [Go (testing + testify)](#go-testing--testify)
    - [2.2 Mock ä¸ Stub ç­–ç•¥](#22-mock-ä¸-stub-ç­–ç•¥)
      - [ä½¿ç”¨ Mock éš”ç¦»å¤–éƒ¨ä¾èµ–](#ä½¿ç”¨-mock-éš”ç¦»å¤–éƒ¨ä¾èµ–)
    - [2.3 æµ‹è¯•è¦†ç›–ç‡ä¸è´¨é‡é—¨ç¦](#23-æµ‹è¯•è¦†ç›–ç‡ä¸è´¨é‡é—¨ç¦)
      - [é…ç½®è¦†ç›–ç‡å·¥å…·](#é…ç½®è¦†ç›–ç‡å·¥å…·)
  - [3. é›†æˆæµ‹è¯•æ¡†æ¶](#3-é›†æˆæµ‹è¯•æ¡†æ¶)
    - [3.1 OTLP Collector é›†æˆæµ‹è¯•](#31-otlp-collector-é›†æˆæµ‹è¯•)
      - [ä½¿ç”¨ Testcontainers è¿›è¡Œé›†æˆæµ‹è¯•](#ä½¿ç”¨-testcontainers-è¿›è¡Œé›†æˆæµ‹è¯•)
    - [3.2 å¤šç»„ä»¶è”è°ƒæµ‹è¯•](#32-å¤šç»„ä»¶è”è°ƒæµ‹è¯•)
  - [4. ç«¯åˆ°ç«¯æµ‹è¯• (E2E)](#4-ç«¯åˆ°ç«¯æµ‹è¯•-e2e)
    - [4.1 åˆ†å¸ƒå¼è¿½è¸ªåœºæ™¯æµ‹è¯•](#41-åˆ†å¸ƒå¼è¿½è¸ªåœºæ™¯æµ‹è¯•)
    - [4.2 è·¨æœåŠ¡é“¾è·¯éªŒè¯](#42-è·¨æœåŠ¡é“¾è·¯éªŒè¯)
    - [4.3 æ€§èƒ½åŸºå‡†æµ‹è¯•](#43-æ€§èƒ½åŸºå‡†æµ‹è¯•)
      - [K6 æ€§èƒ½æµ‹è¯•è„šæœ¬](#k6-æ€§èƒ½æµ‹è¯•è„šæœ¬)
      - [JMeter æ€§èƒ½æµ‹è¯•è®¡åˆ’](#jmeter-æ€§èƒ½æµ‹è¯•è®¡åˆ’)
    - [4.4 æ··æ²Œå·¥ç¨‹ (Chaos Engineering)](#44-æ··æ²Œå·¥ç¨‹-chaos-engineering)
  - [5. åè®®ä¸€è‡´æ€§éªŒè¯](#5-åè®®ä¸€è‡´æ€§éªŒè¯)
    - [5.1 OTLP/gRPC åè®®éªŒè¯](#51-otlpgrpc-åè®®éªŒè¯)
  - [7. è‡ªåŠ¨åŒ–æµ‹è¯•æµæ°´çº¿](#7-è‡ªåŠ¨åŒ–æµ‹è¯•æµæ°´çº¿)
    - [7.1 CI/CD é›†æˆ (GitHub Actions)](#71-cicd-é›†æˆ-github-actions)
  - [8. å·¥å…·é“¾ä¸ç”Ÿæ€](#8-å·¥å…·é“¾ä¸ç”Ÿæ€)
    - [8.1 æ¨èæµ‹è¯•å·¥å…·çŸ©é˜µ](#81-æ¨èæµ‹è¯•å·¥å…·çŸ©é˜µ)
    - [8.2 è‡ªå®šä¹‰æµ‹è¯•å·¥å…·å¼€å‘](#82-è‡ªå®šä¹‰æµ‹è¯•å·¥å…·å¼€å‘)
  - [9. æœ€ä½³å®è·µä¸æ¡ˆä¾‹ç ”ç©¶](#9-æœ€ä½³å®è·µä¸æ¡ˆä¾‹ç ”ç©¶)
    - [9.1 è¡Œä¸šæ ‡æ†æ¡ˆä¾‹](#91-è¡Œä¸šæ ‡æ†æ¡ˆä¾‹)
      - [æ¡ˆä¾‹ 1: Uber - å¤§è§„æ¨¡åˆ†å¸ƒå¼è¿½è¸ªæµ‹è¯•](#æ¡ˆä¾‹-1-uber---å¤§è§„æ¨¡åˆ†å¸ƒå¼è¿½è¸ªæµ‹è¯•)
      - [æ¡ˆä¾‹ 2: Netflix - æ··æ²Œå·¥ç¨‹é©±åŠ¨çš„å¯è§‚æµ‹æ€§æµ‹è¯•](#æ¡ˆä¾‹-2-netflix---æ··æ²Œå·¥ç¨‹é©±åŠ¨çš„å¯è§‚æµ‹æ€§æµ‹è¯•)
    - [9.2 å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ](#92-å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ)
      - [é™·é˜± 1: æµ‹è¯•ç¯å¢ƒä¸ç”Ÿäº§ç¯å¢ƒå·®å¼‚è¿‡å¤§](#é™·é˜±-1-æµ‹è¯•ç¯å¢ƒä¸ç”Ÿäº§ç¯å¢ƒå·®å¼‚è¿‡å¤§)
      - [é™·é˜± 2: æµ‹è¯•æ•°æ®ä¸çœŸå®](#é™·é˜±-2-æµ‹è¯•æ•°æ®ä¸çœŸå®)
      - [é™·é˜± 3: æµ‹è¯•æ•°æ®æ±¡æŸ“](#é™·é˜±-3-æµ‹è¯•æ•°æ®æ±¡æŸ“)
    - [9.3 æµ‹è¯•ç­–ç•¥æ¼”è¿›è·¯å¾„](#93-æµ‹è¯•ç­–ç•¥æ¼”è¿›è·¯å¾„)
      - [é˜¶æ®µ 1: åŸºç¡€æµ‹è¯• (0-3 ä¸ªæœˆ)](#é˜¶æ®µ-1-åŸºç¡€æµ‹è¯•-0-3-ä¸ªæœˆ)
      - [é˜¶æ®µ 2: è‡ªåŠ¨åŒ–å¢å¼º (3-6 ä¸ªæœˆ)](#é˜¶æ®µ-2-è‡ªåŠ¨åŒ–å¢å¼º-3-6-ä¸ªæœˆ)
      - [é˜¶æ®µ 3: ç”Ÿäº§çº§æµ‹è¯• (6-12 ä¸ªæœˆ)](#é˜¶æ®µ-3-ç”Ÿäº§çº§æµ‹è¯•-6-12-ä¸ªæœˆ)
      - [é˜¶æ®µ 4: æŒç»­ä¼˜åŒ– (12+ ä¸ªæœˆ)](#é˜¶æ®µ-4-æŒç»­ä¼˜åŒ–-12-ä¸ªæœˆ)
    - [9.4 ROI ä¸è´¨é‡åº¦é‡](#94-roi-ä¸è´¨é‡åº¦é‡)
      - [æµ‹è¯•æŠ•èµ„å›æŠ¥ç‡è®¡ç®—](#æµ‹è¯•æŠ•èµ„å›æŠ¥ç‡è®¡ç®—)
      - [è´¨é‡åº¦é‡æŒ‡æ ‡](#è´¨é‡åº¦é‡æŒ‡æ ‡)
  - [10. æ€»ç»“ä¸å±•æœ›](#10-æ€»ç»“ä¸å±•æœ›)
    - [10.1 æ ¸å¿ƒè¦ç‚¹](#101-æ ¸å¿ƒè¦ç‚¹)
    - [10.2 æœªæ¥è¶‹åŠ¿](#102-æœªæ¥è¶‹åŠ¿)
  - [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)

---

## 1. OTLP æµ‹è¯•ä½“ç³»æ¦‚è¿°

### 1.1 æµ‹è¯•é‡‘å­—å¡”ä¸æµ‹è¯•ç­–ç•¥

OTLP æµ‹è¯•ä½“ç³»éµå¾ª**æµ‹è¯•é‡‘å­—å¡”**åŸåˆ™,æ„å»ºåˆ†å±‚æµ‹è¯•ç­–ç•¥:

```text
          â•±â•²
         â•±E2Eâ•²          å°‘é‡ (5-10%) - é«˜ä»·å€¼,é«˜æˆæœ¬
        â•±â”€â”€â”€â”€â”€â”€â•²
       â•± é›†æˆæµ‹è¯• â•²       é€‚ä¸­ (20-30%) - éªŒè¯ç»„ä»¶äº¤äº’
      â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
     â•±   å•å…ƒæµ‹è¯•   â•±      å¤§é‡ (60-75%) - å¿«é€Ÿåé¦ˆ,ä½æˆæœ¬
    â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±
```

#### åˆ†å±‚æµ‹è¯•ç­–ç•¥

| æµ‹è¯•å±‚çº§ | æµ‹è¯•å¯¹è±¡ | æµ‹è¯•å·¥å…· | æ‰§è¡Œé¢‘ç‡ | åé¦ˆæ—¶é—´ |
|---------|---------|---------|---------|---------|
| **å•å…ƒæµ‹è¯•** | å‡½æ•°/æ–¹æ³•/ç±» | JUnit, pytest, Go testing, Jest | æ¯æ¬¡æäº¤ | < 5 åˆ†é’Ÿ |
| **é›†æˆæµ‹è¯•** | ç»„ä»¶äº¤äº’ | Testcontainers, Docker Compose | æ¯æ¬¡ PR | 10-30 åˆ†é’Ÿ |
| **å¥‘çº¦æµ‹è¯•** | API å…¼å®¹æ€§ | Pact, Spring Cloud Contract | æ¯æ¬¡ PR | 5-15 åˆ†é’Ÿ |
| **E2E æµ‹è¯•** | å®Œæ•´åœºæ™¯ | Playwright, K6, Custom Framework | æ¯æ—¥/æ¯å‘¨ | 30-120 åˆ†é’Ÿ |
| **æ€§èƒ½æµ‹è¯•** | ååé‡/å»¶è¿Ÿ | K6, JMeter, Gatling | æ¯å‘¨/å‘å¸ƒå‰ | 60-240 åˆ†é’Ÿ |
| **æ··æ²Œæµ‹è¯•** | æ•…éšœæ¢å¤ | Chaos Mesh, Litmus | æ¯å‘¨/æŒ‰éœ€ | å¯å˜ |

### 1.2 OTLP ç‰¹å®šçš„æµ‹è¯•æŒ‘æˆ˜

#### æŒ‘æˆ˜ 1: åˆ†å¸ƒå¼ç³»ç»Ÿçš„ä¸ç¡®å®šæ€§

**é—®é¢˜**: åˆ†å¸ƒå¼è¿½è¸ªæ•°æ®çš„ç”Ÿæˆå’Œä¼ è¾“å…·æœ‰å¼‚æ­¥æ€§å’Œä¸ç¡®å®šæ€§ã€‚

**è§£å†³æ–¹æ¡ˆ**:

- **æœ€ç»ˆä¸€è‡´æ€§éªŒè¯**: ä½¿ç”¨è½®è¯¢æœºåˆ¶ç­‰å¾…æ•°æ®åˆ°è¾¾
- **æ—¶é—´çª—å£å®¹å¿**: å…è®¸æ—¶é—´æˆ³åœ¨åˆç†èŒƒå›´å†…æ³¢åŠ¨
- **é‡è¯•ä¸å¹‚ç­‰æ€§**: æµ‹è¯•ç”¨ä¾‹æ”¯æŒé‡è¯•,éªŒè¯å¹‚ç­‰æ€§

```python
# Python - æœ€ç»ˆä¸€è‡´æ€§éªŒè¯ç¤ºä¾‹
import time
from typing import Optional, Callable
from dataclasses import dataclass

@dataclass
class RetryConfig:
    """é‡è¯•é…ç½®"""
    max_attempts: int = 10
    initial_delay: float = 0.5
    backoff_factor: float = 1.5
    max_delay: float = 10.0

def wait_for_condition(
    condition: Callable[[], bool],
    config: RetryConfig = RetryConfig(),
    timeout: float = 30.0
) -> bool:
    """
    ç­‰å¾…æ¡ä»¶æ»¡è¶³ (æœ€ç»ˆä¸€è‡´æ€§éªŒè¯)
    
    Args:
        condition: æ£€æŸ¥æ¡ä»¶çš„å‡½æ•°
        config: é‡è¯•é…ç½®
        timeout: è¶…æ—¶æ—¶é—´ (ç§’)
    
    Returns:
        æ¡ä»¶æ˜¯å¦æ»¡è¶³
    """
    start_time = time.time()
    delay = config.initial_delay
    
    for attempt in range(config.max_attempts):
        # æ£€æŸ¥è¶…æ—¶
        if time.time() - start_time > timeout:
            return False
        
        # æ£€æŸ¥æ¡ä»¶
        try:
            if condition():
                return True
        except Exception as e:
            # è®°å½•ä½†ä¸ä¸­æ–­é‡è¯•
            print(f"Condition check failed (attempt {attempt + 1}): {e}")
        
        # ç­‰å¾…åé‡è¯•
        time.sleep(delay)
        delay = min(delay * config.backoff_factor, config.max_delay)
    
    return False

# ä½¿ç”¨ç¤ºä¾‹
def verify_trace_received(trace_id: str, backend_api: str) -> None:
    """éªŒè¯è¿½è¸ªæ•°æ®å·²æ¥æ”¶"""
    import requests
    
    def check_trace() -> bool:
        response = requests.get(f"{backend_api}/traces/{trace_id}", timeout=5)
        return response.status_code == 200 and response.json().get("spans")
    
    assert wait_for_condition(check_trace, timeout=30.0), \
        f"Trace {trace_id} not received within 30 seconds"
```

#### æŒ‘æˆ˜ 2: å¤šåè®®å¤šè¯­è¨€å…¼å®¹æ€§

**é—®é¢˜**: OTLP æ”¯æŒ gRPC å’Œ HTTP ä¸¤ç§åè®®,ä»¥åŠå¤šç§ç¼–ç¨‹è¯­è¨€ SDKã€‚

**è§£å†³æ–¹æ¡ˆ**:

- **åè®®ä¸€è‡´æ€§æµ‹è¯•å¥—ä»¶**: å¯¹ gRPC å’Œ HTTP ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•ç”¨ä¾‹
- **è·¨è¯­è¨€äº’æ“ä½œæ€§æµ‹è¯•**: éªŒè¯ä¸åŒè¯­è¨€ SDK ç”Ÿæˆæ•°æ®çš„å…¼å®¹æ€§
- **å‚æ•°åŒ–æµ‹è¯•**: ä½¿ç”¨æµ‹è¯•æ¡†æ¶çš„å‚æ•°åŒ–åŠŸèƒ½è¦†ç›–å¤šç§åœºæ™¯

```go
// Go - å¤šåè®®å‚æ•°åŒ–æµ‹è¯•ç¤ºä¾‹
package otlp_test

import (
    "context"
    "testing"
    "time"

    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp"
    "go.opentelemetry.io/otel/sdk/trace"
    sdktrace "go.opentelemetry.io/otel/sdk/trace"
)

// ExporterFactory å®šä¹‰å¯¼å‡ºå™¨å·¥å‚æ¥å£
type ExporterFactory func(endpoint string) (trace.SpanExporter, error)

// TestExporterProtocols æµ‹è¯•å¤šç§åè®®
func TestExporterProtocols(t *testing.T) {
    testCases := []struct {
        name    string
        factory ExporterFactory
        endpoint string
    }{
        {
            name: "OTLP/gRPC",
            factory: func(endpoint string) (trace.SpanExporter, error) {
                return otlptracegrpc.New(
                    context.Background(),
                    otlptracegrpc.WithEndpoint(endpoint),
                    otlptracegrpc.WithInsecure(),
                )
            },
            endpoint: "localhost:4317",
        },
        {
            name: "OTLP/HTTP",
            factory: func(endpoint string) (trace.SpanExporter, error) {
                return otlptracehttp.New(
                    context.Background(),
                    otlptracehttp.WithEndpoint(endpoint),
                    otlptracehttp.WithInsecure(),
                )
            },
            endpoint: "localhost:4318",
        },
    }

    for _, tc := range testCases {
        t.Run(tc.name, func(t *testing.T) {
            // åˆ›å»ºå¯¼å‡ºå™¨
            exporter, err := tc.factory(tc.endpoint)
            if err != nil {
                t.Fatalf("Failed to create exporter: %v", err)
            }
            defer exporter.Shutdown(context.Background())

            // åˆ›å»ºè¿½è¸ªæä¾›å™¨
            tp := sdktrace.NewTracerProvider(
                sdktrace.WithBatcher(exporter),
            )
            defer tp.Shutdown(context.Background())

            // ç”Ÿæˆæµ‹è¯•è¿½è¸ª
            tracer := tp.Tracer("test-tracer")
            ctx, span := tracer.Start(context.Background(), "test-span")
            span.SetAttributes(/* ... */)
            time.Sleep(10 * time.Millisecond)
            span.End()

            // éªŒè¯å¯¼å‡ºæˆåŠŸ
            if err := tp.ForceFlush(ctx); err != nil {
                t.Errorf("Failed to flush traces: %v", err)
            }

            // éªŒè¯åç«¯æ¥æ”¶ (ä½¿ç”¨å‰é¢çš„ wait_for_condition æ¨¡å¼)
            // ...
        })
    }
}
```

#### æŒ‘æˆ˜ 3: æ€§èƒ½ä¸æ­£ç¡®æ€§çš„å¹³è¡¡

**é—®é¢˜**: æµ‹è¯•éœ€è¦åŒæ—¶éªŒè¯åŠŸèƒ½æ­£ç¡®æ€§å’Œæ€§èƒ½æŒ‡æ ‡ã€‚

**è§£å†³æ–¹æ¡ˆ**:

- **åˆ†ç¦»åŠŸèƒ½æµ‹è¯•ä¸æ€§èƒ½æµ‹è¯•**: ä¸åŒçš„æµ‹è¯•å¥—ä»¶å’Œæ‰§è¡Œé¢‘ç‡
- **æ€§èƒ½åŸºå‡†å›å½’æ£€æµ‹**: å»ºç«‹æ€§èƒ½åŸºçº¿,æ£€æµ‹é€€åŒ–
- **èµ„æºé™åˆ¶æµ‹è¯•**: åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹éªŒè¯æ­£ç¡®æ€§

```typescript
// TypeScript (Node.js) - æ€§èƒ½åŸºå‡†æµ‹è¯•ç¤ºä¾‹
import { performance } from 'perf_hooks';
import { trace, SpanStatusCode } from '@opentelemetry/api';
import { Resource } from '@opentelemetry/resources';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';
import { BasicTracerProvider, BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';

interface BenchmarkResult {
  totalSpans: number;
  durationMs: number;
  throughputPerSecond: number;
  avgLatencyMs: number;
  p95LatencyMs: number;
  p99LatencyMs: number;
}

/**
 * æ€§èƒ½åŸºå‡†æµ‹è¯•
 */
class OTLPBenchmark {
  private provider: BasicTracerProvider;
  private exporter: OTLPTraceExporter;
  private latencies: number[] = [];

  constructor(collectorEndpoint: string) {
    this.exporter = new OTLPTraceExporter({
      url: collectorEndpoint,
    });

    this.provider = new BasicTracerProvider({
      resource: new Resource({
        [SemanticResourceAttributes.SERVICE_NAME]: 'benchmark-service',
      }),
    });

    this.provider.addSpanProcessor(new BatchSpanProcessor(this.exporter, {
      maxQueueSize: 10000,
      maxExportBatchSize: 512,
      scheduledDelayMillis: 1000,
    }));
  }

  /**
   * è¿è¡ŒåŸºå‡†æµ‹è¯•
   */
  async runBenchmark(totalSpans: number, concurrency: number = 10): Promise<BenchmarkResult> {
    const tracer = this.provider.getTracer('benchmark-tracer');
    const startTime = performance.now();
    this.latencies = [];

    // å¹¶å‘ç”Ÿæˆ Span
    const batchSize = Math.ceil(totalSpans / concurrency);
    const promises: Promise<void>[] = [];

    for (let i = 0; i < concurrency; i++) {
      const start = i * batchSize;
      const end = Math.min(start + batchSize, totalSpans);
      
      promises.push(this.generateSpanBatch(tracer, start, end));
    }

    await Promise.all(promises);

    // å¼ºåˆ¶åˆ·æ–°
    await this.provider.forceFlush();

    const endTime = performance.now();
    const durationMs = endTime - startTime;

    // è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    this.latencies.sort((a, b) => a - b);
    const avgLatencyMs = this.latencies.reduce((a, b) => a + b, 0) / this.latencies.length;
    const p95Index = Math.floor(this.latencies.length * 0.95);
    const p99Index = Math.floor(this.latencies.length * 0.99);

    return {
      totalSpans,
      durationMs,
      throughputPerSecond: (totalSpans / durationMs) * 1000,
      avgLatencyMs,
      p95LatencyMs: this.latencies[p95Index],
      p99LatencyMs: this.latencies[p99Index],
    };
  }

  /**
   * ç”Ÿæˆä¸€æ‰¹ Span
   */
  private async generateSpanBatch(tracer: any, start: number, end: number): Promise<void> {
    for (let i = start; i < end; i++) {
      const spanStartTime = performance.now();
      
      const span = tracer.startSpan(`operation-${i}`, {
        attributes: {
          'span.index': i,
          'benchmark.batch': Math.floor(i / 100),
        },
      });

      // æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
      await this.simulateWork();

      span.setStatus({ code: SpanStatusCode.OK });
      span.end();

      const spanEndTime = performance.now();
      this.latencies.push(spanEndTime - spanStartTime);
    }
  }

  /**
   * æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½
   */
  private async simulateWork(): Promise<void> {
    // æ¨¡æ‹Ÿ 1-5ms çš„å¼‚æ­¥å·¥ä½œ
    const delay = Math.random() * 4 + 1;
    await new Promise(resolve => setTimeout(resolve, delay));
  }

  /**
   * æ¸…ç†èµ„æº
   */
  async shutdown(): Promise<void> {
    await this.provider.shutdown();
  }
}

// ä½¿ç”¨ç¤ºä¾‹å’Œæ–­è¨€
async function runPerformanceTest(): Promise<void> {
  const benchmark = new OTLPBenchmark('http://localhost:4318/v1/traces');

  try {
    console.log('Starting OTLP performance benchmark...');
    
    const result = await benchmark.runBenchmark(10000, 20);

    console.log('Benchmark Results:');
    console.log(`  Total Spans: ${result.totalSpans}`);
    console.log(`  Duration: ${result.durationMs.toFixed(2)} ms`);
    console.log(`  Throughput: ${result.throughputPerSecond.toFixed(2)} spans/sec`);
    console.log(`  Avg Latency: ${result.avgLatencyMs.toFixed(2)} ms`);
    console.log(`  P95 Latency: ${result.p95LatencyMs.toFixed(2)} ms`);
    console.log(`  P99 Latency: ${result.p99LatencyMs.toFixed(2)} ms`);

    // æ€§èƒ½æ–­è¨€ (åŸºäºå†å²åŸºçº¿)
    const baseline = {
      throughputPerSecond: 5000,  // æœ€ä½ååé‡
      p99LatencyMs: 50,            // æœ€å¤§ P99 å»¶è¿Ÿ
    };

    if (result.throughputPerSecond < baseline.throughputPerSecond) {
      throw new Error(
        `Performance regression: Throughput ${result.throughputPerSecond.toFixed(2)} ` +
        `< baseline ${baseline.throughputPerSecond}`
      );
    }

    if (result.p99LatencyMs > baseline.p99LatencyMs) {
      throw new Error(
        `Performance regression: P99 latency ${result.p99LatencyMs.toFixed(2)}ms ` +
        `> baseline ${baseline.p99LatencyMs}ms`
      );
    }

    console.log('âœ… Performance test passed');

  } finally {
    await benchmark.shutdown();
  }
}

// å¯¼å‡ºä¾›æµ‹è¯•æ¡†æ¶ä½¿ç”¨
export { OTLPBenchmark, BenchmarkResult, runPerformanceTest };
```

### 1.3 ç«¯åˆ°ç«¯è´¨é‡ä¿éšœæµç¨‹

```mermaid
graph TB
    A[ä»£ç æäº¤] --> B[é™æ€åˆ†æ<br/>Linting/SAST]
    B --> C[å•å…ƒæµ‹è¯•<br/>è¦†ç›–ç‡ > 80%]
    C --> D[é›†æˆæµ‹è¯•<br/>ç»„ä»¶äº¤äº’]
    D --> E[å¥‘çº¦æµ‹è¯•<br/>API å…¼å®¹æ€§]
    E --> F[E2E æµ‹è¯•<br/>å…³é”®åœºæ™¯]
    F --> G[æ€§èƒ½æµ‹è¯•<br/>åŸºå‡†å›å½’]
    G --> H[å®‰å…¨æ‰«æ<br/>æ¼æ´æ£€æµ‹]
    H --> I{æ‰€æœ‰æµ‹è¯•é€šè¿‡?}
    I -->|æ˜¯| J[æ„å»ºåˆ¶å“]
    I -->|å¦| K[é€šçŸ¥å¼€å‘è€…]
    K --> A
    J --> L[éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ]
    L --> M[å†’çƒŸæµ‹è¯•]
    M --> N[å‘å¸ƒåˆ°ç”Ÿäº§]
```

#### è´¨é‡é—¨ç¦æ ‡å‡†

| é˜¶æ®µ | æŒ‡æ ‡ | é˜ˆå€¼ | é˜»æ–­çº§åˆ« |
|------|------|------|---------|
| **ä»£ç è´¨é‡** | Linting é”™è¯¯ | 0 | âŒ é˜»æ–­ |
| **å•å…ƒæµ‹è¯•** | è¦†ç›–ç‡ | â‰¥ 80% | âŒ é˜»æ–­ |
| **å•å…ƒæµ‹è¯•** | é€šè¿‡ç‡ | 100% | âŒ é˜»æ–­ |
| **é›†æˆæµ‹è¯•** | é€šè¿‡ç‡ | 100% | âŒ é˜»æ–­ |
| **å¥‘çº¦æµ‹è¯•** | å…¼å®¹æ€§ | æ— ç ´åæ€§å˜æ›´ | âŒ é˜»æ–­ |
| **E2E æµ‹è¯•** | é€šè¿‡ç‡ | 100% | âŒ é˜»æ–­ |
| **æ€§èƒ½æµ‹è¯•** | ååé‡ä¸‹é™ | < 10% | âš ï¸ è­¦å‘Š |
| **æ€§èƒ½æµ‹è¯•** | P99 å»¶è¿Ÿå¢åŠ  | < 20% | âš ï¸ è­¦å‘Š |
| **å®‰å…¨æ‰«æ** | é«˜å±æ¼æ´ | 0 | âŒ é˜»æ–­ |
| **å®‰å…¨æ‰«æ** | ä¸­å±æ¼æ´ | â‰¤ 5 | âš ï¸ è­¦å‘Š |

### 1.4 æµ‹è¯•ç¯å¢ƒä¸åŸºç¡€è®¾æ–½

#### æ¨èçš„æµ‹è¯•ç¯å¢ƒæ¶æ„

```yaml
# docker-compose.test.yml - å®Œæ•´çš„æµ‹è¯•ç¯å¢ƒ
version: '3.9'

services:
  # OTLP Collector (è¢«æµ‹ç³»ç»Ÿ)
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.108.0
    container_name: otel-collector-test
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./test-configs/collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Prometheus metrics
      - "13133:13133" # Health check
    networks:
      - otlp-test-network

  # Jaeger (åç«¯å­˜å‚¨å’ŒæŸ¥è¯¢)
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger-test
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "16686:16686"  # Jaeger UI
      - "14250:14250"  # Jaeger gRPC
    networks:
      - otlp-test-network

  # Prometheus (æŒ‡æ ‡åç«¯)
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus-test
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - ./test-configs/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - otlp-test-network

  # Redis (ç”¨äºæµ‹è¯•ç¼“å­˜å’ŒçŠ¶æ€)
  redis:
    image: redis:7-alpine
    container_name: redis-test
    ports:
      - "6379:6379"
    networks:
      - otlp-test-network

  # PostgreSQL (ç”¨äºæµ‹è¯•æ•°æ®åº“é›†æˆ)
  postgres:
    image: postgres:16-alpine
    container_name: postgres-test
    environment:
      POSTGRES_USER: testuser
      POSTGRES_PASSWORD: testpass
      POSTGRES_DB: testdb
    ports:
      - "5432:5432"
    networks:
      - otlp-test-network

  # æµ‹è¯•åº”ç”¨ (ç”Ÿæˆæµ‹è¯•æ•°æ®)
  test-app:
    build:
      context: ./test-apps
      dockerfile: Dockerfile
    container_name: test-app
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
      - OTEL_SERVICE_NAME=test-application
    depends_on:
      - otel-collector
      - redis
      - postgres
    networks:
      - otlp-test-network

networks:
  otlp-test-network:
    driver: bridge
```

#### æµ‹è¯•ç¯å¢ƒç®¡ç†è„šæœ¬

```bash
#!/bin/bash
# test-env.sh - æµ‹è¯•ç¯å¢ƒç®¡ç†è„šæœ¬

set -e

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# å¯åŠ¨æµ‹è¯•ç¯å¢ƒ
start_test_env() {
    log_info "Starting OTLP test environment..."
    docker-compose -f docker-compose.test.yml up -d
    
    log_info "Waiting for services to be ready..."
    wait_for_service "http://localhost:13133" "OTLP Collector"
    wait_for_service "http://localhost:16686" "Jaeger"
    wait_for_service "http://localhost:9090" "Prometheus"
    
    log_info "Test environment is ready!"
}

# åœæ­¢æµ‹è¯•ç¯å¢ƒ
stop_test_env() {
    log_info "Stopping OTLP test environment..."
    docker-compose -f docker-compose.test.yml down -v
    log_info "Test environment stopped."
}

# ç­‰å¾…æœåŠ¡å°±ç»ª
wait_for_service() {
    local url=$1
    local name=$2
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if curl -s -o /dev/null -w "%{http_code}" "$url" | grep -q "200\|404"; then
            log_info "$name is ready (attempt $attempt/$max_attempts)"
            return 0
        fi
        
        log_warn "$name not ready yet (attempt $attempt/$max_attempts)"
        sleep 2
        ((attempt++))
    done
    
    log_error "$name failed to start after $max_attempts attempts"
    return 1
}

# æ¸…ç†æµ‹è¯•æ•°æ®
cleanup() {
    log_info "Cleaning up test data..."
    docker-compose -f docker-compose.test.yml exec -T redis redis-cli FLUSHALL
    docker-compose -f docker-compose.test.yml exec -T postgres psql -U testuser -d testdb -c "TRUNCATE TABLE test_table CASCADE;"
    log_info "Cleanup complete."
}

# æŸ¥çœ‹æ—¥å¿—
logs() {
    local service=$1
    if [ -z "$service" ]; then
        docker-compose -f docker-compose.test.yml logs -f
    else
        docker-compose -f docker-compose.test.yml logs -f "$service"
    fi
}

# å¥åº·æ£€æŸ¥
health_check() {
    log_info "Running health checks..."
    
    # Collector health
    if curl -s http://localhost:13133 | grep -q "Server available"; then
        log_info "âœ… OTLP Collector is healthy"
    else
        log_error "âŒ OTLP Collector is unhealthy"
        return 1
    fi
    
    # Jaeger health
    if curl -s http://localhost:16686 > /dev/null; then
        log_info "âœ… Jaeger is healthy"
    else
        log_error "âŒ Jaeger is unhealthy"
        return 1
    fi
    
    # Prometheus health
    if curl -s http://localhost:9090/-/healthy | grep -q "Prometheus is Healthy"; then
        log_info "âœ… Prometheus is healthy"
    else
        log_error "âŒ Prometheus is unhealthy"
        return 1
    fi
    
    log_info "All services are healthy!"
}

# ä¸»å‡½æ•°
main() {
    local command=$1
    
    case "$command" in
        start)
            start_test_env
            ;;
        stop)
            stop_test_env
            ;;
        restart)
            stop_test_env
            start_test_env
            ;;
        cleanup)
            cleanup
            ;;
        logs)
            logs "$2"
            ;;
        health)
            health_check
            ;;
        *)
            echo "Usage: $0 {start|stop|restart|cleanup|logs [service]|health}"
            exit 1
            ;;
    esac
}

main "$@"
```

---

## 2. å•å…ƒæµ‹è¯•æ¡†æ¶

### 2.1 å¤šè¯­è¨€å•å…ƒæµ‹è¯•æœ€ä½³å®è·µ

#### Python (pytest)

```python
# test_trace_processor.py - Python å•å…ƒæµ‹è¯•ç¤ºä¾‹
import pytest
from unittest.mock import Mock, patch, MagicMock
from typing import List
from dataclasses import dataclass

from opentelemetry.proto.trace.v1 import trace_pb2
from opentelemetry.proto.common.v1 import common_pb2

@dataclass
class TraceProcessorConfig:
    """è¿½è¸ªå¤„ç†å™¨é…ç½®"""
    sample_rate: float = 1.0
    max_spans_per_trace: int = 1000
    enable_validation: bool = True

class TraceProcessor:
    """è¿½è¸ªæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, config: TraceProcessorConfig):
        self.config = config
        self.processed_count = 0
    
    def process_spans(self, spans: List[trace_pb2.Span]) -> List[trace_pb2.Span]:
        """
        å¤„ç† Span åˆ—è¡¨
        
        Args:
            spans: åŸå§‹ Span åˆ—è¡¨
        
        Returns:
            å¤„ç†åçš„ Span åˆ—è¡¨
        
        Raises:
            ValueError: å¦‚æœ Span æ— æ•ˆ
        """
        if not spans:
            return []
        
        if len(spans) > self.config.max_spans_per_trace:
            raise ValueError(
                f"Too many spans: {len(spans)} > {self.config.max_spans_per_trace}"
            )
        
        processed = []
        for span in spans:
            if self.config.enable_validation:
                self._validate_span(span)
            
            if self._should_sample():
                processed.append(self._enrich_span(span))
        
        self.processed_count += len(processed)
        return processed
    
    def _validate_span(self, span: trace_pb2.Span) -> None:
        """éªŒè¯ Span æœ‰æ•ˆæ€§"""
        if not span.trace_id:
            raise ValueError("Span missing trace_id")
        if not span.span_id:
            raise ValueError("Span missing span_id")
        if span.end_time_unix_nano <= span.start_time_unix_nano:
            raise ValueError("Invalid span duration")
    
    def _should_sample(self) -> bool:
        """é‡‡æ ·å†³ç­–"""
        import random
        return random.random() < self.config.sample_rate
    
    def _enrich_span(self, span: trace_pb2.Span) -> trace_pb2.Span:
        """ä¸°å¯Œ Span å…ƒæ•°æ®"""
        # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
        span.attributes.append(
            common_pb2.KeyValue(
                key="processed_at",
                value=common_pb2.AnyValue(int_value=int(time.time() * 1e9))
            )
        )
        return span


# ===== æµ‹è¯•å¥—ä»¶ =====

class TestTraceProcessor:
    """TraceProcessor å•å…ƒæµ‹è¯•å¥—ä»¶"""
    
    @pytest.fixture
    def processor(self) -> TraceProcessor:
        """æµ‹è¯•å¤¹å…·: åˆ›å»ºå¤„ç†å™¨å®ä¾‹"""
        config = TraceProcessorConfig(
            sample_rate=1.0,  # 100% é‡‡æ ·ä¾¿äºæµ‹è¯•
            max_spans_per_trace=100,
            enable_validation=True
        )
        return TraceProcessor(config)
    
    @pytest.fixture
    def valid_span(self) -> trace_pb2.Span:
        """æµ‹è¯•å¤¹å…·: åˆ›å»ºæœ‰æ•ˆçš„ Span"""
        return trace_pb2.Span(
            trace_id=b'\x01' * 16,
            span_id=b'\x02' * 8,
            name="test-span",
            kind=trace_pb2.Span.SPAN_KIND_INTERNAL,
            start_time_unix_nano=1000000,
            end_time_unix_nano=2000000,
        )
    
    def test_process_empty_spans(self, processor: TraceProcessor):
        """æµ‹è¯•: å¤„ç†ç©ºåˆ—è¡¨"""
        result = processor.process_spans([])
        assert result == []
        assert processor.processed_count == 0
    
    def test_process_valid_span(self, processor: TraceProcessor, valid_span: trace_pb2.Span):
        """æµ‹è¯•: å¤„ç†æœ‰æ•ˆ Span"""
        result = processor.process_spans([valid_span])
        
        assert len(result) == 1
        assert result[0].trace_id == valid_span.trace_id
        assert result[0].span_id == valid_span.span_id
        assert processor.processed_count == 1
        
        # éªŒè¯ä¸°å¯Œçš„å…ƒæ•°æ®
        processed_at_attr = next(
            (attr for attr in result[0].attributes if attr.key == "processed_at"),
            None
        )
        assert processed_at_attr is not None
        assert processed_at_attr.value.int_value > 0
    
    @pytest.mark.parametrize("missing_field,expected_error", [
        ("trace_id", "missing trace_id"),
        ("span_id", "missing span_id"),
    ])
    def test_validate_span_missing_fields(
        self, 
        processor: TraceProcessor, 
        valid_span: trace_pb2.Span,
        missing_field: str,
        expected_error: str
    ):
        """æµ‹è¯•: éªŒè¯ç¼ºå¤±å¿…éœ€å­—æ®µçš„ Span"""
        setattr(valid_span, missing_field, b'')
        
        with pytest.raises(ValueError, match=expected_error):
            processor.process_spans([valid_span])
    
    def test_validate_span_invalid_duration(self, processor: TraceProcessor, valid_span: trace_pb2.Span):
        """æµ‹è¯•: éªŒè¯æŒç»­æ—¶é—´æ— æ•ˆçš„ Span"""
        valid_span.end_time_unix_nano = valid_span.start_time_unix_nano - 1
        
        with pytest.raises(ValueError, match="Invalid span duration"):
            processor.process_spans([valid_span])
    
    def test_too_many_spans(self, processor: TraceProcessor, valid_span: trace_pb2.Span):
        """æµ‹è¯•: è¶…è¿‡æœ€å¤§ Span æ•°é‡é™åˆ¶"""
        many_spans = [valid_span] * 101  # è¶…è¿‡é…ç½®çš„ 100
        
        with pytest.raises(ValueError, match="Too many spans"):
            processor.process_spans(many_spans)
    
    def test_sampling(self, valid_span: trace_pb2.Span):
        """æµ‹è¯•: é‡‡æ ·é€»è¾‘"""
        # 50% é‡‡æ ·ç‡
        config = TraceProcessorConfig(sample_rate=0.5, enable_validation=False)
        processor = TraceProcessor(config)
        
        # ç”Ÿæˆ 1000 ä¸ª Span,ç»Ÿè®¡é‡‡æ ·æ•°é‡
        spans = [valid_span] * 1000
        
        with patch('random.random', side_effect=[0.3, 0.7] * 500):  # äº¤æ›¿ä½äºå’Œé«˜äº 0.5
            result = processor.process_spans(spans)
        
        # åº”è¯¥å¤§çº¦æœ‰ 500 ä¸ªè¢«é‡‡æ · (å…è®¸ 5% è¯¯å·®)
        assert 475 <= len(result) <= 525
    
    def test_disable_validation(self, valid_span: trace_pb2.Span):
        """æµ‹è¯•: ç¦ç”¨éªŒè¯"""
        config = TraceProcessorConfig(
            sample_rate=1.0,
            enable_validation=False
        )
        processor = TraceProcessor(config)
        
        # åˆ›å»ºæ— æ•ˆ Span (ç¼ºå¤± trace_id)
        invalid_span = trace_pb2.Span(
            span_id=b'\x02' * 8,
            name="invalid-span",
            start_time_unix_nano=1000000,
            end_time_unix_nano=2000000,
        )
        
        # ç¦ç”¨éªŒè¯æ—¶ä¸åº”æŠ›å‡ºå¼‚å¸¸
        result = processor.process_spans([invalid_span])
        assert len(result) == 1
    
    @patch('time.time', return_value=1234567890.123456)
    def test_enrich_span_timestamp(
        self, 
        mock_time: Mock, 
        processor: TraceProcessor, 
        valid_span: trace_pb2.Span
    ):
        """æµ‹è¯•: Span ä¸°å¯ŒåŒ–æ·»åŠ æ—¶é—´æˆ³"""
        result = processor.process_spans([valid_span])
        
        processed_at_attr = next(
            attr for attr in result[0].attributes if attr.key == "processed_at"
        )
        expected_timestamp = int(1234567890.123456 * 1e9)
        assert processed_at_attr.value.int_value == expected_timestamp


# æ€§èƒ½æµ‹è¯• (ä½¿ç”¨ pytest-benchmark)
def test_processor_performance(benchmark, valid_span: trace_pb2.Span):
    """æ€§èƒ½æµ‹è¯•: å¤„ç† 1000 ä¸ª Span çš„æ€§èƒ½"""
    config = TraceProcessorConfig(
        sample_rate=1.0,
        enable_validation=True
    )
    processor = TraceProcessor(config)
    spans = [valid_span] * 1000
    
    result = benchmark(processor.process_spans, spans)
    
    # æ–­è¨€æ€§èƒ½æŒ‡æ ‡
    assert len(result) == 1000
    # benchmark ä¼šè‡ªåŠ¨æŠ¥å‘Šæ‰§è¡Œæ—¶é—´ç»Ÿè®¡
```

#### Go (testing + testify)

```go
// trace_processor_test.go - Go å•å…ƒæµ‹è¯•ç¤ºä¾‹
package processor_test

import (
    "context"
    "errors"
    "testing"
    "time"

    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/mock"
    "github.com/stretchr/testify/require"
    "github.com/stretchr/testify/suite"
    "go.opentelemetry.io/collector/pdata/ptrace"
)

// === è¢«æµ‹ä»£ç  ===

type SpanExporter interface {
    Export(ctx context.Context, spans ptrace.Traces) error
}

type TraceProcessor struct {
    exporter    SpanExporter
    sampleRate  float64
    maxSpans    int
}

func NewTraceProcessor(exporter SpanExporter, sampleRate float64, maxSpans int) *TraceProcessor {
    return &TraceProcessor{
        exporter:   exporter,
        sampleRate: sampleRate,
        maxSpans:   maxSpans,
    }
}

func (p *TraceProcessor) Process(ctx context.Context, traces ptrace.Traces) error {
    spanCount := traces.SpanCount()
    
    if spanCount > p.maxSpans {
        return errors.New("too many spans")
    }
    
    // é‡‡æ ·é€»è¾‘ (ç®€åŒ–)
    if p.sampleRate < 1.0 {
        traces = p.sample(traces)
    }
    
    // å¯¼å‡º
    return p.exporter.Export(ctx, traces)
}

func (p *TraceProcessor) sample(traces ptrace.Traces) ptrace.Traces {
    // ç®€åŒ–çš„é‡‡æ ·å®ç°
    return traces
}

// === Mock å¯¹è±¡ ===

type MockExporter struct {
    mock.Mock
}

func (m *MockExporter) Export(ctx context.Context, spans ptrace.Traces) error {
    args := m.Called(ctx, spans)
    return args.Error(0)
}

// === æµ‹è¯•å¥—ä»¶ (ä½¿ç”¨ testify/suite) ===

type TraceProcessorTestSuite struct {
    suite.Suite
    processor     *TraceProcessor
    mockExporter  *MockExporter
    ctx           context.Context
}

// SetupTest åœ¨æ¯ä¸ªæµ‹è¯•å‰è¿è¡Œ
func (suite *TraceProcessorTestSuite) SetupTest() {
    suite.mockExporter = new(MockExporter)
    suite.processor = NewTraceProcessor(suite.mockExporter, 1.0, 1000)
    suite.ctx = context.Background()
}

// TearDownTest åœ¨æ¯ä¸ªæµ‹è¯•åè¿è¡Œ
func (suite *TraceProcessorTestSuite) TearDownTest() {
    suite.mockExporter.AssertExpectations(suite.T())
}

// TestProcessValidTraces æµ‹è¯•å¤„ç†æœ‰æ•ˆè¿½è¸ª
func (suite *TraceProcessorTestSuite) TestProcessValidTraces() {
    // Arrange
    traces := ptrace.NewTraces()
    rs := traces.ResourceSpans().AppendEmpty()
    ss := rs.ScopeSpans().AppendEmpty()
    span := ss.Spans().AppendEmpty()
    span.SetName("test-span")
    span.SetTraceID([16]byte{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16})
    span.SetSpanID([8]byte{1, 2, 3, 4, 5, 6, 7, 8})

    suite.mockExporter.On("Export", suite.ctx, traces).Return(nil)

    // Act
    err := suite.processor.Process(suite.ctx, traces)

    // Assert
    assert.NoError(suite.T(), err)
}

// TestProcessTooManySpans æµ‹è¯•è¶…è¿‡æœ€å¤§ Span é™åˆ¶
func (suite *TraceProcessorTestSuite) TestProcessTooManySpans() {
    // Arrange
    traces := ptrace.NewTraces()
    rs := traces.ResourceSpans().AppendEmpty()
    ss := rs.ScopeSpans().AppendEmpty()
    
    // åˆ›å»º 1001 ä¸ª Span (è¶…è¿‡é™åˆ¶ 1000)
    for i := 0; i < 1001; i++ {
        span := ss.Spans().AppendEmpty()
        span.SetName("span-" + string(rune(i)))
    }

    // Act
    err := suite.processor.Process(suite.ctx, traces)

    // Assert
    assert.Error(suite.T(), err)
    assert.Contains(suite.T(), err.Error(), "too many spans")
}

// TestProcessExporterError æµ‹è¯•å¯¼å‡ºå™¨é”™è¯¯å¤„ç†
func (suite *TraceProcessorTestSuite) TestProcessExporterError() {
    // Arrange
    traces := ptrace.NewTraces()
    rs := traces.ResourceSpans().AppendEmpty()
    ss := rs.ScopeSpans().AppendEmpty()
    span := ss.Spans().AppendEmpty()
    span.SetName("test-span")

    expectedError := errors.New("export failed")
    suite.mockExporter.On("Export", suite.ctx, traces).Return(expectedError)

    // Act
    err := suite.processor.Process(suite.ctx, traces)

    // Assert
    assert.Error(suite.T(), err)
    assert.Equal(suite.T(), expectedError, err)
}

// è¿è¡Œæµ‹è¯•å¥—ä»¶
func TestTraceProcessorTestSuite(t *testing.T) {
    suite.Run(t, new(TraceProcessorTestSuite))
}

// === è¡¨é©±åŠ¨æµ‹è¯• ===

func TestSamplingDecision(t *testing.T) {
    testCases := []struct {
        name         string
        sampleRate   float64
        shouldSample bool
    }{
        {"Always sample", 1.0, true},
        {"Never sample", 0.0, false},
        {"50% sample", 0.5, true}, // ç®€åŒ–,å®é™…éœ€è¦ç»Ÿè®¡æµ‹è¯•
    }

    for _, tc := range testCases {
        t.Run(tc.name, func(t *testing.T) {
            mockExporter := new(MockExporter)
            processor := NewTraceProcessor(mockExporter, tc.sampleRate, 1000)

            traces := ptrace.NewTraces()
            rs := traces.ResourceSpans().AppendEmpty()
            ss := rs.ScopeSpans().AppendEmpty()
            span := ss.Spans().AppendEmpty()
            span.SetName("test-span")

            if tc.shouldSample {
                mockExporter.On("Export", mock.Anything, mock.Anything).Return(nil)
            }

            err := processor.Process(context.Background(), traces)
            assert.NoError(t, err)

            mockExporter.AssertExpectations(t)
        })
    }
}

// === åŸºå‡†æµ‹è¯• ===

func BenchmarkTraceProcessor(b *testing.B) {
    mockExporter := new(MockExporter)
    mockExporter.On("Export", mock.Anything, mock.Anything).Return(nil)
    
    processor := NewTraceProcessor(mockExporter, 1.0, 10000)
    ctx := context.Background()

    // åˆ›å»ºæµ‹è¯•æ•°æ®
    traces := ptrace.NewTraces()
    rs := traces.ResourceSpans().AppendEmpty()
    ss := rs.ScopeSpans().AppendEmpty()
    for i := 0; i < 100; i++ {
        span := ss.Spans().AppendEmpty()
        span.SetName("benchmark-span")
        span.SetTraceID([16]byte{byte(i)})
        span.SetSpanID([8]byte{byte(i)})
    }

    b.ResetTimer()
    for i := 0; i < b.N; i++ {
        err := processor.Process(ctx, traces)
        if err != nil {
            b.Fatal(err)
        }
    }
}

// === å¹¶å‘æµ‹è¯• ===

func TestConcurrentProcessing(t *testing.T) {
    mockExporter := new(MockExporter)
    mockExporter.On("Export", mock.Anything, mock.Anything).Return(nil)
    
    processor := NewTraceProcessor(mockExporter, 1.0, 10000)

    // å¹¶å‘æ‰§è¡Œ 100 ä¸ª goroutine
    const goroutines = 100
    errors := make(chan error, goroutines)

    for i := 0; i < goroutines; i++ {
        go func() {
            traces := ptrace.NewTraces()
            rs := traces.ResourceSpans().AppendEmpty()
            ss := rs.ScopeSpans().AppendEmpty()
            span := ss.Spans().AppendEmpty()
            span.SetName("concurrent-span")

            err := processor.Process(context.Background(), traces)
            errors <- err
        }()
    }

    // æ”¶é›†ç»“æœ
    for i := 0; i < goroutines; i++ {
        err := <-errors
        assert.NoError(t, err)
    }
}
```

### 2.2 Mock ä¸ Stub ç­–ç•¥

#### ä½¿ç”¨ Mock éš”ç¦»å¤–éƒ¨ä¾èµ–

```java
// Java - ä½¿ç”¨ Mockito è¿›è¡Œå•å…ƒæµ‹è¯•
package io.opentelemetry.exporter;

import io.opentelemetry.api.trace.Span;
import io.opentelemetry.sdk.trace.data.SpanData;
import io.opentelemetry.sdk.trace.export.SpanExporter;
import io.opentelemetry.sdk.common.CompletableResultCode;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;

import java.util.Arrays;
import java.util.Collection;
import java.util.concurrent.TimeUnit;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.*;

/**
 * BatchSpanProcessor å•å…ƒæµ‹è¯•
 */
@ExtendWith(MockitoExtension.class)
class BatchSpanProcessorTest {

    @Mock
    private SpanExporter mockExporter;

    private BatchSpanProcessor processor;

    @BeforeEach
    void setUp() {
        // é…ç½® Mock å¯¼å‡ºå™¨çš„é»˜è®¤è¡Œä¸º
        when(mockExporter.export(anyCollection()))
            .thenReturn(CompletableResultCode.ofSuccess());
        when(mockExporter.flush())
            .thenReturn(CompletableResultCode.ofSuccess());
        when(mockExporter.shutdown())
            .thenReturn(CompletableResultCode.ofSuccess());

        // åˆ›å»ºè¢«æµ‹å¯¹è±¡
        processor = BatchSpanProcessor.builder(mockExporter)
            .setMaxQueueSize(2048)
            .setMaxExportBatchSize(512)
            .setScheduleDelay(1, TimeUnit.SECONDS)
            .build();
    }

    @Test
    void testExportBatch() {
        // Arrange
        SpanData span1 = createTestSpan("span-1");
        SpanData span2 = createTestSpan("span-2");

        // Act
        processor.onEnd(span1);
        processor.onEnd(span2);
        processor.forceFlush().join(5, TimeUnit.SECONDS);

        // Assert
        verify(mockExporter, atLeastOnce()).export(argThat(spans -> 
            spans.size() >= 2
        ));
    }

    @Test
    void testExportFailureHandling() {
        // Arrange - Mock å¯¼å‡ºå¤±è´¥
        when(mockExporter.export(anyCollection()))
            .thenReturn(CompletableResultCode.ofFailure());

        SpanData span = createTestSpan("span-1");

        // Act
        processor.onEnd(span);
        CompletableResultCode result = processor.forceFlush();
        result.join(5, TimeUnit.SECONDS);

        // Assert
        assertFalse(result.isSuccess(), "Flush should fail when export fails");
        verify(mockExporter, atLeastOnce()).export(anyCollection());
    }

    @Test
    void testMaxQueueSize() {
        // Arrange
        int maxQueueSize = 10;
        processor = BatchSpanProcessor.builder(mockExporter)
            .setMaxQueueSize(maxQueueSize)
            .build();

        // Act - æ·»åŠ è¶…è¿‡é˜Ÿåˆ—å¤§å°çš„ Span
        for (int i = 0; i < maxQueueSize + 5; i++) {
            processor.onEnd(createTestSpan("span-" + i));
        }
        processor.forceFlush().join(5, TimeUnit.SECONDS);

        // Assert - éªŒè¯åªå¯¼å‡ºé˜Ÿåˆ—å¤§å°çš„ Span
        verify(mockExporter, atLeastOnce()).export(argThat(spans -> 
            spans.size() <= maxQueueSize
        ));
    }

    @Test
    void testShutdown() {
        // Act
        CompletableResultCode result = processor.shutdown();
        result.join(5, TimeUnit.SECONDS);

        // Assert
        assertTrue(result.isSuccess());
        verify(mockExporter, times(1)).shutdown();
    }

    // æµ‹è¯•è¾…åŠ©æ–¹æ³•
    private SpanData createTestSpan(String name) {
        return TestSpanData.builder()
            .setName(name)
            .setTraceId("00000000000000000000000000000001")
            .setSpanId("0000000000000001")
            .setStartEpochNanos(System.nanoTime())
            .setEndEpochNanos(System.nanoTime() + 1000000)
            .setKind(Span.Kind.INTERNAL)
            .setStatus(StatusData.ok())
            .build();
    }
}
```

### 2.3 æµ‹è¯•è¦†ç›–ç‡ä¸è´¨é‡é—¨ç¦

#### é…ç½®è¦†ç›–ç‡å·¥å…·

```xml
<!-- Maven (pom.xml) - JaCoCo è¦†ç›–ç‡é…ç½® -->
<build>
    <plugins>
        <plugin>
            <groupId>org.jacoco</groupId>
            <artifactId>jacoco-maven-plugin</artifactId>
            <version>0.8.11</version>
            <executions>
                <execution>
                    <goals>
                        <goal>prepare-agent</goal>
                    </goals>
                </execution>
                <execution>
                    <id>report</id>
                    <phase>test</phase>
                    <goals>
                        <goal>report</goal>
                    </goals>
                </execution>
                <execution>
                    <id>jacoco-check</id>
                    <goals>
                        <goal>check</goal>
                    </goals>
                    <configuration>
                        <rules>
                            <rule>
                                <element>PACKAGE</element>
                                <limits>
                                    <limit>
                                        <counter>LINE</counter>
                                        <value>COVEREDRATIO</value>
                                        <minimum>0.80</minimum>
                                    </limit>
                                    <limit>
                                        <counter>BRANCH</counter>
                                        <value>COVEREDRATIO</value>
                                        <minimum>0.75</minimum>
                                    </limit>
                                </limits>
                            </rule>
                        </rules>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```

```python
# pytest.ini - Python è¦†ç›–ç‡é…ç½®
[pytest]
addopts = 
    --cov=src
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80
    --cov-branch
    -v

[coverage:run]
source = src
omit = 
    */tests/*
    */test_*.py
    */__pycache__/*

[coverage:report]
precision = 2
show_missing = True
skip_covered = False
```

```yaml
# .github/workflows/test.yml - GitHub Actions è¦†ç›–ç‡é›†æˆ
name: Test and Coverage

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run tests with coverage
        run: |
          pytest --cov=src --cov-report=xml --cov-report=term
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: true
          flags: unittests
      
      - name: Check coverage threshold
        run: |
          coverage report --fail-under=80
```

---

## 3. é›†æˆæµ‹è¯•æ¡†æ¶

### 3.1 OTLP Collector é›†æˆæµ‹è¯•

#### ä½¿ç”¨ Testcontainers è¿›è¡Œé›†æˆæµ‹è¯•

```go
// collector_integration_test.go - Collector é›†æˆæµ‹è¯•
package integration_test

import (
    "context"
    "fmt"
    "testing"
    "time"

    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/require"
    "github.com/testcontainers/testcontainers-go"
    "github.com/testcontainers/testcontainers-go/wait"
    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
    "go.opentelemetry.io/otel/sdk/trace"
    sdktrace "go.opentelemetry.io/otel/sdk/trace"
)

// CollectorTestSuite é›†æˆæµ‹è¯•å¥—ä»¶
type CollectorTestSuite struct {
    collectorContainer testcontainers.Container
    jaegerContainer    testcontainers.Container
    collectorEndpoint  string
    jaegerUIEndpoint   string
}

// SetupSuite å¯åŠ¨æµ‹è¯•ç¯å¢ƒ
func (suite *CollectorTestSuite) SetupSuite(t *testing.T) {
    ctx := context.Background()

    // å¯åŠ¨ Jaeger (ä½œä¸ºåç«¯)
    jaegerReq := testcontainers.ContainerRequest{
        Image:        "jaegertracing/all-in-one:1.52",
        ExposedPorts: []string{"16686/tcp", "4317/tcp", "4318/tcp"},
        Env: map[string]string{
            "COLLECTOR_OTLP_ENABLED": "true",
        },
        WaitingFor: wait.ForHTTP("/").WithPort("16686/tcp"),
    }

    jaegerContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
        ContainerRequest: jaegerReq,
        Started:          true,
    })
    require.NoError(t, err)
    suite.jaegerContainer = jaegerContainer

    jaegerHost, err := jaegerContainer.Host(ctx)
    require.NoError(t, err)
    jaegerPort, err := jaegerContainer.MappedPort(ctx, "16686")
    require.NoError(t, err)
    suite.jaegerUIEndpoint = fmt.Sprintf("http://%s:%s", jaegerHost, jaegerPort.Port())

    jaegerOTLPPort, err := jaegerContainer.MappedPort(ctx, "4317")
    require.NoError(t, err)
    jaegerOTLPEndpoint := fmt.Sprintf("%s:%s", jaegerHost, jaegerOTLPPort.Port())

    // å¯åŠ¨ OTLP Collector
    collectorConfig := fmt.Sprintf(`
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 1s
    send_batch_size: 100

exporters:
  otlp:
    endpoint: %s
    tls:
      insecure: true
  logging:
    loglevel: debug

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp, logging]
`, jaegerOTLPEndpoint)

    collectorReq := testcontainers.ContainerRequest{
        Image:        "otel/opentelemetry-collector-contrib:0.108.0",
        ExposedPorts: []string{"4317/tcp", "4318/tcp", "8888/tcp"},
        Cmd:          []string{"--config=/etc/otel-collector-config.yaml"},
        Files: []testcontainers.ContainerFile{
            {
                HostFilePath:      "/tmp/otel-collector-config.yaml",
                ContainerFilePath: "/etc/otel-collector-config.yaml",
                FileMode:          0644,
            },
        },
        WaitingFor: wait.ForHTTP("/").WithPort("8888/tcp"),
    }

    // å†™å…¥é…ç½®æ–‡ä»¶
    require.NoError(t, os.WriteFile("/tmp/otel-collector-config.yaml", []byte(collectorConfig), 0644))

    collectorContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
        ContainerRequest: collectorReq,
        Started:          true,
    })
    require.NoError(t, err)
    suite.collectorContainer = collectorContainer

    collectorHost, err := collectorContainer.Host(ctx)
    require.NoError(t, err)
    collectorPort, err := collectorContainer.MappedPort(ctx, "4317")
    require.NoError(t, err)
    suite.collectorEndpoint = fmt.Sprintf("%s:%s", collectorHost, collectorPort.Port())

    // ç­‰å¾…æœåŠ¡å°±ç»ª
    time.Sleep(5 * time.Second)
}

// TearDownSuite æ¸…ç†æµ‹è¯•ç¯å¢ƒ
func (suite *CollectorTestSuite) TearDownSuite(t *testing.T) {
    ctx := context.Background()
    
    if suite.collectorContainer != nil {
        require.NoError(t, suite.collectorContainer.Terminate(ctx))
    }
    if suite.jaegerContainer != nil {
        require.NoError(t, suite.jaegerContainer.Terminate(ctx))
    }
}

// TestEndToEndTracing ç«¯åˆ°ç«¯è¿½è¸ªæµ‹è¯•
func TestEndToEndTracing(t *testing.T) {
    suite := &CollectorTestSuite{}
    suite.SetupSuite(t)
    defer suite.TearDownSuite(t)

    ctx := context.Background()

    // åˆ›å»º OTLP å¯¼å‡ºå™¨
    exporter, err := otlptracegrpc.New(
        ctx,
        otlptracegrpc.WithEndpoint(suite.collectorEndpoint),
        otlptracegrpc.WithInsecure(),
    )
    require.NoError(t, err)
    defer exporter.Shutdown(ctx)

    // åˆ›å»ºè¿½è¸ªæä¾›å™¨
    tp := sdktrace.NewTracerProvider(
        sdktrace.WithBatcher(exporter),
        sdktrace.WithSampler(sdktrace.AlwaysSample()),
    )
    defer tp.Shutdown(ctx)
    otel.SetTracerProvider(tp)

    // ç”Ÿæˆæµ‹è¯•è¿½è¸ª
    tracer := otel.Tracer("integration-test")
    _, span := tracer.Start(ctx, "test-operation")
    span.SetAttributes(
        attribute.String("test.type", "integration"),
        attribute.Int("test.number", 42),
    )
    time.Sleep(100 * time.Millisecond)
    span.End()

    // å¼ºåˆ¶åˆ·æ–°
    require.NoError(t, tp.ForceFlush(ctx))

    // ç­‰å¾…æ•°æ®ä¼ è¾“åˆ° Jaeger
    time.Sleep(5 * time.Second)

    // éªŒè¯è¿½è¸ªæ•°æ®å·²åˆ°è¾¾ Jaeger
    traceID := span.SpanContext().TraceID().String()
    jaegerAPIURL := fmt.Sprintf("%s/api/traces/%s", suite.jaegerUIEndpoint, traceID)

    var resp *http.Response
    for i := 0; i < 10; i++ {
        resp, err = http.Get(jaegerAPIURL)
        if err == nil && resp.StatusCode == 200 {
            break
        }
        time.Sleep(2 * time.Second)
    }

    require.NoError(t, err)
    require.Equal(t, 200, resp.StatusCode)

    // è§£æå“åº”
    var jaegerTrace struct {
        Data []struct {
            TraceID string `json:"traceID"`
            Spans   []struct {
                OperationName string `json:"operationName"`
            } `json:"spans"`
        } `json:"data"`
    }

    require.NoError(t, json.NewDecoder(resp.Body).Decode(&jaegerTrace))
    require.Len(t, jaegerTrace.Data, 1)
    assert.Equal(t, "test-operation", jaegerTrace.Data[0].Spans[0].OperationName)
}
```

### 3.2 å¤šç»„ä»¶è”è°ƒæµ‹è¯•

```python
# test_multi_component.py - å¤šç»„ä»¶è”è°ƒæµ‹è¯•
import pytest
import requests
import time
from typing import Dict, Any
from testcontainers.compose import DockerCompose

class TestMultiComponentIntegration:
    """å¤šç»„ä»¶è”è°ƒé›†æˆæµ‹è¯•"""
    
    @pytest.fixture(scope="class")
    def test_environment(self):
        """å¯åŠ¨å®Œæ•´çš„æµ‹è¯•ç¯å¢ƒ"""
        with DockerCompose(
            filepath=".",
            compose_file_name="docker-compose.test.yml",
            pull=True
        ) as compose:
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            time.sleep(15)
            
            # è·å–æœåŠ¡ç«¯ç‚¹
            endpoints = {
                "collector": "http://localhost:4318",
                "jaeger": "http://localhost:16686",
                "prometheus": "http://localhost:9090",
            }
            
            # å¥åº·æ£€æŸ¥
            for name, url in endpoints.items():
                assert self._wait_for_service(url), f"{name} failed to start"
            
            yield endpoints
    
    def _wait_for_service(self, url: str, timeout: int = 60) -> bool:
        """ç­‰å¾…æœåŠ¡å°±ç»ª"""
        start = time.time()
        while time.time() - start < timeout:
            try:
                response = requests.get(url, timeout=5)
                if response.status_code in [200, 404]:  # 404 ä¹Ÿè¡¨ç¤ºæœåŠ¡è¿è¡Œ
                    return True
            except requests.RequestException:
                pass
            time.sleep(2)
        return False
    
    def test_traces_pipeline(self, test_environment: Dict[str, str]):
        """æµ‹è¯•è¿½è¸ªæ•°æ®ç®¡é“"""
        # 1. ç”Ÿæˆæµ‹è¯•è¿½è¸ª
        trace_data = {
            "resourceSpans": [{
                "resource": {
                    "attributes": [{
                        "key": "service.name",
                        "value": {"stringValue": "test-service"}
                    }]
                },
                "scopeSpans": [{
                    "spans": [{
                        "traceId": "0102030405060708090a0b0c0d0e0f10",
                        "spanId": "0102030405060708",
                        "name": "test-span",
                        "kind": 1,
                        "startTimeUnixNano": str(int(time.time() * 1e9)),
                        "endTimeUnixNano": str(int((time.time() + 0.1) * 1e9)),
                        "attributes": [{
                            "key": "test.attribute",
                            "value": {"stringValue": "test-value"}
                        }]
                    }]
                }]
            }]
        }
        
        # 2. å‘é€åˆ° Collector
        response = requests.post(
            f"{test_environment['collector']}/v1/traces",
            json=trace_data,
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        assert response.status_code == 200, f"Failed to send traces: {response.text}"
        
        # 3. ç­‰å¾…æ•°æ®ä¼ è¾“
        time.sleep(5)
        
        # 4. éªŒè¯æ•°æ®åˆ°è¾¾ Jaeger
        trace_id = "0102030405060708090a0b0c0d0e0f10"
        jaeger_url = f"{test_environment['jaeger']}/api/traces/{trace_id}"
        
        for _ in range(10):
            response = requests.get(jaeger_url, timeout=5)
            if response.status_code == 200:
                trace_result = response.json()
                if trace_result.get("data"):
                    break
            time.sleep(2)
        else:
            pytest.fail("Trace not found in Jaeger after 20 seconds")
        
        # 5. éªŒè¯è¿½è¸ªå†…å®¹
        assert len(trace_result["data"]) == 1
        assert trace_result["data"][0]["traceID"] == trace_id
        spans = trace_result["data"][0]["spans"]
        assert len(spans) == 1
        assert spans[0]["operationName"] == "test-span"
    
    def test_metrics_pipeline(self, test_environment: Dict[str, str]):
        """æµ‹è¯•æŒ‡æ ‡æ•°æ®ç®¡é“"""
        # 1. ç”Ÿæˆæµ‹è¯•æŒ‡æ ‡
        metric_data = {
            "resourceMetrics": [{
                "resource": {
                    "attributes": [{
                        "key": "service.name",
                        "value": {"stringValue": "test-service"}
                    }]
                },
                "scopeMetrics": [{
                    "metrics": [{
                        "name": "test_counter",
                        "unit": "1",
                        "sum": {
                            "dataPoints": [{
                                "asInt": "42",
                                "timeUnixNano": str(int(time.time() * 1e9))
                            }],
                            "aggregationTemporality": 2,
                            "isMonotonic": True
                        }
                    }]
                }]
            }]
        }
        
        # 2. å‘é€åˆ° Collector
        response = requests.post(
            f"{test_environment['collector']}/v1/metrics",
            json=metric_data,
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        assert response.status_code == 200
        
        # 3. ç­‰å¾…æ•°æ®ä¼ è¾“
        time.sleep(10)
        
        # 4. æŸ¥è¯¢ Prometheus
        prom_url = f"{test_environment['prometheus']}/api/v1/query"
        params = {"query": "test_counter"}
        
        for _ in range(10):
            response = requests.get(prom_url, params=params, timeout=5)
            if response.status_code == 200:
                result = response.json()
                if result["data"]["result"]:
                    break
            time.sleep(2)
        else:
            pytest.fail("Metric not found in Prometheus after 20 seconds")
        
        # 5. éªŒè¯æŒ‡æ ‡å€¼
        metric_result = result["data"]["result"][0]
        assert metric_result["metric"]["__name__"] == "test_counter"
        assert int(float(metric_result["value"][1])) == 42
```

---

## 4. ç«¯åˆ°ç«¯æµ‹è¯• (E2E)

### 4.1 åˆ†å¸ƒå¼è¿½è¸ªåœºæ™¯æµ‹è¯•

```typescript
// e2e-tracing.spec.ts - Playwright E2E æµ‹è¯•
import { test, expect } from '@playwright/test';
import { trace, context, SpanStatusCode } from '@opentelemetry/api';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { Resource } from '@opentelemetry/resources';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';
import { BasicTracerProvider, BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';
import axios from 'axios';

// æµ‹è¯•é…ç½®
const OTLP_ENDPOINT = process.env.OTLP_ENDPOINT || 'http://localhost:4318/v1/traces';
const JAEGER_UI = process.env.JAEGER_UI || 'http://localhost:16686';
const TEST_APP_URL = process.env.TEST_APP_URL || 'http://localhost:8080';

test.describe('E2E Distributed Tracing', () => {
  let tracerProvider: BasicTracerProvider;

  test.beforeAll(async () => {
    // åˆå§‹åŒ–è¿½è¸ª
    const exporter = new OTLPTraceExporter({
      url: OTLP_ENDPOINT,
    });

    tracerProvider = new BasicTracerProvider({
      resource: new Resource({
        [SemanticResourceAttributes.SERVICE_NAME]: 'e2e-test-client',
      }),
    });

    tracerProvider.addSpanProcessor(new BatchSpanProcessor(exporter));
    trace.setGlobalTracerProvider(tracerProvider);
  });

  test.afterAll(async () => {
    await tracerProvider.shutdown();
  });

  test('should propagate trace context across services', async () => {
    const tracer = trace.getTracer('e2e-test');
    const parentSpan = tracer.startSpan('e2e-test-request');
    const ctx = trace.setSpan(context.active(), parentSpan);

    let traceId: string;
    let spanId: string;

    await context.with(ctx, async () => {
      traceId = parentSpan.spanContext().traceId;
      spanId = parentSpan.spanContext().spanId;

      // è°ƒç”¨æµ‹è¯•åº”ç”¨ (å¸¦è¿½è¸ª header)
      const response = await axios.get(`${TEST_APP_URL}/api/test`, {
        headers: {
          'traceparent': `00-${traceId}-${spanId}-01`,
        },
      });

      expect(response.status).toBe(200);
      parentSpan.setStatus({ code: SpanStatusCode.OK });
      parentSpan.end();
    });

    // å¼ºåˆ¶åˆ·æ–°è¿½è¸ªæ•°æ®
    await tracerProvider.forceFlush();

    // ç­‰å¾…æ•°æ®ä¼ è¾“
    await new Promise(resolve => setTimeout(resolve, 5000));

    // éªŒè¯è¿½è¸ªé“¾è·¯
    const traceData = await fetchTraceFromJaeger(traceId);
    
    expect(traceData).toBeDefined();
    expect(traceData.traceID).toBe(traceId);
    
    // éªŒè¯æœåŠ¡æ‹“æ‰‘
    const spans = traceData.spans;
    expect(spans.length).toBeGreaterThan(1);
    
    // éªŒè¯æ ¹ Span
    const rootSpan = spans.find((s: any) => !s.references || s.references.length === 0);
    expect(rootSpan).toBeDefined();
    expect(rootSpan.operationName).toBe('e2e-test-request');
    
    // éªŒè¯å­ Span
    const childSpans = spans.filter((s: any) => 
      s.references && s.references.some((r: any) => r.spanID === spanId)
    );
    expect(childSpans.length).toBeGreaterThan(0);
  });

  test('should capture errors in distributed traces', async () => {
    const tracer = trace.getTracer('e2e-test');
    const parentSpan = tracer.startSpan('e2e-test-error');
    const ctx = trace.setSpan(context.active(), parentSpan);

    let traceId: string;

    await context.with(ctx, async () => {
      traceId = parentSpan.spanContext().traceId;

      try {
        // è°ƒç”¨ä¼šå¤±è´¥çš„ç«¯ç‚¹
        await axios.get(`${TEST_APP_URL}/api/error`);
      } catch (error) {
        parentSpan.recordException(error as Error);
        parentSpan.setStatus({
          code: SpanStatusCode.ERROR,
          message: (error as Error).message,
        });
      } finally {
        parentSpan.end();
      }
    });

    await tracerProvider.forceFlush();
    await new Promise(resolve => setTimeout(resolve, 5000));

    // éªŒè¯é”™è¯¯è¿½è¸ª
    const traceData = await fetchTraceFromJaeger(traceId);
    
    const errorSpans = traceData.spans.filter((s: any) => 
      s.tags.some((t: any) => t.key === 'error' && t.value === true)
    );
    
    expect(errorSpans.length).toBeGreaterThan(0);
  });
});

// è¾…åŠ©å‡½æ•°: ä» Jaeger è·å–è¿½è¸ª
async function fetchTraceFromJaeger(traceId: string, maxRetries = 10): Promise<any> {
  for (let i = 0; i < maxRetries; i++) {
    try {
      const response = await axios.get(`${JAEGER_UI}/api/traces/${traceId}`);
      if (response.data && response.data.data && response.data.data.length > 0) {
        return response.data.data[0];
      }
    } catch (error) {
      // å¿½ç•¥é”™è¯¯,ç»§ç»­é‡è¯•
    }
    await new Promise(resolve => setTimeout(resolve, 2000));
  }
  throw new Error(`Trace ${traceId} not found after ${maxRetries} retries`);
}
```

### 4.2 è·¨æœåŠ¡é“¾è·¯éªŒè¯

```python
# test_cross_service.py - è·¨æœåŠ¡é“¾è·¯æµ‹è¯•
import pytest
import requests
import time
from typing import List, Dict
from dataclasses import dataclass
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.resources import Resource, SERVICE_NAME

@dataclass
class ServiceEndpoint:
    """æœåŠ¡ç«¯ç‚¹é…ç½®"""
    name: str
    url: str
    expected_operation: str

class TestCrossServiceTracing:
    """è·¨æœåŠ¡é“¾è·¯æµ‹è¯•"""
    
    @pytest.fixture(scope="class")
    def tracer_provider(self):
        """åˆå§‹åŒ–è¿½è¸ªæä¾›å™¨"""
        resource = Resource(attributes={
            SERVICE_NAME: "test-orchestrator"
        })
        
        provider = TracerProvider(resource=resource)
        otlp_exporter = OTLPSpanExporter(
            endpoint="http://localhost:4317",
            insecure=True
        )
        
        provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
        trace.set_tracer_provider(provider)
        
        yield provider
        
        provider.shutdown()
    
    @pytest.fixture
    def service_chain(self) -> List[ServiceEndpoint]:
        """å®šä¹‰æœåŠ¡è°ƒç”¨é“¾"""
        return [
            ServiceEndpoint("frontend", "http://localhost:8080/api/users", "GET /api/users"),
            ServiceEndpoint("backend", "http://localhost:8081/api/users", "fetch_users"),
            ServiceEndpoint("database", "http://localhost:5432", "SELECT users"),
        ]
    
    def test_complete_service_chain(
        self, 
        tracer_provider: TracerProvider,
        service_chain: List[ServiceEndpoint]
    ):
        """æµ‹è¯•å®Œæ•´çš„æœåŠ¡è°ƒç”¨é“¾"""
        tracer = trace.get_tracer(__name__)
        
        with tracer.start_as_current_span("test-service-chain") as span:
            trace_id = format(span.get_span_context().trace_id, '032x')
            span_id = format(span.get_span_context().span_id, '016x')
            
            # æ„é€  W3C Trace Context header
            traceparent = f"00-{trace_id}-{span_id}-01"
            
            # è°ƒç”¨ç¬¬ä¸€ä¸ªæœåŠ¡ (frontend)
            response = requests.get(
                service_chain[0].url,
                headers={"traceparent": traceparent},
                timeout=10
            )
            
            assert response.status_code == 200, f"Frontend request failed: {response.text}"
            
            # æ·»åŠ è‡ªå®šä¹‰å±æ€§
            span.set_attribute("test.chain_length", len(service_chain))
            span.set_attribute("test.frontend_response_time", response.elapsed.total_seconds())
        
        # åˆ·æ–°è¿½è¸ªæ•°æ®
        tracer_provider.force_flush()
        
        # ç­‰å¾…æ•°æ®ä¼ è¾“
        time.sleep(5)
        
        # éªŒè¯å®Œæ•´é“¾è·¯
        trace_data = self._fetch_trace_from_backend(trace_id)
        
        assert trace_data is not None, f"Trace {trace_id} not found"
        assert len(trace_data["spans"]) >= len(service_chain), \
            f"Expected at least {len(service_chain)} spans, got {len(trace_data['spans'])}"
        
        # éªŒè¯æ¯ä¸ªæœåŠ¡çš„ Span
        span_operations = {span["operationName"] for span in trace_data["spans"]}
        
        for service in service_chain:
            assert any(service.expected_operation in op for op in span_operations), \
                f"Missing span for service {service.name} (operation: {service.expected_operation})"
        
        # éªŒè¯é“¾è·¯å®Œæ•´æ€§ (çˆ¶å­å…³ç³»)
        self._verify_span_relationships(trace_data["spans"])
    
    def test_service_failure_propagation(
        self,
        tracer_provider: TracerProvider,
        service_chain: List[ServiceEndpoint]
    ):
        """æµ‹è¯•æœåŠ¡æ•…éšœä¼ æ’­"""
        tracer = trace.get_tracer(__name__)
        
        with tracer.start_as_current_span("test-failure-propagation") as span:
            trace_id = format(span.get_span_context().trace_id, '032x')
            span_id = format(span.get_span_context().span_id, '016x')
            traceparent = f"00-{trace_id}-{span_id}-01"
            
            # è°ƒç”¨ä¼šå¤±è´¥çš„ç«¯ç‚¹
            try:
                response = requests.get(
                    f"{service_chain[0].url}/fail",
                    headers={"traceparent": traceparent},
                    timeout=10
                )
                # é¢„æœŸä¼šå¤±è´¥
                assert response.status_code >= 400
            except requests.RequestException as e:
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
        
        tracer_provider.force_flush()
        time.sleep(5)
        
        # éªŒè¯é”™è¯¯æ ‡è®°
        trace_data = self._fetch_trace_from_backend(trace_id)
        error_spans = [
            s for s in trace_data["spans"]
            if any(tag["key"] == "error" and tag["value"] for tag in s.get("tags", []))
        ]
        
        assert len(error_spans) > 0, "No error spans found in failed request"
    
    def _fetch_trace_from_backend(self, trace_id: str, max_retries: int = 10) -> Dict:
        """ä»åç«¯è·å–è¿½è¸ªæ•°æ®"""
        jaeger_url = f"http://localhost:16686/api/traces/{trace_id}"
        
        for attempt in range(max_retries):
            try:
                response = requests.get(jaeger_url, timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    if data.get("data") and len(data["data"]) > 0:
                        return data["data"][0]
            except requests.RequestException:
                pass
            
            time.sleep(2)
        
        return None
    
    def _verify_span_relationships(self, spans: List[Dict]) -> None:
        """éªŒè¯ Span çˆ¶å­å…³ç³»å®Œæ•´æ€§"""
        span_map = {span["spanID"]: span for span in spans}
        
        for span in spans:
            # æ£€æŸ¥çˆ¶ Span å¼•ç”¨
            if span.get("references"):
                for ref in span["references"]:
                    if ref["refType"] == "CHILD_OF":
                        parent_id = ref["spanID"]
                        assert parent_id in span_map, \
                            f"Parent span {parent_id} not found for span {span['spanID']}"
```

### 4.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

#### K6 æ€§èƒ½æµ‹è¯•è„šæœ¬

```javascript
// performance-test.js - K6 æ€§èƒ½åŸºå‡†æµ‹è¯•
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';
import { randomItem } from 'https://jslib.k6.io/k6-utils/1.2.0/index.js';

// è‡ªå®šä¹‰æŒ‡æ ‡
const errorRate = new Rate('errors');
const traceDuration = new Trend('trace_duration');

// æµ‹è¯•é…ç½®
export const options = {
  stages: [
    { duration: '2m', target: 100 },  // çº¿æ€§å¢åŠ åˆ° 100 VU
    { duration: '5m', target: 100 },  // ä¿æŒ 100 VU
    { duration: '2m', target: 200 },  // å¢åŠ åˆ° 200 VU
    { duration: '5m', target: 200 },  // ä¿æŒ 200 VU
    { duration: '2m', target: 0 },    // é€æ¸å‡å°‘åˆ° 0
  ],
  thresholds: {
    'http_req_duration': ['p(95)<500', 'p(99)<1000'],  // 95% < 500ms, 99% < 1s
    'errors': ['rate<0.1'],  // é”™è¯¯ç‡ < 10%
    'http_req_failed': ['rate<0.01'],  // å¤±è´¥ç‡ < 1%
  },
};

const OTLP_ENDPOINT = __ENV.OTLP_ENDPOINT || 'http://localhost:4318/v1/traces';

// ç”Ÿæˆæµ‹è¯•è¿½è¸ªæ•°æ®
function generateTraceData() {
  const traceId = Array.from({ length: 32 }, () => 
    Math.floor(Math.random() * 16).toString(16)
  ).join('');
  
  const spanId = Array.from({ length: 16 }, () => 
    Math.floor(Math.random() * 16).toString(16)
  ).join('');
  
  const startTime = Date.now() * 1000000;  // çº³ç§’
  const duration = Math.random() * 100 + 50;  // 50-150ms
  const endTime = startTime + (duration * 1000000);
  
  return {
    resourceSpans: [{
      resource: {
        attributes: [
          { key: 'service.name', value: { stringValue: `perf-test-${__VU}` } },
          { key: 'service.version', value: { stringValue: '1.0.0' } },
        ],
      },
      scopeSpans: [{
        scope: {
          name: 'k6-performance-test',
          version: '1.0.0',
        },
        spans: [{
          traceId,
          spanId,
          name: randomItem(['GET /api/users', 'POST /api/orders', 'GET /api/products']),
          kind: 1,
          startTimeUnixNano: startTime.toString(),
          endTimeUnixNano: endTime.toString(),
          attributes: [
            { key: 'http.method', value: { stringValue: randomItem(['GET', 'POST', 'PUT']) } },
            { key: 'http.status_code', value: { intValue: '200' } },
            { key: 'http.url', value: { stringValue: 'https://api.example.com/users' } },
          ],
        }],
      }],
    }],
  };
}

// ä¸»æµ‹è¯•åœºæ™¯
export default function () {
  const traceData = generateTraceData();
  
  const payload = JSON.stringify(traceData);
  const params = {
    headers: {
      'Content-Type': 'application/json',
    },
  };
  
  // å‘é€è¿½è¸ªæ•°æ®
  const startTime = Date.now();
  const response = http.post(OTLP_ENDPOINT, payload, params);
  const duration = Date.now() - startTime;
  
  // è®°å½•æŒ‡æ ‡
  traceDuration.add(duration);
  
  // æ£€æŸ¥å“åº”
  const success = check(response, {
    'status is 200': (r) => r.status === 200,
    'response time < 500ms': (r) => r.timings.duration < 500,
  });
  
  errorRate.add(!success);
  
  // æ¨¡æ‹Ÿå®é™…è´Ÿè½½ (éšæœºç¡çœ  100-500ms)
  sleep(Math.random() * 0.4 + 0.1);
}

// æµ‹è¯•ç”Ÿå‘½å‘¨æœŸé’©å­
export function setup() {
  console.log('Starting performance test...');
  // é¢„çƒ­
  for (let i = 0; i < 10; i++) {
    http.post(OTLP_ENDPOINT, JSON.stringify(generateTraceData()), {
      headers: { 'Content-Type': 'application/json' },
    });
  }
  return { startTime: Date.now() };
}

export function teardown(data) {
  const totalDuration = (Date.now() - data.startTime) / 1000;
  console.log(`Test completed in ${totalDuration.toFixed(2)} seconds`);
}
```

#### JMeter æ€§èƒ½æµ‹è¯•è®¡åˆ’

```xml
<!-- otlp-performance-test.jmx - JMeter æµ‹è¯•è®¡åˆ’ (ç®€åŒ–ç‰ˆ) -->
<?xml version="1.0" encoding="UTF-8"?>
<jmeterTestPlan version="1.2" properties="5.0" jmeter="5.6.2">
  <hashTree>
    <TestPlan guiclass="TestPlanGui" testclass="TestPlan" testname="OTLP Performance Test">
      <elementProp name="TestPlan.user_defined_variables" elementType="Arguments">
        <collectionProp name="Arguments.arguments">
          <elementProp name="OTLP_HOST" elementType="Argument">
            <stringProp name="Argument.name">OTLP_HOST</stringProp>
            <stringProp name="Argument.value">localhost</stringProp>
          </elementProp>
          <elementProp name="OTLP_PORT" elementType="Argument">
            <stringProp name="Argument.name">OTLP_PORT</stringProp>
            <stringProp name="Argument.value">4318</stringProp>
          </elementProp>
        </collectionProp>
      </elementProp>
    </TestPlan>
    
    <hashTree>
      <ThreadGroup guiclass="ThreadGroupGui" testclass="ThreadGroup" testname="Trace Senders">
        <stringProp name="ThreadGroup.num_threads">100</stringProp>
        <stringProp name="ThreadGroup.ramp_time">60</stringProp>
        <stringProp name="ThreadGroup.duration">300</stringProp>
        <boolProp name="ThreadGroup.scheduler">true</boolProp>
        <stringProp name="ThreadGroup.on_sample_error">continue</stringProp>
      </ThreadGroup>
      
      <hashTree>
        <HTTPSamplerProxy guiclass="HttpTestSampleGui" testclass="HTTPSamplerProxy" testname="Send Trace">
          <stringProp name="HTTPSampler.domain">${OTLP_HOST}</stringProp>
          <stringProp name="HTTPSampler.port">${OTLP_PORT}</stringProp>
          <stringProp name="HTTPSampler.protocol">http</stringProp>
          <stringProp name="HTTPSampler.path">/v1/traces</stringProp>
          <stringProp name="HTTPSampler.method">POST</stringProp>
          <boolProp name="HTTPSampler.use_keepalive">true</boolProp>
          <elementProp name="HTTPsampler.Arguments" elementType="Arguments">
            <collectionProp name="Arguments.arguments">
              <elementProp name="" elementType="HTTPArgument">
                <boolProp name="HTTPArgument.always_encode">false</boolProp>
                <stringProp name="Argument.value">${__BeanShell(new groovy.json.JsonBuilder([
                  resourceSpans: [[
                    resource: [
                      attributes: [
                        [key: 'service.name', value: [stringValue: 'jmeter-test']]
                      ]
                    ],
                    scopeSpans: [[
                      spans: [[
                        traceId: UUID.randomUUID().toString().replace('-', ''),
                        spanId: UUID.randomUUID().toString().replace('-', '').substring(0, 16),
                        name: 'jmeter-test-span',
                        kind: 1,
                        startTimeUnixNano: (System.currentTimeMillis() * 1000000).toString(),
                        endTimeUnixNano: ((System.currentTimeMillis() + 100) * 1000000).toString()
                      ]]
                    ]]
                  ]]
                ]).toString())}</stringProp>
                <stringProp name="Argument.metadata">=</stringProp>
              </elementProp>
            </collectionProp>
          </elementProp>
        </HTTPSamplerProxy>
        
        <hashTree>
          <HeaderManager guiclass="HeaderPanel" testclass="HeaderManager" testname="HTTP Headers">
            <collectionProp name="HeaderManager.headers">
              <elementProp name="" elementType="Header">
                <stringProp name="Header.name">Content-Type</stringProp>
                <stringProp name="Header.value">application/json</stringProp>
              </elementProp>
            </collectionProp>
          </HeaderManager>
          
          <ResponseAssertion guiclass="AssertionGui" testclass="ResponseAssertion" testname="Status Code Assertion">
            <collectionProp name="Asserion.test_strings">
              <stringProp name="49586">200</stringProp>
            </collectionProp>
            <stringProp name="Assertion.test_field">Assertion.response_code</stringProp>
            <intProp name="Assertion.test_type">8</intProp>
          </ResponseAssertion>
        </hashTree>
      </hashTree>
    </hashTree>
  </hashTree>
</jmeterTestPlan>
```

### 4.4 æ··æ²Œå·¥ç¨‹ (Chaos Engineering)

```yaml
# chaos-experiment.yaml - Chaos Mesh æ··æ²Œå®éªŒ
apiVersion: chaos-mesh.org/v1alpha1
kind: Workflow
metadata:
  name: otlp-chaos-experiment
  namespace: otlp-test
spec:
  entry: otlp-resilience-test
  templates:
    - name: otlp-resilience-test
      templateType: Serial
      deadline: 30m
      children:
        - baseline-test
        - network-delay-test
        - pod-failure-test
        - stress-test
        - verification-test

    # åŸºçº¿æµ‹è¯•
    - name: baseline-test
      templateType: Task
      deadline: 5m
      task:
        container:
          name: baseline-test
          image: k6io/k6:latest
          command: ['k6', 'run', '--vus', '50', '--duration', '2m', '/scripts/baseline.js']
          volumeMounts:
            - name: test-scripts
              mountPath: /scripts

    # ç½‘ç»œå»¶è¿Ÿæµ‹è¯•
    - name: network-delay-test
      templateType: Parallel
      deadline: 10m
      children:
        - inject-network-delay
        - run-test-under-delay

    - name: inject-network-delay
      templateType: NetworkChaos
      deadline: 8m
      networkChaos:
        action: delay
        mode: all
        selector:
          namespaces:
            - otlp-test
          labelSelectors:
            'app': 'otel-collector'
        delay:
          latency: '100ms'
          correlation: '50'
          jitter: '50ms'
        duration: 5m

    - name: run-test-under-delay
      templateType: Task
      deadline: 8m
      task:
        container:
          name: test-with-delay
          image: k6io/k6:latest
          command: ['k6', 'run', '--vus', '50', '--duration', '5m', '/scripts/latency-test.js']
          volumeMounts:
            - name: test-scripts
              mountPath: /scripts

    # Pod æ•…éšœæµ‹è¯•
    - name: pod-failure-test
      templateType: Parallel
      deadline: 10m
      children:
        - inject-pod-failure
        - run-test-under-failure

    - name: inject-pod-failure
      templateType: PodChaos
      deadline: 8m
      podChaos:
        action: pod-kill
        mode: one
        selector:
          namespaces:
            - otlp-test
          labelSelectors:
            'app': 'otel-collector'
        duration: 5m
        gracePeriod: 0

    - name: run-test-under-failure
      templateType: Task
      deadline: 8m
      task:
        container:
          name: test-with-failure
          image: k6io/k6:latest
          command: ['k6', 'run', '--vus', '50', '--duration', '5m', '/scripts/resilience-test.js']
          volumeMounts:
            - name: test-scripts
              mountPath: /scripts

    # å‹åŠ›æµ‹è¯•
    - name: stress-test
      templateType: Parallel
      deadline: 10m
      children:
        - inject-stress
        - run-test-under-stress

    - name: inject-stress
      templateType: StressChaos
      deadline: 8m
      stressChaos:
        mode: one
        selector:
          namespaces:
            - otlp-test
          labelSelectors:
            'app': 'otel-collector'
        stressors:
          cpu:
            workers: 4
            load: 80
          memory:
            workers: 2
            size: '512MB'
        duration: 5m

    - name: run-test-under-stress
      templateType: Task
      deadline: 8m
      task:
        container:
          name: test-with-stress
          image: k6io/k6:latest
          command: ['k6', 'run', '--vus', '50', '--duration', '5m', '/scripts/stress-test.js']
          volumeMounts:
            - name: test-scripts
              mountPath: /scripts

    # éªŒè¯æµ‹è¯•
    - name: verification-test
      templateType: Task
      deadline: 5m
      task:
        container:
          name: verification
          image: curlimages/curl:latest
          command: ['sh', '-c', '/scripts/verify-resilience.sh']
          volumeMounts:
            - name: test-scripts
              mountPath: /scripts

  volumes:
    - name: test-scripts
      configMap:
        name: chaos-test-scripts
```

```bash
#!/bin/bash
# verify-resilience.sh - æ··æ²Œå®éªŒéªŒè¯è„šæœ¬

set -e

JAEGER_API="http://jaeger-query:16686/api"
PROMETHEUS_API="http://prometheus:9090/api/v1"

echo "=== Chaos Engineering Verification ==="

# 1. æ£€æŸ¥è¿½è¸ªæ•°æ®å®Œæ•´æ€§
echo "1. Checking trace completeness..."

TOTAL_TRACES=$(curl -s "${JAEGER_API}/services/perf-test/operations" | jq '.data | length')
echo "   Total traces: ${TOTAL_TRACES}"

if [ "${TOTAL_TRACES}" -lt 1000 ]; then
    echo "   âŒ FAIL: Expected at least 1000 traces, got ${TOTAL_TRACES}"
    exit 1
fi

echo "   âœ… PASS: Trace completeness verified"

# 2. æ£€æŸ¥é”™è¯¯ç‡
echo "2. Checking error rate..."

ERROR_RATE=$(curl -s "${PROMETHEUS_API}/query?query=rate(otlp_receiver_refused_spans[5m])" \
    | jq -r '.data.result[0].value[1] // "0"')

echo "   Error rate: ${ERROR_RATE}"

if [ "$(echo "${ERROR_RATE} > 0.05" | bc -l)" -eq 1 ]; then
    echo "   âŒ FAIL: Error rate ${ERROR_RATE} exceeds 5%"
    exit 1
fi

echo "   âœ… PASS: Error rate acceptable"

# 3. æ£€æŸ¥å»¶è¿Ÿ P99
echo "3. Checking P99 latency..."

P99_LATENCY=$(curl -s "${PROMETHEUS_API}/query?query=histogram_quantile(0.99, rate(otlp_receiver_request_duration_seconds_bucket[5m]))" \
    | jq -r '.data.result[0].value[1] // "0"')

echo "   P99 latency: ${P99_LATENCY}s"

if [ "$(echo "${P99_LATENCY} > 2.0" | bc -l)" -eq 1 ]; then
    echo "   âš ï¸  WARN: P99 latency ${P99_LATENCY}s exceeds 2s"
else
    echo "   âœ… PASS: P99 latency acceptable"
fi

# 4. æ£€æŸ¥ Collector æ¢å¤
echo "4. Checking Collector recovery..."

COLLECTOR_UP=$(curl -s "${PROMETHEUS_API}/query?query=up{job='otel-collector'}" \
    | jq -r '.data.result[0].value[1]')

if [ "${COLLECTOR_UP}" != "1" ]; then
    echo "   âŒ FAIL: Collector not recovered"
    exit 1
fi

echo "   âœ… PASS: Collector recovered successfully"

echo ""
echo "=== âœ… All chaos engineering tests passed ==="
```

---

## 5. åè®®ä¸€è‡´æ€§éªŒè¯

### 5.1 OTLP/gRPC åè®®éªŒè¯

```python
# test_otlp_grpc_protocol.py - OTLP gRPC åè®®ä¸€è‡´æ€§æµ‹è¯•
import pytest
import grpc
from opentelemetry.proto.collector.trace.v1 import trace_service_pb2, trace_service_pb2_grpc
from opentelemetry.proto.trace.v1 import trace_pb2
from opentelemetry.proto.common.v1 import common_pb2
from opentelemetry.proto.resource.v1 import resource_pb2
from typing import Iterator
import time

class TestOTLPgRPCProtocol:
    """OTLP gRPC åè®®ä¸€è‡´æ€§æµ‹è¯•"""
    
    @pytest.fixture(scope="class")
    def grpc_channel(self):
        """åˆ›å»º gRPC é€šé“"""
        channel = grpc.insecure_channel('localhost:4317')
        yield channel
        channel.close()
    
    @pytest.fixture(scope="class")
    def trace_service_stub(self, grpc_channel):
        """åˆ›å»º TraceService stub"""
        return trace_service_pb2_grpc.TraceServiceStub(grpc_channel)
    
    def create_valid_trace_request(self) -> trace_service_pb2.ExportTraceServiceRequest:
        """åˆ›å»ºæœ‰æ•ˆçš„è¿½è¸ªå¯¼å‡ºè¯·æ±‚"""
        return trace_service_pb2.ExportTraceServiceRequest(
            resource_spans=[
                trace_pb2.ResourceSpans(
                    resource=resource_pb2.Resource(
                        attributes=[
                            common_pb2.KeyValue(
                                key='service.name',
                                value=common_pb2.AnyValue(string_value='test-service')
                            ),
                            common_pb2.KeyValue(
                                key='service.version',
                                value=common_pb2.AnyValue(string_value='1.0.0')
                            ),
                        ]
                    ),
                    scope_spans=[
                        trace_pb2.ScopeSpans(
                            scope=common_pb2.InstrumentationScope(
                                name='test-instrumentation',
                                version='1.0.0'
                            ),
                            spans=[
                                trace_pb2.Span(
                                    trace_id=b'\x01' * 16,
                                    span_id=b'\x02' * 8,
                                    name='test-operation',
                                    kind=trace_pb2.Span.SPAN_KIND_INTERNAL,
                                    start_time_unix_nano=int(time.time() * 1e9),
                                    end_time_unix_nano=int((time.time() + 0.1) * 1e9),
                                    attributes=[
                                        common_pb2.KeyValue(
                                            key='http.method',
                                            value=common_pb2.AnyValue(string_value='GET')
                                        ),
                                        common_pb2.KeyValue(
                                            key='http.status_code',
                                            value=common_pb2.AnyValue(int_value=200)
                                        ),
                                    ],
                                )
                            ]
                        )
                    ]
                )
            ]
        )
    
    def test_export_valid_trace(self, trace_service_stub):
        """æµ‹è¯•: å¯¼å‡ºæœ‰æ•ˆè¿½è¸ª"""
        request = self.create_valid_trace_request()
        
        try:
            response = trace_service_stub.Export(request, timeout=10)
            
            # éªŒè¯å“åº”ç±»å‹
            assert isinstance(response, trace_service_pb2.ExportTraceServiceResponse)
            
            # éªŒè¯ PartialSuccess (å¦‚æœå­˜åœ¨)
            if response.HasField('partial_success'):
                assert response.partial_success.rejected_spans == 0, \
                    f"Unexpected rejected spans: {response.partial_success.error_message}"
            
        except grpc.RpcError as e:
            pytest.fail(f"gRPC export failed: {e.code()} - {e.details()}")
    
    def test_empty_request(self, trace_service_stub):
        """æµ‹è¯•: ç©ºè¯·æ±‚"""
        request = trace_service_pb2.ExportTraceServiceRequest()
        
        # ç©ºè¯·æ±‚åº”è¯¥è¢«æ¥å— (æ ¹æ® OTLP è§„èŒƒ)
        response = trace_service_stub.Export(request, timeout=10)
        assert isinstance(response, trace_service_pb2.ExportTraceServiceResponse)
    
    def test_invalid_trace_id(self, trace_service_stub):
        """æµ‹è¯•: æ— æ•ˆçš„ trace_id (é•¿åº¦ä¸æ­£ç¡®)"""
        request = self.create_valid_trace_request()
        # è®¾ç½®æ— æ•ˆçš„ trace_id (ä¸æ˜¯ 16 å­—èŠ‚)
        request.resource_spans[0].scope_spans[0].spans[0].trace_id = b'\x01' * 8  # åªæœ‰ 8 å­—èŠ‚
        
        try:
            response = trace_service_stub.Export(request, timeout=10)
            
            # æŸäº›å®ç°å¯èƒ½æ‹’ç»,æŸäº›å¯èƒ½æ¥å—ä½†æ ‡è®°ä¸ºéƒ¨åˆ†æˆåŠŸ
            if response.HasField('partial_success'):
                assert response.partial_success.rejected_spans > 0
            
        except grpc.RpcError as e:
            # é¢„æœŸå¯èƒ½å¤±è´¥ (æ ¹æ®å®ç°)
            assert e.code() == grpc.StatusCode.INVALID_ARGUMENT
    
    def test_compression_support(self, grpc_channel):
        """æµ‹è¯•: gRPC å‹ç¼©æ”¯æŒ"""
        stub = trace_service_pb2_grpc.TraceServiceStub(grpc_channel)
        request = self.create_valid_trace_request()
        
        # æµ‹è¯• gzip å‹ç¼©
        response = stub.Export(
            request,
            compression=grpc.Compression.Gzip,
            timeout=10
        )
        
        assert isinstance(response, trace_service_pb2.ExportTraceServiceResponse)
    
    def test_metadata_propagation(self, trace_service_stub):
        """æµ‹è¯•: å…ƒæ•°æ®ä¼ æ’­"""
        request = self.create_valid_trace_request()
        
        metadata = (
            ('x-custom-header', 'test-value'),
            ('authorization', 'Bearer test-token'),
        )
        
        response = trace_service_stub.Export(request, metadata=metadata, timeout=10)
        assert isinstance(response, trace_service_pb2.ExportTraceServiceResponse)
    
    def test_timeout_handling(self, trace_service_stub):
        """æµ‹è¯•: è¶…æ—¶å¤„ç†"""
        request = self.create_valid_trace_request()
        
        try:
            # è®¾ç½®æçŸ­çš„è¶…æ—¶
            response = trace_service_stub.Export(request, timeout=0.001)
            # å¦‚æœæ²¡æœ‰è¶…æ—¶,ä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„ (è¯·æ±‚è¶³å¤Ÿå¿«)
            assert isinstance(response, trace_service_pb2.ExportTraceServiceResponse)
        except grpc.RpcError as e:
            # é¢„æœŸè¶…æ—¶
            assert e.code() == grpc.StatusCode.DEADLINE_EXCEEDED
    
    def test_concurrent_exports(self, trace_service_stub):
        """æµ‹è¯•: å¹¶å‘å¯¼å‡º"""
        import concurrent.futures
        
        requests = [self.create_valid_trace_request() for _ in range(100)]
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            futures = [
                executor.submit(trace_service_stub.Export, req, timeout=10)
                for req in requests
            ]
            
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        # æ‰€æœ‰è¯·æ±‚éƒ½åº”è¯¥æˆåŠŸ
        assert len(results) == 100
        assert all(isinstance(r, trace_service_pb2.ExportTraceServiceResponse) for r in results)
    
    def test_large_payload(self, trace_service_stub):
        """æµ‹è¯•: å¤§è´Ÿè½½ (1000 spans)"""
        request = trace_service_pb2.ExportTraceServiceRequest(
            resource_spans=[
                trace_pb2.ResourceSpans(
                    resource=resource_pb2.Resource(
                        attributes=[
                            common_pb2.KeyValue(
                                key='service.name',
                                value=common_pb2.AnyValue(string_value='test-service')
                            ),
                        ]
                    ),
                    scope_spans=[
                        trace_pb2.ScopeSpans(
                            spans=[
                                trace_pb2.Span(
                                    trace_id=b'\x01' * 16,
                                    span_id=i.to_bytes(8, 'big'),
                                    name=f'span-{i}',
                                    kind=trace_pb2.Span.SPAN_KIND_INTERNAL,
                                    start_time_unix_nano=int(time.time() * 1e9),
                                    end_time_unix_nano=int((time.time() + 0.1) * 1e9),
                                )
                                for i in range(1000)
                            ]
                        )
                    ]
                )
            ]
        )
        
        response = trace_service_stub.Export(request, timeout=30)
        assert isinstance(response, trace_service_pb2.ExportTraceServiceResponse)

### 5.2 OTLP/HTTP åè®®éªŒè¯

```python
# test_otlp_http_protocol.py - OTLP HTTP åè®®ä¸€è‡´æ€§æµ‹è¯•
import pytest
import requests
import json
import time
from typing import Dict, Any

class TestOTLPHTTPProtocol:
    """OTLP HTTP åè®®ä¸€è‡´æ€§æµ‹è¯•"""
    
    BASE_URL = "http://localhost:4318"
    
    def create_valid_trace_json(self) -> Dict[str, Any]:
        """åˆ›å»ºæœ‰æ•ˆçš„è¿½è¸ª JSON"""
        return {
            "resourceSpans": [{
                "resource": {
                    "attributes": [
                        {"key": "service.name", "value": {"stringValue": "test-service"}},
                        {"key": "service.version", "value": {"stringValue": "1.0.0"}},
                    ]
                },
                "scopeSpans": [{
                    "scope": {
                        "name": "test-instrumentation",
                        "version": "1.0.0"
                    },
                    "spans": [{
                        "traceId": "01020304050607080910111213141516",
                        "spanId": "0102030405060708",
                        "name": "test-operation",
                        "kind": 1,
                        "startTimeUnixNano": str(int(time.time() * 1e9)),
                        "endTimeUnixNano": str(int((time.time() + 0.1) * 1e9)),
                        "attributes": [
                            {"key": "http.method", "value": {"stringValue": "GET"}},
                            {"key": "http.status_code", "value": {"intValue": "200"}},
                        ],
                    }]
                }]
            }]
        }
    
    def test_json_content_type(self):
        """æµ‹è¯•: application/json Content-Type"""
        data = self.create_valid_trace_json()
        
        response = requests.post(
            f"{self.BASE_URL}/v1/traces",
            json=data,
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        
        assert response.status_code == 200, f"Unexpected status: {response.status_code} - {response.text}"
        
        # éªŒè¯å“åº”æ ¼å¼
        if response.text:
            response_data = response.json()
            assert isinstance(response_data, dict)
    
    def test_protobuf_content_type(self):
        """æµ‹è¯•: application/x-protobuf Content-Type"""
        from opentelemetry.proto.collector.trace.v1 import trace_service_pb2
        from opentelemetry.proto.trace.v1 import trace_pb2
        from opentelemetry.proto.common.v1 import common_pb2
        from opentelemetry.proto.resource.v1 import resource_pb2
        
        request = trace_service_pb2.ExportTraceServiceRequest(
            resource_spans=[
                trace_pb2.ResourceSpans(
                    resource=resource_pb2.Resource(
                        attributes=[
                            common_pb2.KeyValue(
                                key='service.name',
                                value=common_pb2.AnyValue(string_value='test-service')
                            ),
                        ]
                    ),
                    scope_spans=[
                        trace_pb2.ScopeSpans(
                            spans=[
                                trace_pb2.Span(
                                    trace_id=b'\x01' * 16,
                                    span_id=b'\x02' * 8,
                                    name='test-operation',
                                    kind=trace_pb2.Span.SPAN_KIND_INTERNAL,
                                    start_time_unix_nano=int(time.time() * 1e9),
                                    end_time_unix_nano=int((time.time() + 0.1) * 1e9),
                                )
                            ]
                        )
                    ]
                )
            ]
        )
        
        response = requests.post(
            f"{self.BASE_URL}/v1/traces",
            data=request.SerializeToString(),
            headers={"Content-Type": "application/x-protobuf"},
            timeout=10
        )
        
        assert response.status_code == 200
    
    def test_gzip_compression(self):
        """æµ‹è¯•: gzip å‹ç¼©"""
        import gzip
        
        data = self.create_valid_trace_json()
        json_data = json.dumps(data).encode('utf-8')
        compressed_data = gzip.compress(json_data)
        
        response = requests.post(
            f"{self.BASE_URL}/v1/traces",
            data=compressed_data,
            headers={
                "Content-Type": "application/json",
                "Content-Encoding": "gzip",
            },
            timeout=10
        )
        
        assert response.status_code == 200
    
    def test_custom_headers(self):
        """æµ‹è¯•: è‡ªå®šä¹‰ HTTP headers"""
        data = self.create_valid_trace_json()
        
        response = requests.post(
            f"{self.BASE_URL}/v1/traces",
            json=data,
            headers={
                "Content-Type": "application/json",
                "X-Custom-Header": "test-value",
                "Authorization": "Bearer test-token",
            },
            timeout=10
        )
        
        assert response.status_code == 200
    
    def test_cors_headers(self):
        """æµ‹è¯•: CORS headers"""
        # OPTIONS é¢„æ£€è¯·æ±‚
        response = requests.options(
            f"{self.BASE_URL}/v1/traces",
            headers={
                "Origin": "http://example.com",
                "Access-Control-Request-Method": "POST",
                "Access-Control-Request-Headers": "Content-Type",
            },
            timeout=10
        )
        
        # éªŒè¯ CORS headers (å¦‚æœæ”¯æŒ)
        if response.status_code == 200:
            assert "Access-Control-Allow-Origin" in response.headers or \
                   response.status_code == 404  # æŸäº›å®ç°ä¸æ”¯æŒ OPTIONS
    
    def test_http_methods(self):
        """æµ‹è¯•: åªæ”¯æŒ POST æ–¹æ³•"""
        data = self.create_valid_trace_json()
        
        # GET åº”è¯¥å¤±è´¥
        response = requests.get(f"{self.BASE_URL}/v1/traces", timeout=10)
        assert response.status_code in [404, 405]
        
        # PUT åº”è¯¥å¤±è´¥
        response = requests.put(
            f"{self.BASE_URL}/v1/traces",
            json=data,
            timeout=10
        )
        assert response.status_code in [404, 405]
        
        # DELETE åº”è¯¥å¤±è´¥
        response = requests.delete(f"{self.BASE_URL}/v1/traces", timeout=10)
        assert response.status_code in [404, 405]
    
    def test_error_responses(self):
        """æµ‹è¯•: é”™è¯¯å“åº”æ ¼å¼"""
        # å‘é€æ— æ•ˆ JSON
        response = requests.post(
            f"{self.BASE_URL}/v1/traces",
            data="invalid json",
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        
        # åº”è¯¥è¿”å› 4xx é”™è¯¯
        assert 400 <= response.status_code < 500
    
    def test_partial_success_response(self):
        """æµ‹è¯•: éƒ¨åˆ†æˆåŠŸå“åº”"""
        # å‘é€åŒ…å«æ— æ•ˆæ•°æ®çš„è¯·æ±‚
        data = self.create_valid_trace_json()
        data["resourceSpans"][0]["scopeSpans"][0]["spans"][0]["traceId"] = "invalid"  # æ— æ•ˆé•¿åº¦
        
        response = requests.post(
            f"{self.BASE_URL}/v1/traces",
            json=data,
            timeout=10
        )
        
        # å¯èƒ½è¿”å› 200 (éƒ¨åˆ†æˆåŠŸ) æˆ– 400 (å®Œå…¨å¤±è´¥),å–å†³äºå®ç°
        assert response.status_code in [200, 400]
        
        if response.status_code == 200 and response.text:
            response_data = response.json()
            if "partialSuccess" in response_data:
                assert response_data["partialSuccess"]["rejectedSpans"] > 0

---

## 6. æ€§èƒ½æµ‹è¯•ä¸åŸºå‡†

### 6.1 ååé‡ä¸å»¶è¿Ÿæµ‹è¯•

```python
# benchmark_throughput.py - ååé‡åŸºå‡†æµ‹è¯•
import asyncio
import aiohttp
import time
import statistics
from typing import List, Dict
from dataclasses import dataclass
import json

@dataclass
class BenchmarkConfig:
    """åŸºå‡†æµ‹è¯•é…ç½®"""
    endpoint: str = "http://localhost:4318/v1/traces"
    total_requests: int = 10000
    concurrency: int = 100
    payload_size: str = "small"  # small, medium, large

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_duration_seconds: float
    requests_per_second: float
    avg_latency_ms: float
    median_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    min_latency_ms: float
    max_latency_ms: float

class ThroughputBenchmark:
    """ååé‡åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.latencies: List[float] = []
        self.success_count = 0
        self.failure_count = 0
    
    def generate_payload(self) -> Dict:
        """ç”Ÿæˆæµ‹è¯•è´Ÿè½½"""
        base_payload = {
            "resourceSpans": [{
                "resource": {
                    "attributes": [
                        {"key": "service.name", "value": {"stringValue": "benchmark-service"}},
                    ]
                },
                "scopeSpans": [{
                    "spans": []
                }]
            }]
        }
        
        # æ ¹æ® payload_size è°ƒæ•´ span æ•°é‡
        span_counts = {"small": 1, "medium": 10, "large": 100}
        num_spans = span_counts.get(self.config.payload_size, 1)
        
        for i in range(num_spans):
            span = {
                "traceId": f"{i:032x}",
                "spanId": f"{i:016x}",
                "name": f"benchmark-span-{i}",
                "kind": 1,
                "startTimeUnixNano": str(int(time.time() * 1e9)),
                "endTimeUnixNano": str(int((time.time() + 0.1) * 1e9)),
            }
            base_payload["resourceSpans"][0]["scopeSpans"][0]["spans"].append(span)
        
        return base_payload
    
    async def send_request(self, session: aiohttp.ClientSession) -> None:
        """å‘é€å•ä¸ªè¯·æ±‚"""
        payload = self.generate_payload()
        
        start_time = time.perf_counter()
        
        try:
            async with session.post(
                self.config.endpoint,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                await response.read()
                
                end_time = time.perf_counter()
                latency_ms = (end_time - start_time) * 1000
                
                self.latencies.append(latency_ms)
                
                if response.status == 200:
                    self.success_count += 1
                else:
                    self.failure_count += 1
        
        except Exception as e:
            self.failure_count += 1
            print(f"Request failed: {e}")
    
    async def run_benchmark(self) -> BenchmarkResult:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        print(f"Starting benchmark: {self.config.total_requests} requests with concurrency {self.config.concurrency}")
        
        connector = aiohttp.TCPConnector(limit=self.config.concurrency)
        async with aiohttp.ClientSession(connector=connector) as session:
            start_time = time.time()
            
            # åˆ›å»ºä»»åŠ¡
            tasks = [
                self.send_request(session)
                for _ in range(self.config.total_requests)
            ]
            
            # æ‰§è¡Œä»»åŠ¡ (é™åˆ¶å¹¶å‘)
            for i in range(0, len(tasks), self.config.concurrency):
                batch = tasks[i:i + self.config.concurrency]
                await asyncio.gather(*batch, return_exceptions=True)
            
            end_time = time.time()
            total_duration = end_time - start_time
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        self.latencies.sort()
        
        return BenchmarkResult(
            total_requests=self.config.total_requests,
            successful_requests=self.success_count,
            failed_requests=self.failure_count,
            total_duration_seconds=total_duration,
            requests_per_second=self.config.total_requests / total_duration,
            avg_latency_ms=statistics.mean(self.latencies),
            median_latency_ms=statistics.median(self.latencies),
            p95_latency_ms=self.latencies[int(len(self.latencies) * 0.95)],
            p99_latency_ms=self.latencies[int(len(self.latencies) * 0.99)],
            min_latency_ms=min(self.latencies),
            max_latency_ms=max(self.latencies),
        )
    
    def print_results(self, result: BenchmarkResult) -> None:
        """æ‰“å°ç»“æœ"""
        print("\n" + "="*60)
        print("BENCHMARK RESULTS")
        print("="*60)
        print(f"Total Requests:        {result.total_requests}")
        print(f"Successful:            {result.successful_requests} ({result.successful_requests/result.total_requests*100:.2f}%)")
        print(f"Failed:                {result.failure_count}")
        print(f"Total Duration:        {result.total_duration_seconds:.2f}s")
        print(f"Throughput:            {result.requests_per_second:.2f} req/s")
        print(f"Average Latency:       {result.avg_latency_ms:.2f} ms")
        print(f"Median Latency:        {result.median_latency_ms:.2f} ms")
        print(f"P95 Latency:           {result.p95_latency_ms:.2f} ms")
        print(f"P99 Latency:           {result.p99_latency_ms:.2f} ms")
        print(f"Min Latency:           {result.min_latency_ms:.2f} ms")
        print(f"Max Latency:           {result.max_latency_ms:.2f} ms")
        print("="*60 + "\n")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    configs = [
        BenchmarkConfig(total_requests=1000, concurrency=10, payload_size="small"),
        BenchmarkConfig(total_requests=1000, concurrency=50, payload_size="small"),
        BenchmarkConfig(total_requests=1000, concurrency=100, payload_size="small"),
        BenchmarkConfig(total_requests=1000, concurrency=100, payload_size="medium"),
        BenchmarkConfig(total_requests=500, concurrency=100, payload_size="large"),
    ]
    
    for config in configs:
        print(f"\nRunning benchmark: concurrency={config.concurrency}, payload={config.payload_size}")
        benchmark = ThroughputBenchmark(config)
        result = await benchmark.run_benchmark()
        benchmark.print_results(result)
        
        # ç­‰å¾…ç³»ç»Ÿæ¢å¤
        await asyncio.sleep(5)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## 7. è‡ªåŠ¨åŒ–æµ‹è¯•æµæ°´çº¿

### 7.1 CI/CD é›†æˆ (GitHub Actions)

```yaml
# .github/workflows/otlp-test-pipeline.yml
name: OTLP Test Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

env:
  OTLP_VERSION: "0.108.0"

jobs:
  # é™æ€åˆ†æ
  static-analysis:
    name: Static Analysis
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install linters
        run: |
          pip install pylint mypy black isort
      
      - name: Run Black (formatting check)
        run: black --check src/
      
      - name: Run isort (import sorting)
        run: isort --check-only src/
      
      - name: Run Pylint
        run: pylint src/
      
      - name: Run MyPy (type checking)
        run: mypy src/

  # å•å…ƒæµ‹è¯•
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run unit tests
        run: |
          pytest tests/unit \
            --cov=src \
            --cov-report=xml \
            --cov-report=term \
            --junitxml=test-results/junit.xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: python-${{ matrix.python-version }}
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: test-results/

  # é›†æˆæµ‹è¯•
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
      
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: testuser
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Start OTLP Collector
        run: |
          docker run -d --name otel-collector \
            -p 4317:4317 -p 4318:4318 -p 8888:8888 \
            -v $(pwd)/test-configs/collector-config.yaml:/etc/otel-collector-config.yaml \
            otel/opentelemetry-collector-contrib:${{ env.OTLP_VERSION }} \
            --config=/etc/otel-collector-config.yaml
          
          # Wait for collector
          sleep 10
          curl -f http://localhost:8888/
      
      - name: Start Jaeger
        run: |
          docker run -d --name jaeger \
            -p 16686:16686 -p 14250:14250 \
            -e COLLECTOR_OTLP_ENABLED=true \
            jaegertracing/all-in-one:1.52
          
          sleep 5
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run integration tests
        env:
          OTLP_ENDPOINT: http://localhost:4318
          JAEGER_UI: http://localhost:16686
          REDIS_URL: redis://localhost:6379
          DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
        run: |
          pytest tests/integration \
            --junitxml=test-results/integration-junit.xml
      
      - name: Collect logs
        if: failure()
        run: |
          docker logs otel-collector > collector-logs.txt
          docker logs jaeger > jaeger-logs.txt
      
      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: integration-test-logs
          path: |
            collector-logs.txt
            jaeger-logs.txt

  # E2E æµ‹è¯•
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [integration-tests]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Start test environment
        run: |
          docker-compose -f docker-compose.test.yml up -d
          sleep 20
      
      - name: Install Playwright
        run: |
          npm install -D @playwright/test
          npx playwright install --with-deps
      
      - name: Run E2E tests
        run: |
          npx playwright test tests/e2e/
      
      - name: Upload Playwright report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: playwright-report
          path: playwright-report/

  # æ€§èƒ½æµ‹è¯•
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [integration-tests]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Start test environment
        run: |
          docker-compose -f docker-compose.test.yml up -d
          sleep 20
      
      - name: Install K6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg \
            --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" \
            | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
      
      - name: Run performance tests
        run: |
          k6 run tests/performance/performance-test.js \
            --out json=performance-results.json
      
      - name: Analyze performance regression
        run: |
          python scripts/analyze_performance_regression.py \
            --current performance-results.json \
            --baseline performance-baseline.json \
            --threshold 0.10  # 10% regression threshold
      
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: performance-results.json

  # å®‰å…¨æ‰«æ
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Trivy (vulnerability scanner)
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
      
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'
      
      - name: Run Bandit (Python security linter)
        run: |
          pip install bandit
          bandit -r src/ -f json -o bandit-results.json
      
      - name: Upload Bandit results
        uses: actions/upload-artifact@v3
        with:
          name: bandit-results
          path: bandit-results.json

  # æŠ¥å‘Šç”Ÿæˆ
  generate-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, security-scan]
    if: always()
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download all artifacts
        uses: actions/download-artifact@v3
      
      - name: Generate unified report
        run: |
          python scripts/generate_test_report.py \
            --output test-report.html
      
      - name: Upload test report
        uses: actions/upload-artifact@v3
        with:
          name: test-report
          path: test-report.html
      
      - name: Comment PR with report
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('test-report-summary.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
```

---

## 8. å·¥å…·é“¾ä¸ç”Ÿæ€

### 8.1 æ¨èæµ‹è¯•å·¥å…·çŸ©é˜µ

| æµ‹è¯•ç±»å‹ | è¯­è¨€/æ¡†æ¶ | æ¨èå·¥å…· | ä¼˜åŠ¿ | é›†æˆéš¾åº¦ |
|---------|---------|---------|------|---------|
| **å•å…ƒæµ‹è¯•** | Python | pytest, unittest | ä¸°å¯Œçš„æ’ä»¶ç”Ÿæ€, ç®€æ´è¯­æ³• | â­ ä½ |
| | Go | testing, testify | å†…ç½®æ ‡å‡†åº“, å¼ºå¤§æ–­è¨€ | â­ ä½ |
| | Java | JUnit 5, Mockito | æˆç†Ÿç¨³å®š, ä¼ä¸šçº§æ”¯æŒ | â­â­ ä¸­ |
| | Node.js/TypeScript | Jest, Mocha | å¿«é€Ÿæ‰§è¡Œ, è‰¯å¥½ TypeScript æ”¯æŒ | â­ ä½ |
| **Mock/Stub** | è·¨è¯­è¨€ | Mockito, unittest.mock, testify/mock | çµæ´»çš„æ¨¡æ‹Ÿç­–ç•¥ | â­ ä½ |
| **é›†æˆæµ‹è¯•** | å®¹å™¨ç¼–æ’ | Testcontainers | çœŸå®ç¯å¢ƒéš”ç¦», å¯é‡å¤ | â­â­ ä¸­ |
| | Docker | Docker Compose | ç®€å•å¿«é€Ÿ, å¹¿æ³›æ”¯æŒ | â­ ä½ |
| **å¥‘çº¦æµ‹è¯•** | è·¨æœåŠ¡ | Pact, Spring Cloud Contract | API å…¼å®¹æ€§ä¿è¯ | â­â­â­ é«˜ |
| **E2E æµ‹è¯•** | Web UI | Playwright, Cypress | ç°ä»£æµè§ˆå™¨è‡ªåŠ¨åŒ– | â­â­ ä¸­ |
| | API | Postman, Newman | æ˜“ç”¨çš„ API æµ‹è¯• | â­ ä½ |
| **æ€§èƒ½æµ‹è¯•** | è´Ÿè½½ç”Ÿæˆ | K6, JMeter, Gatling | å¼ºå¤§çš„è´Ÿè½½æ¨¡æ‹Ÿ | â­â­ ä¸­ |
| | åŸºå‡†æµ‹è¯• | pytest-benchmark, Go benchmarks | ç²¾ç¡®çš„æ€§èƒ½åº¦é‡ | â­ ä½ |
| **æ··æ²Œå·¥ç¨‹** | Kubernetes | Chaos Mesh, Litmus | åˆ†å¸ƒå¼æ•…éšœæ³¨å…¥ | â­â­â­ é«˜ |
| | é€šç”¨ | Gremlin, Pumba | å¤šç§æ··æ²Œåœºæ™¯ | â­â­ ä¸­ |
| **è¦†ç›–ç‡** | Python | coverage.py, pytest-cov | è¯¦ç»†çš„è¦†ç›–ç‡æŠ¥å‘Š | â­ ä½ |
| | Go | go test -cover | å†…ç½®è¦†ç›–ç‡æ”¯æŒ | â­ ä½ |
| | Java | JaCoCo | ä¼ä¸šçº§è¦†ç›–ç‡åˆ†æ | â­â­ ä¸­ |
| **å®‰å…¨æµ‹è¯•** | æ¼æ´æ‰«æ | Trivy, Snyk | è‡ªåŠ¨ä¾èµ–æ‰«æ | â­ ä½ |
| | SAST | Bandit, SonarQube | é™æ€ä»£ç å®‰å…¨åˆ†æ | â­â­ ä¸­ |
| **åè®®æµ‹è¯•** | OTLP | Custom scripts, grpcurl | åè®®çº§åˆ«éªŒè¯ | â­â­ ä¸­ |

### 8.2 è‡ªå®šä¹‰æµ‹è¯•å·¥å…·å¼€å‘

```python
# otlp_test_framework.py - è‡ªå®šä¹‰ OTLP æµ‹è¯•æ¡†æ¶
"""
OTLP æµ‹è¯•æ¡†æ¶ - æä¾›ç®€åŒ–çš„æµ‹è¯• API å’Œå¸¸ç”¨æµ‹è¯•åœºæ™¯

ä½¿ç”¨ç¤ºä¾‹:
    with OTLPTestEnvironment() as env:
        client = env.create_trace_client()
        trace_id = client.send_trace("test-operation")
        env.verify_trace_received(trace_id, timeout=10)
"""

import time
import uuid
import requests
from typing import Optional, Dict, Any, List
from contextlib import contextmanager
from dataclasses import dataclass
import docker
from opentelemetry.proto.collector.trace.v1 import trace_service_pb2
from opentelemetry.proto.trace.v1 import trace_pb2
from opentelemetry.proto.common.v1 import common_pb2
from opentelemetry.proto.resource.v1 import resource_pb2

@dataclass
class OTLPTestConfig:
    """æµ‹è¯•ç¯å¢ƒé…ç½®"""
    collector_image: str = "otel/opentelemetry-collector-contrib:0.108.0"
    jaeger_image: str = "jaegertracing/all-in-one:1.52"
    collector_grpc_port: int = 4317
    collector_http_port: int = 4318
    jaeger_ui_port: int = 16686
    startup_timeout: int = 30

class OTLPTestEnvironment:
    """OTLP æµ‹è¯•ç¯å¢ƒç®¡ç†å™¨"""
    
    def __init__(self, config: Optional[OTLPTestConfig] = None):
        self.config = config or OTLPTestConfig()
        self.docker_client = docker.from_env()
        self.containers: Dict[str, Any] = {}
        self._started = False
    
    def __enter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡æ—¶å¯åŠ¨æµ‹è¯•ç¯å¢ƒ"""
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡æ—¶æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        self.cleanup()
    
    def start(self) -> None:
        """å¯åŠ¨æµ‹è¯•ç¯å¢ƒ"""
        if self._started:
            return
        
        print("Starting OTLP test environment...")
        
        # å¯åŠ¨ Jaeger
        self.containers['jaeger'] = self.docker_client.containers.run(
            self.config.jaeger_image,
            name=f"otlp-test-jaeger-{uuid.uuid4().hex[:8]}",
            ports={
                '16686/tcp': self.config.jaeger_ui_port,
                '4317/tcp': 14317,  # å†…éƒ¨ OTLP gRPC
            },
            environment={'COLLECTOR_OTLP_ENABLED': 'true'},
            detach=True,
            remove=True,
        )
        
        # å¯åŠ¨ Collector
        collector_config = self._generate_collector_config()
        self.containers['collector'] = self.docker_client.containers.run(
            self.config.collector_image,
            name=f"otlp-test-collector-{uuid.uuid4().hex[:8]}",
            ports={
                '4317/tcp': self.config.collector_grpc_port,
                '4318/tcp': self.config.collector_http_port,
            },
            command=['--config=yaml:' + collector_config],
            detach=True,
            remove=True,
        )
        
        # ç­‰å¾…æœåŠ¡å°±ç»ª
        self._wait_for_readiness()
        self._started = True
        print("âœ… Test environment ready")
    
    def cleanup(self) -> None:
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if not self._started:
            return
        
        print("Cleaning up test environment...")
        for name, container in self.containers.items():
            try:
                container.stop(timeout=5)
                print(f"  Stopped {name}")
            except Exception as e:
                print(f"  Error stopping {name}: {e}")
        
        self.containers.clear()
        self._started = False
        print("âœ… Cleanup complete")
    
    def _generate_collector_config(self) -> str:
        """ç”Ÿæˆ Collector é…ç½®"""
        jaeger_endpoint = f"{self.containers['jaeger'].name}:4317"
        
        return f"""
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 1s
    send_batch_size: 100

exporters:
  otlp:
    endpoint: {jaeger_endpoint}
    tls:
      insecure: true
  logging:
    loglevel: debug

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp, logging]
"""
    
    def _wait_for_readiness(self) -> None:
        """ç­‰å¾…æœåŠ¡å°±ç»ª"""
        start = time.time()
        
        # ç­‰å¾… Collector
        collector_ready = False
        while time.time() - start < self.config.startup_timeout:
            try:
                response = requests.get(
                    f"http://localhost:{self.config.collector_http_port}/",
                    timeout=2
                )
                if response.status_code in [200, 404]:
                    collector_ready = True
                    break
            except requests.RequestException:
                pass
            time.sleep(1)
        
        if not collector_ready:
            raise TimeoutError("Collector failed to start")
        
        # ç­‰å¾… Jaeger
        jaeger_ready = False
        while time.time() - start < self.config.startup_timeout:
            try:
                response = requests.get(
                    f"http://localhost:{self.config.jaeger_ui_port}/",
                    timeout=2
                )
                if response.status_code == 200:
                    jaeger_ready = True
                    break
            except requests.RequestException:
                pass
            time.sleep(1)
        
        if not jaeger_ready:
            raise TimeoutError("Jaeger failed to start")
    
    def create_trace_client(self, service_name: str = "test-service") -> 'OTLPTraceClient':
        """åˆ›å»ºè¿½è¸ªå®¢æˆ·ç«¯"""
        return OTLPTraceClient(
            endpoint=f"http://localhost:{self.config.collector_http_port}",
            service_name=service_name
        )
    
    def verify_trace_received(
        self, 
        trace_id: str, 
        timeout: int = 10
    ) -> bool:
        """éªŒè¯è¿½è¸ªæ•°æ®å·²æ¥æ”¶"""
        jaeger_api = f"http://localhost:{self.config.jaeger_ui_port}/api/traces/{trace_id}"
        
        start = time.time()
        while time.time() - start < timeout:
            try:
                response = requests.get(jaeger_api, timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    if data.get('data') and len(data['data']) > 0:
                        return True
            except requests.RequestException:
                pass
            time.sleep(1)
        
        return False
    
    def get_trace(self, trace_id: str) -> Optional[Dict]:
        """è·å–è¿½è¸ªæ•°æ®"""
        jaeger_api = f"http://localhost:{self.config.jaeger_ui_port}/api/traces/{trace_id}"
        
        try:
            response = requests.get(jaeger_api, timeout=5)
            if response.status_code == 200:
                data = response.json()
                if data.get('data') and len(data['data']) > 0:
                    return data['data'][0]
        except requests.RequestException:
            pass
        
        return None

class OTLPTraceClient:
    """OTLP è¿½è¸ªå®¢æˆ·ç«¯ (ç®€åŒ–ç‰ˆ)"""
    
    def __init__(self, endpoint: str, service_name: str):
        self.endpoint = f"{endpoint}/v1/traces"
        self.service_name = service_name
    
    def send_trace(
        self, 
        operation_name: str,
        attributes: Optional[Dict[str, Any]] = None,
        duration_ms: float = 100.0
    ) -> str:
        """å‘é€æµ‹è¯•è¿½è¸ª"""
        trace_id = uuid.uuid4().hex
        span_id = uuid.uuid4().hex[:16]
        
        start_time = int(time.time() * 1e9)
        end_time = int((time.time() + duration_ms / 1000) * 1e9)
        
        span_attributes = [
            {"key": "test.type", "value": {"stringValue": "automated"}}
        ]
        
        if attributes:
            for key, value in attributes.items():
                if isinstance(value, str):
                    span_attributes.append({
                        "key": key,
                        "value": {"stringValue": value}
                    })
                elif isinstance(value, int):
                    span_attributes.append({
                        "key": key,
                        "value": {"intValue": str(value)}
                    })
        
        payload = {
            "resourceSpans": [{
                "resource": {
                    "attributes": [
                        {"key": "service.name", "value": {"stringValue": self.service_name}}
                    ]
                },
                "scopeSpans": [{
                    "spans": [{
                        "traceId": trace_id,
                        "spanId": span_id,
                        "name": operation_name,
                        "kind": 1,
                        "startTimeUnixNano": str(start_time),
                        "endTimeUnixNano": str(end_time),
                        "attributes": span_attributes,
                    }]
                }]
            }]
        }
        
        response = requests.post(
            self.endpoint,
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=10
        )
        
        if response.status_code != 200:
            raise RuntimeError(f"Failed to send trace: {response.status_code} - {response.text}")
        
        return trace_id

# ===== ä½¿ç”¨ç¤ºä¾‹ =====

def test_otlp_framework_example():
    """æµ‹è¯•æ¡†æ¶ä½¿ç”¨ç¤ºä¾‹"""
    with OTLPTestEnvironment() as env:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = env.create_trace_client(service_name="example-service")
        
        # å‘é€è¿½è¸ª
        trace_id = client.send_trace(
            operation_name="example-operation",
            attributes={
                "http.method": "GET",
                "http.status_code": 200,
            },
            duration_ms=50.0
        )
        
        print(f"Sent trace: {trace_id}")
        
        # éªŒè¯æ¥æ”¶
        assert env.verify_trace_received(trace_id, timeout=10), \
            f"Trace {trace_id} not received"
        
        # è·å–å¹¶éªŒè¯å†…å®¹
        trace_data = env.get_trace(trace_id)
        assert trace_data is not None
        assert len(trace_data['spans']) == 1
        assert trace_data['spans'][0]['operationName'] == 'example-operation'
        
        print("âœ… Test passed!")

if __name__ == "__main__":
    test_otlp_framework_example()
```

---

## 9. æœ€ä½³å®è·µä¸æ¡ˆä¾‹ç ”ç©¶

### 9.1 è¡Œä¸šæ ‡æ†æ¡ˆä¾‹

#### æ¡ˆä¾‹ 1: Uber - å¤§è§„æ¨¡åˆ†å¸ƒå¼è¿½è¸ªæµ‹è¯•

**æŒ‘æˆ˜**:

- æ¯ç§’ç™¾ä¸‡çº§è¿½è¸ªæ•°æ®
- å¤æ‚çš„å¾®æœåŠ¡æ‹“æ‰‘ (2000+ æœåŠ¡)
- å¤šè¯­è¨€ã€å¤šæ¡†æ¶æ··åˆ

**æµ‹è¯•ç­–ç•¥**:

1. **åˆ†å±‚æµ‹è¯•é‡‘å­—å¡”**:
   - å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 85%
   - é›†æˆæµ‹è¯•è¦†ç›–å…³é”®è·¯å¾„
   - E2E æµ‹è¯•è¦†ç›–ç”¨æˆ·æ ¸å¿ƒæµç¨‹

2. **å½±å­æµé‡æµ‹è¯•** (Shadow Testing):
   - å¤åˆ¶ç”Ÿäº§æµé‡åˆ°æµ‹è¯•ç¯å¢ƒ
   - å¯¹æ¯”æ–°è€ç‰ˆæœ¬çš„è¿½è¸ªæ•°æ®å®Œæ•´æ€§

3. **æ€§èƒ½åŸºå‡†å›å½’**:
   - æ¯æ¬¡ PR è‡ªåŠ¨è¿è¡Œæ€§èƒ½æµ‹è¯•
   - P99 å»¶è¿Ÿä¸å¾—è¶…è¿‡åŸºçº¿ 10%

**æˆæœ**:

- ç”Ÿäº§æ•…éšœç‡é™ä½ 70%
- æµ‹è¯•è¦†ç›–ç‡æå‡åˆ° 87%
- å‘å¸ƒé¢‘ç‡æå‡ 3 å€

#### æ¡ˆä¾‹ 2: Netflix - æ··æ²Œå·¥ç¨‹é©±åŠ¨çš„å¯è§‚æµ‹æ€§æµ‹è¯•

**æŒ‘æˆ˜**:

- å…¨çƒåˆ†å¸ƒå¼æ¶æ„
- å¤æ‚çš„æ•…éšœåœºæ™¯
- é«˜å¯ç”¨æ€§è¦æ±‚ (99.99%)

**æµ‹è¯•ç­–ç•¥**:

1. **æ··æ²Œå·¥ç¨‹é›†æˆ**:
   - Chaos Monkey: éšæœºç»ˆæ­¢æœåŠ¡å®ä¾‹
   - Latency Monkey: æ³¨å…¥ç½‘ç»œå»¶è¿Ÿ
   - éªŒè¯è¿½è¸ªæ•°æ®åœ¨æ•…éšœåœºæ™¯ä¸‹çš„å®Œæ•´æ€§

2. **Canary éƒ¨ç½²æµ‹è¯•**:
   - 1% æµé‡å…ˆå¯¼æµ‹è¯•
   - å®æ—¶ç›‘æ§è¿½è¸ªæ•°æ®è´¨é‡æŒ‡æ ‡
   - è‡ªåŠ¨å›æ»šæœºåˆ¶

3. **è·¨åŒºåŸŸæµ‹è¯•**:
   - æ¨¡æ‹Ÿè·¨åŒºåŸŸç½‘ç»œåˆ†åŒº
   - éªŒè¯è¿½è¸ªä¸Šä¸‹æ–‡ä¼ æ’­çš„é²æ£’æ€§

**æˆæœ**:

- æ•…éšœæ¢å¤æ—¶é—´ (MTTR) å‡å°‘ 60%
- è¿½è¸ªæ•°æ®ä¸¢å¤±ç‡ < 0.1%

### 9.2 å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ

#### é™·é˜± 1: æµ‹è¯•ç¯å¢ƒä¸ç”Ÿäº§ç¯å¢ƒå·®å¼‚è¿‡å¤§

**é—®é¢˜**:

- æµ‹è¯•ç¯å¢ƒä½¿ç”¨å•æœº Collector
- ç”Ÿäº§ç¯å¢ƒä½¿ç”¨åˆ†å¸ƒå¼ Collector é›†ç¾¤
- å¯¼è‡´æµ‹è¯•é€šè¿‡ä½†ç”Ÿäº§å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:

```yaml
# docker-compose.test-production-like.yml
# ç”Ÿäº§åŒ–çš„æµ‹è¯•ç¯å¢ƒ
version: '3.9'

services:
  # Collector é›†ç¾¤ (3 èŠ‚ç‚¹)
  otel-collector-1:
    image: otel/opentelemetry-collector-contrib:0.108.0
    <<: &collector-common
      volumes:
        - ./collector-config.yaml:/etc/config.yaml
      command: ["--config=/etc/config.yaml"]
  
  otel-collector-2:
    image: otel/opentelemetry-collector-contrib:0.108.0
    <<: *collector-common
  
  otel-collector-3:
    image: otel/opentelemetry-collector-contrib:0.108.0
    <<: *collector-common
  
  # è´Ÿè½½å‡è¡¡å™¨
  nginx-lb:
    image: nginx:alpine
    ports:
      - "4317:4317"
      - "4318:4318"
    volumes:
      - ./nginx-lb.conf:/etc/nginx/nginx.conf
    depends_on:
      - otel-collector-1
      - otel-collector-2
      - otel-collector-3
```

#### é™·é˜± 2: æµ‹è¯•æ•°æ®ä¸çœŸå®

**é—®é¢˜**:

- æµ‹è¯•ç”¨ä¾‹ä½¿ç”¨ç®€å•ã€è§„æ•´çš„æ•°æ®
- ç”Ÿäº§ç¯å¢ƒæœ‰å„ç§è¾¹ç•Œæƒ…å†µã€å¼‚å¸¸æ•°æ®

**è§£å†³æ–¹æ¡ˆ**:

```python
# test_data_generator.py - çœŸå®æ•°æ®ç”Ÿæˆå™¨
import random
import string
from typing import Dict, Any

class RealisticTraceGenerator:
    """ç”Ÿæˆæ›´çœŸå®çš„æµ‹è¯•è¿½è¸ªæ•°æ®"""
    
    def generate_realistic_span(self) -> Dict[str, Any]:
        """ç”ŸæˆçœŸå®çš„ Span æ•°æ®"""
        span = self._base_span()
        
        # éšæœºæ·»åŠ çœŸå®åœºæ™¯çš„å¤æ‚æ€§
        scenarios = [
            self._add_error_scenario,
            self._add_long_duration,
            self._add_many_attributes,
            self._add_unicode_characters,
            self._add_large_events,
        ]
        
        # 20% æ¦‚ç‡è§¦å‘ç‰¹æ®Šåœºæ™¯
        if random.random() < 0.2:
            random.choice(scenarios)(span)
        
        return span
    
    def _base_span(self) -> Dict[str, Any]:
        """åŸºç¡€ Span"""
        return {
            "traceId": self._random_hex(32),
            "spanId": self._random_hex(16),
            "name": random.choice([
                "GET /api/users",
                "POST /api/orders",
                "database query",
                "cache lookup",
            ]),
            "kind": random.choice([1, 2, 3]),
            "startTimeUnixNano": str(int(time.time() * 1e9)),
            "endTimeUnixNano": str(int((time.time() + random.uniform(0.01, 2.0)) * 1e9)),
        }
    
    def _add_error_scenario(self, span: Dict[str, Any]) -> None:
        """æ·»åŠ é”™è¯¯åœºæ™¯"""
        span["status"] = {
            "code": 2,  # ERROR
            "message": random.choice([
                "Connection timeout",
                "Internal server error",
                "Resource not found",
            ])
        }
        span["events"] = [{
            "name": "exception",
            "timeUnixNano": span["startTimeUnixNano"],
            "attributes": [
                {"key": "exception.type", "value": {"stringValue": "TimeoutError"}},
                {"key": "exception.message", "value": {"stringValue": "Request timed out after 30s"}},
                {"key": "exception.stacktrace", "value": {"stringValue": self._generate_stacktrace()}},
            ]
        }]
    
    def _add_unicode_characters(self, span: Dict[str, Any]) -> None:
        """æ·»åŠ  Unicode å­—ç¬¦ (æµ‹è¯•ç¼–ç å¤„ç†)"""
        span["name"] = "ç”¨æˆ·æŸ¥è¯¢ ğŸ”"
        span["attributes"] = [
            {"key": "user.name", "value": {"stringValue": "å¼ ä¸‰"}},
            {"key": "emoji", "value": {"stringValue": "ğŸ‘ğŸ˜€ğŸš€"}},
        ]
    
    def _add_many_attributes(self, span: Dict[str, Any]) -> None:
        """æ·»åŠ å¤§é‡å±æ€§ (æµ‹è¯•å®¹é‡é™åˆ¶)"""
        span["attributes"] = [
            {"key": f"attr_{i}", "value": {"stringValue": f"value_{i}"}}
            for i in range(100)
        ]
    
    def _random_hex(self, length: int) -> str:
        """ç”Ÿæˆéšæœºåå…­è¿›åˆ¶å­—ç¬¦ä¸²"""
        return ''.join(random.choices('0123456789abcdef', k=length))
    
    def _generate_stacktrace(self) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå †æ ˆè·Ÿè¸ª"""
        return "\n".join([
            f"  File '/app/service.py', line {random.randint(10, 500)}, in process_request"
            for _ in range(random.randint(3, 10))
        ])
```

#### é™·é˜± 3: æµ‹è¯•æ•°æ®æ±¡æŸ“

**é—®é¢˜**:

- æµ‹è¯•ä¹‹é—´å…±äº«çŠ¶æ€
- å‰ä¸€ä¸ªæµ‹è¯•å¤±è´¥å¯¼è‡´åç»­æµ‹è¯•å—å½±å“

**è§£å†³æ–¹æ¡ˆ**:

```python
# test_isolation.py - æµ‹è¯•éš”ç¦»æœ€ä½³å®è·µ
import pytest
from typing import Generator

class TestWithProperIsolation:
    """æµ‹è¯•éš”ç¦»æœ€ä½³å®è·µ"""
    
    @pytest.fixture(autouse=True)
    def setup_and_teardown(self) -> Generator:
        """æ¯ä¸ªæµ‹è¯•å‰åè‡ªåŠ¨æ¸…ç†"""
        # Setup: åˆ›å»ºéš”ç¦»çš„æµ‹è¯•æ•°æ®
        self.test_id = uuid.uuid4().hex
        self.trace_ids = []
        
        yield
        
        # Teardown: æ¸…ç†æµ‹è¯•æ•°æ®
        self._cleanup_test_data()
    
    def _cleanup_test_data(self) -> None:
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        for trace_id in self.trace_ids:
            # æ¸…ç†è¿½è¸ªæ•°æ® (å¦‚æœåç«¯æ”¯æŒ)
            try:
                requests.delete(f"http://backend/traces/{trace_id}")
            except:
                pass
    
    def test_isolated_1(self):
        """éš”ç¦»çš„æµ‹è¯• 1"""
        trace_id = self._send_test_trace(f"test-1-{self.test_id}")
        self.trace_ids.append(trace_id)
        # ... æµ‹è¯•é€»è¾‘ ...
    
    def test_isolated_2(self):
        """éš”ç¦»çš„æµ‹è¯• 2"""
        trace_id = self._send_test_trace(f"test-2-{self.test_id}")
        self.trace_ids.append(trace_id)
        # ... æµ‹è¯•é€»è¾‘ ...
```

### 9.3 æµ‹è¯•ç­–ç•¥æ¼”è¿›è·¯å¾„

#### é˜¶æ®µ 1: åŸºç¡€æµ‹è¯• (0-3 ä¸ªæœˆ)

**ç›®æ ‡**: å»ºç«‹åŸºæœ¬æµ‹è¯•è¦†ç›–

- âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 70%
- âœ… å…³é”®è·¯å¾„çš„é›†æˆæµ‹è¯•
- âœ… åŸºæœ¬çš„ CI é›†æˆ

**å·¥å…·**:

- pytest / JUnit / Go testing
- Docker Compose

#### é˜¶æ®µ 2: è‡ªåŠ¨åŒ–å¢å¼º (3-6 ä¸ªæœˆ)

**ç›®æ ‡**: æå‡æµ‹è¯•è‡ªåŠ¨åŒ–å’Œè¦†ç›–ç‡

- âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
- âœ… å®Œæ•´çš„é›†æˆæµ‹è¯•å¥—ä»¶
- âœ… åˆæ­¥çš„æ€§èƒ½åŸºå‡†æµ‹è¯•
- âœ… è‡ªåŠ¨åŒ– E2E æµ‹è¯•

**å·¥å…·**:

- Testcontainers
- K6 / JMeter
- Playwright

#### é˜¶æ®µ 3: ç”Ÿäº§çº§æµ‹è¯• (6-12 ä¸ªæœˆ)

**ç›®æ ‡**: è¾¾åˆ°ç”Ÿäº§çº§æµ‹è¯•æ ‡å‡†

- âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 85%
- âœ… æ··æ²Œå·¥ç¨‹æµ‹è¯•
- âœ… æ€§èƒ½å›å½’æ£€æµ‹
- âœ… å®‰å…¨æ‰«æé›†æˆ
- âœ… å½±å­æµé‡æµ‹è¯•

**å·¥å…·**:

- Chaos Mesh
- æ€§èƒ½å›å½’åˆ†ææ¡†æ¶
- Trivy / Snyk

#### é˜¶æ®µ 4: æŒç»­ä¼˜åŒ– (12+ ä¸ªæœˆ)

**ç›®æ ‡**: æŒç»­æ”¹è¿›æµ‹è¯•æ•ˆç‡å’Œè´¨é‡

- âœ… æ™ºèƒ½æµ‹è¯•é€‰æ‹© (åªè¿è¡Œç›¸å…³æµ‹è¯•)
- âœ… æµ‹è¯•å¹¶è¡ŒåŒ–ä¼˜åŒ–
- âœ… AI è¾…åŠ©æµ‹è¯•ç”Ÿæˆ
- âœ… ç”Ÿäº§ç¯å¢ƒæµ‹è¯•éªŒè¯

**å·¥å…·**:

- è‡ªå®šä¹‰æµ‹è¯•æ¡†æ¶
- AI/ML å·¥å…·

### 9.4 ROI ä¸è´¨é‡åº¦é‡

#### æµ‹è¯•æŠ•èµ„å›æŠ¥ç‡è®¡ç®—

**æˆæœ¬**:

- æµ‹è¯•åŸºç¡€è®¾æ–½: $5,000/æœˆ
- æµ‹è¯•å·¥å…·è®¸å¯è¯: $2,000/æœˆ
- æµ‹è¯•ç»´æŠ¤æ—¶é—´: 2 å·¥ç¨‹å¸ˆ Ã— 20% = 0.4 FTE Ã— $120,000/å¹´ = $48,000/å¹´

**æ€»å¹´æˆæœ¬**: $5,000Ã—12 + $2,000Ã—12 + $48,000 = **$132,000/å¹´**

**æ”¶ç›Š**:

- ç”Ÿäº§æ•…éšœå‡å°‘ 70%: èŠ‚çœ $250,000/å¹´ (æ•…éšœæˆæœ¬)
- å‘å¸ƒé€Ÿåº¦æå‡ 3 å€: å¢åŠ  $150,000/å¹´ (ä¸šåŠ¡ä»·å€¼)
- å®¢æˆ·æ»¡æ„åº¦æå‡: $100,000/å¹´ (å®¢æˆ·ç•™å­˜)

**æ€»å¹´æ”¶ç›Š**: **$500,000/å¹´**

**ROI**: ($500,000 - $132,000) / $132,000 = **278%**

#### è´¨é‡åº¦é‡æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å½“å‰å€¼ | è¶‹åŠ¿ |
|------|--------|--------|------|
| **ä»£ç è¦†ç›–ç‡** | > 85% | 87% | âœ… è¾¾æ ‡ |
| **P0 Bug é€ƒé€¸ç‡** | < 1% | 0.5% | âœ… è¾¾æ ‡ |
| **æµ‹è¯•æ‰§è¡Œæ—¶é—´** | < 30min | 25min | âœ… è¾¾æ ‡ |
| **æ€§èƒ½å›å½’æ£€æµ‹ç‡** | 100% | 100% | âœ… è¾¾æ ‡ |
| **ç”Ÿäº§æ•…éšœç‡** | < 0.1% | 0.08% | âœ… è¾¾æ ‡ |
| **MTTR (å¹³å‡ä¿®å¤æ—¶é—´)** | < 1h | 45min | âœ… è¾¾æ ‡ |

---

## 10. æ€»ç»“ä¸å±•æœ›

### 10.1 æ ¸å¿ƒè¦ç‚¹

1. **åˆ†å±‚æµ‹è¯•ç­–ç•¥**: å•å…ƒæµ‹è¯•ä¸ºåŸºç¡€,é›†æˆ/E2E æµ‹è¯•ä¸ºä¿éšœ
2. **è‡ªåŠ¨åŒ–ä¼˜å…ˆ**: CI/CD é›†æˆ,å¿«é€Ÿåé¦ˆ
3. **çœŸå®ç¯å¢ƒæ¨¡æ‹Ÿ**: ä½¿ç”¨ Testcontainers ç­‰å·¥å…·
4. **æŒç»­ä¼˜åŒ–**: æ€§èƒ½åŸºå‡†ã€æ··æ²Œå·¥ç¨‹ã€å®‰å…¨æ‰«æ

### 10.2 æœªæ¥è¶‹åŠ¿

- **AI è¾…åŠ©æµ‹è¯•**: è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ã€æ™ºèƒ½æ•…éšœå®šä½
- **äº‘åŸç”Ÿæµ‹è¯•**: Kubernetes-native æµ‹è¯•æ¡†æ¶
- **eBPF æµ‹è¯•**: é›¶ä¾µå…¥çš„ç”Ÿäº§ç¯å¢ƒéªŒè¯
- **æ•°å­—å­ªç”Ÿ**: å®Œæ•´çš„ç”Ÿäº§ç¯å¢ƒå…‹éš†ç”¨äºæµ‹è¯•

---

## ç›¸å…³æ–‡æ¡£

- [ğŸ¤–_OTLPè‡ªä¸»è¿ç»´èƒ½åŠ›å®Œæ•´æ¶æ„_AIOpså¹³å°è®¾è®¡.md](./ğŸ¤–_OTLPè‡ªä¸»è¿ç»´èƒ½åŠ›å®Œæ•´æ¶æ„_AIOpså¹³å°è®¾è®¡.md) - AIOps å¹³å°çš„è‡ªåŠ¨åŒ–æµ‹è¯•ç­–ç•¥
- [ğŸ› ï¸_äº¤äº’å¼é…ç½®ç”Ÿæˆå™¨_OTLP_Collectoré…ç½®å‘å¯¼.md](./ğŸ› ï¸_äº¤äº’å¼é…ç½®ç”Ÿæˆå™¨_OTLP_Collectoré…ç½®å‘å¯¼.md) - é…ç½®ç”Ÿæˆå™¨çš„æµ‹è¯•éªŒè¯
- [ğŸ“š_OTLP_SDKæœ€ä½³å®è·µæŒ‡å—_å¤šè¯­è¨€å…¨æ ˆå®ç°.md](./ğŸ“š_OTLP_SDKæœ€ä½³å®è·µæŒ‡å—_å¤šè¯­è¨€å…¨æ ˆå®ç°.md) - SDK å¼€å‘çš„å•å…ƒæµ‹è¯•æœ€ä½³å®è·µ
- [ğŸ“Š_æ¶æ„å›¾è¡¨ä¸å¯è§†åŒ–æŒ‡å—_Mermaidå®Œæ•´ç‰ˆ.md](./ğŸ“Š_æ¶æ„å›¾è¡¨ä¸å¯è§†åŒ–æŒ‡å—_Mermaidå®Œæ•´ç‰ˆ.md) - æµ‹è¯•æ¶æ„ä¸æµç¨‹å¯è§†åŒ–
- [ğŸ_eBPFå¯è§‚æµ‹æ€§æ·±åº¦æŠ€æœ¯æŒ‡å—_é›¶ä¾µå…¥å¼è¿½è¸ª.md](./ğŸ_eBPFå¯è§‚æµ‹æ€§æ·±åº¦æŠ€æœ¯æŒ‡å—_é›¶ä¾µå…¥å¼è¿½è¸ª.md) - eBPF ç»„ä»¶çš„æµ‹è¯•ç­–ç•¥
- [ğŸ•¸ï¸_æœåŠ¡ç½‘æ ¼å¯è§‚æµ‹æ€§å®Œæ•´æŒ‡å—_Istio_Linkerdæ·±åº¦é›†æˆ.md](./ğŸ•¸ï¸_æœåŠ¡ç½‘æ ¼å¯è§‚æµ‹æ€§å®Œæ•´æŒ‡å—_Istio_Linkerdæ·±åº¦é›†æˆ.md) - Service Mesh é›†æˆæµ‹è¯•

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-09  
**è´¡çŒ®è€…**: OTLP æµ‹è¯•æ¡†æ¶å·¥ä½œç»„
