# æ—¥å¿—å…¨æ–‡æœç´¢ - Rust å®Œæ•´å®ç°

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
> **Rust ç‰ˆæœ¬**: 1.90+  
> **æœ€åæ›´æ–°**: 2025-10-10

---

## ğŸ“‹ ç›®å½•

- [æ—¥å¿—å…¨æ–‡æœç´¢ - Rust å®Œæ•´å®ç°](#æ—¥å¿—å…¨æ–‡æœç´¢---rust-å®Œæ•´å®ç°)
  - [ğŸ“‹ ç›®å½•](#-ç›®å½•)
  - [ğŸ¯ æ¦‚è¿°](#-æ¦‚è¿°)
    - [æ ¸å¿ƒåŠŸèƒ½](#æ ¸å¿ƒåŠŸèƒ½)
    - [åº”ç”¨åœºæ™¯](#åº”ç”¨åœºæ™¯)
  - [ğŸ” å…¨æ–‡ç´¢å¼•](#-å…¨æ–‡ç´¢å¼•)
    - [1. å€’æ’ç´¢å¼•æ„å»º](#1-å€’æ’ç´¢å¼•æ„å»º)
    - [2. åˆ†è¯å™¨](#2-åˆ†è¯å™¨)
    - [3. ç´¢å¼•ä¼˜åŒ–](#3-ç´¢å¼•ä¼˜åŒ–)
  - [ğŸ” æœç´¢æŸ¥è¯¢](#-æœç´¢æŸ¥è¯¢)
    - [1. å…³é”®è¯æœç´¢](#1-å…³é”®è¯æœç´¢)
    - [2. çŸ­è¯­æœç´¢](#2-çŸ­è¯­æœç´¢)
    - [3. æ¨¡ç³Šæœç´¢](#3-æ¨¡ç³Šæœç´¢)
  - [ğŸ“ æ­£åˆ™è¡¨è¾¾å¼æœç´¢](#-æ­£åˆ™è¡¨è¾¾å¼æœç´¢)
    - [1. æ­£åˆ™æŸ¥è¯¢å¼•æ“](#1-æ­£åˆ™æŸ¥è¯¢å¼•æ“)
    - [2. æ­£åˆ™ä¼˜åŒ–](#2-æ­£åˆ™ä¼˜åŒ–)
  - [ğŸ¯ é«˜çº§æŸ¥è¯¢](#-é«˜çº§æŸ¥è¯¢)
    - [1. å¸ƒå°”æŸ¥è¯¢](#1-å¸ƒå°”æŸ¥è¯¢)
    - [2. èŒƒå›´æŸ¥è¯¢](#2-èŒƒå›´æŸ¥è¯¢)
    - [3. èšåˆæŸ¥è¯¢](#3-èšåˆæŸ¥è¯¢)
  - [ğŸ“Š æ—¥å¿—åˆ†æ](#-æ—¥å¿—åˆ†æ)
    - [1. æ—¥å¿—æ¨¡å¼è¯†åˆ«](#1-æ—¥å¿—æ¨¡å¼è¯†åˆ«)
    - [2. å¼‚å¸¸æ£€æµ‹](#2-å¼‚å¸¸æ£€æµ‹)
  - [âš¡ æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
    - [1. æŸ¥è¯¢ç¼“å­˜](#1-æŸ¥è¯¢ç¼“å­˜)
    - [2. åˆ†ç‰‡æœç´¢](#2-åˆ†ç‰‡æœç´¢)
  - [ğŸ“Š å®Œæ•´ç¤ºä¾‹ï¼šæ—¥å¿—æœç´¢ç³»ç»Ÿ](#-å®Œæ•´ç¤ºä¾‹æ—¥å¿—æœç´¢ç³»ç»Ÿ)
  - [ğŸ¯ æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)
    - [ç´¢å¼•ä¼˜åŒ–](#ç´¢å¼•ä¼˜åŒ–)
    - [æŸ¥è¯¢ä¼˜åŒ–](#æŸ¥è¯¢ä¼˜åŒ–)
    - [å­˜å‚¨ä¼˜åŒ–](#å­˜å‚¨ä¼˜åŒ–)
  - [ğŸ“š å‚è€ƒèµ„æº](#-å‚è€ƒèµ„æº)

---

## ğŸ¯ æ¦‚è¿°

æ—¥å¿—å…¨æ–‡æœç´¢æ˜¯ OTLP Logs ç³»ç»Ÿçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œé€šè¿‡é«˜æ•ˆçš„å…¨æ–‡ç´¢å¼•å’Œçµæ´»çš„æŸ¥è¯¢è¯­è¨€ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿå®šä½å’Œåˆ†ææµ·é‡æ—¥å¿—æ•°æ®ã€‚

### æ ¸å¿ƒåŠŸèƒ½

- âœ… **å…¨æ–‡ç´¢å¼•** - å€’æ’ç´¢å¼•ã€N-Gramã€å‰ç¼€ç´¢å¼•
- âœ… **å…³é”®è¯æœç´¢** - ç²¾ç¡®åŒ¹é…ã€æ¨¡ç³ŠåŒ¹é…
- âœ… **æ­£åˆ™è¡¨è¾¾å¼** - å¤æ‚æ¨¡å¼åŒ¹é…
- âœ… **å¸ƒå°”æŸ¥è¯¢** - AND/OR/NOT ç»„åˆæŸ¥è¯¢
- âœ… **æ—¥å¿—åˆ†æ** - æ¨¡å¼è¯†åˆ«ã€å¼‚å¸¸æ£€æµ‹
- âœ… **æ€§èƒ½ä¼˜åŒ–** - æŸ¥è¯¢ç¼“å­˜ã€åˆ†ç‰‡æœç´¢

### åº”ç”¨åœºæ™¯

- ğŸ”§ **æ•…éšœæ’æŸ¥** - å¿«é€Ÿå®šä½é”™è¯¯æ—¥å¿—
- ğŸ”§ **å®‰å…¨å®¡è®¡** - å®‰å…¨äº‹ä»¶æ£€ç´¢
- ğŸ”§ **ä¸šåŠ¡åˆ†æ** - ç”¨æˆ·è¡Œä¸ºæ—¥å¿—åˆ†æ
- ğŸ”§ **æ€§èƒ½è°ƒä¼˜** - æ…¢æŸ¥è¯¢æ—¥å¿—åˆ†æ

---

## ğŸ” å…¨æ–‡ç´¢å¼•

### 1. å€’æ’ç´¢å¼•æ„å»º

```rust
use std::sync::Arc;
use std::collections::{HashMap, HashSet};
use tokio::sync::RwLock;
use chrono::{DateTime, Utc};

/// å€’æ’ç´¢å¼•æ„å»ºå™¨
pub struct InvertedIndexBuilder {
    index: Arc<RwLock<HashMap<String, PostingList>>>,
    tokenizer: Arc<Tokenizer>,
}

impl InvertedIndexBuilder {
    pub fn new(tokenizer: Arc<Tokenizer>) -> Self {
        Self {
            index: Arc::new(RwLock::new(HashMap::new())),
            tokenizer,
        }
    }
    
    /// ç´¢å¼•å•æ¡æ—¥å¿—
    pub async fn index_log(&self, log: &LogRecord) {
        // åˆ†è¯
        let tokens = self.tokenizer.tokenize(&log.message);
        
        let mut index = self.index.write().await;
        
        // æ„å»ºå€’æ’ç´¢å¼•
        for (position, token) in tokens.iter().enumerate() {
            let posting_list = index
                .entry(token.clone())
                .or_insert_with(PostingList::new);
            
            posting_list.add_posting(Posting {
                log_id: log.id.clone(),
                position,
                timestamp: log.timestamp,
            });
        }
    }
    
    /// æ‰¹é‡ç´¢å¼•
    pub async fn index_logs_batch(&self, logs: Vec<LogRecord>) {
        for log in logs {
            self.index_log(&log).await;
        }
    }
    
    /// æœç´¢
    pub async fn search(&self, query: &str) -> Vec<String> {
        let tokens = self.tokenizer.tokenize(query);
        
        if tokens.is_empty() {
            return Vec::new();
        }
        
        let index = self.index.read().await;
        
        // è·å–ç¬¬ä¸€ä¸ªè¯çš„posting list
        let mut result_set: Option<HashSet<String>> = None;
        
        for token in &tokens {
            if let Some(posting_list) = index.get(token) {
                let log_ids: HashSet<String> = posting_list
                    .postings
                    .iter()
                    .map(|p| p.log_id.clone())
                    .collect();
                
                result_set = match result_set {
                    Some(existing) => {
                        // è®¡ç®—äº¤é›†ï¼ˆAND è¯­ä¹‰ï¼‰
                        Some(existing.intersection(&log_ids).cloned().collect())
                    }
                    None => Some(log_ids),
                };
            } else {
                // è¯ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºç»“æœ
                return Vec::new();
            }
        }
        
        result_set.unwrap_or_default().into_iter().collect()
    }
}

#[derive(Debug, Clone)]
pub struct LogRecord {
    pub id: String,
    pub timestamp: DateTime<Utc>,
    pub level: LogLevel,
    pub message: String,
    pub attributes: HashMap<String, String>,
}

#[derive(Debug, Clone, PartialEq)]
pub enum LogLevel {
    Trace,
    Debug,
    Info,
    Warn,
    Error,
}

#[derive(Debug, Clone)]
pub struct PostingList {
    pub postings: Vec<Posting>,
}

impl PostingList {
    fn new() -> Self {
        Self {
            postings: Vec::new(),
        }
    }
    
    fn add_posting(&mut self, posting: Posting) {
        self.postings.push(posting);
    }
}

#[derive(Debug, Clone)]
pub struct Posting {
    pub log_id: String,
    pub position: usize,
    pub timestamp: DateTime<Utc>,
}
```

---

### 2. åˆ†è¯å™¨

```rust
/// åˆ†è¯å™¨
pub struct Tokenizer {
    stop_words: HashSet<String>,
}

impl Tokenizer {
    pub fn new() -> Self {
        let stop_words: HashSet<String> = [
            "the", "a", "an", "and", "or", "but", "is", "are", "was", "were",
            "in", "on", "at", "to", "from", "with", "by", "for",
        ]
        .iter()
        .map(|s| s.to_string())
        .collect();
        
        Self { stop_words }
    }
    
    /// åˆ†è¯
    pub fn tokenize(&self, text: &str) -> Vec<String> {
        text.to_lowercase()
            .split(|c: char| !c.is_alphanumeric())
            .filter(|s| !s.is_empty())
            .filter(|s| !self.stop_words.contains(*s))
            .map(|s| s.to_string())
            .collect()
    }
    
    /// å¸¦ä½ç½®çš„åˆ†è¯
    pub fn tokenize_with_positions(&self, text: &str) -> Vec<(String, usize)> {
        text.to_lowercase()
            .split(|c: char| !c.is_alphanumeric())
            .filter(|s| !s.is_empty())
            .filter(|s| !self.stop_words.contains(*s))
            .enumerate()
            .map(|(pos, s)| (s.to_string(), pos))
            .collect()
    }
    
    /// N-Gram åˆ†è¯
    pub fn ngram_tokenize(&self, text: &str, n: usize) -> Vec<String> {
        let chars: Vec<char> = text.chars().collect();
        
        if chars.len() < n {
            return vec![text.to_string()];
        }
        
        (0..=chars.len() - n)
            .map(|i| chars[i..i + n].iter().collect())
            .collect()
    }
}
```

---

### 3. ç´¢å¼•ä¼˜åŒ–

```rust
/// ç´¢å¼•å‹ç¼©å™¨ï¼ˆå‰ç«¯ç¼–ç ï¼‰
pub struct IndexCompressor;

impl IndexCompressor {
    /// å‹ç¼© posting listï¼ˆå¢é‡ç¼–ç ï¼‰
    pub fn compress_postings(postings: &[Posting]) -> Vec<u8> {
        if postings.is_empty() {
            return Vec::new();
        }
        
        let mut compressed = Vec::new();
        
        // ç¬¬ä¸€ä¸ªæ–‡æ¡£ID
        compressed.extend_from_slice(&postings[0].log_id.as_bytes());
        compressed.push(b'\0'); // åˆ†éš”ç¬¦
        
        // åç»­æ–‡æ¡£ä½¿ç”¨å¢é‡ç¼–ç ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        for i in 1..postings.len() {
            compressed.extend_from_slice(&postings[i].log_id.as_bytes());
            compressed.push(b'\0');
        }
        
        compressed
    }
    
    /// è§£å‹ posting list
    pub fn decompress_postings(compressed: &[u8]) -> Vec<String> {
        compressed
            .split(|&b| b == b'\0')
            .filter(|s| !s.is_empty())
            .map(|s| String::from_utf8_lossy(s).to_string())
            .collect()
    }
}

/// ç´¢å¼•åˆå¹¶å™¨
pub struct IndexMerger;

impl IndexMerger {
    /// åˆå¹¶å¤šä¸ªç´¢å¼•
    pub fn merge_indices(
        indices: Vec<HashMap<String, PostingList>>,
    ) -> HashMap<String, PostingList> {
        let mut merged = HashMap::new();
        
        for index in indices {
            for (term, posting_list) in index {
                merged
                    .entry(term)
                    .or_insert_with(PostingList::new)
                    .postings
                    .extend(posting_list.postings);
            }
        }
        
        // æ’åºæ¯ä¸ªposting list
        for posting_list in merged.values_mut() {
            posting_list.postings.sort_by_key(|p| p.timestamp);
        }
        
        merged
    }
}
```

---

## ğŸ” æœç´¢æŸ¥è¯¢

### 1. å…³é”®è¯æœç´¢

```rust
/// å…³é”®è¯æœç´¢å¼•æ“
pub struct KeywordSearchEngine {
    index_builder: Arc<InvertedIndexBuilder>,
}

impl KeywordSearchEngine {
    pub fn new(index_builder: Arc<InvertedIndexBuilder>) -> Self {
        Self { index_builder }
    }
    
    /// ç²¾ç¡®æœç´¢
    pub async fn exact_search(&self, keyword: &str) -> Vec<String> {
        self.index_builder.search(keyword).await
    }
    
    /// å‰ç¼€æœç´¢
    pub async fn prefix_search(&self, prefix: &str) -> Vec<String> {
        let index = self.index_builder.index.read().await;
        
        let mut result_set = HashSet::new();
        
        for (term, posting_list) in index.iter() {
            if term.starts_with(prefix) {
                for posting in &posting_list.postings {
                    result_set.insert(posting.log_id.clone());
                }
            }
        }
        
        result_set.into_iter().collect()
    }
    
    /// é€šé…ç¬¦æœç´¢
    pub async fn wildcard_search(&self, pattern: &str) -> Vec<String> {
        let index = self.index_builder.index.read().await;
        
        let mut result_set = HashSet::new();
        
        // å°†é€šé…ç¬¦è½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼
        let regex_pattern = pattern.replace('*', ".*").replace('?', ".");
        let regex = regex::Regex::new(&regex_pattern).unwrap();
        
        for (term, posting_list) in index.iter() {
            if regex.is_match(term) {
                for posting in &posting_list.postings {
                    result_set.insert(posting.log_id.clone());
                }
            }
        }
        
        result_set.into_iter().collect()
    }
}
```

---

### 2. çŸ­è¯­æœç´¢

```rust
/// çŸ­è¯­æœç´¢å¼•æ“
pub struct PhraseSearchEngine {
    index_builder: Arc<InvertedIndexBuilder>,
}

impl PhraseSearchEngine {
    pub fn new(index_builder: Arc<InvertedIndexBuilder>) -> Self {
        Self { index_builder }
    }
    
    /// çŸ­è¯­æœç´¢ï¼ˆç²¾ç¡®ä½ç½®åŒ¹é…ï¼‰
    pub async fn phrase_search(&self, phrase: &str) -> Vec<String> {
        let tokens = self.index_builder.tokenizer.tokenize(phrase);
        
        if tokens.is_empty() {
            return Vec::new();
        }
        
        let index = self.index_builder.index.read().await;
        
        // è·å–ç¬¬ä¸€ä¸ªè¯çš„ posting list
        let first_postings = match index.get(&tokens[0]) {
            Some(list) => &list.postings,
            None => return Vec::new(),
        };
        
        let mut result_set = Vec::new();
        
        // å¯¹æ¯ä¸ªæ–‡æ¡£ï¼Œæ£€æŸ¥åç»­è¯æ˜¯å¦åœ¨è¿ç»­ä½ç½®
        for posting in first_postings {
            let mut all_found = true;
            
            for (i, token) in tokens.iter().enumerate().skip(1) {
                if let Some(posting_list) = index.get(token) {
                    // æ£€æŸ¥æ˜¯å¦æœ‰åœ¨æ­£ç¡®ä½ç½®çš„ posting
                    let found = posting_list.postings.iter().any(|p| {
                        p.log_id == posting.log_id && p.position == posting.position + i
                    });
                    
                    if !found {
                        all_found = false;
                        break;
                    }
                } else {
                    all_found = false;
                    break;
                }
            }
            
            if all_found {
                result_set.push(posting.log_id.clone());
            }
        }
        
        result_set
    }
}
```

---

### 3. æ¨¡ç³Šæœç´¢

```rust
/// æ¨¡ç³Šæœç´¢å¼•æ“ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰
pub struct FuzzySearchEngine {
    index_builder: Arc<InvertedIndexBuilder>,
    max_distance: usize,
}

impl FuzzySearchEngine {
    pub fn new(index_builder: Arc<InvertedIndexBuilder>, max_distance: usize) -> Self {
        Self {
            index_builder,
            max_distance,
        }
    }
    
    /// æ¨¡ç³Šæœç´¢
    pub async fn fuzzy_search(&self, query: &str) -> Vec<String> {
        let index = self.index_builder.index.read().await;
        
        let mut result_set = HashSet::new();
        
        // æŸ¥æ‰¾ç¼–è¾‘è·ç¦»åœ¨é˜ˆå€¼å†…çš„æ‰€æœ‰è¯
        for (term, posting_list) in index.iter() {
            let distance = Self::levenshtein_distance(query, term);
            
            if distance <= self.max_distance {
                for posting in &posting_list.postings {
                    result_set.insert(posting.log_id.clone());
                }
            }
        }
        
        result_set.into_iter().collect()
    }
    
    /// Levenshtein ç¼–è¾‘è·ç¦»
    fn levenshtein_distance(s1: &str, s2: &str) -> usize {
        let len1 = s1.len();
        let len2 = s2.len();
        
        let mut matrix = vec![vec![0; len2 + 1]; len1 + 1];
        
        for i in 0..=len1 {
            matrix[i][0] = i;
        }
        
        for j in 0..=len2 {
            matrix[0][j] = j;
        }
        
        let s1_chars: Vec<char> = s1.chars().collect();
        let s2_chars: Vec<char> = s2.chars().collect();
        
        for i in 1..=len1 {
            for j in 1..=len2 {
                let cost = if s1_chars[i - 1] == s2_chars[j - 1] {
                    0
                } else {
                    1
                };
                
                matrix[i][j] = (matrix[i - 1][j] + 1)
                    .min(matrix[i][j - 1] + 1)
                    .min(matrix[i - 1][j - 1] + cost);
            }
        }
        
        matrix[len1][len2]
    }
}
```

---

## ğŸ“ æ­£åˆ™è¡¨è¾¾å¼æœç´¢

### 1. æ­£åˆ™æŸ¥è¯¢å¼•æ“

```rust
use regex::Regex;

/// æ­£åˆ™æŸ¥è¯¢å¼•æ“
pub struct RegexQueryEngine {
    log_storage: Arc<LogStorage>,
}

impl RegexQueryEngine {
    pub fn new(log_storage: Arc<LogStorage>) -> Self {
        Self { log_storage }
    }
    
    /// æ­£åˆ™æœç´¢
    pub async fn regex_search(&self, pattern: &str) -> Result<Vec<LogRecord>, Box<dyn std::error::Error>> {
        let regex = Regex::new(pattern)?;
        
        let all_logs = self.log_storage.get_all_logs().await;
        
        let matching_logs: Vec<LogRecord> = all_logs
            .into_iter()
            .filter(|log| regex.is_match(&log.message))
            .collect();
        
        Ok(matching_logs)
    }
    
    /// æ­£åˆ™æœç´¢ï¼ˆå¹¶è¡Œï¼‰
    pub async fn regex_search_parallel(
        &self,
        pattern: &str,
    ) -> Result<Vec<LogRecord>, Box<dyn std::error::Error>> {
        let regex = Arc::new(Regex::new(pattern)?);
        
        let all_logs = self.log_storage.get_all_logs().await;
        
        let chunk_size = (all_logs.len() + 7) / 8; // 8 ä¸ªçº¿ç¨‹
        let mut handles = Vec::new();
        
        for chunk in all_logs.chunks(chunk_size) {
            let regex = regex.clone();
            let chunk = chunk.to_vec();
            
            let handle = tokio::spawn(async move {
                chunk
                    .into_iter()
                    .filter(|log| regex.is_match(&log.message))
                    .collect::<Vec<LogRecord>>()
            });
            
            handles.push(handle);
        }
        
        let mut results = Vec::new();
        
        for handle in handles {
            if let Ok(chunk_results) = handle.await {
                results.extend(chunk_results);
            }
        }
        
        Ok(results)
    }
}
```

---

### 2. æ­£åˆ™ä¼˜åŒ–

```rust
/// æ­£åˆ™æŸ¥è¯¢ä¼˜åŒ–å™¨
pub struct RegexOptimizer;

impl RegexOptimizer {
    /// æå–æ­£åˆ™ä¸­çš„å›ºå®šå‰ç¼€
    pub fn extract_prefix(pattern: &str) -> Option<String> {
        let mut prefix = String::new();
        
        for ch in pattern.chars() {
            if ch.is_alphanumeric() || ch == '_' {
                prefix.push(ch);
            } else {
                break;
            }
        }
        
        if prefix.is_empty() {
            None
        } else {
            Some(prefix)
        }
    }
    
    /// ä¼˜åŒ–æ­£åˆ™æŸ¥è¯¢ï¼ˆå…ˆç”¨å‰ç¼€ç´¢å¼•è¿‡æ»¤ï¼‰
    pub async fn optimized_regex_search(
        &self,
        pattern: &str,
        index_builder: Arc<InvertedIndexBuilder>,
        log_storage: Arc<LogStorage>,
    ) -> Result<Vec<LogRecord>, Box<dyn std::error::Error>> {
        // æå–å›ºå®šå‰ç¼€
        if let Some(prefix) = Self::extract_prefix(pattern) {
            // ä½¿ç”¨å‰ç¼€ç´¢å¼•é¢„è¿‡æ»¤
            let candidate_ids = KeywordSearchEngine::new(index_builder)
                .prefix_search(&prefix)
                .await;
            
            // åªå¯¹å€™é€‰æ—¥å¿—åº”ç”¨æ­£åˆ™
            let regex = Regex::new(pattern)?;
            
            let mut results = Vec::new();
            
            for log_id in candidate_ids {
                if let Some(log) = log_storage.get_log(&log_id).await {
                    if regex.is_match(&log.message) {
                        results.push(log);
                    }
                }
            }
            
            Ok(results)
        } else {
            // æ— æ³•ä¼˜åŒ–ï¼Œå…¨é‡æ‰«æ
            let regex_engine = RegexQueryEngine::new(log_storage);
            regex_engine.regex_search(pattern).await
        }
    }
}
```

---

## ğŸ¯ é«˜çº§æŸ¥è¯¢

### 1. å¸ƒå°”æŸ¥è¯¢

```rust
/// å¸ƒå°”æŸ¥è¯¢å¼•æ“
pub struct BooleanQueryEngine {
    index_builder: Arc<InvertedIndexBuilder>,
}

impl BooleanQueryEngine {
    pub fn new(index_builder: Arc<InvertedIndexBuilder>) -> Self {
        Self { index_builder }
    }
    
    /// AND æŸ¥è¯¢
    pub async fn and_query(&self, terms: Vec<String>) -> Vec<String> {
        if terms.is_empty() {
            return Vec::new();
        }
        
        let index = self.index_builder.index.read().await;
        
        // è·å–ç¬¬ä¸€ä¸ªè¯çš„ç»“æœ
        let mut result_set: HashSet<String> = match index.get(&terms[0]) {
            Some(posting_list) => posting_list
                .postings
                .iter()
                .map(|p| p.log_id.clone())
                .collect(),
            None => return Vec::new(),
        };
        
        // å¯¹åç»­è¯å–äº¤é›†
        for term in terms.iter().skip(1) {
            if let Some(posting_list) = index.get(term) {
                let term_set: HashSet<String> = posting_list
                    .postings
                    .iter()
                    .map(|p| p.log_id.clone())
                    .collect();
                
                result_set = result_set.intersection(&term_set).cloned().collect();
            } else {
                return Vec::new();
            }
        }
        
        result_set.into_iter().collect()
    }
    
    /// OR æŸ¥è¯¢
    pub async fn or_query(&self, terms: Vec<String>) -> Vec<String> {
        let index = self.index_builder.index.read().await;
        
        let mut result_set = HashSet::new();
        
        for term in terms {
            if let Some(posting_list) = index.get(&term) {
                for posting in &posting_list.postings {
                    result_set.insert(posting.log_id.clone());
                }
            }
        }
        
        result_set.into_iter().collect()
    }
    
    /// NOT æŸ¥è¯¢
    pub async fn not_query(&self, include: Vec<String>, exclude: Vec<String>) -> Vec<String> {
        let include_set: HashSet<String> = self.or_query(include).await.into_iter().collect();
        let exclude_set: HashSet<String> = self.or_query(exclude).await.into_iter().collect();
        
        include_set.difference(&exclude_set).cloned().collect()
    }
    
    /// å¤åˆæŸ¥è¯¢
    pub async fn complex_query(&self, query: BooleanQuery) -> Vec<String> {
        match query {
            BooleanQuery::And(queries) => {
                let mut results = Vec::new();
                
                for q in queries {
                    let sub_results = self.complex_query(*q).await;
                    results.push(sub_results);
                }
                
                // è®¡ç®—äº¤é›†
                if results.is_empty() {
                    return Vec::new();
                }
                
                let mut intersection: HashSet<String> = results[0].iter().cloned().collect();
                
                for result in results.iter().skip(1) {
                    let set: HashSet<String> = result.iter().cloned().collect();
                    intersection = intersection.intersection(&set).cloned().collect();
                }
                
                intersection.into_iter().collect()
            }
            BooleanQuery::Or(queries) => {
                let mut result_set = HashSet::new();
                
                for q in queries {
                    let sub_results = self.complex_query(*q).await;
                    result_set.extend(sub_results);
                }
                
                result_set.into_iter().collect()
            }
            BooleanQuery::Not(query) => {
                // ç®€åŒ–å®ç°
                Vec::new()
            }
            BooleanQuery::Term(term) => {
                self.and_query(vec![term]).await
            }
        }
    }
}

#[derive(Debug, Clone)]
pub enum BooleanQuery {
    And(Vec<Box<BooleanQuery>>),
    Or(Vec<Box<BooleanQuery>>),
    Not(Box<BooleanQuery>),
    Term(String),
}
```

---

### 2. èŒƒå›´æŸ¥è¯¢

```rust
/// èŒƒå›´æŸ¥è¯¢å¼•æ“
pub struct RangeQueryEngine {
    log_storage: Arc<LogStorage>,
}

impl RangeQueryEngine {
    pub fn new(log_storage: Arc<LogStorage>) -> Self {
        Self { log_storage }
    }
    
    /// æ—¶é—´èŒƒå›´æŸ¥è¯¢
    pub async fn time_range_query(
        &self,
        start: DateTime<Utc>,
        end: DateTime<Utc>,
    ) -> Vec<LogRecord> {
        let all_logs = self.log_storage.get_all_logs().await;
        
        all_logs
            .into_iter()
            .filter(|log| log.timestamp >= start && log.timestamp <= end)
            .collect()
    }
    
    /// çº§åˆ«è¿‡æ»¤
    pub async fn level_filter(
        &self,
        min_level: LogLevel,
    ) -> Vec<LogRecord> {
        let all_logs = self.log_storage.get_all_logs().await;
        
        all_logs
            .into_iter()
            .filter(|log| Self::level_ge(&log.level, &min_level))
            .collect()
    }
    
    fn level_ge(level: &LogLevel, min: &LogLevel) -> bool {
        let level_order = |l: &LogLevel| match l {
            LogLevel::Trace => 0,
            LogLevel::Debug => 1,
            LogLevel::Info => 2,
            LogLevel::Warn => 3,
            LogLevel::Error => 4,
        };
        
        level_order(level) >= level_order(min)
    }
}
```

---

### 3. èšåˆæŸ¥è¯¢

```rust
/// èšåˆæŸ¥è¯¢å¼•æ“
pub struct AggregationQueryEngine {
    log_storage: Arc<LogStorage>,
}

impl AggregationQueryEngine {
    pub fn new(log_storage: Arc<LogStorage>) -> Self {
        Self { log_storage }
    }
    
    /// æŒ‰çº§åˆ«èšåˆ
    pub async fn aggregate_by_level(&self) -> HashMap<LogLevel, usize> {
        let all_logs = self.log_storage.get_all_logs().await;
        
        let mut counts = HashMap::new();
        
        for log in all_logs {
            *counts.entry(log.level).or_insert(0) += 1;
        }
        
        counts
    }
    
    /// æŒ‰æ—¶é—´çª—å£èšåˆ
    pub async fn aggregate_by_time_window(
        &self,
        window_size: chrono::Duration,
    ) -> HashMap<DateTime<Utc>, usize> {
        let all_logs = self.log_storage.get_all_logs().await;
        
        let mut counts = HashMap::new();
        
        for log in all_logs {
            let window_key = log.timestamp.timestamp() / window_size.num_seconds();
            let window_start = DateTime::from_timestamp(window_key * window_size.num_seconds(), 0).unwrap();
            
            *counts.entry(window_start).or_insert(0) += 1;
        }
        
        counts
    }
    
    /// Top N é”™è¯¯æ¶ˆæ¯
    pub async fn top_errors(&self, n: usize) -> Vec<(String, usize)> {
        let all_logs = self.log_storage.get_all_logs().await;
        
        let mut error_counts = HashMap::new();
        
        for log in all_logs {
            if log.level == LogLevel::Error {
                *error_counts.entry(log.message.clone()).or_insert(0) += 1;
            }
        }
        
        let mut sorted: Vec<(String, usize)> = error_counts.into_iter().collect();
        sorted.sort_by(|a, b| b.1.cmp(&a.1));
        
        sorted.into_iter().take(n).collect()
    }
}
```

---

## ğŸ“Š æ—¥å¿—åˆ†æ

### 1. æ—¥å¿—æ¨¡å¼è¯†åˆ«

```rust
/// æ—¥å¿—æ¨¡å¼è¯†åˆ«å™¨
pub struct LogPatternRecognizer {
    patterns: Vec<LogPattern>,
}

impl LogPatternRecognizer {
    pub fn new() -> Self {
        Self {
            patterns: Vec::new(),
        }
    }
    
    /// è®­ç»ƒï¼ˆæå–å¸¸è§æ¨¡å¼ï¼‰
    pub fn train(&mut self, logs: Vec<LogRecord>) {
        // ç®€åŒ–ç‰ˆï¼šåŸºäºè¯é¢‘ç»Ÿè®¡
        let mut pattern_counts: HashMap<String, usize> = HashMap::new();
        
        for log in logs {
            // æå–æ¨¡å¼ï¼ˆç§»é™¤æ•°å­—å’Œå˜é‡ï¼‰
            let pattern = Self::extract_pattern(&log.message);
            *pattern_counts.entry(pattern).or_insert(0) += 1;
        }
        
        // ä¿ç•™é¢‘ç¹æ¨¡å¼
        self.patterns = pattern_counts
            .into_iter()
            .filter(|(_, count)| *count >= 10) // è‡³å°‘å‡ºç°10æ¬¡
            .map(|(pattern, count)| LogPattern { pattern, count })
            .collect();
        
        // æŒ‰é¢‘ç‡æ’åº
        self.patterns.sort_by(|a, b| b.count.cmp(&a.count));
    }
    
    /// æå–æ¨¡å¼ï¼ˆç§»é™¤å˜é‡éƒ¨åˆ†ï¼‰
    fn extract_pattern(message: &str) -> String {
        message
            .split_whitespace()
            .map(|word| {
                if word.chars().all(|c| c.is_numeric() || c == '.') {
                    "<NUM>"
                } else if word.starts_with('"') && word.ends_with('"') {
                    "<STR>"
                } else {
                    word
                }
            })
            .collect::<Vec<&str>>()
            .join(" ")
    }
    
    /// è¯†åˆ«æ—¥å¿—æ‰€å±æ¨¡å¼
    pub fn recognize(&self, log: &LogRecord) -> Option<usize> {
        let pattern = Self::extract_pattern(&log.message);
        
        self.patterns
            .iter()
            .position(|p| p.pattern == pattern)
    }
}

#[derive(Debug, Clone)]
pub struct LogPattern {
    pub pattern: String,
    pub count: usize,
}
```

---

### 2. å¼‚å¸¸æ£€æµ‹

```rust
/// å¼‚å¸¸æ£€æµ‹å™¨
pub struct AnomalyDetector {
    normal_patterns: HashSet<String>,
    threshold: f64,
}

impl AnomalyDetector {
    pub fn new(threshold: f64) -> Self {
        Self {
            normal_patterns: HashSet::new(),
            threshold,
        }
    }
    
    /// è®­ç»ƒæ­£å¸¸æ¨¡å¼
    pub fn train(&mut self, logs: Vec<LogRecord>) {
        let mut pattern_freq: HashMap<String, usize> = HashMap::new();
        
        for log in &logs {
            let pattern = Self::extract_pattern(&log.message);
            *pattern_freq.entry(pattern).or_insert(0) += 1;
        }
        
        let total = logs.len() as f64;
        
        // ä¿å­˜é¢‘ç‡è¶…è¿‡é˜ˆå€¼çš„æ¨¡å¼
        for (pattern, count) in pattern_freq {
            if (count as f64 / total) >= self.threshold {
                self.normal_patterns.insert(pattern);
            }
        }
    }
    
    /// æ£€æµ‹å¼‚å¸¸æ—¥å¿—
    pub fn detect_anomalies(&self, logs: Vec<LogRecord>) -> Vec<LogRecord> {
        logs.into_iter()
            .filter(|log| {
                let pattern = Self::extract_pattern(&log.message);
                !self.normal_patterns.contains(&pattern)
            })
            .collect()
    }
    
    fn extract_pattern(message: &str) -> String {
        LogPatternRecognizer::extract_pattern(message)
    }
}
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. æŸ¥è¯¢ç¼“å­˜

```rust
use lru::LruCache;
use std::num::NonZeroUsize;

/// æŸ¥è¯¢ç¼“å­˜
pub struct QueryCache {
    cache: Arc<tokio::sync::Mutex<LruCache<String, Vec<String>>>>,
}

impl QueryCache {
    pub fn new(capacity: usize) -> Self {
        Self {
            cache: Arc::new(tokio::sync::Mutex::new(
                LruCache::new(NonZeroUsize::new(capacity).unwrap())
            )),
        }
    }
    
    /// è·å–ç¼“å­˜
    pub async fn get(&self, query: &str) -> Option<Vec<String>> {
        let mut cache = self.cache.lock().await;
        cache.get(query).cloned()
    }
    
    /// æ”¾å…¥ç¼“å­˜
    pub async fn put(&self, query: String, results: Vec<String>) {
        let mut cache = self.cache.lock().await;
        cache.put(query, results);
    }
}
```

---

### 2. åˆ†ç‰‡æœç´¢

```rust
/// åˆ†ç‰‡æœç´¢å¼•æ“
pub struct ShardedSearchEngine {
    shards: Vec<Arc<InvertedIndexBuilder>>,
}

impl ShardedSearchEngine {
    pub fn new(num_shards: usize, tokenizer: Arc<Tokenizer>) -> Self {
        let shards = (0..num_shards)
            .map(|_| Arc::new(InvertedIndexBuilder::new(tokenizer.clone())))
            .collect();
        
        Self { shards }
    }
    
    /// ç´¢å¼•æ—¥å¿—ï¼ˆè‡ªåŠ¨åˆ†ç‰‡ï¼‰
    pub async fn index_log(&self, log: &LogRecord) {
        let shard_id = Self::get_shard_id(&log.id, self.shards.len());
        self.shards[shard_id].index_log(log).await;
    }
    
    /// å¹¶è¡Œæœç´¢æ‰€æœ‰åˆ†ç‰‡
    pub async fn search(&self, query: &str) -> Vec<String> {
        let mut handles = Vec::new();
        
        for shard in &self.shards {
            let shard = shard.clone();
            let query = query.to_string();
            
            let handle = tokio::spawn(async move {
                shard.search(&query).await
            });
            
            handles.push(handle);
        }
        
        let mut results = Vec::new();
        
        for handle in handles {
            if let Ok(shard_results) = handle.await {
                results.extend(shard_results);
            }
        }
        
        results
    }
    
    fn get_shard_id(log_id: &str, num_shards: usize) -> usize {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        log_id.hash(&mut hasher);
        
        (hasher.finish() as usize) % num_shards
    }
}
```

---

## ğŸ“Š å®Œæ•´ç¤ºä¾‹ï¼šæ—¥å¿—æœç´¢ç³»ç»Ÿ

```rust
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    tracing_subscriber::fmt::init();
    
    println!("ğŸ¯ æ—¥å¿—æœç´¢ç³»ç»Ÿå¯åŠ¨ä¸­...\n");
    
    // åˆ›å»ºåˆ†è¯å™¨
    let tokenizer = Arc::new(Tokenizer::new());
    
    // åˆ›å»ºå€’æ’ç´¢å¼•
    let index_builder = Arc::new(InvertedIndexBuilder::new(tokenizer.clone()));
    
    // åˆ›å»ºæ—¥å¿—å­˜å‚¨
    let log_storage = Arc::new(LogStorage::new());
    
    // æ¨¡æ‹Ÿæ—¥å¿—æ•°æ®
    let logs = vec![
        LogRecord {
            id: "1".to_string(),
            timestamp: Utc::now(),
            level: LogLevel::Error,
            message: "Database connection failed".to_string(),
            attributes: HashMap::new(),
        },
        LogRecord {
            id: "2".to_string(),
            timestamp: Utc::now(),
            level: LogLevel::Info,
            message: "User logged in successfully".to_string(),
            attributes: HashMap::new(),
        },
        LogRecord {
            id: "3".to_string(),
            timestamp: Utc::now(),
            level: LogLevel::Warn,
            message: "Database query slow".to_string(),
            attributes: HashMap::new(),
        },
    ];
    
    // ç´¢å¼•æ—¥å¿—
    index_builder.index_logs_batch(logs.clone()).await;
    log_storage.store_logs(logs.clone()).await;
    
    println!("âœ… ç´¢å¼•äº† {} æ¡æ—¥å¿—\n", logs.len());
    
    // 1. å…³é”®è¯æœç´¢
    let keyword_engine = KeywordSearchEngine::new(index_builder.clone());
    let results = keyword_engine.exact_search("database").await;
    
    println!("ğŸ” å…³é”®è¯æœç´¢ 'database': {} æ¡ç»“æœ", results.len());
    
    // 2. çŸ­è¯­æœç´¢
    let phrase_engine = PhraseSearchEngine::new(index_builder.clone());
    let results = phrase_engine.phrase_search("connection failed").await;
    
    println!("ğŸ” çŸ­è¯­æœç´¢ 'connection failed': {} æ¡ç»“æœ", results.len());
    
    // 3. æ¨¡ç³Šæœç´¢
    let fuzzy_engine = FuzzySearchEngine::new(index_builder.clone(), 2);
    let results = fuzzy_engine.fuzzy_search("databse").await; // æ‹¼å†™é”™è¯¯
    
    println!("ğŸ” æ¨¡ç³Šæœç´¢ 'databse': {} æ¡ç»“æœ", results.len());
    
    // 4. èšåˆæŸ¥è¯¢
    let aggregation_engine = AggregationQueryEngine::new(log_storage.clone());
    let level_counts = aggregation_engine.aggregate_by_level().await;
    
    println!("\nğŸ“Š æŒ‰çº§åˆ«èšåˆ:");
    for (level, count) in level_counts {
        println!("  {:?}: {}", level, count);
    }
    
    println!("\nâœ… æœç´¢ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼");
    
    Ok(())
}

// å ä½å®ç°
pub struct LogStorage {
    logs: Arc<RwLock<Vec<LogRecord>>>,
}

impl LogStorage {
    pub fn new() -> Self {
        Self {
            logs: Arc::new(RwLock::new(Vec::new())),
        }
    }
    
    pub async fn store_logs(&self, logs: Vec<LogRecord>) {
        let mut storage = self.logs.write().await;
        storage.extend(logs);
    }
    
    pub async fn get_all_logs(&self) -> Vec<LogRecord> {
        let storage = self.logs.read().await;
        storage.clone()
    }
    
    pub async fn get_log(&self, log_id: &str) -> Option<LogRecord> {
        let storage = self.logs.read().await;
        storage.iter().find(|log| log.id == log_id).cloned()
    }
}
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### ç´¢å¼•ä¼˜åŒ–

1. **åˆ†ç‰‡ç´¢å¼•** - æŒ‰æ—¶é—´æˆ–æ—¥å¿—é‡åˆ†ç‰‡
2. **å¢é‡ç´¢å¼•** - å®æ—¶ç´¢å¼•æ–°æ—¥å¿—
3. **å®šæœŸåˆå¹¶** - åˆå¹¶å°ç´¢å¼•æ®µ

### æŸ¥è¯¢ä¼˜åŒ–

1. **å‰ç¼€è¿‡æ»¤** - æ­£åˆ™æŸ¥è¯¢å…ˆç”¨å‰ç¼€ç´¢å¼•è¿‡æ»¤
2. **å¹¶è¡Œæœç´¢** - å¤šåˆ†ç‰‡å¹¶è¡ŒæŸ¥è¯¢
3. **æŸ¥è¯¢ç¼“å­˜** - ç¼“å­˜çƒ­ç‚¹æŸ¥è¯¢ç»“æœ

### å­˜å‚¨ä¼˜åŒ–

1. **å‹ç¼©å­˜å‚¨** - ä½¿ç”¨ Snappy/Zstd å‹ç¼©
2. **TTL ç­–ç•¥** - å®šæœŸæ¸…ç†æ—§æ—¥å¿—
3. **å†·çƒ­åˆ†ç¦»** - çƒ­æ•°æ®å†…å­˜ï¼Œå†·æ•°æ®ç£ç›˜

---

## ğŸ“š å‚è€ƒèµ„æº

- [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Lucene](https://lucene.apache.org/)
- [Tantivy](https://github.com/quickwit-oss/tantivy)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0  
**æœ€åæ›´æ–°**: 2025-10-10  
**ä½œè€…**: OTLP Rust é¡¹ç›®ç»„
