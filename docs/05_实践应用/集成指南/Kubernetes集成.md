# Kubernetes é›†æˆæŒ‡å—

## ğŸ“š æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†å¦‚ä½•åœ¨Kubernetesç¯å¢ƒä¸­éƒ¨ç½²å’Œé…ç½®OTLP Ruståº”ç”¨ï¼ŒåŒ…æ‹¬å®¹å™¨åŒ–ã€æœåŠ¡å‘ç°ã€é…ç½®ç®¡ç†ã€ç›‘æ§å’Œæ—¥å¿—æ”¶é›†ç­‰ã€‚

## ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²

### 1. Dockerfile é…ç½®

```dockerfile
# å¤šé˜¶æ®µæ„å»ºDockerfile
FROM rust:1.90-slim as builder

# å®‰è£…æ„å»ºä¾èµ–
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶Cargoæ–‡ä»¶
COPY Cargo.toml Cargo.lock ./

# æ„å»ºä¾èµ–ï¼ˆåˆ©ç”¨Dockerç¼“å­˜ï¼‰
RUN mkdir src && echo "fn main() {}" > src/main.rs
RUN cargo build --release && rm -rf src

# å¤åˆ¶æºä»£ç 
COPY src ./src

# æ„å»ºåº”ç”¨
RUN cargo build --release

# è¿è¡Œæ—¶é•œåƒ
FROM debian:bookworm-slim

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y \
    ca-certificates \
    libssl3 \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -r -s /bin/false appuser

# å¤åˆ¶äºŒè¿›åˆ¶æ–‡ä»¶
COPY --from=builder /app/target/release/my-otlp-app /usr/local/bin/
RUN chmod +x /usr/local/bin/my-otlp-app

# è®¾ç½®ç”¨æˆ·
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["my-otlp-app"]
```

### 2. æ„å»ºå’Œæ¨é€é•œåƒ

```bash
# æ„å»ºé•œåƒ
docker build -t my-registry/my-otlp-app:latest .

# æ¨é€é•œåƒ
docker push my-registry/my-otlp-app:latest
```

## â˜¸ï¸ Kubernetes éƒ¨ç½²

### 1. åŸºç¡€éƒ¨ç½²é…ç½®

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-app
  namespace: default
  labels:
    app: otlp-app
    version: v1.0.0
spec:
  replicas: 3
  selector:
    matchLabels:
      app: otlp-app
  template:
    metadata:
      labels:
        app: otlp-app
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: otlp-app
        image: my-registry/my-otlp-app:latest
        ports:
        - containerPort: 8080
          name: http
        env:
        - name: OTLP_ENDPOINT
          value: "http://otel-collector:4317"
        - name: SERVICE_NAME
          value: "otlp-app"
        - name: SERVICE_VERSION
          value: "1.0.0"
        - name: DEPLOYMENT_ENVIRONMENT
          value: "production"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: otlp-app-config
---
apiVersion: v1
kind: Service
metadata:
  name: otlp-app-service
  labels:
    app: otlp-app
spec:
  selector:
    app: otlp-app
  ports:
  - name: http
    port: 80
    targetPort: 8080
    protocol: TCP
  type: ClusterIP
```

### 2. é…ç½®ç®¡ç†

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otlp-app-config
  namespace: default
data:
  config.yaml: |
    endpoint: "http://otel-collector:4317"
    protocol: "grpc"
    service_name: "otlp-app"
    service_version: "1.0.0"
    sampling_ratio: 0.1
    batch_config:
      max_export_batch_size: 512
      export_timeout_ms: 5000
      max_queue_size: 2048
      scheduled_delay_ms: 5000
    retry_config:
      max_retries: 5
      initial_retry_delay_ms: 1000
      max_retry_delay_ms: 30000
      retry_delay_multiplier: 2.0
    security:
      tls_enabled: false
---
apiVersion: v1
kind: Secret
metadata:
  name: otlp-app-secrets
  namespace: default
type: Opaque
data:
  api-key: <base64-encoded-api-key>
  bearer-token: <base64-encoded-bearer-token>
```

### 3. ç¯å¢ƒç‰¹å®šé…ç½®

```yaml
# deployment-dev.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-app-dev
  namespace: development
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otlp-app-dev
  template:
    metadata:
      labels:
        app: otlp-app-dev
        environment: development
    spec:
      containers:
      - name: otlp-app
        image: my-registry/my-otlp-app:dev
        env:
        - name: OTLP_ENDPOINT
          value: "http://otel-collector-dev:4317"
        - name: DEBUG
          value: "true"
        - name: SAMPLING_RATIO
          value: "1.0"  # 100%é‡‡æ ·ç”¨äºå¼€å‘
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "256Mi"
            cpu: "200m"
---
# deployment-prod.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-app-prod
  namespace: production
spec:
  replicas: 5
  selector:
    matchLabels:
      app: otlp-app-prod
  template:
    metadata:
      labels:
        app: otlp-app-prod
        environment: production
    spec:
      containers:
      - name: otlp-app
        image: my-registry/my-otlp-app:latest
        env:
        - name: OTLP_ENDPOINT
          value: "https://api.honeycomb.io:443"
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: otlp-app-secrets
              key: api-key
        - name: SAMPLING_RATIO
          value: "0.1"  # 10%é‡‡æ ·ç”¨äºç”Ÿäº§
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 3
```

## ğŸ“Š OpenTelemetry Collector é›†æˆ

### 1. Collector éƒ¨ç½²

```yaml
# otel-collector.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector:latest
        ports:
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        - containerPort: 8888
          name: metrics
        - containerPort: 8889
          name: pprof
        env:
        - name: HONEYCOMB_API_KEY
          valueFrom:
            secretKeyRef:
              name: honeycomb-secrets
              key: api-key
        volumeMounts:
        - name: config-volume
          mountPath: /etc/collector-config.yaml
          subPath: collector-config.yaml
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
      volumes:
      - name: config-volume
        configMap:
          name: otel-collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
spec:
  selector:
    app: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
  - name: otlp-http
    port: 4318
    targetPort: 4318
  - name: metrics
    port: 8888
    targetPort: 8888
  type: ClusterIP
```

### 2. Collector é…ç½®

```yaml
# otel-collector-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
data:
  collector-config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    
    processors:
      batch:
        send_batch_size: 1024
        timeout: 1s
      memory_limiter:
        limit_mib: 512
      resource:
        attributes:
        - key: k8s.cluster.name
          value: "production-cluster"
          action: insert
        - key: k8s.namespace.name
          from_attribute: k8s.namespace.name
          action: insert
      k8sattributes:
        auth_type: serviceAccount
        passthrough: false
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.uid
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
              - from: resource_attribute
                name: k8s.namespace.name
    
    exporters:
      logging:
        loglevel: debug
      otlp:
        endpoint: "https://api.honeycomb.io:443"
        headers:
          "x-honeycomb-team": "${HONEYCOMB_API_KEY}"
          "x-honeycomb-dataset": "kubernetes"
        tls:
          insecure: false
      prometheus:
        endpoint: "0.0.0.0:8889"
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, resource, batch]
          exporters: [otlp, logging]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, resource, batch]
          exporters: [otlp, prometheus]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, resource, batch]
          exporters: [otlp, logging]
```

## ğŸ”„ è‡ªåŠ¨æ‰©ç¼©å®¹

### 1. Horizontal Pod Autoscaler

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: otlp-app-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otlp-app
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
```

### 2. Vertical Pod Autoscaler

```yaml
# vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: otlp-app-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: otlp-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: otlp-app
      maxAllowed:
        cpu: "2"
        memory: "2Gi"
      minAllowed:
        cpu: "100m"
        memory: "128Mi"
      controlledResources: ["cpu", "memory"]
```

## ğŸ“ˆ ç›‘æ§å’Œå‘Šè­¦

### 1. ServiceMonitor (Prometheus)

```yaml
# servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otlp-app-monitor
  namespace: default
  labels:
    app: otlp-app
spec:
  selector:
    matchLabels:
      app: otlp-app
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
    scrapeTimeout: 10s
    honorLabels: true
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod_name
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_service_name]
      targetLabel: service_name
```

### 2. PrometheusRule

```yaml
# prometheusrule.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: otlp-app-rules
  namespace: default
  labels:
    app: otlp-app
spec:
  groups:
  - name: otlp-app.rules
    rules:
    - alert: OtlpAppHighErrorRate
      expr: rate(otlp_requests_total{status="error"}[5m]) > 0.1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value }} errors per second"
    
    - alert: OtlpAppHighLatency
      expr: histogram_quantile(0.95, rate(otlp_request_duration_seconds_bucket[5m])) > 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High latency detected"
        description: "95th percentile latency is {{ $value }} seconds"
    
    - alert: OtlpAppDown
      expr: up{job="otlp-app"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "OTLP App is down"
        description: "OTLP App has been down for more than 1 minute"
```

### 3. Grafana Dashboard

```yaml
# grafana-dashboard.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otlp-app-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  otlp-app-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "OTLP App Dashboard",
        "tags": ["otlp", "rust"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(otlp_requests_total[5m])",
                "legendFormat": "Requests/sec"
              }
            ]
          },
          {
            "id": 2,
            "title": "Error Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(otlp_requests_total{status=\"error\"}[5m])",
                "legendFormat": "Errors/sec"
              }
            ]
          },
          {
            "id": 3,
            "title": "Response Time",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(otlp_request_duration_seconds_bucket[5m]))",
                "legendFormat": "95th percentile"
              },
              {
                "expr": "histogram_quantile(0.50, rate(otlp_request_duration_seconds_bucket[5m]))",
                "legendFormat": "50th percentile"
              }
            ]
          }
        ]
      }
    }
```

## ğŸ” å®‰å…¨é…ç½®

### 1. NetworkPolicy

```yaml
# networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: otlp-app-netpol
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: otlp-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    - podSelector:
        matchLabels:
          app: otel-collector
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: otel-collector
    ports:
    - protocol: TCP
      port: 4317
  - to: []
    ports:
    - protocol: TCP
      port: 443  # HTTPS for external APIs
```

### 2. PodSecurityPolicy

```yaml
# psp.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: otlp-app-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
```

### 3. RBAC

```yaml
# rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otlp-app-sa
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: otlp-app-role
  namespace: default
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: otlp-app-rolebinding
  namespace: default
subjects:
- kind: ServiceAccount
  name: otlp-app-sa
  namespace: default
roleRef:
  kind: Role
  name: otlp-app-role
  apiGroup: rbac.authorization.k8s.io
```

## ğŸš€ éƒ¨ç½²è„šæœ¬

### 1. éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy.sh

set -e

NAMESPACE=${1:-default}
ENVIRONMENT=${2:-development}
IMAGE_TAG=${3:-latest}

echo "Deploying OTLP App to namespace: $NAMESPACE"
echo "Environment: $ENVIRONMENT"
echo "Image tag: $IMAGE_TAG"

# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

# åº”ç”¨é…ç½®
kubectl apply -f configmap.yaml -n $NAMESPACE
kubectl apply -f secret.yaml -n $NAMESPACE

# åº”ç”¨RBAC
kubectl apply -f rbac.yaml -n $NAMESPACE

# éƒ¨ç½²åº”ç”¨
kubectl apply -f deployment-$ENVIRONMENT.yaml -n $NAMESPACE

# åº”ç”¨æœåŠ¡
kubectl apply -f service.yaml -n $NAMESPACE

# åº”ç”¨ç½‘ç»œç­–ç•¥
kubectl apply -f networkpolicy.yaml -n $NAMESPACE

# ç­‰å¾…éƒ¨ç½²å®Œæˆ
kubectl wait --for=condition=available --timeout=300s deployment/otlp-app -n $NAMESPACE

echo "Deployment completed successfully!"
```

### 2. å›æ»šè„šæœ¬

```bash
#!/bin/bash
# rollback.sh

NAMESPACE=${1:-default}
REVISION=${2:-}

if [ -z "$REVISION" ]; then
    echo "Usage: $0 <namespace> <revision>"
    echo "Available revisions:"
    kubectl rollout history deployment/otlp-app -n $NAMESPACE
    exit 1
fi

echo "Rolling back to revision $REVISION in namespace $NAMESPACE"

kubectl rollout undo deployment/otlp-app --to-revision=$REVISION -n $NAMESPACE

kubectl rollout status deployment/otlp-app -n $NAMESPACE

echo "Rollback completed!"
```

## ğŸ“Š ç›‘æ§è„šæœ¬

### 1. å¥åº·æ£€æŸ¥è„šæœ¬

```bash
#!/bin/bash
# health-check.sh

NAMESPACE=${1:-default}
POD_NAME=$(kubectl get pods -n $NAMESPACE -l app=otlp-app -o jsonpath='{.items[0].metadata.name}')

if [ -z "$POD_NAME" ]; then
    echo "No OTLP app pods found in namespace $NAMESPACE"
    exit 1
fi

echo "Checking health of pod: $POD_NAME"

# æ£€æŸ¥podçŠ¶æ€
kubectl get pod $POD_NAME -n $NAMESPACE

# æ£€æŸ¥æ—¥å¿—
echo "Recent logs:"
kubectl logs $POD_NAME -n $NAMESPACE --tail=20

# æ£€æŸ¥å¥åº·ç«¯ç‚¹
kubectl exec $POD_NAME -n $NAMESPACE -- curl -f http://localhost:8080/health

echo "Health check completed!"
```

### 2. æ€§èƒ½ç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# performance-monitor.sh

NAMESPACE=${1:-default}

echo "Performance monitoring for OTLP App in namespace: $NAMESPACE"

# è·å–èµ„æºä½¿ç”¨æƒ…å†µ
echo "Resource usage:"
kubectl top pods -n $NAMESPACE -l app=otlp-app

# è·å–æŒ‡æ ‡
echo "Metrics:"
kubectl port-forward -n $NAMESPACE svc/otlp-app-service 8080:80 &
PORT_FORWARD_PID=$!

sleep 5

curl -s http://localhost:8080/metrics | grep otlp_

kill $PORT_FORWARD_PID

echo "Performance monitoring completed!"
```

## ğŸ“š æœ€ä½³å®è·µ

### 1. èµ„æºç®¡ç†

- **åˆç†è®¾ç½®èµ„æºé™åˆ¶**: é¿å…èµ„æºäº‰ç”¨
- **ä½¿ç”¨HPAå’ŒVPA**: è‡ªåŠ¨è°ƒæ•´èµ„æºä½¿ç”¨
- **ç›‘æ§èµ„æºä½¿ç”¨**: åŠæ—¶å‘ç°èµ„æºç“¶é¢ˆ

### 2. é…ç½®ç®¡ç†1

- **ä½¿ç”¨ConfigMapå’ŒSecret**: åˆ†ç¦»é…ç½®å’Œæ•æ„Ÿä¿¡æ¯
- **ç¯å¢ƒç‰¹å®šé…ç½®**: ä¸åŒç¯å¢ƒä½¿ç”¨ä¸åŒé…ç½®
- **é…ç½®çƒ­æ›´æ–°**: æ”¯æŒé…ç½®çš„åŠ¨æ€æ›´æ–°

### 3. ç›‘æ§å’Œå‘Šè­¦

- **å…¨é¢ç›‘æ§**: ç›‘æ§åº”ç”¨ã€åŸºç¡€è®¾æ–½å’Œä¸šåŠ¡æŒ‡æ ‡
- **åˆç†å‘Šè­¦**: è®¾ç½®åˆé€‚çš„å‘Šè­¦é˜ˆå€¼
- **æ—¥å¿—æ”¶é›†**: ç»Ÿä¸€æ—¥å¿—æ”¶é›†å’Œåˆ†æ

### 4. å®‰å…¨å®è·µ

- **æœ€å°æƒé™åŸåˆ™**: ä½¿ç”¨RBACé™åˆ¶æƒé™
- **ç½‘ç»œéš”ç¦»**: ä½¿ç”¨NetworkPolicyé™åˆ¶ç½‘ç»œè®¿é—®
- **å®‰å…¨é•œåƒ**: ä½¿ç”¨å®‰å…¨çš„åŸºç¡€é•œåƒ

## ğŸš€ ä¸‹ä¸€æ­¥

### æ·±å…¥å­¦ä¹ 

1. **[å¾®æœåŠ¡é›†æˆ](å¾®æœåŠ¡é›†æˆ.md)** - å¾®æœåŠ¡ç¯å¢ƒéƒ¨ç½²
2. **[æœåŠ¡ç½‘æ ¼é›†æˆ](æœåŠ¡ç½‘æ ¼é›†æˆ.md)** - Istioé›†æˆ
3. **[æ•°æ®åº“é›†æˆ](æ•°æ®åº“é›†æˆ.md)** - æ•°æ®åº“æ“ä½œç›‘æ§

### è¿ç»´å®è·µ

1. **[ç›‘æ§å‘Šè­¦è®¾ç½®](../éƒ¨ç½²è¿ç»´/ç›‘æ§å‘Šè­¦.md)** - å»ºç«‹ç›‘æ§ä½“ç³»
2. **[æ•…éšœæ’æŸ¥æŒ‡å—](../éƒ¨ç½²è¿ç»´/æ•…éšœæ’æŸ¥.md)** - å¿«é€Ÿå®šä½é—®é¢˜
3. **[æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](../æ€§èƒ½è°ƒä¼˜/ä¼˜åŒ–ç­–ç•¥.md)** - æå‡åº”ç”¨æ€§èƒ½

---

**Kubernetesé›†æˆæŒ‡å—ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´1æœˆ27æ—¥  
**ç»´æŠ¤è€…**: OTLP 2025 æ–‡æ¡£å›¢é˜Ÿ
