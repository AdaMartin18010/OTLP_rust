# OTLP Rust 云原生微服务架构指南

## 📚 概述

本文档详细介绍了OTLP Rust在云原生环境中的微服务架构设计，包括容器化、服务网格、云原生模式、CI/CD流水线等现代云原生架构实践。

## ☁️ 云原生架构概览

### 1. 云原生架构原则

| 原则 | 描述 | 实现方式 |
|------|------|----------|
| **容器化** | 应用打包在容器中 | Docker + Kubernetes |
| **微服务** | 拆分为小型独立服务 | 服务拆分 + API网关 |
| **可观测性** | 完整的监控和追踪 | OTLP + Prometheus + Grafana |
| **自动化** | 自动化的部署和运维 | CI/CD + GitOps |
| **弹性** | 自动扩缩容和故障恢复 | HPA + VPA + 熔断器 |
| **安全** | 零信任安全模型 | mTLS + RBAC + 网络策略 |

### 2. 云原生架构层次

```text
┌─────────────────────────────────────────┐
│           应用层 (Application)           │
├─────────────────────────────────────────┤
│         服务网格层 (Service Mesh)        │
├─────────────────────────────────────────┤
│         容器编排层 (Orchestration)       │
├─────────────────────────────────────────┤
│         基础设施层 (Infrastructure)      │
└─────────────────────────────────────────┘
```

## 🐳 容器化架构

### 1. 多阶段Docker构建

```dockerfile
# Dockerfile.otlp-rust
# 构建阶段
FROM rust:1.90-slim as builder

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# 设置工作目录
WORKDIR /usr/src/app

# 复制Cargo文件
COPY Cargo.toml Cargo.lock ./

# 创建虚拟依赖来利用Docker缓存
RUN mkdir src && echo "fn main() {}" > src/main.rs

# 构建依赖
RUN cargo build --release && rm -rf src

# 复制源代码
COPY src ./src
COPY examples ./examples
COPY benches ./benches

# 重新构建应用
RUN cargo build --release

# 运行时阶段
FROM debian:bookworm-slim

# 安装运行时依赖
RUN apt-get update && apt-get install -y \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# 创建非root用户
RUN groupadd -r otlp && useradd -r -g otlp otlp

# 设置工作目录
WORKDIR /app

# 复制二进制文件
COPY --from=builder /usr/src/app/target/release/otlp /app/otlp

# 复制配置文件
COPY --from=builder /usr/src/app/examples/config.yaml /app/config.yaml

# 设置权限
RUN chown -R otlp:otlp /app
USER otlp

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# 暴露端口
EXPOSE 8080 4317 4318

# 启动命令
CMD ["./otlp"]
```

### 2. 容器编排配置

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: otlp-system
  labels:
    name: otlp-system
    istio-injection: enabled

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otlp-config
  namespace: otlp-system
data:
  config.yaml: |
    otlp:
      endpoint: "http://otel-collector:4317"
      service_name: "otlp-rust-service"
      service_version: "1.0.0"
      environment: "production"
      
    batch:
      max_export_batch_size: 1000
      export_timeout: 5s
      max_queue_size: 10000
      scheduled_delay: 1s
      
    retry:
      enabled: true
      initial_interval: 1s
      max_interval: 30s
      max_elapsed_time: 300s
      multiplier: 2.0
      
    compression:
      enabled: true
      algorithm: "gzip"
      level: 6

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: otlp-secrets
  namespace: otlp-system
type: Opaque
data:
  api-key: <base64-encoded-api-key>
  tls-cert: <base64-encoded-cert>
  tls-key: <base64-encoded-key>

---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otlp-app
  namespace: otlp-system
  labels:
    app: otlp-app
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: otlp-app
  template:
    metadata:
      labels:
        app: otlp-app
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: otlp-app-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      containers:
      - name: otlp-app
        image: otlp-rust:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        env:
        - name: RUST_LOG
          value: "info"
        - name: OTLP_ENDPOINT
          valueFrom:
            configMapKeyRef:
              name: otlp-config
              key: otlp.endpoint
        - name: SERVICE_NAME
          valueFrom:
            configMapKeyRef:
              name: otlp-config
              key: otlp.service_name
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: otlp-secrets
              key: api-key
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: config-volume
          mountPath: /app/config.yaml
          subPath: config.yaml
      volumes:
      - name: config-volume
        configMap:
          name: otlp-config
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - otlp-app
              topologyKey: kubernetes.io/hostname
```

## 🌐 服务网格架构

### 1. Istio服务网格配置

```yaml
# istio/gateway.yaml
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: otlp-gateway
  namespace: otlp-system
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "otlp.example.com"
    tls:
      httpsRedirect: true
  - port:
      number: 443
      name: https
      protocol: HTTPS
    hosts:
    - "otlp.example.com"
    tls:
      mode: SIMPLE
      credentialName: otlp-tls-cert

---
# istio/virtualservice.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: otlp-vs
  namespace: otlp-system
spec:
  hosts:
  - "otlp.example.com"
  gateways:
  - otlp-gateway
  http:
  - match:
    - uri:
        prefix: /api/v1/traces
    route:
    - destination:
        host: otlp-app
        port:
          number: 4317
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
  - match:
    - uri:
        prefix: /api/v1/metrics
    route:
    - destination:
        host: otlp-app
        port:
          number: 4318
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
  - match:
    - uri:
        prefix: /health
    route:
    - destination:
        host: otlp-app
        port:
          number: 8080
    timeout: 5s

---
# istio/destinationrule.yaml
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: otlp-app-dr
  namespace: otlp-system
spec:
  host: otlp-app
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
        maxRequestsPerConnection: 10
        maxRetries: 3
    circuitBreaker:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    loadBalancer:
      simple: LEAST_CONN
    outlierDetection:
      consecutive5xxErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
    tls:
      mode: ISTIO_MUTUAL
```

### 2. 服务网格监控

```yaml
# istio/telemetry.yaml
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: otlp-telemetry
  namespace: otlp-system
spec:
  metrics:
  - providers:
    - name: prometheus
  - overrides:
    - match:
        metric: ALL_METRICS
      tagOverrides:
        destination_service:
          value: "otlp-app"
        source_service:
          value: "client"
  tracing:
  - providers:
    - name: otlp
  - customTags:
    environment:
      literal:
        value: "production"
    service_version:
      header:
        name: "x-service-version"
        defaultValue: "unknown"
```

## 🔄 微服务架构实现

### 1. 微服务拆分策略

```rust
// src/microservices/mod.rs
use std::collections::HashMap;
use tokio::sync::RwLock;
use serde::{Serialize, Deserialize};

pub mod trace_service;
pub mod metric_service;
pub mod log_service;
pub mod config_service;
pub mod health_service;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MicroserviceConfig {
    pub service_name: String,
    pub version: String,
    pub port: u16,
    pub dependencies: Vec<ServiceDependency>,
    pub health_check: HealthCheckConfig,
    pub metrics: MetricsConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceDependency {
    pub service_name: String,
    pub endpoint: String,
    pub timeout: u64,
    pub retry_count: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheckConfig {
    pub enabled: bool,
    pub interval: u64,
    pub timeout: u64,
    pub failure_threshold: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MetricsConfig {
    pub enabled: bool,
    pub port: u16,
    pub path: String,
}

pub struct MicroserviceManager {
    services: RwLock<HashMap<String, Box<dyn Microservice + Send + Sync>>>,
    config: MicroserviceConfig,
}

impl MicroserviceManager {
    pub fn new(config: MicroserviceConfig) -> Self {
        Self {
            services: RwLock::new(HashMap::new()),
            config,
        }
    }
    
    pub async fn register_service<T>(&self, name: &str, service: T)
    where
        T: Microservice + Send + Sync + 'static,
    {
        let mut services = self.services.write().await;
        services.insert(name.to_string(), Box::new(service));
    }
    
    pub async fn start_all_services(&self) -> Result<(), Box<dyn std::error::Error>> {
        let services = self.services.read().await;
        
        for (name, service) in services.iter() {
            log::info!("Starting service: {}", name);
            service.start().await?;
        }
        
        Ok(())
    }
    
    pub async fn stop_all_services(&self) -> Result<(), Box<dyn std::error::Error>> {
        let services = self.services.read().await;
        
        for (name, service) in services.iter() {
            log::info!("Stopping service: {}", name);
            service.stop().await?;
        }
        
        Ok(())
    }
}

pub trait Microservice {
    async fn start(&self) -> Result<(), Box<dyn std::error::Error>>;
    async fn stop(&self) -> Result<(), Box<dyn std::error::Error>>;
    async fn health_check(&self) -> Result<HealthStatus, Box<dyn std::error::Error>>;
    async fn get_metrics(&self) -> Result<String, Box<dyn std::error::Error>>;
}

#[derive(Debug, Clone)]
pub enum HealthStatus {
    Healthy,
    Unhealthy,
    Degraded,
}
```

### 2. 追踪服务实现

```rust
// src/microservices/trace_service.rs
use super::{Microservice, HealthStatus};
use crate::data::TelemetryData;
use std::collections::HashMap;
use tokio::sync::RwLock;

pub struct TraceService {
    traces: RwLock<HashMap<String, TelemetryData>>,
    metrics: TraceMetrics,
}

struct TraceMetrics {
    total_traces: std::sync::atomic::AtomicU64,
    active_traces: std::sync::atomic::AtomicU64,
    failed_traces: std::sync::atomic::AtomicU64,
}

impl TraceService {
    pub fn new() -> Self {
        Self {
            traces: RwLock::new(HashMap::new()),
            metrics: TraceMetrics {
                total_traces: std::sync::atomic::AtomicU64::new(0),
                active_traces: std::sync::atomic::AtomicU64::new(0),
                failed_traces: std::sync::atomic::AtomicU64::new(0),
            },
        }
    }
    
    pub async fn create_trace(&self, service_name: &str, operation_name: &str) -> Result<String, Box<dyn std::error::Error>> {
        let trace_id = uuid::Uuid::new_v4().to_string();
        let trace = TelemetryData::trace(format!("{}-{}", service_name, operation_name))
            .with_attribute("service.name", service_name)
            .with_attribute("operation.name", operation_name)
            .with_attribute("trace.id", &trace_id);
        
        let mut traces = self.traces.write().await;
        traces.insert(trace_id.clone(), trace);
        
        self.metrics.total_traces.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        self.metrics.active_traces.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
        
        Ok(trace_id)
    }
    
    pub async fn complete_trace(&self, trace_id: &str, status: &str) -> Result<(), Box<dyn std::error::Error>> {
        let mut traces = self.traces.write().await;
        if let Some(trace) = traces.get_mut(trace_id) {
            trace.with_attribute("status", status);
            self.metrics.active_traces.fetch_sub(1, std::sync::atomic::Ordering::Relaxed);
            
            if status == "error" {
                self.metrics.failed_traces.fetch_add(1, std::sync::atomic::Ordering::Relaxed);
            }
        }
        
        Ok(())
    }
    
    pub async fn get_trace(&self, trace_id: &str) -> Result<Option<TelemetryData>, Box<dyn std::error::Error>> {
        let traces = self.traces.read().await;
        Ok(traces.get(trace_id).cloned())
    }
}

#[async_trait::async_trait]
impl Microservice for TraceService {
    async fn start(&self) -> Result<(), Box<dyn std::error::Error>> {
        log::info!("Trace service started");
        Ok(())
    }
    
    async fn stop(&self) -> Result<(), Box<dyn std::error::Error>> {
        log::info!("Trace service stopped");
        Ok(())
    }
    
    async fn health_check(&self) -> Result<HealthStatus, Box<dyn std::error::Error>> {
        let traces_count = self.traces.read().await.len();
        if traces_count < 10000 {
            Ok(HealthStatus::Healthy)
        } else {
            Ok(HealthStatus::Degraded)
        }
    }
    
    async fn get_metrics(&self) -> Result<String, Box<dyn std::error::Error>> {
        let metrics = format!(
            "# HELP otlp_traces_total Total number of traces\n\
             # TYPE otlp_traces_total counter\n\
             otlp_traces_total {}\n\
             # HELP otlp_traces_active Active number of traces\n\
             # TYPE otlp_traces_active gauge\n\
             otlp_traces_active {}\n\
             # HELP otlp_traces_failed Failed number of traces\n\
             # TYPE otlp_traces_failed counter\n\
             otlp_traces_failed {}",
            self.metrics.total_traces.load(std::sync::atomic::Ordering::Relaxed),
            self.metrics.active_traces.load(std::sync::atomic::Ordering::Relaxed),
            self.metrics.failed_traces.load(std::sync::atomic::Ordering::Relaxed)
        );
        
        Ok(metrics)
    }
}
```

## 🚀 CI/CD流水线

### 1. GitOps配置

```yaml
# .github/workflows/ci-cd.yaml
name: OTLP Rust CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        components: rustfmt, clippy
    
    - name: Cache cargo registry
      uses: actions/cache@v3
      with:
        path: ~/.cargo/registry
        key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Cache cargo index
      uses: actions/cache@v3
      with:
        path: ~/.cargo/git
        key: ${{ runner.os }}-cargo-index-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Cache cargo build
      uses: actions/cache@v3
      with:
        path: target
        key: ${{ runner.os }}-cargo-build-target-${{ hashFiles('**/Cargo.lock') }}
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libssl-dev pkg-config
    
    - name: Run tests
      run: cargo test --verbose
    
    - name: Run clippy
      run: cargo clippy --all-targets --all-features -- -D warnings
    
    - name: Run fmt
      run: cargo fmt --all -- --check
    
    - name: Run security audit
      run: |
        cargo install cargo-audit
        cargo audit

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.otlp-rust
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: 'v3.12.0'
    
    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig
    
    - name: Deploy to Kubernetes
      run: |
        # Update image tag in deployment
        sed -i "s|image: otlp-rust:.*|image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest|g" k8s/deployment.yaml
        
        # Apply configurations
        kubectl apply -f k8s/namespace.yaml
        kubectl apply -f k8s/configmap.yaml
        kubectl apply -f k8s/secret.yaml
        kubectl apply -f k8s/service.yaml
        kubectl apply -f k8s/deployment.yaml
        kubectl apply -f istio/gateway.yaml
        kubectl apply -f istio/virtualservice.yaml
        kubectl apply -f istio/destinationrule.yaml
        
        # Wait for deployment to be ready
        kubectl rollout status deployment/otlp-app -n otlp-system --timeout=300s
    
    - name: Run smoke tests
      run: |
        # Wait for service to be ready
        kubectl wait --for=condition=available --timeout=300s deployment/otlp-app -n otlp-system
        
        # Get service endpoint
        SERVICE_IP=$(kubectl get svc otlp-app-service -n otlp-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
        
        # Run smoke tests
        curl -f http://$SERVICE_IP:8080/health || exit 1
        curl -f http://$SERVICE_IP:8080/ready || exit 1
        curl -f http://$SERVICE_IP:8080/metrics || exit 1
```

### 2. ArgoCD应用配置

```yaml
# argocd/application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: otlp-rust-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/otlp-rust
    targetRevision: HEAD
    path: k8s
  destination:
    server: https://kubernetes.default.svc
    namespace: otlp-system
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
    - PrunePropagationPolicy=foreground
    - PruneLast=true
  revisionHistoryLimit: 10
```

## 📊 云原生监控

### 1. Prometheus配置

```yaml
# monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "otlp_rules.yml"
    
    scrape_configs:
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name
    
    - job_name: 'istio-mesh'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - istio-system
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: istio-telemetry;prometheus
    
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otlp-rules
  namespace: monitoring
data:
  otlp_rules.yml: |
    groups:
    - name: otlp.rules
      rules:
      - alert: OtlpServiceDown
        expr: up{job="otlp-app"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "OTLP service is down"
          description: "OTLP service has been down for more than 1 minute."
      
      - alert: OtlpHighErrorRate
        expr: rate(otlp_requests_total{status="error"}[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate in OTLP service"
          description: "Error rate is {{ $value }} errors per second."
      
      - alert: OtlpHighLatency
        expr: histogram_quantile(0.95, rate(otlp_request_duration_seconds_bucket[5m])) > 1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High latency in OTLP service"
          description: "95th percentile latency is {{ $value }} seconds."
      
      - alert: OtlpMemoryUsage
        expr: (container_memory_usage_bytes{pod=~"otlp-app-.*"} / container_spec_memory_limit_bytes{pod=~"otlp-app-.*"}) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage in OTLP service"
          description: "Memory usage is {{ $value }}% of limit."
```

### 2. Grafana仪表盘

```json
{
  "dashboard": {
    "id": null,
    "title": "OTLP Rust Service Dashboard",
    "tags": ["otlp", "rust", "microservices"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(otlp_requests_total[5m])",
            "legendFormat": "Requests/sec"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "reqps"
          }
        }
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(otlp_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          },
          {
            "expr": "histogram_quantile(0.95, rate(otlp_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.99, rate(otlp_request_duration_seconds_bucket[5m]))",
            "legendFormat": "99th percentile"
          }
        ],
        "yAxes": [
          {
            "unit": "s"
          }
        ]
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(otlp_requests_total{status=\"error\"}[5m]) / rate(otlp_requests_total[5m])",
            "legendFormat": "Error Rate"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit"
          }
        }
      },
      {
        "id": 4,
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "container_memory_usage_bytes{pod=~\"otlp-app-.*\"}",
            "legendFormat": "{{pod}}"
          }
        ],
        "yAxes": [
          {
            "unit": "bytes"
          }
        ]
      },
      {
        "id": 5,
        "title": "CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(container_cpu_usage_seconds_total{pod=~\"otlp-app-.*\"}[5m])",
            "legendFormat": "{{pod}}"
          }
        ],
        "yAxes": [
          {
            "unit": "percent"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

## 🔒 云原生安全

### 1. 安全策略配置

```yaml
# security/network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: otlp-network-policy
  namespace: otlp-system
spec:
  podSelector:
    matchLabels:
      app: otlp-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: istio-system
    - podSelector:
        matchLabels:
          app: otel-collector
    ports:
    - protocol: TCP
      port: 8080
    - protocol: TCP
      port: 4317
    - protocol: TCP
      port: 4318
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: otel-collector
    ports:
    - protocol: TCP
      port: 4317
  - to: []
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53

---
# security/pod-security-policy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: otlp-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'

---
# security/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: otlp-app-role
  namespace: otlp-system
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: otlp-app-rolebinding
  namespace: otlp-system
subjects:
- kind: ServiceAccount
  name: otlp-app-sa
  namespace: otlp-system
roleRef:
  kind: Role
  name: otlp-app-role
  apiGroup: rbac.authorization.k8s.io

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otlp-app-sa
  namespace: otlp-system
automountServiceAccountToken: false
```

## 📚 最佳实践总结

### 1. 云原生架构原则1

- **容器优先**: 所有应用都容器化部署
- **微服务架构**: 拆分为小型独立服务
- **API驱动**: 通过API进行服务间通信
- **自动化运维**: 实现自动化的部署和运维
- **可观测性**: 建立完整的监控和追踪体系
- **安全优先**: 实施零信任安全模型

### 2. 实施建议

- **渐进式迁移**: 从单体应用逐步迁移到微服务
- **服务网格**: 使用Istio等服务网格管理微服务
- **GitOps**: 采用GitOps模式管理配置和部署
- **监控告警**: 建立完善的监控告警体系
- **安全策略**: 实施多层安全防护策略
- **性能优化**: 持续优化系统性能

### 3. 运维最佳实践

- **自动化测试**: 建立完整的自动化测试体系
- **持续集成**: 实现持续集成和持续部署
- **配置管理**: 统一管理配置和密钥
- **日志聚合**: 集中收集和分析日志
- **故障恢复**: 建立快速故障恢复机制
- **容量规划**: 合理规划资源容量

---

**云原生微服务架构版本**: v1.0  
**最后更新**: 2025年1月27日  
**维护者**: OTLP 2025 文档团队
